{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEG CNN based.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuantingC/positioning/blob/master/EEG_CNN_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyBqi0PbnnrZ",
        "colab_type": "code",
        "outputId": "4115ae5f-6b88-4913-d963-6007ff728277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install h5py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.16.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzHM9RXIET0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install -U -q PyDrive ## you will have install for every colab session\n",
        "\n",
        "# from pydrive.auth import GoogleAuth\n",
        "# from pydrive.drive import GoogleDrive\n",
        "# from google.colab import auth\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# # 1. Authenticate and create the PyDrive client.\n",
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)\n",
        "# mat_import = drive.CreateFile({'id':'16NbTvbchzCrftTP2iVKbmA2a54fQ4nNV'})\n",
        "# mat_import.GetContentFile('project_data.zip') # - 'sample.json' is the file name that will be accessible in the notebook\n",
        "# # print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "# # data = sio.loadmat('sample.mat') # load the .mat file.\n",
        "# # R = data['R'][0,:]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlkDmnDKFfua",
        "colab_type": "code",
        "outputId": "5294331f-91bb-4200-e72e-4f40156b3120",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/My Drive/UCLA/EE239AS Deep learning/project/code"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/UCLA/EE239AS Deep learning/project/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSBDSgqTaIEf",
        "colab_type": "code",
        "outputId": "925592a8-bd57-4fbd-938d-54251c556932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"project_data.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "\n",
        "%cd project\n",
        "\n",
        "X_test = np.load(\"X_test.npy\")\n",
        "y_test = np.load(\"y_test.npy\")\n",
        "person_train_valid = np.load(\"person_train_valid.npy\")\n",
        "X_train_valid = np.load(\"X_train_valid.npy\")\n",
        "y_train_valid = np.load(\"y_train_valid.npy\")\n",
        "person_test = np.load(\"person_test.npy\")\n",
        "\n",
        "%cd .."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/UCLA/EE239AS Deep learning/project/code/project\n",
            "/content/drive/My Drive/UCLA/EE239AS Deep learning/project/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoH1EUNnaJrp",
        "colab_type": "code",
        "outputId": "03ab224e-76a0-4e44-f547-6c200354daaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
        "print ('Test data shape: {}'.format(X_test.shape))\n",
        "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
        "print ('Test target shape: {}'.format(y_test.shape))\n",
        "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
        "print ('Person test shape: {}'.format(person_test.shape))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training/Valid data shape: (2115, 25, 1000)\n",
            "Test data shape: (443, 25, 1000)\n",
            "Training/Valid target shape: (2115,)\n",
            "Test target shape: (443,)\n",
            "Person train/valid shape: (2115, 1)\n",
            "Person test shape: (443, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM4CEGPBDMW0",
        "colab_type": "text"
      },
      "source": [
        "## preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3pG-GNDoCID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def onehotencoding(y_train_valid):\n",
        "  num_train = y_train_valid.shape[0]\n",
        "  num_class = len(np.unique(y_train_valid))\n",
        "  y_train = np.zeros((num_train, num_class))\n",
        "  y_train[range(num_train), y_train_valid-769] = 1\n",
        "  return y_train\n",
        "\n",
        "Y_train = onehotencoding(y_train_valid)\n",
        "Y_test = onehotencoding(y_test)\n",
        "\n",
        "X_train = X_train_valid[:,0:22,:]\n",
        "X_test = X_test[:,0:22,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC-xPuDNkZcj",
        "colab_type": "text"
      },
      "source": [
        "### show the Fourier transform for a single recording"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4Lu_lLCBwUi",
        "colab_type": "code",
        "outputId": "72fe9b7e-3d12-4ca0-bae7-4142bfded7a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# plt.plot(2.0/N * np.abs(X_train_freq[0,0,:]))\n",
        "# X_train_freq[0,0,:].shape\n",
        "# X_train_freq[0,0,0:N//2].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaAJIJ_7_gib",
        "colab_type": "code",
        "outputId": "5bb508ce-8edb-4a58-d586-547a919513dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train_freq = np.fft.rfft(X_train)\n",
        "X_train_freq.shape\n",
        "# freqs = np.arange(0,1/2*fs,fs/N)\n",
        "# plt.plot(freqs, 2.0/N * np.abs(X_train_freq[0,0,0:N//2]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2115, 22, 501)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z-BtmC-Wc-T",
        "colab_type": "code",
        "outputId": "aced7722-9326-49c9-dab3-418d7794f112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "fs = 500\n",
        "x = np.arange(0,4,1/fs)\n",
        "y = X_train[0,0,:]\n",
        "N = len(y)\n",
        "yf = np.fft.fft(y)\n",
        "xf = np.arange(0,1/2*fs,fs/N)\n",
        "\n",
        "def findFFT(X):\n",
        "  N = X.shape[-1]\n",
        "  Xf = np.fft.fft(X)\n",
        "\n",
        "def running_mean(x, N):\n",
        "  \n",
        "  cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
        "  return (cumsum[N:] - cumsum[:-N]) / N\n",
        "\n",
        "window = 5\n",
        "yf_hat = running_mean(2.0 / N * np.abs(yf[0:N//2]), window)\n",
        "\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(xf, 2.0/N * np.abs(yf[0:N//2]))\n",
        "ax.grid(True)\n",
        "ax.set(xlabel = '(Hz)', ylabel = 'Amp')\n",
        "ax.plot(xf[window-3:-window+3], yf_hat)\n",
        "ax.legend(['FFT','smooth FFT'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f000611f128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAENCAYAAADnrmWtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78LholZuXMvE",
        "colab_type": "code",
        "outputId": "2afbcbaf-4a1d-487e-e91d-0887b975882a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "# pd.DataFrame(X_train)\n",
        "print(X_train.shape)\n",
        "fig, ax = plt.subplots(figsize = (7,7))\n",
        "plt.imshow(X_train[0], aspect='auto')\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2115, 22, 1000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7fdabed25780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAGfCAYAAAAUBHZmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 504x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1IO_qeVJ6jh",
        "colab_type": "text"
      },
      "source": [
        "## CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnakzPN9OiDE",
        "colab_type": "text"
      },
      "source": [
        "### Trail-wise Deep CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPUspEUzF0u0",
        "colab_type": "text"
      },
      "source": [
        "### import keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHU3bSrQJ56n",
        "colab_type": "code",
        "outputId": "e31dff9f-1a05-40c0-f82f-deb80bfba8a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential, model_from_json\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.layers import *\n",
        "from keras import regularizers\n",
        "from keras.callbacks import EarlyStopping\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w6XzzdxA4PF",
        "colab_type": "text"
      },
      "source": [
        "### Without gaussian noise\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP8oWP7vAzmF",
        "colab_type": "code",
        "outputId": "cb14edd6-9c7f-4dcf-c5fb-17394a9063dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dropout_rate = 0.3\n",
        "\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=25, kernel_size=(1,10), \n",
        "                strides=(1, 1), input_shape=(22,1000,1),\n",
        "                padding ='valid'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(GaussianNoise(0.1))\n",
        "model.add(Conv2D(filters=25, kernel_size=(22,1), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(GaussianNoise(0.1))\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(MaxPool2D(pool_size=(1, 3), strides=(1, 3)))\n",
        "model.add(Reshape((-1,25,1)))\n",
        "model.add(Conv2D(filters=50, kernel_size=(10,25), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(Reshape((-1,50,1)))\n",
        "model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "model.add(Conv2D(filters=100, kernel_size=(10,50), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "# model.add(Dropout(dropout_rate))\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "\n",
        "model.add(Reshape((-1,100,1)))\n",
        "model.add(MaxPool2D(pool_size=(2, 1), strides=(2, 1)))\n",
        "model.add(Conv2D(filters=200, kernel_size=(3,100), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "\n",
        "model.add(Reshape((-1,200,1)))\n",
        "model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "model.add(Flatten())\n",
        "# model.add(Dense(units = 20))\n",
        "# model.add(Activation('elu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(Dense(units = 4, activation='softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])\n",
        "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
        "\n",
        "history = model.fit(X_train.reshape(-1,22,1000,1), Y_train, \n",
        "                    batch_size=256, epochs = 200, validation_split = 0.2,\n",
        "                    callbacks = [early_stop])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 22, 991, 25)       275       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 991, 25)        13775     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1, 991, 25)        100       \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_1 (Spatial (None, 1, 991, 25)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1, 330, 25)        0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 330, 25, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 321, 1, 50)        12550     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 321, 1, 50)        200       \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_2 (Spatial (None, 321, 1, 50)        0         \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 321, 50, 1)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 107, 50, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 98, 1, 100)        50100     \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_3 (Spatial (None, 98, 1, 100)        0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 98, 100, 1)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 49, 100, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 47, 1, 200)        60200     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 47, 1, 200)        800       \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_4 (Spatial (None, 47, 1, 200)        0         \n",
            "_________________________________________________________________\n",
            "reshape_4 (Reshape)          (None, 47, 200, 1)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 15, 200, 1)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3000)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 12004     \n",
            "=================================================================\n",
            "Total params: 150,004\n",
            "Trainable params: 149,454\n",
            "Non-trainable params: 550\n",
            "_________________________________________________________________\n",
            "Train on 1692 samples, validate on 423 samples\n",
            "Epoch 1/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.7747 - acc: 0.3103 - val_loss: 2.0180 - val_acc: 0.3121\n",
            "Epoch 2/200\n",
            "1692/1692 [==============================] - 1s 393us/step - loss: 1.6020 - acc: 0.3599 - val_loss: 1.7759 - val_acc: 0.3712\n",
            "Epoch 3/200\n",
            "1692/1692 [==============================] - 1s 391us/step - loss: 1.4643 - acc: 0.4108 - val_loss: 1.5534 - val_acc: 0.4090\n",
            "Epoch 4/200\n",
            "1692/1692 [==============================] - 1s 387us/step - loss: 1.3117 - acc: 0.4704 - val_loss: 1.5431 - val_acc: 0.4350\n",
            "Epoch 5/200\n",
            "1692/1692 [==============================] - 1s 392us/step - loss: 1.2312 - acc: 0.5030 - val_loss: 1.5318 - val_acc: 0.4350\n",
            "Epoch 6/200\n",
            "1692/1692 [==============================] - 1s 393us/step - loss: 1.1643 - acc: 0.5160 - val_loss: 1.4316 - val_acc: 0.4492\n",
            "Epoch 7/200\n",
            "1692/1692 [==============================] - 1s 387us/step - loss: 1.0585 - acc: 0.5691 - val_loss: 1.4137 - val_acc: 0.4823\n",
            "Epoch 8/200\n",
            "1692/1692 [==============================] - 1s 394us/step - loss: 0.9264 - acc: 0.6206 - val_loss: 1.5024 - val_acc: 0.4846\n",
            "Epoch 9/200\n",
            "1692/1692 [==============================] - 1s 394us/step - loss: 0.9198 - acc: 0.6212 - val_loss: 1.3988 - val_acc: 0.4846\n",
            "Epoch 10/200\n",
            "1692/1692 [==============================] - 1s 397us/step - loss: 0.8420 - acc: 0.6584 - val_loss: 1.4303 - val_acc: 0.4799\n",
            "Epoch 11/200\n",
            "1692/1692 [==============================] - 1s 392us/step - loss: 0.8024 - acc: 0.6850 - val_loss: 1.3806 - val_acc: 0.5154\n",
            "Epoch 12/200\n",
            "1692/1692 [==============================] - 1s 396us/step - loss: 0.7322 - acc: 0.7033 - val_loss: 1.4456 - val_acc: 0.4988\n",
            "Epoch 13/200\n",
            "1692/1692 [==============================] - 1s 393us/step - loss: 0.6707 - acc: 0.7340 - val_loss: 1.4635 - val_acc: 0.4846\n",
            "Epoch 14/200\n",
            "1692/1692 [==============================] - 1s 392us/step - loss: 0.6050 - acc: 0.7707 - val_loss: 1.4005 - val_acc: 0.4917\n",
            "Epoch 15/200\n",
            "1692/1692 [==============================] - 1s 394us/step - loss: 0.5837 - acc: 0.7819 - val_loss: 1.4545 - val_acc: 0.5035\n",
            "Epoch 16/200\n",
            "1692/1692 [==============================] - 1s 392us/step - loss: 0.5605 - acc: 0.7961 - val_loss: 1.3927 - val_acc: 0.5154\n",
            "Epoch 17/200\n",
            "1692/1692 [==============================] - 1s 397us/step - loss: 0.5172 - acc: 0.8174 - val_loss: 1.5036 - val_acc: 0.5012\n",
            "Epoch 18/200\n",
            "1692/1692 [==============================] - 1s 394us/step - loss: 0.5058 - acc: 0.8144 - val_loss: 1.5433 - val_acc: 0.4941\n",
            "Epoch 19/200\n",
            "1692/1692 [==============================] - 1s 395us/step - loss: 0.4714 - acc: 0.8227 - val_loss: 1.4944 - val_acc: 0.4988\n",
            "Epoch 20/200\n",
            "1692/1692 [==============================] - 1s 398us/step - loss: 0.4366 - acc: 0.8357 - val_loss: 1.4764 - val_acc: 0.4894\n",
            "Epoch 21/200\n",
            "1692/1692 [==============================] - 1s 392us/step - loss: 0.4533 - acc: 0.8292 - val_loss: 1.5227 - val_acc: 0.5130\n",
            "Epoch 22/200\n",
            "1692/1692 [==============================] - 1s 393us/step - loss: 0.4010 - acc: 0.8611 - val_loss: 1.5126 - val_acc: 0.5130\n",
            "Epoch 23/200\n",
            "1692/1692 [==============================] - 1s 392us/step - loss: 0.4024 - acc: 0.8505 - val_loss: 1.6603 - val_acc: 0.4752\n",
            "Epoch 24/200\n",
            "1692/1692 [==============================] - 1s 398us/step - loss: 0.3552 - acc: 0.8788 - val_loss: 1.5332 - val_acc: 0.4988\n",
            "Epoch 25/200\n",
            "1692/1692 [==============================] - 1s 403us/step - loss: 0.3373 - acc: 0.8877 - val_loss: 1.5591 - val_acc: 0.5130\n",
            "Epoch 26/200\n",
            "1692/1692 [==============================] - 1s 406us/step - loss: 0.3170 - acc: 0.8948 - val_loss: 1.4993 - val_acc: 0.5319\n",
            "Epoch 27/200\n",
            "1692/1692 [==============================] - 1s 403us/step - loss: 0.2989 - acc: 0.9090 - val_loss: 1.5471 - val_acc: 0.5296\n",
            "Epoch 28/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.2956 - acc: 0.9013 - val_loss: 1.5029 - val_acc: 0.5390\n",
            "Epoch 29/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.2712 - acc: 0.9238 - val_loss: 1.5576 - val_acc: 0.5177\n",
            "Epoch 30/200\n",
            "1692/1692 [==============================] - 1s 397us/step - loss: 0.2665 - acc: 0.9125 - val_loss: 1.4880 - val_acc: 0.5272\n",
            "Epoch 31/200\n",
            "1692/1692 [==============================] - 1s 396us/step - loss: 0.2478 - acc: 0.9243 - val_loss: 1.5645 - val_acc: 0.5343\n",
            "Epoch 32/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.2631 - acc: 0.9066 - val_loss: 1.6121 - val_acc: 0.5059\n",
            "Epoch 33/200\n",
            "1692/1692 [==============================] - 1s 395us/step - loss: 0.2372 - acc: 0.9243 - val_loss: 1.6910 - val_acc: 0.4846\n",
            "Epoch 34/200\n",
            "1692/1692 [==============================] - 1s 395us/step - loss: 0.2165 - acc: 0.9374 - val_loss: 1.6730 - val_acc: 0.5106\n",
            "Epoch 35/200\n",
            "1692/1692 [==============================] - 1s 397us/step - loss: 0.2166 - acc: 0.9385 - val_loss: 1.6368 - val_acc: 0.5248\n",
            "Epoch 36/200\n",
            "1692/1692 [==============================] - 1s 394us/step - loss: 0.1937 - acc: 0.9444 - val_loss: 1.6738 - val_acc: 0.5201\n",
            "Epoch 37/200\n",
            "1692/1692 [==============================] - 1s 394us/step - loss: 0.1914 - acc: 0.9433 - val_loss: 1.7041 - val_acc: 0.5201\n",
            "Epoch 38/200\n",
            "1692/1692 [==============================] - 1s 396us/step - loss: 0.1812 - acc: 0.9498 - val_loss: 1.6759 - val_acc: 0.5437\n",
            "Epoch 39/200\n",
            "1692/1692 [==============================] - 1s 397us/step - loss: 0.1923 - acc: 0.9427 - val_loss: 1.6105 - val_acc: 0.5437\n",
            "Epoch 40/200\n",
            "1692/1692 [==============================] - 1s 393us/step - loss: 0.2006 - acc: 0.9344 - val_loss: 1.6668 - val_acc: 0.5177\n",
            "Epoch 41/200\n",
            "1692/1692 [==============================] - 1s 403us/step - loss: 0.1775 - acc: 0.9498 - val_loss: 1.6727 - val_acc: 0.4988\n",
            "Epoch 42/200\n",
            "1692/1692 [==============================] - 1s 398us/step - loss: 0.1706 - acc: 0.9480 - val_loss: 1.8410 - val_acc: 0.4823\n",
            "Epoch 43/200\n",
            "1692/1692 [==============================] - 1s 400us/step - loss: 0.1631 - acc: 0.9574 - val_loss: 1.7669 - val_acc: 0.5035\n",
            "Epoch 44/200\n",
            "1692/1692 [==============================] - 1s 398us/step - loss: 0.1528 - acc: 0.9592 - val_loss: 1.9407 - val_acc: 0.4988\n",
            "Epoch 45/200\n",
            "1692/1692 [==============================] - 1s 395us/step - loss: 0.1548 - acc: 0.9557 - val_loss: 1.7837 - val_acc: 0.5343\n",
            "Epoch 46/200\n",
            "1692/1692 [==============================] - 1s 395us/step - loss: 0.1463 - acc: 0.9563 - val_loss: 1.7340 - val_acc: 0.5296\n",
            "Epoch 47/200\n",
            "1692/1692 [==============================] - 1s 403us/step - loss: 0.1448 - acc: 0.9622 - val_loss: 1.7998 - val_acc: 0.5201\n",
            "Epoch 48/200\n",
            "1692/1692 [==============================] - 1s 398us/step - loss: 0.1280 - acc: 0.9681 - val_loss: 1.8018 - val_acc: 0.5272\n",
            "Epoch 49/200\n",
            "1692/1692 [==============================] - 1s 398us/step - loss: 0.1379 - acc: 0.9574 - val_loss: 1.8094 - val_acc: 0.5366\n",
            "Epoch 50/200\n",
            "1692/1692 [==============================] - 1s 397us/step - loss: 0.1406 - acc: 0.9580 - val_loss: 1.7424 - val_acc: 0.5508\n",
            "Epoch 51/200\n",
            "1692/1692 [==============================] - 1s 395us/step - loss: 0.1363 - acc: 0.9557 - val_loss: 1.8503 - val_acc: 0.4988\n",
            "Epoch 52/200\n",
            "1692/1692 [==============================] - 1s 397us/step - loss: 0.1370 - acc: 0.9592 - val_loss: 1.9065 - val_acc: 0.5319\n",
            "Epoch 53/200\n",
            "1692/1692 [==============================] - 1s 395us/step - loss: 0.1391 - acc: 0.9610 - val_loss: 2.0189 - val_acc: 0.5106\n",
            "Epoch 54/200\n",
            "1692/1692 [==============================] - 1s 400us/step - loss: 0.1285 - acc: 0.9574 - val_loss: 1.9770 - val_acc: 0.5059\n",
            "Epoch 55/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.1302 - acc: 0.9616 - val_loss: 1.9885 - val_acc: 0.5059\n",
            "Epoch 56/200\n",
            "1692/1692 [==============================] - 1s 406us/step - loss: 0.1408 - acc: 0.9563 - val_loss: 1.8517 - val_acc: 0.5201\n",
            "Epoch 57/200\n",
            "1692/1692 [==============================] - 1s 393us/step - loss: 0.1183 - acc: 0.9675 - val_loss: 1.8776 - val_acc: 0.5272\n",
            "Epoch 58/200\n",
            "1692/1692 [==============================] - 1s 400us/step - loss: 0.1137 - acc: 0.9687 - val_loss: 1.9001 - val_acc: 0.5201\n",
            "Epoch 59/200\n",
            "1692/1692 [==============================] - 1s 398us/step - loss: 0.1088 - acc: 0.9728 - val_loss: 1.8432 - val_acc: 0.5201\n",
            "Epoch 60/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.1106 - acc: 0.9693 - val_loss: 1.7772 - val_acc: 0.5248\n",
            "Epoch 61/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.1061 - acc: 0.9669 - val_loss: 1.8014 - val_acc: 0.5248\n",
            "Epoch 62/200\n",
            "1692/1692 [==============================] - 1s 400us/step - loss: 0.1000 - acc: 0.9710 - val_loss: 1.7861 - val_acc: 0.5461\n",
            "Epoch 63/200\n",
            "1692/1692 [==============================] - 1s 397us/step - loss: 0.0970 - acc: 0.9699 - val_loss: 1.7679 - val_acc: 0.5248\n",
            "Epoch 64/200\n",
            "1692/1692 [==============================] - 1s 402us/step - loss: 0.1172 - acc: 0.9622 - val_loss: 1.7773 - val_acc: 0.5437\n",
            "Epoch 65/200\n",
            "1692/1692 [==============================] - 1s 397us/step - loss: 0.1132 - acc: 0.9657 - val_loss: 1.8484 - val_acc: 0.5272\n",
            "Epoch 66/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.1023 - acc: 0.9716 - val_loss: 1.8898 - val_acc: 0.5225\n",
            "Epoch 67/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.0981 - acc: 0.9722 - val_loss: 1.8459 - val_acc: 0.5366\n",
            "Epoch 68/200\n",
            "1692/1692 [==============================] - 1s 401us/step - loss: 0.0972 - acc: 0.9704 - val_loss: 1.8912 - val_acc: 0.5272\n",
            "Epoch 69/200\n",
            "1692/1692 [==============================] - 1s 400us/step - loss: 0.0903 - acc: 0.9710 - val_loss: 1.9845 - val_acc: 0.5437\n",
            "Epoch 70/200\n",
            "1692/1692 [==============================] - 1s 400us/step - loss: 0.0992 - acc: 0.9716 - val_loss: 1.8550 - val_acc: 0.5343\n",
            "Epoch 71/200\n",
            "1692/1692 [==============================] - 1s 404us/step - loss: 0.0826 - acc: 0.9728 - val_loss: 1.8112 - val_acc: 0.5296\n",
            "Epoch 72/200\n",
            "1692/1692 [==============================] - 1s 401us/step - loss: 0.0907 - acc: 0.9699 - val_loss: 1.8805 - val_acc: 0.5437\n",
            "Epoch 73/200\n",
            "1692/1692 [==============================] - 1s 400us/step - loss: 0.0824 - acc: 0.9805 - val_loss: 1.9228 - val_acc: 0.5296\n",
            "Epoch 74/200\n",
            "1692/1692 [==============================] - 1s 403us/step - loss: 0.0935 - acc: 0.9704 - val_loss: 1.9694 - val_acc: 0.5248\n",
            "Epoch 75/200\n",
            "1692/1692 [==============================] - 1s 396us/step - loss: 0.0821 - acc: 0.9805 - val_loss: 1.8952 - val_acc: 0.5343\n",
            "Epoch 76/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.0745 - acc: 0.9811 - val_loss: 1.9266 - val_acc: 0.5154\n",
            "Epoch 77/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.0738 - acc: 0.9793 - val_loss: 1.9049 - val_acc: 0.5130\n",
            "Epoch 78/200\n",
            "1692/1692 [==============================] - 1s 398us/step - loss: 0.0875 - acc: 0.9722 - val_loss: 1.8967 - val_acc: 0.5343\n",
            "Epoch 79/200\n",
            "1692/1692 [==============================] - 1s 403us/step - loss: 0.0717 - acc: 0.9781 - val_loss: 2.0110 - val_acc: 0.4965\n",
            "Epoch 80/200\n",
            "1692/1692 [==============================] - 1s 401us/step - loss: 0.0730 - acc: 0.9787 - val_loss: 1.9778 - val_acc: 0.5248\n",
            "Epoch 81/200\n",
            "1692/1692 [==============================] - 1s 394us/step - loss: 0.0764 - acc: 0.9764 - val_loss: 1.8377 - val_acc: 0.5532\n",
            "Epoch 82/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.0783 - acc: 0.9770 - val_loss: 1.9205 - val_acc: 0.5154\n",
            "Epoch 83/200\n",
            "1692/1692 [==============================] - 1s 395us/step - loss: 0.0739 - acc: 0.9775 - val_loss: 2.0280 - val_acc: 0.5248\n",
            "Epoch 84/200\n",
            "1692/1692 [==============================] - 1s 393us/step - loss: 0.0735 - acc: 0.9775 - val_loss: 2.0083 - val_acc: 0.5106\n",
            "Epoch 85/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.0671 - acc: 0.9817 - val_loss: 2.0129 - val_acc: 0.5272\n",
            "Epoch 86/200\n",
            "1692/1692 [==============================] - 1s 397us/step - loss: 0.0646 - acc: 0.9829 - val_loss: 2.0382 - val_acc: 0.5319\n",
            "Epoch 87/200\n",
            "1692/1692 [==============================] - 1s 397us/step - loss: 0.0638 - acc: 0.9840 - val_loss: 2.0185 - val_acc: 0.5461\n",
            "Epoch 88/200\n",
            "1692/1692 [==============================] - 1s 402us/step - loss: 0.0557 - acc: 0.9852 - val_loss: 2.0170 - val_acc: 0.5414\n",
            "Epoch 89/200\n",
            "1692/1692 [==============================] - 1s 401us/step - loss: 0.0791 - acc: 0.9752 - val_loss: 2.0237 - val_acc: 0.5414\n",
            "Epoch 90/200\n",
            "1692/1692 [==============================] - 1s 401us/step - loss: 0.0672 - acc: 0.9781 - val_loss: 2.0458 - val_acc: 0.5414\n",
            "Epoch 91/200\n",
            "1692/1692 [==============================] - 1s 400us/step - loss: 0.0633 - acc: 0.9829 - val_loss: 2.0074 - val_acc: 0.5532\n",
            "Epoch 92/200\n",
            "1692/1692 [==============================] - 1s 400us/step - loss: 0.0604 - acc: 0.9829 - val_loss: 2.0311 - val_acc: 0.5390\n",
            "Epoch 93/200\n",
            "1692/1692 [==============================] - 1s 401us/step - loss: 0.0554 - acc: 0.9864 - val_loss: 2.0772 - val_acc: 0.5248\n",
            "Epoch 94/200\n",
            "1692/1692 [==============================] - 1s 402us/step - loss: 0.0608 - acc: 0.9817 - val_loss: 2.0807 - val_acc: 0.5248\n",
            "Epoch 95/200\n",
            "1692/1692 [==============================] - 1s 401us/step - loss: 0.0631 - acc: 0.9829 - val_loss: 2.0238 - val_acc: 0.5366\n",
            "Epoch 96/200\n",
            "1692/1692 [==============================] - 1s 404us/step - loss: 0.0525 - acc: 0.9888 - val_loss: 2.0966 - val_acc: 0.5272\n",
            "Epoch 97/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.0507 - acc: 0.9870 - val_loss: 2.0595 - val_acc: 0.5390\n",
            "Epoch 98/200\n",
            "1692/1692 [==============================] - 1s 397us/step - loss: 0.0608 - acc: 0.9852 - val_loss: 2.0957 - val_acc: 0.5154\n",
            "Epoch 99/200\n",
            "1692/1692 [==============================] - 1s 397us/step - loss: 0.0501 - acc: 0.9888 - val_loss: 2.0283 - val_acc: 0.5343\n",
            "Epoch 100/200\n",
            "1692/1692 [==============================] - 1s 401us/step - loss: 0.0658 - acc: 0.9823 - val_loss: 2.1151 - val_acc: 0.5083\n",
            "Epoch 101/200\n",
            "1692/1692 [==============================] - 1s 403us/step - loss: 0.0636 - acc: 0.9799 - val_loss: 2.0700 - val_acc: 0.5272\n",
            "Epoch 102/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.0501 - acc: 0.9817 - val_loss: 2.0170 - val_acc: 0.5296\n",
            "Epoch 103/200\n",
            "1692/1692 [==============================] - 1s 398us/step - loss: 0.0534 - acc: 0.9840 - val_loss: 2.1031 - val_acc: 0.5201\n",
            "Epoch 104/200\n",
            "1692/1692 [==============================] - 1s 398us/step - loss: 0.0464 - acc: 0.9911 - val_loss: 2.1567 - val_acc: 0.5272\n",
            "Epoch 105/200\n",
            "1692/1692 [==============================] - 1s 395us/step - loss: 0.0523 - acc: 0.9870 - val_loss: 2.1263 - val_acc: 0.5130\n",
            "Epoch 106/200\n",
            "1692/1692 [==============================] - 1s 395us/step - loss: 0.0603 - acc: 0.9799 - val_loss: 2.0844 - val_acc: 0.5154\n",
            "Epoch 107/200\n",
            "1692/1692 [==============================] - 1s 400us/step - loss: 0.0616 - acc: 0.9793 - val_loss: 2.1836 - val_acc: 0.5059\n",
            "Epoch 108/200\n",
            "1692/1692 [==============================] - 1s 402us/step - loss: 0.0499 - acc: 0.9870 - val_loss: 2.1123 - val_acc: 0.5059\n",
            "Epoch 109/200\n",
            "1692/1692 [==============================] - 1s 400us/step - loss: 0.0622 - acc: 0.9835 - val_loss: 2.0836 - val_acc: 0.5437\n",
            "Epoch 110/200\n",
            "1692/1692 [==============================] - 1s 401us/step - loss: 0.0473 - acc: 0.9888 - val_loss: 2.1240 - val_acc: 0.5248\n",
            "Epoch 111/200\n",
            "1692/1692 [==============================] - 1s 400us/step - loss: 0.0613 - acc: 0.9793 - val_loss: 2.1115 - val_acc: 0.5201\n",
            "Epoch 112/200\n",
            "1692/1692 [==============================] - 1s 404us/step - loss: 0.0521 - acc: 0.9846 - val_loss: 2.2605 - val_acc: 0.5035\n",
            "Epoch 113/200\n",
            "1692/1692 [==============================] - 1s 398us/step - loss: 0.0630 - acc: 0.9805 - val_loss: 2.1745 - val_acc: 0.5272\n",
            "Epoch 114/200\n",
            "1692/1692 [==============================] - 1s 401us/step - loss: 0.0622 - acc: 0.9787 - val_loss: 2.0709 - val_acc: 0.5106\n",
            "Epoch 115/200\n",
            "1692/1692 [==============================] - 1s 400us/step - loss: 0.0472 - acc: 0.9882 - val_loss: 2.2294 - val_acc: 0.5083\n",
            "Epoch 116/200\n",
            "1692/1692 [==============================] - 1s 395us/step - loss: 0.0589 - acc: 0.9811 - val_loss: 2.1997 - val_acc: 0.5248\n",
            "Epoch 117/200\n",
            "1692/1692 [==============================] - 1s 400us/step - loss: 0.0564 - acc: 0.9835 - val_loss: 2.1361 - val_acc: 0.5319\n",
            "Epoch 118/200\n",
            "1692/1692 [==============================] - 1s 401us/step - loss: 0.0555 - acc: 0.9829 - val_loss: 2.1670 - val_acc: 0.5366\n",
            "Epoch 119/200\n",
            "1692/1692 [==============================] - 1s 397us/step - loss: 0.0536 - acc: 0.9829 - val_loss: 2.0923 - val_acc: 0.5390\n",
            "Epoch 120/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.0582 - acc: 0.9817 - val_loss: 2.0172 - val_acc: 0.5390\n",
            "Epoch 121/200\n",
            "1692/1692 [==============================] - 1s 401us/step - loss: 0.0652 - acc: 0.9805 - val_loss: 2.0985 - val_acc: 0.5296\n",
            "Epoch 122/200\n",
            "1692/1692 [==============================] - 1s 399us/step - loss: 0.0648 - acc: 0.9787 - val_loss: 2.1496 - val_acc: 0.5390\n",
            "Epoch 123/200\n",
            "1692/1692 [==============================] - 1s 403us/step - loss: 0.0527 - acc: 0.9840 - val_loss: 2.1934 - val_acc: 0.5106\n",
            "Epoch 124/200\n",
            "1692/1692 [==============================] - 1s 393us/step - loss: 0.0574 - acc: 0.9799 - val_loss: 2.2452 - val_acc: 0.5177\n",
            "Epoch 00124: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZt5MtYpCDny",
        "colab_type": "code",
        "outputId": "968aa52d-262e-487a-cd9c-0c9bb3a0b62e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "_, accu = model.evaluate(X_test.reshape(-1,22,1000,1), Y_test)\n",
        "\n",
        "print(\"training accu is : {:2.2%}\".format(history.history['acc'][-1]))\n",
        "print(\"val accu is : {:2.2%}\".format(history.history['val_acc'][-1]))\n",
        "print(\"test accu is : {:2.2%}\".format(accu))\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model acc')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "443/443 [==============================] - 0s 270us/step\n",
            "training accu is : 97.99%\n",
            "val accu is : 51.77%\n",
            "test accu is : 46.28%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0YAAAFRCAYAAABQaL0SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niMS9/wNGwkMBpfDJYAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1008x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWYmYM3aA0A9",
        "colab_type": "text"
      },
      "source": [
        "### With gaussian noise\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eqYg8UVKpZe",
        "colab_type": "code",
        "outputId": "653cc6fb-9bb1-411a-8ad3-dd16274496e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dropout_rate = 0.6\n",
        "\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=25, kernel_size=(1,10), \n",
        "                strides=(1, 1), input_shape=(22,1000,1),\n",
        "                 activation = 'elu'\n",
        "                padding ='valid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(GaussianNoise(0.1))\n",
        "model.add(Conv2D(filters=25, kernel_size=(22,1), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(GaussianNoise(0.1))\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(MaxPool2D(pool_size=(1, 3), strides=(1, 3)))\n",
        "model.add(Reshape((-1,25,1)))\n",
        "model.add(Conv2D(filters=50, kernel_size=(10,25), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(Reshape((-1,50,1)))\n",
        "model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "model.add(Conv2D(filters=100, kernel_size=(10,50), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "# model.add(Dropout(dropout_rate))\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "\n",
        "model.add(Reshape((-1,100,1)))\n",
        "model.add(MaxPool2D(pool_size=(2, 1), strides=(2, 1)))\n",
        "model.add(Conv2D(filters=200, kernel_size=(3,100), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "\n",
        "model.add(Reshape((-1,200,1)))\n",
        "model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "model.add(Flatten())\n",
        "# model.add(Dense(units = 20))\n",
        "# model.add(Activation('elu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(Dense(units = 4, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 22, 991, 25)       275       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 22, 991, 25)       100       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_1 (GaussianNo (None, 22, 991, 25)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 991, 25)        13775     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1, 991, 25)        100       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_2 (GaussianNo (None, 1, 991, 25)        0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_1 (Spatial (None, 1, 991, 25)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1, 330, 25)        0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 330, 25, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 321, 1, 50)        12550     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 321, 1, 50)        200       \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_2 (Spatial (None, 321, 1, 50)        0         \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 321, 50, 1)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 107, 50, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 98, 1, 100)        50100     \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_3 (Spatial (None, 98, 1, 100)        0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 98, 100, 1)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 49, 100, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 47, 1, 200)        60200     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 47, 1, 200)        800       \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_4 (Spatial (None, 47, 1, 200)        0         \n",
            "_________________________________________________________________\n",
            "reshape_4 (Reshape)          (None, 47, 200, 1)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 15, 200, 1)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3000)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 12004     \n",
            "=================================================================\n",
            "Total params: 150,104\n",
            "Trainable params: 149,504\n",
            "Non-trainable params: 600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R3R7L64L9MS",
        "colab_type": "code",
        "outputId": "d8019085-4f28-4aa3-ecfd-a4b7cd8c2ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
        "\n",
        "history = model.fit(X_train.reshape(2115,22,1000,1), Y_train, \n",
        "                    batch_size=256, epochs = 200, validation_split = 0.2,\n",
        "                    callbacks = [early_stop])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1692 samples, validate on 423 samples\n",
            "Epoch 1/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 2.2121 - acc: 0.2494 - val_loss: 1.5802 - val_acc: 0.2766\n",
            "Epoch 2/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.9634 - acc: 0.2807 - val_loss: 1.5998 - val_acc: 0.3617\n",
            "Epoch 3/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.7692 - acc: 0.3061 - val_loss: 1.4532 - val_acc: 0.3688\n",
            "Epoch 4/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.6821 - acc: 0.3115 - val_loss: 1.3606 - val_acc: 0.3641\n",
            "Epoch 5/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.6471 - acc: 0.2973 - val_loss: 1.3849 - val_acc: 0.3475\n",
            "Epoch 6/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.5973 - acc: 0.3109 - val_loss: 1.4247 - val_acc: 0.3759\n",
            "Epoch 7/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.5543 - acc: 0.3162 - val_loss: 1.3941 - val_acc: 0.3546\n",
            "Epoch 8/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.5279 - acc: 0.3174 - val_loss: 1.3396 - val_acc: 0.3783\n",
            "Epoch 9/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.4639 - acc: 0.3363 - val_loss: 1.3228 - val_acc: 0.3735\n",
            "Epoch 10/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.4541 - acc: 0.3546 - val_loss: 1.2872 - val_acc: 0.3924\n",
            "Epoch 11/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.4339 - acc: 0.3564 - val_loss: 1.2725 - val_acc: 0.4019\n",
            "Epoch 12/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.4146 - acc: 0.3871 - val_loss: 1.2759 - val_acc: 0.4137\n",
            "Epoch 13/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.4605 - acc: 0.3641 - val_loss: 1.3126 - val_acc: 0.4255\n",
            "Epoch 14/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.4283 - acc: 0.3812 - val_loss: 1.2501 - val_acc: 0.4444\n",
            "Epoch 15/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.4066 - acc: 0.3865 - val_loss: 1.3021 - val_acc: 0.4208\n",
            "Epoch 16/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.4263 - acc: 0.3753 - val_loss: 1.2466 - val_acc: 0.4634\n",
            "Epoch 17/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.4217 - acc: 0.3865 - val_loss: 1.2303 - val_acc: 0.4586\n",
            "Epoch 18/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3616 - acc: 0.4007 - val_loss: 1.2154 - val_acc: 0.4350\n",
            "Epoch 19/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3839 - acc: 0.3966 - val_loss: 1.2125 - val_acc: 0.4397\n",
            "Epoch 20/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3719 - acc: 0.4013 - val_loss: 1.2239 - val_acc: 0.4539\n",
            "Epoch 21/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3320 - acc: 0.3972 - val_loss: 1.2018 - val_acc: 0.4752\n",
            "Epoch 22/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3396 - acc: 0.4031 - val_loss: 1.2041 - val_acc: 0.4515\n",
            "Epoch 23/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3579 - acc: 0.4072 - val_loss: 1.2468 - val_acc: 0.4374\n",
            "Epoch 24/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3425 - acc: 0.4096 - val_loss: 1.2272 - val_acc: 0.4799\n",
            "Epoch 25/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3377 - acc: 0.4202 - val_loss: 1.3330 - val_acc: 0.4374\n",
            "Epoch 26/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3158 - acc: 0.4391 - val_loss: 1.1839 - val_acc: 0.4586\n",
            "Epoch 27/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3366 - acc: 0.4184 - val_loss: 1.2823 - val_acc: 0.4539\n",
            "Epoch 28/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3482 - acc: 0.4143 - val_loss: 1.1948 - val_acc: 0.4823\n",
            "Epoch 29/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3053 - acc: 0.4409 - val_loss: 1.2555 - val_acc: 0.4704\n",
            "Epoch 30/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3257 - acc: 0.4155 - val_loss: 1.1750 - val_acc: 0.4870\n",
            "Epoch 31/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3324 - acc: 0.4143 - val_loss: 1.2637 - val_acc: 0.4586\n",
            "Epoch 32/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2964 - acc: 0.4350 - val_loss: 1.2139 - val_acc: 0.4728\n",
            "Epoch 33/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2345 - acc: 0.4580 - val_loss: 1.2283 - val_acc: 0.4823\n",
            "Epoch 34/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2550 - acc: 0.4474 - val_loss: 1.2063 - val_acc: 0.4752\n",
            "Epoch 35/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2643 - acc: 0.4639 - val_loss: 1.1754 - val_acc: 0.4775\n",
            "Epoch 36/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2769 - acc: 0.4338 - val_loss: 1.2603 - val_acc: 0.4846\n",
            "Epoch 37/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2888 - acc: 0.4468 - val_loss: 1.1758 - val_acc: 0.4775\n",
            "Epoch 38/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2381 - acc: 0.4746 - val_loss: 1.2088 - val_acc: 0.4728\n",
            "Epoch 39/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2514 - acc: 0.4551 - val_loss: 1.2211 - val_acc: 0.4634\n",
            "Epoch 40/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2568 - acc: 0.4539 - val_loss: 1.1408 - val_acc: 0.5130\n",
            "Epoch 41/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2481 - acc: 0.4598 - val_loss: 1.1636 - val_acc: 0.4965\n",
            "Epoch 42/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2344 - acc: 0.4628 - val_loss: 1.1322 - val_acc: 0.5012\n",
            "Epoch 43/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2024 - acc: 0.4770 - val_loss: 1.1474 - val_acc: 0.5083\n",
            "Epoch 44/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2306 - acc: 0.4722 - val_loss: 1.1530 - val_acc: 0.5130\n",
            "Epoch 45/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2450 - acc: 0.4764 - val_loss: 1.1134 - val_acc: 0.5225\n",
            "Epoch 46/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2127 - acc: 0.4870 - val_loss: 1.1505 - val_acc: 0.4988\n",
            "Epoch 47/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1938 - acc: 0.4935 - val_loss: 1.1162 - val_acc: 0.5154\n",
            "Epoch 48/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2040 - acc: 0.4941 - val_loss: 1.2182 - val_acc: 0.4917\n",
            "Epoch 49/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2277 - acc: 0.4858 - val_loss: 1.1373 - val_acc: 0.5035\n",
            "Epoch 50/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1787 - acc: 0.5053 - val_loss: 1.2043 - val_acc: 0.5035\n",
            "Epoch 51/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1759 - acc: 0.5035 - val_loss: 1.1355 - val_acc: 0.4988\n",
            "Epoch 52/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1615 - acc: 0.5047 - val_loss: 1.1279 - val_acc: 0.5272\n",
            "Epoch 53/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1470 - acc: 0.5142 - val_loss: 1.2020 - val_acc: 0.5272\n",
            "Epoch 54/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1482 - acc: 0.5254 - val_loss: 1.1207 - val_acc: 0.5225\n",
            "Epoch 55/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1709 - acc: 0.5041 - val_loss: 1.1566 - val_acc: 0.5319\n",
            "Epoch 56/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1584 - acc: 0.5118 - val_loss: 1.1110 - val_acc: 0.5201\n",
            "Epoch 57/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1409 - acc: 0.5219 - val_loss: 1.1028 - val_acc: 0.5461\n",
            "Epoch 58/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1411 - acc: 0.5225 - val_loss: 1.1281 - val_acc: 0.5508\n",
            "Epoch 59/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1684 - acc: 0.5041 - val_loss: 1.1079 - val_acc: 0.5343\n",
            "Epoch 60/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1172 - acc: 0.5331 - val_loss: 1.1362 - val_acc: 0.5225\n",
            "Epoch 61/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1280 - acc: 0.5319 - val_loss: 1.1113 - val_acc: 0.5556\n",
            "Epoch 62/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1362 - acc: 0.5136 - val_loss: 1.1067 - val_acc: 0.5485\n",
            "Epoch 63/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1211 - acc: 0.5195 - val_loss: 1.1380 - val_acc: 0.5272\n",
            "Epoch 64/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0980 - acc: 0.5349 - val_loss: 1.1836 - val_acc: 0.5012\n",
            "Epoch 65/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0945 - acc: 0.5621 - val_loss: 1.1291 - val_acc: 0.5461\n",
            "Epoch 66/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0833 - acc: 0.5426 - val_loss: 1.1540 - val_acc: 0.5390\n",
            "Epoch 67/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1317 - acc: 0.5331 - val_loss: 1.1148 - val_acc: 0.5579\n",
            "Epoch 68/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0891 - acc: 0.5479 - val_loss: 1.1548 - val_acc: 0.5272\n",
            "Epoch 69/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1055 - acc: 0.5437 - val_loss: 1.0940 - val_acc: 0.5650\n",
            "Epoch 70/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0812 - acc: 0.5579 - val_loss: 1.1274 - val_acc: 0.5437\n",
            "Epoch 71/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0928 - acc: 0.5437 - val_loss: 1.1442 - val_acc: 0.5556\n",
            "Epoch 72/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0800 - acc: 0.5532 - val_loss: 1.1081 - val_acc: 0.5437\n",
            "Epoch 73/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0891 - acc: 0.5455 - val_loss: 1.2045 - val_acc: 0.5248\n",
            "Epoch 74/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0592 - acc: 0.5709 - val_loss: 1.1240 - val_acc: 0.5508\n",
            "Epoch 75/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0416 - acc: 0.5715 - val_loss: 1.1403 - val_acc: 0.5532\n",
            "Epoch 76/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0100 - acc: 0.5934 - val_loss: 1.1550 - val_acc: 0.5414\n",
            "Epoch 77/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0419 - acc: 0.5786 - val_loss: 1.1239 - val_acc: 0.5414\n",
            "Epoch 78/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0073 - acc: 0.5780 - val_loss: 1.1578 - val_acc: 0.5437\n",
            "Epoch 79/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0445 - acc: 0.5561 - val_loss: 1.1542 - val_acc: 0.5296\n",
            "Epoch 80/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0467 - acc: 0.5680 - val_loss: 1.1173 - val_acc: 0.5461\n",
            "Epoch 81/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9730 - acc: 0.6028 - val_loss: 1.1530 - val_acc: 0.5390\n",
            "Epoch 82/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0490 - acc: 0.5804 - val_loss: 1.1798 - val_acc: 0.5414\n",
            "Epoch 83/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0231 - acc: 0.5827 - val_loss: 1.1267 - val_acc: 0.5579\n",
            "Epoch 84/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9858 - acc: 0.5975 - val_loss: 1.1505 - val_acc: 0.5414\n",
            "Epoch 85/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0450 - acc: 0.5768 - val_loss: 1.1361 - val_acc: 0.5485\n",
            "Epoch 86/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9775 - acc: 0.6011 - val_loss: 1.2195 - val_acc: 0.5390\n",
            "Epoch 87/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0159 - acc: 0.5922 - val_loss: 1.1679 - val_acc: 0.5248\n",
            "Epoch 88/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0319 - acc: 0.5745 - val_loss: 1.1391 - val_acc: 0.5177\n",
            "Epoch 89/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0205 - acc: 0.5774 - val_loss: 1.2397 - val_acc: 0.5154\n",
            "Epoch 90/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9604 - acc: 0.6158 - val_loss: 1.1577 - val_acc: 0.5201\n",
            "Epoch 91/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9857 - acc: 0.5934 - val_loss: 1.1725 - val_acc: 0.5366\n",
            "Epoch 92/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9413 - acc: 0.6164 - val_loss: 1.1841 - val_acc: 0.5603\n",
            "Epoch 93/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0333 - acc: 0.5987 - val_loss: 1.2068 - val_acc: 0.5650\n",
            "Epoch 94/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9500 - acc: 0.6147 - val_loss: 1.1778 - val_acc: 0.5532\n",
            "Epoch 95/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9601 - acc: 0.6105 - val_loss: 1.1974 - val_acc: 0.5414\n",
            "Epoch 96/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9769 - acc: 0.6123 - val_loss: 1.1834 - val_acc: 0.5366\n",
            "Epoch 97/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9612 - acc: 0.6093 - val_loss: 1.2102 - val_acc: 0.5461\n",
            "Epoch 98/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9559 - acc: 0.6082 - val_loss: 1.1849 - val_acc: 0.5390\n",
            "Epoch 99/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9579 - acc: 0.6170 - val_loss: 1.2598 - val_acc: 0.5248\n",
            "Epoch 100/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9440 - acc: 0.6147 - val_loss: 1.1499 - val_acc: 0.5461\n",
            "Epoch 101/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9449 - acc: 0.6052 - val_loss: 1.1973 - val_acc: 0.5414\n",
            "Epoch 102/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9289 - acc: 0.6182 - val_loss: 1.1939 - val_acc: 0.5414\n",
            "Epoch 103/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9177 - acc: 0.6229 - val_loss: 1.1995 - val_acc: 0.5177\n",
            "Epoch 104/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9126 - acc: 0.6365 - val_loss: 1.2050 - val_acc: 0.5674\n",
            "Epoch 105/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8884 - acc: 0.6389 - val_loss: 1.1727 - val_acc: 0.5603\n",
            "Epoch 106/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8872 - acc: 0.6413 - val_loss: 1.1636 - val_acc: 0.5556\n",
            "Epoch 107/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8987 - acc: 0.6401 - val_loss: 1.2677 - val_acc: 0.5366\n",
            "Epoch 108/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9266 - acc: 0.6259 - val_loss: 1.1814 - val_acc: 0.5485\n",
            "Epoch 109/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8851 - acc: 0.6478 - val_loss: 1.2398 - val_acc: 0.5272\n",
            "Epoch 110/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8776 - acc: 0.6572 - val_loss: 1.1765 - val_acc: 0.5532\n",
            "Epoch 111/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8877 - acc: 0.6371 - val_loss: 1.2263 - val_acc: 0.5366\n",
            "Epoch 112/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9121 - acc: 0.6495 - val_loss: 1.1976 - val_acc: 0.5532\n",
            "Epoch 113/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8690 - acc: 0.6507 - val_loss: 1.2011 - val_acc: 0.5579\n",
            "Epoch 114/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8996 - acc: 0.6448 - val_loss: 1.2178 - val_acc: 0.5603\n",
            "Epoch 115/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8591 - acc: 0.6673 - val_loss: 1.2027 - val_acc: 0.5485\n",
            "Epoch 116/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8800 - acc: 0.6478 - val_loss: 1.2065 - val_acc: 0.5461\n",
            "Epoch 117/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8727 - acc: 0.6537 - val_loss: 1.2763 - val_acc: 0.5390\n",
            "Epoch 118/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9244 - acc: 0.6407 - val_loss: 1.2306 - val_acc: 0.5508\n",
            "Epoch 119/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9058 - acc: 0.6383 - val_loss: 1.2376 - val_acc: 0.5366\n",
            "Epoch 120/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8265 - acc: 0.6613 - val_loss: 1.2369 - val_acc: 0.5343\n",
            "Epoch 121/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8557 - acc: 0.6507 - val_loss: 1.2297 - val_acc: 0.5272\n",
            "Epoch 122/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8335 - acc: 0.6655 - val_loss: 1.2665 - val_acc: 0.5272\n",
            "Epoch 123/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8738 - acc: 0.6543 - val_loss: 1.2741 - val_acc: 0.5272\n",
            "Epoch 124/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8587 - acc: 0.6596 - val_loss: 1.2393 - val_acc: 0.5390\n",
            "Epoch 125/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8263 - acc: 0.6844 - val_loss: 1.3275 - val_acc: 0.4941\n",
            "Epoch 126/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8661 - acc: 0.6584 - val_loss: 1.2466 - val_acc: 0.5154\n",
            "Epoch 127/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7981 - acc: 0.6868 - val_loss: 1.2974 - val_acc: 0.5083\n",
            "Epoch 128/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8371 - acc: 0.6631 - val_loss: 1.2660 - val_acc: 0.5106\n",
            "Epoch 129/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8113 - acc: 0.6832 - val_loss: 1.3267 - val_acc: 0.5083\n",
            "Epoch 130/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8239 - acc: 0.6649 - val_loss: 1.3151 - val_acc: 0.5248\n",
            "Epoch 131/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8067 - acc: 0.6809 - val_loss: 1.2823 - val_acc: 0.5225\n",
            "Epoch 132/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7841 - acc: 0.6844 - val_loss: 1.2871 - val_acc: 0.5154\n",
            "Epoch 133/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7941 - acc: 0.6838 - val_loss: 1.2803 - val_acc: 0.5130\n",
            "Epoch 134/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8076 - acc: 0.6998 - val_loss: 1.2622 - val_acc: 0.5177\n",
            "Epoch 135/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7999 - acc: 0.6927 - val_loss: 1.2449 - val_acc: 0.5296\n",
            "Epoch 136/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8167 - acc: 0.6803 - val_loss: 1.2676 - val_acc: 0.5225\n",
            "Epoch 137/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7573 - acc: 0.6956 - val_loss: 1.2544 - val_acc: 0.5296\n",
            "Epoch 138/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7636 - acc: 0.7051 - val_loss: 1.2608 - val_acc: 0.5248\n",
            "Epoch 139/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7740 - acc: 0.6956 - val_loss: 1.2771 - val_acc: 0.5296\n",
            "Epoch 140/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8258 - acc: 0.6631 - val_loss: 1.2794 - val_acc: 0.5225\n",
            "Epoch 141/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7796 - acc: 0.6832 - val_loss: 1.2983 - val_acc: 0.5343\n",
            "Epoch 142/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7861 - acc: 0.6785 - val_loss: 1.3114 - val_acc: 0.5248\n",
            "Epoch 143/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7591 - acc: 0.7074 - val_loss: 1.2796 - val_acc: 0.5248\n",
            "Epoch 144/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7443 - acc: 0.7051 - val_loss: 1.3409 - val_acc: 0.5225\n",
            "Epoch 145/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7417 - acc: 0.7033 - val_loss: 1.3154 - val_acc: 0.5012\n",
            "Epoch 146/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7556 - acc: 0.7045 - val_loss: 1.3276 - val_acc: 0.5225\n",
            "Epoch 147/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7221 - acc: 0.7074 - val_loss: 1.3196 - val_acc: 0.5296\n",
            "Epoch 148/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7947 - acc: 0.6809 - val_loss: 1.3717 - val_acc: 0.5272\n",
            "Epoch 149/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7052 - acc: 0.7340 - val_loss: 1.3648 - val_acc: 0.5343\n",
            "Epoch 150/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7761 - acc: 0.6974 - val_loss: 1.3458 - val_acc: 0.5343\n",
            "Epoch 151/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7708 - acc: 0.7051 - val_loss: 1.3489 - val_acc: 0.5201\n",
            "Epoch 152/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7326 - acc: 0.7092 - val_loss: 1.3398 - val_acc: 0.5154\n",
            "Epoch 153/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7765 - acc: 0.7021 - val_loss: 1.3882 - val_acc: 0.5083\n",
            "Epoch 154/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7373 - acc: 0.7139 - val_loss: 1.3492 - val_acc: 0.5225\n",
            "Epoch 155/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7678 - acc: 0.6903 - val_loss: 1.4029 - val_acc: 0.5035\n",
            "Epoch 156/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7225 - acc: 0.7169 - val_loss: 1.3833 - val_acc: 0.5083\n",
            "Epoch 157/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7161 - acc: 0.7157 - val_loss: 1.3636 - val_acc: 0.5201\n",
            "Epoch 158/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7520 - acc: 0.7199 - val_loss: 1.3752 - val_acc: 0.5366\n",
            "Epoch 159/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6854 - acc: 0.7405 - val_loss: 1.3714 - val_acc: 0.5319\n",
            "Epoch 160/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7175 - acc: 0.7104 - val_loss: 1.3688 - val_acc: 0.5177\n",
            "Epoch 161/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7416 - acc: 0.7193 - val_loss: 1.3437 - val_acc: 0.5106\n",
            "Epoch 162/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7101 - acc: 0.7199 - val_loss: 1.3946 - val_acc: 0.5154\n",
            "Epoch 163/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7037 - acc: 0.7293 - val_loss: 1.3614 - val_acc: 0.5035\n",
            "Epoch 164/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7141 - acc: 0.7264 - val_loss: 1.3627 - val_acc: 0.5059\n",
            "Epoch 165/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7392 - acc: 0.7098 - val_loss: 1.4046 - val_acc: 0.5130\n",
            "Epoch 166/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7245 - acc: 0.7204 - val_loss: 1.3836 - val_acc: 0.5130\n",
            "Epoch 167/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7301 - acc: 0.7199 - val_loss: 1.3401 - val_acc: 0.5296\n",
            "Epoch 168/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6975 - acc: 0.7258 - val_loss: 1.3469 - val_acc: 0.5225\n",
            "Epoch 169/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7306 - acc: 0.7311 - val_loss: 1.3430 - val_acc: 0.5083\n",
            "Epoch 170/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7276 - acc: 0.7122 - val_loss: 1.3621 - val_acc: 0.4965\n",
            "Epoch 171/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7107 - acc: 0.7346 - val_loss: 1.3796 - val_acc: 0.5201\n",
            "Epoch 172/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6972 - acc: 0.7346 - val_loss: 1.3709 - val_acc: 0.5106\n",
            "Epoch 173/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7266 - acc: 0.7246 - val_loss: 1.3890 - val_acc: 0.4965\n",
            "Epoch 174/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7084 - acc: 0.7258 - val_loss: 1.3710 - val_acc: 0.5012\n",
            "Epoch 175/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6897 - acc: 0.7299 - val_loss: 1.3760 - val_acc: 0.4965\n",
            "Epoch 176/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6720 - acc: 0.7465 - val_loss: 1.3752 - val_acc: 0.5130\n",
            "Epoch 177/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6774 - acc: 0.7270 - val_loss: 1.3564 - val_acc: 0.4988\n",
            "Epoch 178/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6649 - acc: 0.7352 - val_loss: 1.3763 - val_acc: 0.5177\n",
            "Epoch 179/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6894 - acc: 0.7370 - val_loss: 1.4381 - val_acc: 0.5083\n",
            "Epoch 180/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6902 - acc: 0.7335 - val_loss: 1.4491 - val_acc: 0.4941\n",
            "Epoch 181/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6661 - acc: 0.7447 - val_loss: 1.4240 - val_acc: 0.5059\n",
            "Epoch 182/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7070 - acc: 0.7364 - val_loss: 1.4264 - val_acc: 0.4965\n",
            "Epoch 183/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6825 - acc: 0.7234 - val_loss: 1.3991 - val_acc: 0.4941\n",
            "Epoch 184/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6799 - acc: 0.7358 - val_loss: 1.4026 - val_acc: 0.5035\n",
            "Epoch 185/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6542 - acc: 0.7411 - val_loss: 1.3635 - val_acc: 0.5012\n",
            "Epoch 186/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7188 - acc: 0.7246 - val_loss: 1.3830 - val_acc: 0.5296\n",
            "Epoch 187/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6631 - acc: 0.7364 - val_loss: 1.3767 - val_acc: 0.5201\n",
            "Epoch 188/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6697 - acc: 0.7376 - val_loss: 1.4311 - val_acc: 0.5083\n",
            "Epoch 189/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6628 - acc: 0.7453 - val_loss: 1.4500 - val_acc: 0.5012\n",
            "Epoch 190/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6519 - acc: 0.7488 - val_loss: 1.3697 - val_acc: 0.5272\n",
            "Epoch 191/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6517 - acc: 0.7583 - val_loss: 1.4499 - val_acc: 0.5083\n",
            "Epoch 192/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6474 - acc: 0.7405 - val_loss: 1.4367 - val_acc: 0.4965\n",
            "Epoch 193/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6576 - acc: 0.7376 - val_loss: 1.4908 - val_acc: 0.4917\n",
            "Epoch 194/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6458 - acc: 0.7470 - val_loss: 1.4141 - val_acc: 0.5035\n",
            "Epoch 195/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6922 - acc: 0.7323 - val_loss: 1.4498 - val_acc: 0.5035\n",
            "Epoch 196/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6725 - acc: 0.7335 - val_loss: 1.4340 - val_acc: 0.5059\n",
            "Epoch 197/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6369 - acc: 0.7465 - val_loss: 1.4996 - val_acc: 0.4870\n",
            "Epoch 198/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6742 - acc: 0.7453 - val_loss: 1.4526 - val_acc: 0.4941\n",
            "Epoch 199/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6425 - acc: 0.7453 - val_loss: 1.4476 - val_acc: 0.4917\n",
            "Epoch 200/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6694 - acc: 0.7429 - val_loss: 1.4765 - val_acc: 0.4894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8oFJLLS6rIZ",
        "colab_type": "code",
        "outputId": "cb8fe228-c3d4-48e1-c0c3-4e6efffe8375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "_, accu = model.evaluate(X_test.reshape(-1,22,1000,1), Y_test)\n",
        "\n",
        "print(\"training accu is : {:2.2%}\".format(history.history['acc'][-1]))\n",
        "print(\"val accu is : {:2.2%}\".format(history.history['val_acc'][-1]))\n",
        "print(\"test accu is : {:2.2%}\".format(accu))\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model acc')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "443/443 [==============================] - 0s 1ms/step\n",
            "training accu is : 74.29%\n",
            "val accu is : 48.94%\n",
            "test accu is : 45.60%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0YAAAFRCAYAAABQaL0SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1008x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yCPF0HMACrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KEa5yjVRyrB",
        "colab_type": "text"
      },
      "source": [
        "### Trail-wise Shallow CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17yqEtgUCd-B",
        "colab_type": "text"
      },
      "source": [
        "#### without noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpDxqU3mCc3P",
        "colab_type": "code",
        "outputId": "18dbcee3-202b-4b87-9e92-426acc6156fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "dropout_rate = 0.6\n",
        "\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=40, kernel_size=(1,25), \n",
        "                strides=(1, 1), input_shape=(22,1000,1),\n",
        "                padding ='valid'))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(GaussianNoise(0.1))\n",
        "model.add(Conv2D(filters=40, kernel_size=(22,1), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid'))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(GaussianNoise(0.1))\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(Reshape((-1,40,1)))\n",
        "model.add(AveragePooling2D(pool_size=(75,1), strides = (15,1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 4, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7ff2e832c8d0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1455, in __del__\n",
            "    self._session._session, self._handle, status)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
            "    c_api.TF_GetCode(self.status.status))\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: No such callable handle: 2199956768\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 22, 976, 40)       1040      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 22, 976, 40)       160       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 976, 40)        35240     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1, 976, 40)        160       \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_1 (Spatial (None, 1, 976, 40)        0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 976, 40, 1)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 61, 40, 1)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2440)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 9764      \n",
            "=================================================================\n",
            "Total params: 46,364\n",
            "Trainable params: 46,204\n",
            "Non-trainable params: 160\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlp_jNKACkB7",
        "colab_type": "code",
        "outputId": "98284291-935d-4f6c-9aa9-b7bce7e1846a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6700
        }
      },
      "source": [
        "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
        "\n",
        "history = model.fit(X_train.reshape(2115,22,1000,1), Y_train, \n",
        "                    batch_size=256, epochs = 200, validation_split = 0.2,\n",
        "                    callbacks = [early_stop])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1692 samples, validate on 423 samples\n",
            "Epoch 1/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.4535 - acc: 0.2707 - val_loss: 1.3316 - val_acc: 0.3641\n",
            "Epoch 2/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.3315 - acc: 0.3617 - val_loss: 1.3287 - val_acc: 0.3735\n",
            "Epoch 3/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2904 - acc: 0.3859 - val_loss: 1.3103 - val_acc: 0.3877\n",
            "Epoch 4/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2677 - acc: 0.4226 - val_loss: 1.2759 - val_acc: 0.3853\n",
            "Epoch 5/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.2292 - acc: 0.4651 - val_loss: 1.2318 - val_acc: 0.4397\n",
            "Epoch 6/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1887 - acc: 0.4752 - val_loss: 1.2246 - val_acc: 0.4303\n",
            "Epoch 7/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1570 - acc: 0.5059 - val_loss: 1.2054 - val_acc: 0.4563\n",
            "Epoch 8/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1502 - acc: 0.5112 - val_loss: 1.1745 - val_acc: 0.4894\n",
            "Epoch 9/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1308 - acc: 0.5248 - val_loss: 1.1499 - val_acc: 0.4965\n",
            "Epoch 10/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.1043 - acc: 0.5225 - val_loss: 1.1359 - val_acc: 0.4870\n",
            "Epoch 11/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0905 - acc: 0.5384 - val_loss: 1.1500 - val_acc: 0.4775\n",
            "Epoch 12/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0570 - acc: 0.5538 - val_loss: 1.1524 - val_acc: 0.4870\n",
            "Epoch 13/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0518 - acc: 0.5491 - val_loss: 1.0992 - val_acc: 0.5059\n",
            "Epoch 14/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0457 - acc: 0.5709 - val_loss: 1.1201 - val_acc: 0.4941\n",
            "Epoch 15/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0425 - acc: 0.5881 - val_loss: 1.1024 - val_acc: 0.5012\n",
            "Epoch 16/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9991 - acc: 0.5916 - val_loss: 1.0936 - val_acc: 0.5083\n",
            "Epoch 17/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9943 - acc: 0.5928 - val_loss: 1.1010 - val_acc: 0.5012\n",
            "Epoch 18/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0082 - acc: 0.5952 - val_loss: 1.1048 - val_acc: 0.5083\n",
            "Epoch 19/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 1.0078 - acc: 0.5916 - val_loss: 1.0956 - val_acc: 0.5083\n",
            "Epoch 20/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9791 - acc: 0.6005 - val_loss: 1.0804 - val_acc: 0.5130\n",
            "Epoch 21/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9562 - acc: 0.6117 - val_loss: 1.0766 - val_acc: 0.4965\n",
            "Epoch 22/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9538 - acc: 0.6076 - val_loss: 1.1027 - val_acc: 0.5106\n",
            "Epoch 23/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9494 - acc: 0.6247 - val_loss: 1.1079 - val_acc: 0.5177\n",
            "Epoch 24/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9564 - acc: 0.6147 - val_loss: 1.0785 - val_acc: 0.5059\n",
            "Epoch 25/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9432 - acc: 0.6147 - val_loss: 1.0945 - val_acc: 0.5059\n",
            "Epoch 26/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9344 - acc: 0.6265 - val_loss: 1.1276 - val_acc: 0.5201\n",
            "Epoch 27/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9405 - acc: 0.6259 - val_loss: 1.1202 - val_acc: 0.5035\n",
            "Epoch 28/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9171 - acc: 0.6353 - val_loss: 1.0795 - val_acc: 0.5366\n",
            "Epoch 29/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9126 - acc: 0.6359 - val_loss: 1.0654 - val_acc: 0.5296\n",
            "Epoch 30/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9021 - acc: 0.6330 - val_loss: 1.0503 - val_acc: 0.5461\n",
            "Epoch 31/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9137 - acc: 0.6418 - val_loss: 1.0912 - val_acc: 0.5343\n",
            "Epoch 32/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9110 - acc: 0.6353 - val_loss: 1.0698 - val_acc: 0.5225\n",
            "Epoch 33/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9158 - acc: 0.6259 - val_loss: 1.0940 - val_acc: 0.5272\n",
            "Epoch 34/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9030 - acc: 0.6442 - val_loss: 1.0819 - val_acc: 0.5390\n",
            "Epoch 35/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8932 - acc: 0.6365 - val_loss: 1.0629 - val_acc: 0.5343\n",
            "Epoch 36/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8803 - acc: 0.6519 - val_loss: 1.1016 - val_acc: 0.5272\n",
            "Epoch 37/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9014 - acc: 0.6383 - val_loss: 1.0657 - val_acc: 0.5508\n",
            "Epoch 38/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.9111 - acc: 0.6353 - val_loss: 1.0624 - val_acc: 0.5248\n",
            "Epoch 39/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8746 - acc: 0.6531 - val_loss: 1.0953 - val_acc: 0.5248\n",
            "Epoch 40/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8830 - acc: 0.6424 - val_loss: 1.0801 - val_acc: 0.5248\n",
            "Epoch 41/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8525 - acc: 0.6661 - val_loss: 1.1134 - val_acc: 0.5366\n",
            "Epoch 42/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8707 - acc: 0.6513 - val_loss: 1.0481 - val_acc: 0.5508\n",
            "Epoch 43/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8609 - acc: 0.6525 - val_loss: 1.0517 - val_acc: 0.5532\n",
            "Epoch 44/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8600 - acc: 0.6661 - val_loss: 1.0575 - val_acc: 0.5390\n",
            "Epoch 45/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8492 - acc: 0.6619 - val_loss: 1.0762 - val_acc: 0.5366\n",
            "Epoch 46/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8365 - acc: 0.6874 - val_loss: 1.0408 - val_acc: 0.5296\n",
            "Epoch 47/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8530 - acc: 0.6507 - val_loss: 1.0549 - val_acc: 0.5485\n",
            "Epoch 48/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8453 - acc: 0.6690 - val_loss: 1.0717 - val_acc: 0.5177\n",
            "Epoch 49/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8452 - acc: 0.6643 - val_loss: 1.0546 - val_acc: 0.5225\n",
            "Epoch 50/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8188 - acc: 0.6761 - val_loss: 1.0610 - val_acc: 0.5296\n",
            "Epoch 51/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8210 - acc: 0.6921 - val_loss: 1.0553 - val_acc: 0.5461\n",
            "Epoch 52/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8269 - acc: 0.6814 - val_loss: 1.0799 - val_acc: 0.5201\n",
            "Epoch 53/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8431 - acc: 0.6566 - val_loss: 1.1029 - val_acc: 0.5296\n",
            "Epoch 54/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8348 - acc: 0.6844 - val_loss: 1.1001 - val_acc: 0.5366\n",
            "Epoch 55/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8102 - acc: 0.6726 - val_loss: 1.0850 - val_acc: 0.5508\n",
            "Epoch 56/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8289 - acc: 0.6637 - val_loss: 1.0764 - val_acc: 0.5485\n",
            "Epoch 57/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8268 - acc: 0.6797 - val_loss: 1.0446 - val_acc: 0.5461\n",
            "Epoch 58/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8133 - acc: 0.6755 - val_loss: 1.0824 - val_acc: 0.5390\n",
            "Epoch 59/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8008 - acc: 0.6974 - val_loss: 1.0775 - val_acc: 0.5414\n",
            "Epoch 60/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8080 - acc: 0.6720 - val_loss: 1.0656 - val_acc: 0.5532\n",
            "Epoch 61/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8103 - acc: 0.6962 - val_loss: 1.0963 - val_acc: 0.5343\n",
            "Epoch 62/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8128 - acc: 0.6738 - val_loss: 1.0703 - val_acc: 0.5272\n",
            "Epoch 63/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7958 - acc: 0.6850 - val_loss: 1.0691 - val_acc: 0.5414\n",
            "Epoch 64/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7888 - acc: 0.6897 - val_loss: 1.0702 - val_acc: 0.5556\n",
            "Epoch 65/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7895 - acc: 0.7009 - val_loss: 1.1323 - val_acc: 0.5366\n",
            "Epoch 66/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8231 - acc: 0.6720 - val_loss: 1.1060 - val_acc: 0.5461\n",
            "Epoch 67/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7971 - acc: 0.6874 - val_loss: 1.0887 - val_acc: 0.5319\n",
            "Epoch 68/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7949 - acc: 0.6797 - val_loss: 1.0531 - val_acc: 0.5697\n",
            "Epoch 69/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7896 - acc: 0.6980 - val_loss: 1.0604 - val_acc: 0.5390\n",
            "Epoch 70/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7816 - acc: 0.6986 - val_loss: 1.0565 - val_acc: 0.5556\n",
            "Epoch 71/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7801 - acc: 0.7009 - val_loss: 1.0942 - val_acc: 0.5437\n",
            "Epoch 72/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7733 - acc: 0.6933 - val_loss: 1.0818 - val_acc: 0.5319\n",
            "Epoch 73/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.8036 - acc: 0.6891 - val_loss: 1.0661 - val_acc: 0.5556\n",
            "Epoch 74/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7854 - acc: 0.6856 - val_loss: 1.0713 - val_acc: 0.5366\n",
            "Epoch 75/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7984 - acc: 0.6773 - val_loss: 1.0670 - val_acc: 0.5508\n",
            "Epoch 76/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7670 - acc: 0.6974 - val_loss: 1.0707 - val_acc: 0.5319\n",
            "Epoch 77/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7553 - acc: 0.7122 - val_loss: 1.0732 - val_acc: 0.5721\n",
            "Epoch 78/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7743 - acc: 0.6992 - val_loss: 1.0873 - val_acc: 0.5532\n",
            "Epoch 79/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7652 - acc: 0.7009 - val_loss: 1.0513 - val_acc: 0.5603\n",
            "Epoch 80/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7578 - acc: 0.7086 - val_loss: 1.0604 - val_acc: 0.5697\n",
            "Epoch 81/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7649 - acc: 0.7004 - val_loss: 1.0772 - val_acc: 0.5343\n",
            "Epoch 82/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7662 - acc: 0.6950 - val_loss: 1.1002 - val_acc: 0.5508\n",
            "Epoch 83/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7637 - acc: 0.7080 - val_loss: 1.0930 - val_acc: 0.5532\n",
            "Epoch 84/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7704 - acc: 0.6944 - val_loss: 1.1315 - val_acc: 0.5508\n",
            "Epoch 85/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7786 - acc: 0.6909 - val_loss: 1.0893 - val_acc: 0.5485\n",
            "Epoch 86/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7525 - acc: 0.7098 - val_loss: 1.0614 - val_acc: 0.5556\n",
            "Epoch 87/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7736 - acc: 0.6921 - val_loss: 1.0819 - val_acc: 0.5485\n",
            "Epoch 88/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7306 - acc: 0.7258 - val_loss: 1.0926 - val_acc: 0.5508\n",
            "Epoch 89/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7695 - acc: 0.7009 - val_loss: 1.0780 - val_acc: 0.5437\n",
            "Epoch 90/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7391 - acc: 0.7104 - val_loss: 1.0929 - val_acc: 0.5816\n",
            "Epoch 91/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7675 - acc: 0.6891 - val_loss: 1.1104 - val_acc: 0.5343\n",
            "Epoch 92/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7386 - acc: 0.7051 - val_loss: 1.0870 - val_acc: 0.5697\n",
            "Epoch 93/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7359 - acc: 0.7074 - val_loss: 1.0597 - val_acc: 0.5556\n",
            "Epoch 94/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7263 - acc: 0.7210 - val_loss: 1.1095 - val_acc: 0.5579\n",
            "Epoch 95/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7410 - acc: 0.6992 - val_loss: 1.0976 - val_acc: 0.5721\n",
            "Epoch 96/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7295 - acc: 0.7210 - val_loss: 1.0616 - val_acc: 0.5721\n",
            "Epoch 97/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7198 - acc: 0.7204 - val_loss: 1.0833 - val_acc: 0.5721\n",
            "Epoch 98/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7555 - acc: 0.7092 - val_loss: 1.0847 - val_acc: 0.5745\n",
            "Epoch 99/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7425 - acc: 0.7181 - val_loss: 1.0672 - val_acc: 0.5887\n",
            "Epoch 100/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7373 - acc: 0.7098 - val_loss: 1.0866 - val_acc: 0.5721\n",
            "Epoch 101/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7236 - acc: 0.7228 - val_loss: 1.0813 - val_acc: 0.5556\n",
            "Epoch 102/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7331 - acc: 0.7004 - val_loss: 1.0937 - val_acc: 0.5816\n",
            "Epoch 103/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7354 - acc: 0.7134 - val_loss: 1.0660 - val_acc: 0.5650\n",
            "Epoch 104/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7157 - acc: 0.7193 - val_loss: 1.0909 - val_acc: 0.5461\n",
            "Epoch 105/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7270 - acc: 0.7293 - val_loss: 1.0762 - val_acc: 0.5674\n",
            "Epoch 106/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7204 - acc: 0.7134 - val_loss: 1.0969 - val_acc: 0.5579\n",
            "Epoch 107/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7098 - acc: 0.7169 - val_loss: 1.0860 - val_acc: 0.5556\n",
            "Epoch 108/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7256 - acc: 0.7181 - val_loss: 1.0596 - val_acc: 0.5768\n",
            "Epoch 109/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7254 - acc: 0.7252 - val_loss: 1.0920 - val_acc: 0.5556\n",
            "Epoch 110/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7413 - acc: 0.7122 - val_loss: 1.0667 - val_acc: 0.5579\n",
            "Epoch 111/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7041 - acc: 0.7240 - val_loss: 1.0851 - val_acc: 0.5603\n",
            "Epoch 112/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6990 - acc: 0.7264 - val_loss: 1.0659 - val_acc: 0.5603\n",
            "Epoch 113/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7086 - acc: 0.7311 - val_loss: 1.0925 - val_acc: 0.5556\n",
            "Epoch 114/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6965 - acc: 0.7352 - val_loss: 1.1301 - val_acc: 0.5579\n",
            "Epoch 115/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7220 - acc: 0.7246 - val_loss: 1.0896 - val_acc: 0.5603\n",
            "Epoch 116/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7213 - acc: 0.7287 - val_loss: 1.0842 - val_acc: 0.5745\n",
            "Epoch 117/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6883 - acc: 0.7346 - val_loss: 1.0991 - val_acc: 0.5461\n",
            "Epoch 118/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6854 - acc: 0.7370 - val_loss: 1.0871 - val_acc: 0.5556\n",
            "Epoch 119/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6863 - acc: 0.7299 - val_loss: 1.0962 - val_acc: 0.5816\n",
            "Epoch 120/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7115 - acc: 0.7246 - val_loss: 1.0675 - val_acc: 0.5674\n",
            "Epoch 121/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7188 - acc: 0.7122 - val_loss: 1.0940 - val_acc: 0.5745\n",
            "Epoch 122/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6819 - acc: 0.7364 - val_loss: 1.0831 - val_acc: 0.5721\n",
            "Epoch 123/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6766 - acc: 0.7476 - val_loss: 1.0894 - val_acc: 0.5792\n",
            "Epoch 124/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.7000 - acc: 0.7293 - val_loss: 1.0718 - val_acc: 0.5745\n",
            "Epoch 125/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6740 - acc: 0.7340 - val_loss: 1.1001 - val_acc: 0.5674\n",
            "Epoch 126/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6932 - acc: 0.7163 - val_loss: 1.0795 - val_acc: 0.5887\n",
            "Epoch 127/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6771 - acc: 0.7323 - val_loss: 1.0875 - val_acc: 0.5745\n",
            "Epoch 128/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6597 - acc: 0.7441 - val_loss: 1.0860 - val_acc: 0.5981\n",
            "Epoch 129/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6807 - acc: 0.7281 - val_loss: 1.0837 - val_acc: 0.5887\n",
            "Epoch 130/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6844 - acc: 0.7323 - val_loss: 1.1456 - val_acc: 0.5887\n",
            "Epoch 131/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6925 - acc: 0.7340 - val_loss: 1.1255 - val_acc: 0.5697\n",
            "Epoch 132/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6838 - acc: 0.7234 - val_loss: 1.0850 - val_acc: 0.6005\n",
            "Epoch 133/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6760 - acc: 0.7358 - val_loss: 1.1125 - val_acc: 0.5816\n",
            "Epoch 134/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6851 - acc: 0.7246 - val_loss: 1.1076 - val_acc: 0.5839\n",
            "Epoch 135/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6739 - acc: 0.7459 - val_loss: 1.0827 - val_acc: 0.5839\n",
            "Epoch 136/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6628 - acc: 0.7482 - val_loss: 1.0935 - val_acc: 0.5697\n",
            "Epoch 137/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6693 - acc: 0.7388 - val_loss: 1.1357 - val_acc: 0.5721\n",
            "Epoch 138/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6645 - acc: 0.7476 - val_loss: 1.1090 - val_acc: 0.5816\n",
            "Epoch 139/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6639 - acc: 0.7352 - val_loss: 1.1013 - val_acc: 0.5745\n",
            "Epoch 140/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6807 - acc: 0.7453 - val_loss: 1.1068 - val_acc: 0.5887\n",
            "Epoch 141/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6489 - acc: 0.7535 - val_loss: 1.1386 - val_acc: 0.5650\n",
            "Epoch 142/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6714 - acc: 0.7382 - val_loss: 1.1115 - val_acc: 0.5697\n",
            "Epoch 143/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6591 - acc: 0.7559 - val_loss: 1.1107 - val_acc: 0.5863\n",
            "Epoch 144/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6523 - acc: 0.7465 - val_loss: 1.0880 - val_acc: 0.5863\n",
            "Epoch 145/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6776 - acc: 0.7311 - val_loss: 1.0925 - val_acc: 0.5792\n",
            "Epoch 146/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6576 - acc: 0.7388 - val_loss: 1.0975 - val_acc: 0.5603\n",
            "Epoch 147/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6560 - acc: 0.7488 - val_loss: 1.0849 - val_acc: 0.5792\n",
            "Epoch 148/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6335 - acc: 0.7494 - val_loss: 1.1392 - val_acc: 0.5697\n",
            "Epoch 149/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6627 - acc: 0.7535 - val_loss: 1.1025 - val_acc: 0.5697\n",
            "Epoch 150/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6636 - acc: 0.7400 - val_loss: 1.0909 - val_acc: 0.5721\n",
            "Epoch 151/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6495 - acc: 0.7465 - val_loss: 1.1016 - val_acc: 0.5745\n",
            "Epoch 152/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6459 - acc: 0.7476 - val_loss: 1.1215 - val_acc: 0.5626\n",
            "Epoch 153/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6682 - acc: 0.7382 - val_loss: 1.1123 - val_acc: 0.5721\n",
            "Epoch 154/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6706 - acc: 0.7340 - val_loss: 1.1144 - val_acc: 0.5697\n",
            "Epoch 155/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6617 - acc: 0.7453 - val_loss: 1.1160 - val_acc: 0.5721\n",
            "Epoch 156/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6333 - acc: 0.7547 - val_loss: 1.1280 - val_acc: 0.5579\n",
            "Epoch 157/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6583 - acc: 0.7447 - val_loss: 1.0829 - val_acc: 0.5981\n",
            "Epoch 158/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6417 - acc: 0.7476 - val_loss: 1.1014 - val_acc: 0.5839\n",
            "Epoch 159/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6412 - acc: 0.7654 - val_loss: 1.1024 - val_acc: 0.5626\n",
            "Epoch 160/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6398 - acc: 0.7530 - val_loss: 1.0951 - val_acc: 0.5745\n",
            "Epoch 161/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6343 - acc: 0.7470 - val_loss: 1.0992 - val_acc: 0.5721\n",
            "Epoch 162/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6497 - acc: 0.7447 - val_loss: 1.0917 - val_acc: 0.5650\n",
            "Epoch 163/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6309 - acc: 0.7482 - val_loss: 1.0855 - val_acc: 0.5839\n",
            "Epoch 164/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6137 - acc: 0.7736 - val_loss: 1.1143 - val_acc: 0.5650\n",
            "Epoch 165/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6293 - acc: 0.7530 - val_loss: 1.0916 - val_acc: 0.5721\n",
            "Epoch 166/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6543 - acc: 0.7470 - val_loss: 1.1191 - val_acc: 0.5816\n",
            "Epoch 167/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6283 - acc: 0.7660 - val_loss: 1.1331 - val_acc: 0.5792\n",
            "Epoch 168/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6247 - acc: 0.7618 - val_loss: 1.0941 - val_acc: 0.5887\n",
            "Epoch 169/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6358 - acc: 0.7559 - val_loss: 1.1048 - val_acc: 0.5839\n",
            "Epoch 170/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6350 - acc: 0.7612 - val_loss: 1.1025 - val_acc: 0.5792\n",
            "Epoch 171/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6246 - acc: 0.7512 - val_loss: 1.1329 - val_acc: 0.5697\n",
            "Epoch 172/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6386 - acc: 0.7577 - val_loss: 1.1209 - val_acc: 0.5745\n",
            "Epoch 173/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6214 - acc: 0.7695 - val_loss: 1.1037 - val_acc: 0.5745\n",
            "Epoch 174/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6249 - acc: 0.7660 - val_loss: 1.0952 - val_acc: 0.5768\n",
            "Epoch 175/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6123 - acc: 0.7843 - val_loss: 1.1562 - val_acc: 0.5697\n",
            "Epoch 176/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6138 - acc: 0.7654 - val_loss: 1.0923 - val_acc: 0.5745\n",
            "Epoch 177/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6299 - acc: 0.7565 - val_loss: 1.1032 - val_acc: 0.5674\n",
            "Epoch 178/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6501 - acc: 0.7435 - val_loss: 1.1537 - val_acc: 0.5697\n",
            "Epoch 179/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6472 - acc: 0.7547 - val_loss: 1.1198 - val_acc: 0.5768\n",
            "Epoch 180/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6280 - acc: 0.7518 - val_loss: 1.1144 - val_acc: 0.5697\n",
            "Epoch 181/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6312 - acc: 0.7506 - val_loss: 1.1011 - val_acc: 0.5603\n",
            "Epoch 182/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6343 - acc: 0.7465 - val_loss: 1.1100 - val_acc: 0.5745\n",
            "Epoch 183/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6100 - acc: 0.7618 - val_loss: 1.1104 - val_acc: 0.5910\n",
            "Epoch 184/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6238 - acc: 0.7695 - val_loss: 1.1128 - val_acc: 0.5697\n",
            "Epoch 185/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6043 - acc: 0.7725 - val_loss: 1.1054 - val_acc: 0.5768\n",
            "Epoch 186/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6114 - acc: 0.7595 - val_loss: 1.0875 - val_acc: 0.5887\n",
            "Epoch 187/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.5909 - acc: 0.7831 - val_loss: 1.1201 - val_acc: 0.5745\n",
            "Epoch 188/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6249 - acc: 0.7595 - val_loss: 1.0961 - val_acc: 0.5816\n",
            "Epoch 189/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6002 - acc: 0.7760 - val_loss: 1.0834 - val_acc: 0.5887\n",
            "Epoch 190/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.5947 - acc: 0.7730 - val_loss: 1.1005 - val_acc: 0.6005\n",
            "Epoch 191/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6194 - acc: 0.7589 - val_loss: 1.1293 - val_acc: 0.5650\n",
            "Epoch 192/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6246 - acc: 0.7530 - val_loss: 1.0970 - val_acc: 0.5863\n",
            "Epoch 193/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6112 - acc: 0.7695 - val_loss: 1.1113 - val_acc: 0.5816\n",
            "Epoch 194/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6112 - acc: 0.7695 - val_loss: 1.1287 - val_acc: 0.5697\n",
            "Epoch 195/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6235 - acc: 0.7577 - val_loss: 1.1011 - val_acc: 0.5745\n",
            "Epoch 196/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6134 - acc: 0.7618 - val_loss: 1.0999 - val_acc: 0.5745\n",
            "Epoch 197/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.5987 - acc: 0.7784 - val_loss: 1.1117 - val_acc: 0.5508\n",
            "Epoch 198/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.5980 - acc: 0.7784 - val_loss: 1.0955 - val_acc: 0.5768\n",
            "Epoch 199/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6200 - acc: 0.7524 - val_loss: 1.1122 - val_acc: 0.5863\n",
            "Epoch 200/200\n",
            "1692/1692 [==============================] - 2s 1ms/step - loss: 0.6042 - acc: 0.7612 - val_loss: 1.1008 - val_acc: 0.5697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1DHIMXfCpXD",
        "colab_type": "code",
        "outputId": "5ac38ca4-38a6-41cc-d0de-55680e3a3b1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "_, accu = model.evaluate(X_test.reshape(-1,22,1000,1), Y_test)\n",
        "\n",
        "print(\"training accu is : {:2.2%}\".format(history.history['acc'][-1]))\n",
        "print(\"val accu is : {:2.2%}\".format(history.history['val_acc'][-1]))\n",
        "print(\"test accu is : {:2.2%}\".format(accu))\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model acc')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "443/443 [==============================] - 0s 1ms/step\n",
            "training accu is : 77.48%\n",
            "val accu is : 56.74%\n",
            "test accu is : 59.59%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz4AAAFMCAYAAAAKvmk7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3v8HC6WzaLDMIQsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1008x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMDhfe4DCsMw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4hmrH_FR3A_",
        "colab_type": "code",
        "outputId": "b97c2192-51a4-4dd8-fd14-e36036f3eb23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        }
      },
      "source": [
        "dropout_rate = 0.6\n",
        "\n",
        "K.clear_session()\n",
        "model2 = Sequential()\n",
        "model2.add(Conv2D(filters=40, kernel_size=(1,25), \n",
        "                strides=(1, 1), input_shape=(22,1000,1),\n",
        "                padding ='valid'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(GaussianNoise(0.1))\n",
        "model2.add(Conv2D(filters=40, kernel_size=(22,1), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(GaussianNoise(0.1))\n",
        "model2.add(SpatialDropout2D(dropout_rate))\n",
        "model2.add(Reshape((-1,40,1)))\n",
        "model2.add(AveragePooling2D(pool_size=(75,1), strides = (15,1)))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(units = 4, activation='softmax'))\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "model2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 22, 976, 40)       1040      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 22, 976, 40)       160       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_1 (GaussianNo (None, 22, 976, 40)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 976, 40)        35240     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1, 976, 40)        160       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_2 (GaussianNo (None, 1, 976, 40)        0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_1 (Spatial (None, 1, 976, 40)        0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 976, 40, 1)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 61, 40, 1)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2440)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 9764      \n",
            "=================================================================\n",
            "Total params: 46,364\n",
            "Trainable params: 46,204\n",
            "Non-trainable params: 160\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-30LVMpSthh",
        "colab_type": "code",
        "outputId": "fb943daa-f9aa-4f5f-ed5d-f63a714f17a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6700
        }
      },
      "source": [
        "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
        "\n",
        "history2 = model2.fit(X_train.reshape(-1,22,1000,1), Y_train, \n",
        "                    batch_size=256, epochs = 200, validation_split = 0.2,\n",
        "                    callbacks = [early_stop])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1692 samples, validate on 423 samples\n",
            "Epoch 1/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.4681 - acc: 0.2937 - val_loss: 1.3651 - val_acc: 0.3404\n",
            "Epoch 2/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.3544 - acc: 0.3511 - val_loss: 1.3408 - val_acc: 0.3735\n",
            "Epoch 3/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.2999 - acc: 0.3765 - val_loss: 1.3382 - val_acc: 0.3783\n",
            "Epoch 4/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.2883 - acc: 0.4019 - val_loss: 1.2868 - val_acc: 0.3948\n",
            "Epoch 5/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.2371 - acc: 0.4498 - val_loss: 1.2466 - val_acc: 0.4657\n",
            "Epoch 6/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.1935 - acc: 0.4699 - val_loss: 1.2253 - val_acc: 0.4539\n",
            "Epoch 7/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.1554 - acc: 0.4846 - val_loss: 1.1600 - val_acc: 0.4894\n",
            "Epoch 8/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.1243 - acc: 0.5095 - val_loss: 1.2003 - val_acc: 0.4468\n",
            "Epoch 9/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.1233 - acc: 0.5100 - val_loss: 1.1675 - val_acc: 0.4941\n",
            "Epoch 10/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.0945 - acc: 0.5355 - val_loss: 1.1261 - val_acc: 0.4988\n",
            "Epoch 11/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.0976 - acc: 0.5384 - val_loss: 1.1380 - val_acc: 0.5035\n",
            "Epoch 12/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.0803 - acc: 0.5556 - val_loss: 1.1174 - val_acc: 0.4775\n",
            "Epoch 13/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.0219 - acc: 0.5745 - val_loss: 1.0999 - val_acc: 0.4917\n",
            "Epoch 14/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.0198 - acc: 0.5898 - val_loss: 1.0909 - val_acc: 0.4965\n",
            "Epoch 15/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.0206 - acc: 0.5863 - val_loss: 1.1349 - val_acc: 0.4870\n",
            "Epoch 16/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 1.0183 - acc: 0.5804 - val_loss: 1.0879 - val_acc: 0.5106\n",
            "Epoch 17/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.9859 - acc: 0.6052 - val_loss: 1.1126 - val_acc: 0.4799\n",
            "Epoch 18/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.9867 - acc: 0.6040 - val_loss: 1.0921 - val_acc: 0.5083\n",
            "Epoch 19/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.9887 - acc: 0.5975 - val_loss: 1.0889 - val_acc: 0.5225\n",
            "Epoch 20/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.9796 - acc: 0.5987 - val_loss: 1.0864 - val_acc: 0.4965\n",
            "Epoch 21/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.9704 - acc: 0.6147 - val_loss: 1.1193 - val_acc: 0.4988\n",
            "Epoch 22/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.9675 - acc: 0.6070 - val_loss: 1.0952 - val_acc: 0.5059\n",
            "Epoch 23/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.9590 - acc: 0.6117 - val_loss: 1.1365 - val_acc: 0.4988\n",
            "Epoch 24/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.9514 - acc: 0.6241 - val_loss: 1.0815 - val_acc: 0.5248\n",
            "Epoch 25/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.9358 - acc: 0.6288 - val_loss: 1.0512 - val_acc: 0.5154\n",
            "Epoch 26/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.9225 - acc: 0.6241 - val_loss: 1.0793 - val_acc: 0.5225\n",
            "Epoch 27/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.9243 - acc: 0.6424 - val_loss: 1.0653 - val_acc: 0.5272\n",
            "Epoch 28/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.9141 - acc: 0.6152 - val_loss: 1.0643 - val_acc: 0.5177\n",
            "Epoch 29/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.9187 - acc: 0.6306 - val_loss: 1.0516 - val_acc: 0.5272\n",
            "Epoch 30/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8940 - acc: 0.6531 - val_loss: 1.0792 - val_acc: 0.5225\n",
            "Epoch 31/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.9236 - acc: 0.6288 - val_loss: 1.0846 - val_acc: 0.5248\n",
            "Epoch 32/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8773 - acc: 0.6495 - val_loss: 1.0636 - val_acc: 0.5296\n",
            "Epoch 33/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8844 - acc: 0.6466 - val_loss: 1.1079 - val_acc: 0.5201\n",
            "Epoch 34/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8996 - acc: 0.6324 - val_loss: 1.0731 - val_acc: 0.5390\n",
            "Epoch 35/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8982 - acc: 0.6365 - val_loss: 1.0583 - val_acc: 0.5508\n",
            "Epoch 36/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8936 - acc: 0.6395 - val_loss: 1.0854 - val_acc: 0.5343\n",
            "Epoch 37/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8955 - acc: 0.6501 - val_loss: 1.0881 - val_acc: 0.5366\n",
            "Epoch 38/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8901 - acc: 0.6566 - val_loss: 1.0790 - val_acc: 0.5319\n",
            "Epoch 39/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8885 - acc: 0.6424 - val_loss: 1.0417 - val_acc: 0.5414\n",
            "Epoch 40/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8612 - acc: 0.6690 - val_loss: 1.0540 - val_acc: 0.5319\n",
            "Epoch 41/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8808 - acc: 0.6596 - val_loss: 1.0644 - val_acc: 0.5390\n",
            "Epoch 42/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8800 - acc: 0.6637 - val_loss: 1.0622 - val_acc: 0.5319\n",
            "Epoch 43/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8733 - acc: 0.6584 - val_loss: 1.1099 - val_acc: 0.5390\n",
            "Epoch 44/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8559 - acc: 0.6684 - val_loss: 1.0675 - val_acc: 0.5366\n",
            "Epoch 45/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8459 - acc: 0.6608 - val_loss: 1.0712 - val_acc: 0.5272\n",
            "Epoch 46/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8342 - acc: 0.6702 - val_loss: 1.0507 - val_acc: 0.5414\n",
            "Epoch 47/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8489 - acc: 0.6773 - val_loss: 1.0541 - val_acc: 0.5319\n",
            "Epoch 48/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8381 - acc: 0.6684 - val_loss: 1.1084 - val_acc: 0.5248\n",
            "Epoch 49/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8566 - acc: 0.6637 - val_loss: 1.0900 - val_acc: 0.5366\n",
            "Epoch 50/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8640 - acc: 0.6584 - val_loss: 1.0503 - val_acc: 0.5556\n",
            "Epoch 51/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8342 - acc: 0.6767 - val_loss: 1.0736 - val_acc: 0.5390\n",
            "Epoch 52/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8348 - acc: 0.6696 - val_loss: 1.1264 - val_acc: 0.5130\n",
            "Epoch 53/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8348 - acc: 0.6625 - val_loss: 1.0867 - val_acc: 0.5414\n",
            "Epoch 54/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8377 - acc: 0.6684 - val_loss: 1.0852 - val_acc: 0.5248\n",
            "Epoch 55/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8178 - acc: 0.6814 - val_loss: 1.0811 - val_acc: 0.5225\n",
            "Epoch 56/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8148 - acc: 0.6743 - val_loss: 1.0906 - val_acc: 0.5296\n",
            "Epoch 57/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8374 - acc: 0.6649 - val_loss: 1.0769 - val_acc: 0.5485\n",
            "Epoch 58/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8202 - acc: 0.6738 - val_loss: 1.0575 - val_acc: 0.5556\n",
            "Epoch 59/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8138 - acc: 0.6927 - val_loss: 1.0824 - val_acc: 0.5390\n",
            "Epoch 60/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8194 - acc: 0.6891 - val_loss: 1.0759 - val_acc: 0.5556\n",
            "Epoch 61/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8019 - acc: 0.6921 - val_loss: 1.0595 - val_acc: 0.5556\n",
            "Epoch 62/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8003 - acc: 0.6944 - val_loss: 1.0795 - val_acc: 0.5414\n",
            "Epoch 63/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7942 - acc: 0.6838 - val_loss: 1.0699 - val_acc: 0.5508\n",
            "Epoch 64/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.8029 - acc: 0.6803 - val_loss: 1.0664 - val_acc: 0.5532\n",
            "Epoch 65/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7819 - acc: 0.7009 - val_loss: 1.0590 - val_acc: 0.5485\n",
            "Epoch 66/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7945 - acc: 0.6933 - val_loss: 1.0859 - val_acc: 0.5319\n",
            "Epoch 67/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7974 - acc: 0.6803 - val_loss: 1.0877 - val_acc: 0.5414\n",
            "Epoch 68/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7939 - acc: 0.6891 - val_loss: 1.0627 - val_acc: 0.5603\n",
            "Epoch 69/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7855 - acc: 0.6939 - val_loss: 1.0797 - val_acc: 0.5437\n",
            "Epoch 70/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7826 - acc: 0.7009 - val_loss: 1.0741 - val_acc: 0.5485\n",
            "Epoch 71/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7856 - acc: 0.6992 - val_loss: 1.0637 - val_acc: 0.5556\n",
            "Epoch 72/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7911 - acc: 0.6927 - val_loss: 1.0633 - val_acc: 0.5366\n",
            "Epoch 73/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7837 - acc: 0.6749 - val_loss: 1.0978 - val_acc: 0.5437\n",
            "Epoch 74/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7862 - acc: 0.7021 - val_loss: 1.0642 - val_acc: 0.5626\n",
            "Epoch 75/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7752 - acc: 0.6868 - val_loss: 1.0664 - val_acc: 0.5650\n",
            "Epoch 76/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7740 - acc: 0.7009 - val_loss: 1.0996 - val_acc: 0.5437\n",
            "Epoch 77/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7788 - acc: 0.6950 - val_loss: 1.0602 - val_acc: 0.5532\n",
            "Epoch 78/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7412 - acc: 0.7122 - val_loss: 1.0817 - val_acc: 0.5674\n",
            "Epoch 79/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7585 - acc: 0.7128 - val_loss: 1.0776 - val_acc: 0.5626\n",
            "Epoch 80/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7414 - acc: 0.7092 - val_loss: 1.0637 - val_acc: 0.5508\n",
            "Epoch 81/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7543 - acc: 0.6950 - val_loss: 1.0571 - val_acc: 0.5697\n",
            "Epoch 82/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7375 - acc: 0.7163 - val_loss: 1.0970 - val_acc: 0.5556\n",
            "Epoch 83/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7611 - acc: 0.7015 - val_loss: 1.0645 - val_acc: 0.5626\n",
            "Epoch 84/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7534 - acc: 0.7128 - val_loss: 1.0878 - val_acc: 0.5485\n",
            "Epoch 85/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7445 - acc: 0.7080 - val_loss: 1.1067 - val_acc: 0.5390\n",
            "Epoch 86/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7497 - acc: 0.7015 - val_loss: 1.0862 - val_acc: 0.5674\n",
            "Epoch 87/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7522 - acc: 0.7104 - val_loss: 1.0702 - val_acc: 0.5508\n",
            "Epoch 88/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7347 - acc: 0.7228 - val_loss: 1.1116 - val_acc: 0.5366\n",
            "Epoch 89/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7560 - acc: 0.6992 - val_loss: 1.0925 - val_acc: 0.5437\n",
            "Epoch 90/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7369 - acc: 0.7287 - val_loss: 1.0644 - val_acc: 0.5697\n",
            "Epoch 91/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7372 - acc: 0.7116 - val_loss: 1.0921 - val_acc: 0.5603\n",
            "Epoch 92/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7386 - acc: 0.7187 - val_loss: 1.0933 - val_acc: 0.5603\n",
            "Epoch 93/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7438 - acc: 0.7116 - val_loss: 1.1086 - val_acc: 0.5343\n",
            "Epoch 94/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7349 - acc: 0.7175 - val_loss: 1.1018 - val_acc: 0.5508\n",
            "Epoch 95/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7598 - acc: 0.7116 - val_loss: 1.0849 - val_acc: 0.5721\n",
            "Epoch 96/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7361 - acc: 0.7163 - val_loss: 1.0775 - val_acc: 0.5745\n",
            "Epoch 97/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7345 - acc: 0.7110 - val_loss: 1.0633 - val_acc: 0.5674\n",
            "Epoch 98/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7218 - acc: 0.7252 - val_loss: 1.0937 - val_acc: 0.5556\n",
            "Epoch 99/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7254 - acc: 0.7275 - val_loss: 1.0688 - val_acc: 0.5792\n",
            "Epoch 100/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7305 - acc: 0.7157 - val_loss: 1.0611 - val_acc: 0.5816\n",
            "Epoch 101/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7307 - acc: 0.7181 - val_loss: 1.0890 - val_acc: 0.5721\n",
            "Epoch 102/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7237 - acc: 0.7134 - val_loss: 1.1093 - val_acc: 0.5626\n",
            "Epoch 103/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7291 - acc: 0.7222 - val_loss: 1.0704 - val_acc: 0.5697\n",
            "Epoch 104/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7208 - acc: 0.7234 - val_loss: 1.0829 - val_acc: 0.5745\n",
            "Epoch 105/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7433 - acc: 0.7104 - val_loss: 1.1252 - val_acc: 0.5556\n",
            "Epoch 106/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7351 - acc: 0.7092 - val_loss: 1.0989 - val_acc: 0.5626\n",
            "Epoch 107/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7447 - acc: 0.7110 - val_loss: 1.0737 - val_acc: 0.5745\n",
            "Epoch 108/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7292 - acc: 0.7128 - val_loss: 1.0911 - val_acc: 0.5603\n",
            "Epoch 109/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7164 - acc: 0.7329 - val_loss: 1.1121 - val_acc: 0.5579\n",
            "Epoch 110/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7207 - acc: 0.7258 - val_loss: 1.0783 - val_acc: 0.5674\n",
            "Epoch 111/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7109 - acc: 0.7204 - val_loss: 1.0769 - val_acc: 0.5697\n",
            "Epoch 112/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7035 - acc: 0.7382 - val_loss: 1.0941 - val_acc: 0.5556\n",
            "Epoch 113/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7083 - acc: 0.7193 - val_loss: 1.0943 - val_acc: 0.5532\n",
            "Epoch 114/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6990 - acc: 0.7311 - val_loss: 1.0704 - val_acc: 0.5768\n",
            "Epoch 115/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6994 - acc: 0.7258 - val_loss: 1.0936 - val_acc: 0.5721\n",
            "Epoch 116/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7028 - acc: 0.7287 - val_loss: 1.0878 - val_acc: 0.5697\n",
            "Epoch 117/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7261 - acc: 0.7335 - val_loss: 1.0790 - val_acc: 0.5816\n",
            "Epoch 118/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7177 - acc: 0.7187 - val_loss: 1.0688 - val_acc: 0.5721\n",
            "Epoch 119/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.7100 - acc: 0.7199 - val_loss: 1.0870 - val_acc: 0.5485\n",
            "Epoch 120/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6978 - acc: 0.7228 - val_loss: 1.0989 - val_acc: 0.5603\n",
            "Epoch 121/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6882 - acc: 0.7352 - val_loss: 1.1011 - val_acc: 0.5792\n",
            "Epoch 122/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6961 - acc: 0.7305 - val_loss: 1.0899 - val_acc: 0.5697\n",
            "Epoch 123/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6952 - acc: 0.7275 - val_loss: 1.0756 - val_acc: 0.5674\n",
            "Epoch 124/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6900 - acc: 0.7358 - val_loss: 1.0758 - val_acc: 0.5650\n",
            "Epoch 125/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6764 - acc: 0.7323 - val_loss: 1.0794 - val_acc: 0.5745\n",
            "Epoch 126/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6849 - acc: 0.7329 - val_loss: 1.0928 - val_acc: 0.5792\n",
            "Epoch 127/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6864 - acc: 0.7453 - val_loss: 1.0903 - val_acc: 0.5674\n",
            "Epoch 128/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6789 - acc: 0.7405 - val_loss: 1.0575 - val_acc: 0.5626\n",
            "Epoch 129/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6834 - acc: 0.7382 - val_loss: 1.0553 - val_acc: 0.5650\n",
            "Epoch 130/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6510 - acc: 0.7518 - val_loss: 1.0859 - val_acc: 0.5626\n",
            "Epoch 131/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6935 - acc: 0.7335 - val_loss: 1.0829 - val_acc: 0.5721\n",
            "Epoch 132/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6891 - acc: 0.7441 - val_loss: 1.0782 - val_acc: 0.5626\n",
            "Epoch 133/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6638 - acc: 0.7394 - val_loss: 1.0891 - val_acc: 0.5745\n",
            "Epoch 134/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6625 - acc: 0.7476 - val_loss: 1.0815 - val_acc: 0.5768\n",
            "Epoch 135/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6834 - acc: 0.7305 - val_loss: 1.0814 - val_acc: 0.5863\n",
            "Epoch 136/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6549 - acc: 0.7494 - val_loss: 1.0806 - val_acc: 0.5603\n",
            "Epoch 137/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6860 - acc: 0.7293 - val_loss: 1.0965 - val_acc: 0.5745\n",
            "Epoch 138/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6887 - acc: 0.7281 - val_loss: 1.0907 - val_acc: 0.5768\n",
            "Epoch 139/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6540 - acc: 0.7482 - val_loss: 1.0953 - val_acc: 0.5626\n",
            "Epoch 140/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6552 - acc: 0.7329 - val_loss: 1.1080 - val_acc: 0.5697\n",
            "Epoch 141/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6665 - acc: 0.7429 - val_loss: 1.0866 - val_acc: 0.5674\n",
            "Epoch 142/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6875 - acc: 0.7394 - val_loss: 1.1284 - val_acc: 0.5674\n",
            "Epoch 143/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6647 - acc: 0.7417 - val_loss: 1.0971 - val_acc: 0.5910\n",
            "Epoch 144/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6540 - acc: 0.7518 - val_loss: 1.0991 - val_acc: 0.5674\n",
            "Epoch 145/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6588 - acc: 0.7447 - val_loss: 1.0797 - val_acc: 0.5674\n",
            "Epoch 146/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6532 - acc: 0.7559 - val_loss: 1.1011 - val_acc: 0.5721\n",
            "Epoch 147/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6518 - acc: 0.7595 - val_loss: 1.0868 - val_acc: 0.5863\n",
            "Epoch 148/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6435 - acc: 0.7589 - val_loss: 1.0892 - val_acc: 0.5839\n",
            "Epoch 149/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6709 - acc: 0.7405 - val_loss: 1.0924 - val_acc: 0.5863\n",
            "Epoch 150/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6500 - acc: 0.7459 - val_loss: 1.1034 - val_acc: 0.5721\n",
            "Epoch 151/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6587 - acc: 0.7441 - val_loss: 1.0913 - val_acc: 0.5745\n",
            "Epoch 152/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6632 - acc: 0.7476 - val_loss: 1.0792 - val_acc: 0.5745\n",
            "Epoch 153/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6603 - acc: 0.7459 - val_loss: 1.0899 - val_acc: 0.5674\n",
            "Epoch 154/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6517 - acc: 0.7465 - val_loss: 1.0949 - val_acc: 0.5721\n",
            "Epoch 155/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6453 - acc: 0.7500 - val_loss: 1.0828 - val_acc: 0.5697\n",
            "Epoch 156/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6691 - acc: 0.7388 - val_loss: 1.0861 - val_acc: 0.5697\n",
            "Epoch 157/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6494 - acc: 0.7618 - val_loss: 1.1166 - val_acc: 0.5603\n",
            "Epoch 158/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6614 - acc: 0.7317 - val_loss: 1.1156 - val_acc: 0.5579\n",
            "Epoch 159/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6461 - acc: 0.7518 - val_loss: 1.0876 - val_acc: 0.5697\n",
            "Epoch 160/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6337 - acc: 0.7595 - val_loss: 1.0958 - val_acc: 0.5745\n",
            "Epoch 161/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6265 - acc: 0.7642 - val_loss: 1.1299 - val_acc: 0.5745\n",
            "Epoch 162/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6571 - acc: 0.7447 - val_loss: 1.0809 - val_acc: 0.5816\n",
            "Epoch 163/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6516 - acc: 0.7494 - val_loss: 1.0877 - val_acc: 0.5697\n",
            "Epoch 164/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6348 - acc: 0.7482 - val_loss: 1.1149 - val_acc: 0.5626\n",
            "Epoch 165/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6458 - acc: 0.7435 - val_loss: 1.1201 - val_acc: 0.5556\n",
            "Epoch 166/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6313 - acc: 0.7535 - val_loss: 1.1037 - val_acc: 0.5603\n",
            "Epoch 167/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6448 - acc: 0.7547 - val_loss: 1.0974 - val_acc: 0.5934\n",
            "Epoch 168/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6682 - acc: 0.7476 - val_loss: 1.0951 - val_acc: 0.5768\n",
            "Epoch 169/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6473 - acc: 0.7642 - val_loss: 1.1068 - val_acc: 0.5745\n",
            "Epoch 170/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6398 - acc: 0.7465 - val_loss: 1.1006 - val_acc: 0.5697\n",
            "Epoch 171/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6140 - acc: 0.7500 - val_loss: 1.0884 - val_acc: 0.5674\n",
            "Epoch 172/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6393 - acc: 0.7559 - val_loss: 1.1261 - val_acc: 0.5745\n",
            "Epoch 173/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6600 - acc: 0.7299 - val_loss: 1.0963 - val_acc: 0.5697\n",
            "Epoch 174/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6426 - acc: 0.7541 - val_loss: 1.1201 - val_acc: 0.5532\n",
            "Epoch 175/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6229 - acc: 0.7595 - val_loss: 1.0930 - val_acc: 0.5650\n",
            "Epoch 176/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6300 - acc: 0.7476 - val_loss: 1.1017 - val_acc: 0.5721\n",
            "Epoch 177/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6227 - acc: 0.7636 - val_loss: 1.1293 - val_acc: 0.5579\n",
            "Epoch 178/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6441 - acc: 0.7524 - val_loss: 1.1137 - val_acc: 0.5768\n",
            "Epoch 179/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6386 - acc: 0.7630 - val_loss: 1.1052 - val_acc: 0.5745\n",
            "Epoch 180/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6346 - acc: 0.7547 - val_loss: 1.1000 - val_acc: 0.5626\n",
            "Epoch 181/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6313 - acc: 0.7589 - val_loss: 1.1143 - val_acc: 0.5626\n",
            "Epoch 182/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6101 - acc: 0.7606 - val_loss: 1.0952 - val_acc: 0.5626\n",
            "Epoch 183/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6234 - acc: 0.7630 - val_loss: 1.1135 - val_acc: 0.5626\n",
            "Epoch 184/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6080 - acc: 0.7671 - val_loss: 1.0904 - val_acc: 0.5697\n",
            "Epoch 185/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6021 - acc: 0.7707 - val_loss: 1.0767 - val_acc: 0.5745\n",
            "Epoch 186/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6059 - acc: 0.7677 - val_loss: 1.0997 - val_acc: 0.5745\n",
            "Epoch 187/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6126 - acc: 0.7600 - val_loss: 1.0886 - val_acc: 0.5792\n",
            "Epoch 188/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6078 - acc: 0.7677 - val_loss: 1.1371 - val_acc: 0.5745\n",
            "Epoch 189/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6244 - acc: 0.7636 - val_loss: 1.1346 - val_acc: 0.5556\n",
            "Epoch 190/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6100 - acc: 0.7648 - val_loss: 1.1023 - val_acc: 0.5626\n",
            "Epoch 191/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6326 - acc: 0.7506 - val_loss: 1.1068 - val_acc: 0.5957\n",
            "Epoch 192/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6304 - acc: 0.7589 - val_loss: 1.0639 - val_acc: 0.5957\n",
            "Epoch 193/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6311 - acc: 0.7654 - val_loss: 1.1087 - val_acc: 0.5721\n",
            "Epoch 194/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6022 - acc: 0.7665 - val_loss: 1.1122 - val_acc: 0.5816\n",
            "Epoch 195/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6180 - acc: 0.7577 - val_loss: 1.1019 - val_acc: 0.5650\n",
            "Epoch 196/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.5932 - acc: 0.7671 - val_loss: 1.1197 - val_acc: 0.5839\n",
            "Epoch 197/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.5986 - acc: 0.7577 - val_loss: 1.1150 - val_acc: 0.5816\n",
            "Epoch 198/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.5935 - acc: 0.7660 - val_loss: 1.0962 - val_acc: 0.5721\n",
            "Epoch 199/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6160 - acc: 0.7547 - val_loss: 1.1147 - val_acc: 0.5792\n",
            "Epoch 200/200\n",
            "1692/1692 [==============================] - 3s 2ms/step - loss: 0.6156 - acc: 0.7636 - val_loss: 1.1357 - val_acc: 0.5768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSEfdJJ7TLjk",
        "colab_type": "code",
        "outputId": "0b9adc75-f7c6-4452-8370-b87566d0b149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "_, accu = model.evaluate(X_test.reshape(-1,22,1000,1), Y_test)\n",
        "\n",
        "print(\"training accu is : {:2.2%}\".format(history2.history['acc'][-1]))\n",
        "print(\"val accu is : {:2.2%}\".format(history2.history['val_acc'][-1]))\n",
        "print(\"test accu is : {:2.2%}\".format(accu))\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.plot(history2.history['acc'])\n",
        "plt.plot(history2.history['val_acc'])\n",
        "plt.title('model acc')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val','train_{noise}', 'val_{noise}'], loc='upper left')\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(history2.history['loss'])\n",
        "plt.plot(history2.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val','train_{noise}', 'val_{noise}'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "443/443 [==============================] - 0s 729us/step\n",
            "training accu is : 76.36%\n",
            "val accu is : 57.68%\n",
            "test accu is : 57.34%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz4AAAFMCAYAAAAKvmk7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1008x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruUfQY5K3MGM",
        "colab_type": "text"
      },
      "source": [
        "### Data augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm-YSxTj3LOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataargment(X_train, Y_train, window, stride, person_idx):\n",
        "  num_trail, num_eletrode, num_bin = X_train.shape\n",
        "  num_class = Y_train.shape[1]\n",
        "  N = (num_bin - window)//stride+1\n",
        "  X_train_argment = np.empty((num_trail*N, num_eletrode, window))\n",
        "  Y_train_argment = np.empty((num_trail*N, num_class))\n",
        "  person_idx_argment = np.empty(num_trail*N)\n",
        "  for idx_trail in range(num_trail):\n",
        "    for n in range(N):\n",
        "      X_train_argment[idx_trail*N+n,:,:] = X_train[idx_trail,:,n*stride:n*stride+window]\n",
        "      Y_train_argment[idx_trail*N+n] = Y_train[idx_trail]\n",
        "      person_idx_argment[idx_trail*N+n] = person_idx[idx_trail]\n",
        "  return X_train_argment, Y_train_argment, person_idx_argment\n",
        "\n",
        "def findaccu(y_pred, y_label, window, stride):\n",
        "  num_trial, num_class = y_label.shape\n",
        "  N = (1000 - window)//stride+1\n",
        "  y_label = np.argmax(y_label, axis = 1)\n",
        "  \n",
        "  diff = (y_pred - y_label)\n",
        "  diff = diff.reshape(-1,N)\n",
        "  result = np.sum(np.count_nonzero(diff, axis = 1) <= np.floor(N//2)) /diff.shape[0]\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "window = 750\n",
        "stride = 25\n",
        "X_train_argment, Y_train_argment, person_train_idx_argment = dataargment(X_train, Y_train, window, stride,person_train_valid)\n",
        "X_test_argment, Y_test_argment, person_test_idx_argment = dataargment(X_test, Y_test, window, stride,person_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmnLl6Nt7A7I",
        "colab_type": "text"
      },
      "source": [
        "## Deep CNN with data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtyGau53OYJO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c27e2943-89cd-4a15-be31-61ac8404e86f"
      },
      "source": [
        "dropout_rate = 0.5\n",
        "l2_reg = 0.001\n",
        "\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=25, kernel_size=(1,10), \n",
        "                 strides=(1, 1), input_shape=(22,window,1),\n",
        "                 activation = 'linear',\n",
        "                 kernel_regularizer = regularizers.l2(l2_reg),\n",
        "                 padding ='valid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(GaussianNoise(0.1))\n",
        "model.add(Conv2D(filters=25, kernel_size=(22,1), \n",
        "                strides=(1, 1),\n",
        "                kernel_regularizer = regularizers.l2(l2_reg),\n",
        "                padding ='valid')\n",
        "         )\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(GaussianNoise(0.1))\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(MaxPool2D(pool_size=(1, 3), strides=(1, 3)))\n",
        "model.add(Reshape((-1,25,1)))\n",
        "model.add(Conv2D(filters=50, kernel_size=(10,25), \n",
        "                strides=(1, 1),\n",
        "                kernel_regularizer = regularizers.l2(l2_reg),                 \n",
        "                padding ='valid'))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(Reshape((-1,50,1)))\n",
        "model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "model.add(Conv2D(filters=100, kernel_size=(10,50), \n",
        "                strides=(1, 1),\n",
        "                kernel_regularizer = regularizers.l2(l2_reg),  \n",
        "                padding ='valid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(Reshape((-1,100,1)))\n",
        "model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "model.add(Conv2D(filters=200, kernel_size=(3,100), \n",
        "                strides=(1, 1),\n",
        "                kernel_regularizer = regularizers.l2(l2_reg),\n",
        "                padding ='valid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(Reshape((-1,200,1)))\n",
        "model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "# model.add(Flatten())\n",
        "model.add(Conv2D(filters = 400, kernel_size = (7,200),\n",
        "                strides = (1,1),\n",
        "                kernel_regularizer = regularizers.l2(l2_reg),\n",
        "                 padding = 'valid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('elu'))\n",
        "model.add(Dropout(dropout_rate))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 22, 741, 25)       275       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 22, 741, 25)       100       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_1 (GaussianNo (None, 22, 741, 25)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 741, 25)        13775     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1, 741, 25)        100       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1, 741, 25)        0         \n",
            "_________________________________________________________________\n",
            "gaussian_noise_2 (GaussianNo (None, 1, 741, 25)        0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_1 (Spatial (None, 1, 741, 25)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1, 247, 25)        0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 247, 25, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 238, 1, 50)        12550     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 238, 1, 50)        200       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 238, 1, 50)        0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_2 (Spatial (None, 238, 1, 50)        0         \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 238, 50, 1)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 79, 50, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 70, 1, 100)        50100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 70, 1, 100)        400       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 70, 1, 100)        0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_3 (Spatial (None, 70, 1, 100)        0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 70, 100, 1)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 23, 100, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 21, 1, 200)        60200     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 21, 1, 200)        800       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 21, 1, 200)        0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_4 (Spatial (None, 21, 1, 200)        0         \n",
            "_________________________________________________________________\n",
            "reshape_4 (Reshape)          (None, 21, 200, 1)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 7, 200, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 1, 1, 400)         560400    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1, 1, 400)         1600      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 1, 1, 400)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 1, 400)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 1604      \n",
            "=================================================================\n",
            "Total params: 702,104\n",
            "Trainable params: 700,504\n",
            "Non-trainable params: 1,600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkMdgUskgNqO",
        "colab_type": "code",
        "outputId": "94af3983-3621-4544-f3d1-d0e5598ac5d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "window = 750\n",
        "stride = 25\n",
        "X_train_argment, Y_train_argment, person_train_idx_argment = dataargment(X_train, Y_train, window, stride,person_train_valid)\n",
        "X_test_argment, Y_test_argment, person_test_idx_argment = dataargment(X_test, Y_test, window, stride,person_test)\n",
        "\n",
        "\n",
        "filepath=\"weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "early_stop = EarlyStopping(monitor='loss', patience=15, verbose=1)\n",
        "\n",
        "history = model.fit(X_train_argment.reshape(-1,22,window,1), Y_train_argment, \n",
        "                    batch_size=256, epochs = 500, validation_split = 0.2,\n",
        "                    callbacks = callbacks_list,\n",
        "                    verbose = 1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 18612 samples, validate on 4653 samples\n",
            "Epoch 1/500\n",
            "18612/18612 [==============================] - 24s 1ms/step - loss: 2.0376 - acc: 0.2562 - val_loss: 1.4760 - val_acc: 0.2734\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.27337, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 2/500\n",
            "18612/18612 [==============================] - 19s 997us/step - loss: 1.6025 - acc: 0.2666 - val_loss: 1.4530 - val_acc: 0.3026\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.27337 to 0.30260, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 3/500\n",
            "18612/18612 [==============================] - 19s 997us/step - loss: 1.5427 - acc: 0.2835 - val_loss: 1.4260 - val_acc: 0.3114\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.30260 to 0.31141, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 4/500\n",
            "18612/18612 [==============================] - 19s 999us/step - loss: 1.4947 - acc: 0.2978 - val_loss: 1.4169 - val_acc: 0.2897\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.31141\n",
            "Epoch 5/500\n",
            "18612/18612 [==============================] - 19s 1000us/step - loss: 1.4490 - acc: 0.3215 - val_loss: 1.3844 - val_acc: 0.3565\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.31141 to 0.35654, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 6/500\n",
            "18612/18612 [==============================] - 19s 1000us/step - loss: 1.4155 - acc: 0.3429 - val_loss: 1.3625 - val_acc: 0.3714\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.35654 to 0.37137, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 7/500\n",
            "18612/18612 [==============================] - 19s 1ms/step - loss: 1.3934 - acc: 0.3514 - val_loss: 1.3484 - val_acc: 0.3815\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.37137 to 0.38147, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 8/500\n",
            "18612/18612 [==============================] - 19s 998us/step - loss: 1.3800 - acc: 0.3634 - val_loss: 1.3364 - val_acc: 0.4025\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.38147 to 0.40254, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 9/500\n",
            "18612/18612 [==============================] - 19s 1000us/step - loss: 1.3595 - acc: 0.3797 - val_loss: 1.3320 - val_acc: 0.3924\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.40254\n",
            "Epoch 10/500\n",
            "18612/18612 [==============================] - 19s 1000us/step - loss: 1.3313 - acc: 0.3983 - val_loss: 1.3107 - val_acc: 0.4197\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.40254 to 0.41973, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 11/500\n",
            "18612/18612 [==============================] - 19s 1ms/step - loss: 1.3175 - acc: 0.4169 - val_loss: 1.2980 - val_acc: 0.4264\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.41973 to 0.42639, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 12/500\n",
            "18612/18612 [==============================] - 19s 1ms/step - loss: 1.3009 - acc: 0.4255 - val_loss: 1.2645 - val_acc: 0.4464\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.42639 to 0.44638, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 13/500\n",
            "18612/18612 [==============================] - 19s 1ms/step - loss: 1.2822 - acc: 0.4434 - val_loss: 1.2433 - val_acc: 0.4683\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.44638 to 0.46830, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 14/500\n",
            "18612/18612 [==============================] - 19s 1ms/step - loss: 1.2604 - acc: 0.4581 - val_loss: 1.2213 - val_acc: 0.4771\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.46830 to 0.47711, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 15/500\n",
            "18612/18612 [==============================] - 19s 998us/step - loss: 1.2364 - acc: 0.4713 - val_loss: 1.1745 - val_acc: 0.5218\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.47711 to 0.52181, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 16/500\n",
            "18612/18612 [==============================] - 19s 998us/step - loss: 1.2189 - acc: 0.4913 - val_loss: 1.1381 - val_acc: 0.5394\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.52181 to 0.53944, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 17/500\n",
            "18612/18612 [==============================] - 19s 996us/step - loss: 1.2007 - acc: 0.4993 - val_loss: 1.1092 - val_acc: 0.5560\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.53944 to 0.55599, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 18/500\n",
            "18612/18612 [==============================] - 19s 996us/step - loss: 1.1855 - acc: 0.5093 - val_loss: 1.1239 - val_acc: 0.5403\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.55599\n",
            "Epoch 19/500\n",
            "18612/18612 [==============================] - 19s 997us/step - loss: 1.1799 - acc: 0.5171 - val_loss: 1.1031 - val_acc: 0.5596\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.55599 to 0.55964, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 20/500\n",
            "18612/18612 [==============================] - 19s 999us/step - loss: 1.1635 - acc: 0.5273 - val_loss: 1.0865 - val_acc: 0.5607\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.55964 to 0.56071, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 21/500\n",
            "18612/18612 [==============================] - 19s 999us/step - loss: 1.1590 - acc: 0.5307 - val_loss: 1.0681 - val_acc: 0.5790\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.56071 to 0.57898, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 22/500\n",
            "18612/18612 [==============================] - 19s 1ms/step - loss: 1.1468 - acc: 0.5345 - val_loss: 1.0596 - val_acc: 0.5790\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.57898\n",
            "Epoch 23/500\n",
            "18612/18612 [==============================] - 19s 999us/step - loss: 1.1374 - acc: 0.5458 - val_loss: 1.0949 - val_acc: 0.5571\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.57898\n",
            "Epoch 24/500\n",
            "18612/18612 [==============================] - 19s 998us/step - loss: 1.1354 - acc: 0.5500 - val_loss: 1.0850 - val_acc: 0.5583\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.57898\n",
            "Epoch 25/500\n",
            "18612/18612 [==============================] - 19s 1ms/step - loss: 1.1227 - acc: 0.5535 - val_loss: 1.0441 - val_acc: 0.5981\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.57898 to 0.59811, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 26/500\n",
            "18612/18612 [==============================] - 19s 1ms/step - loss: 1.1164 - acc: 0.5599 - val_loss: 1.0519 - val_acc: 0.5878\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.59811\n",
            "Epoch 27/500\n",
            "18612/18612 [==============================] - 19s 998us/step - loss: 1.1209 - acc: 0.5527 - val_loss: 1.0751 - val_acc: 0.5740\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.59811\n",
            "Epoch 28/500\n",
            "18612/18612 [==============================] - 19s 996us/step - loss: 1.1093 - acc: 0.5685 - val_loss: 1.0293 - val_acc: 0.6024\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.59811 to 0.60241, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 29/500\n",
            "18612/18612 [==============================] - 19s 1000us/step - loss: 1.0997 - acc: 0.5737 - val_loss: 1.0566 - val_acc: 0.5809\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.60241\n",
            "Epoch 30/500\n",
            "18612/18612 [==============================] - 19s 1ms/step - loss: 1.0931 - acc: 0.5789 - val_loss: 1.0586 - val_acc: 0.5770\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.60241\n",
            "Epoch 31/500\n",
            "18612/18612 [==============================] - 19s 994us/step - loss: 1.0902 - acc: 0.5808 - val_loss: 1.0205 - val_acc: 0.6091\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.60241 to 0.60907, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 32/500\n",
            "18612/18612 [==============================] - 19s 999us/step - loss: 1.0859 - acc: 0.5831 - val_loss: 1.0235 - val_acc: 0.6151\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.60907 to 0.61509, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 33/500\n",
            "18612/18612 [==============================] - 19s 996us/step - loss: 1.0791 - acc: 0.5895 - val_loss: 1.0255 - val_acc: 0.6041\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.61509\n",
            "Epoch 34/500\n",
            "18612/18612 [==============================] - 19s 1ms/step - loss: 1.0872 - acc: 0.5828 - val_loss: 0.9980 - val_acc: 0.6209\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.61509 to 0.62089, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 35/500\n",
            "18612/18612 [==============================] - 19s 999us/step - loss: 1.0729 - acc: 0.5905 - val_loss: 1.0110 - val_acc: 0.6230\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.62089 to 0.62304, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 36/500\n",
            "18612/18612 [==============================] - 19s 997us/step - loss: 1.0795 - acc: 0.5883 - val_loss: 1.0120 - val_acc: 0.6119\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.62304\n",
            "Epoch 37/500\n",
            "18612/18612 [==============================] - 19s 995us/step - loss: 1.0721 - acc: 0.5894 - val_loss: 1.0285 - val_acc: 0.6151\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.62304\n",
            "Epoch 38/500\n",
            "18612/18612 [==============================] - 19s 1ms/step - loss: 1.0681 - acc: 0.5957 - val_loss: 1.0265 - val_acc: 0.6048\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.62304\n",
            "Epoch 39/500\n",
            "18612/18612 [==============================] - 19s 999us/step - loss: 1.0663 - acc: 0.5980 - val_loss: 1.0052 - val_acc: 0.6138\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.62304\n",
            "Epoch 40/500\n",
            "18612/18612 [==============================] - 19s 996us/step - loss: 1.0722 - acc: 0.5949 - val_loss: 1.0009 - val_acc: 0.6314\n",
            "\n",
            "Epoch 00040: val_acc improved from 0.62304 to 0.63142, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 41/500\n",
            "18612/18612 [==============================] - 19s 997us/step - loss: 1.0657 - acc: 0.5996 - val_loss: 1.0240 - val_acc: 0.6162\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.63142\n",
            "Epoch 42/500\n",
            "18612/18612 [==============================] - 19s 998us/step - loss: 1.0554 - acc: 0.6091 - val_loss: 1.0039 - val_acc: 0.6282\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.63142\n",
            "Epoch 43/500\n",
            "18612/18612 [==============================] - 19s 994us/step - loss: 1.0532 - acc: 0.6085 - val_loss: 1.0012 - val_acc: 0.6314\n",
            "\n",
            "Epoch 00043: val_acc improved from 0.63142 to 0.63142, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 44/500\n",
            "18612/18612 [==============================] - 19s 998us/step - loss: 1.0559 - acc: 0.6064 - val_loss: 0.9905 - val_acc: 0.6353\n",
            "\n",
            "Epoch 00044: val_acc improved from 0.63142 to 0.63529, saving model to weights.best_w_l2_BNafterAct_cnnreplacefc.hdf5\n",
            "Epoch 45/500\n",
            "18612/18612 [==============================] - 19s 1ms/step - loss: 1.0525 - acc: 0.6122 - val_loss: 1.0088 - val_acc: 0.6301\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.63529\n",
            "Epoch 46/500\n",
            "13312/18612 [====================>.........] - ETA: 4s - loss: 1.0470 - acc: 0.6113"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K6mb5rBCF8H",
        "colab_type": "code",
        "outputId": "8a783226-955b-46fb-e39c-1bb1a4512c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\"model = keras.models.load_model('weights.best_w_l2_BNafterAct.hdf5')\n",
        "y_pred = model.predict_classes(X_test_argment.reshape(-1,22,window,1))\n",
        "print(\"test_accu is : {:2.2%}\".format(findaccu(y_pred, Y_test_argment, window, stride)))\n",
        "\n",
        "\n",
        "model2 = keras.models.load_model('weights.best_w_l2_stride10.hdf5')\n",
        "y_pred = model2.predict_classes(X_test_argment.reshape(-1,22,window,1))\n",
        "print(\"test_accu is : {:2.2%}\".format(findaccu(y_pred, Y_test_argment, window, stride)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_accu is : 70.88%\n",
            "test_accu is : 69.53%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfcJWPYKbvUr",
        "colab_type": "code",
        "outputId": "5ec81023-26d3-4b8c-9843-69dc7532df8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model2 = keras.models.load_model('weights.best_w_l2.hdf5')\n",
        "y_pred = model2.predict_classes(X_test_argment.reshape(-1,22,window,1))\n",
        "print(\"test_accu is : {:2.2%}\".format(findaccu(y_pred, Y_test_argment, window, stride)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_accu is : 71.11%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZG15BFEEtHn",
        "colab_type": "code",
        "outputId": "173870be-55e2-4741-c082-4c9dbef5d51b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model2 = keras.models.load_model('weights.best.hdf5')\n",
        "y_pred = model2.predict_classes(X_test_argment.reshape(-1,22,window,1))\n",
        "print(\"test_accu is : {:2.2%}\".format(findaccu(y_pred, Y_test_argment, window, stride)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_accu is : 69.53%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDtb1LtRVgHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "|# Majority vote on prediction\n",
        "X_test_argment, Y_test_argment, person_test_idx_argment = dataargment(X_test, Y_test, window, stride,person_test)\n",
        "\n",
        "\n",
        "y_pred = model.predict_classes(X_test_argment.reshape(-1,22,window,1))\n",
        "acc_test = findaccu(y_pred, Y_test_argment, window, stride)\n",
        "# score, accu = model.evaluate(X_test_argment.reshape(-1,22,window,1), Y_test_argment)\n",
        "print(\"best val_accu is : {:2.2%}\".format(np.max(history.history['val_acc'])))\n",
        "print(\"test_accu is : {:2.2%}\".format(acc_test))\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model acc')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02SXSWOHctGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('model_deep_arg.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALIAm10t2_Kl",
        "colab_type": "code",
        "outputId": "297ec229-589d-4fad-9fb7-fa3998e0618e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "num_bin = 1000\n",
        "window = 750\n",
        "stride = 25\n",
        "num_test = Y_test.shape[0]\n",
        "N = (num_bin - window)//stride+1\n",
        "y_pred = model.predict_classes(X_test_argment.reshape(-1,22,window,1))\n",
        "y_pred_trialwise = np.zeros(num_test)\n",
        "for idx_trial in np.arange(num_test):\n",
        "# for idx_trial in [128]:\n",
        "  (values,counts) = np.unique(y_pred[idx_trial:idx_trial+N],return_counts=True)\n",
        "  ind=np.argmax(counts)\n",
        "  y_pred_trialwise[idx_trial] = values[ind]\n",
        "  \n",
        "np.sum(y_pred_trialwise == np.argmax(Y_test)) / num_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40632054176072235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfjZSrJ839Eb",
        "colab_type": "code",
        "outputId": "b6cab79b-cf12-464c-ebed-b6e6c8fce777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "750"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka_Lo7WY7I-9",
        "colab_type": "text"
      },
      "source": [
        "#### Shallow CNN with data argmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d37oOUzm7K8-",
        "colab_type": "code",
        "outputId": "d2d9413c-d6ca-4893-e1dd-58e8d2445343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "\n",
        "dropout_rate = 0.6\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=40, kernel_size=(1,25), \n",
        "                strides=(1, 1), input_shape=(22,window,1),\n",
        "                padding ='valid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters=40, kernel_size=(22,1), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(Reshape((-1,40,1)))\n",
        "model.add(AveragePooling2D(pool_size=(75,1), strides = (15,1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 4, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='loss', patience=15, verbose=1)\n",
        "\n",
        "history = model.fit(X_train_argment.reshape(X_train_argment.shape[0],22,window,1), Y_train_argment, \n",
        "                    batch_size=256, epochs = 500, validation_split = 0.2,\n",
        "                    callbacks = [early_stop], verbose = 0)\n",
        "print(\"dropout rate = {} is done.\".format(dropout_rate))\n",
        "\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model_shallow_arg.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 00167: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-fef114965698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     callbacks = [early_stop], verbose = 0)\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mhisstorydict_shallow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout rate = {} is done.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hisstorydict_shallow' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKZoBW9EmcmY",
        "colab_type": "code",
        "outputId": "ad8bfebf-e370-4013-dcae-90667582c056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(443, 22, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLAilrm-7x8W",
        "colab_type": "code",
        "outputId": "3c46316c-80c9-4bde-ef7a-9da41508e065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "_, accu = model.evaluate(X_test_argment.reshape(-1,22,750,1), Y_test_argment)\n",
        "\n",
        "print(\"training accu is : {:2.2%}\".format(history.history['acc'][-1]))\n",
        "print(\"val accu is : {:2.2%}\".format(history.history['val_acc'][-1]))\n",
        "print(\"test accu is : {:2.2%}\".format(accu))\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model acc')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2658/2658 [==============================] - 1s 499us/step\n",
            "training accu is : 48.46%\n",
            "val accu is : 42.12%\n",
            "test accu is : 41.38%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0kAAAFMCAYAAAAeK/faAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xl4VOXZ+PHvLJnseyY7ISSELewI\noiwFBEHcsG5UCrW2ta21tfb1V62t1dZKbV/1bSndxB2sgpoqKgquiMhOMBACWYDsmcwkM5NlZjLb\n+f0xyZBIAkEyCYH7c11cV+bMc865zyTMmXue57kflaIoCkIIIYQQQgghAFAPdABCCCGEEEIIcT6R\nJEkIIYQQQgghOpEkSQghhBBCCCE6kSRJCCGEEEIIITqRJEkIIYQQQgghOpEkSQghhBBCCCE6kSRJ\niAH261//mr/97W+nbZOXl8ftt9/ePwEJIYQQ7c7lHiX3LjGYSZIkhBBCCCGEEJ1IkiTEWaiqqmLm\nzJmsWbOGhQsXsnDhQg4cOMCdd97JrFmz+NWvfuVv+95773HNNdewaNEiVqxYQUVFBQBms5k77riD\nefPmceedd9Lc3Ozfp7S0lG9/+9ssXLiQa6+9loMHD54xpr///e8sXLiQ+fPn88Mf/pCmpiYAHA4H\nv/zlL5k3bx5XXXUVb7311mm3CyGEGNzOx3tUB4vFwj333MPChQtZvHgxTz/9tP+5//u///PHu2LF\nCgwGw2m3C9EftAMdgBCDjdlsRq/Xs3nzZn72s59x77338sYbb6BSqZg9ezY//vGP0Wq1PPTQQ7zx\nxhsMHTqU5557jt/+9re88MILrFmzhtjYWJ577jmqqqq47rrryMnJwev18pOf/ITvf//73Hzzzezb\nt4+77rqLTz75pMdYDh06xMsvv8yWLVsICwvje9/7HuvWreOuu+7iueeew+Vy8fHHH1NXV8c111zD\n9OnTeeONN7rdnpSU1I+vohBCiEA4n+5RnT311FNER0ezefNmLBYLN9xwA5MnTyY6Opr333+fd955\nh6CgINauXcuOHTvIzc3tdvuSJUsC/AoK4SM9SUKcJbfbzaJFiwAYMWIE48aNIy4ujtjYWPR6PfX1\n9Wzfvp1LL72UoUOHAnDzzTeza9cu3G43e/fu5aqrrgIgPT2dadOmAXDs2DEaGhq46aabAJgyZQpx\ncXHk5+f3GMvYsWP59NNPiYiIQK1WM2nSJCorKwH47LPPuPrqqwFITk5m69atJCUl9bhdCCHE4Hc+\n3aM627p1K7fddhsAMTExLFiwgO3btxMVFUVjYyNvv/02VquV5cuXs2TJkh63C9FfpCdJiLOk0WgI\nCQkBQK1WExYW1uU5j8eD2WwmKirKvz0yMhJFUTCbzVitViIjI/3PdbRramrC4XD4b04ALS0tWCyW\nHmOx2+388Y9/ZNeuXQBYrVbmzJkD+L5N7Hye8PDw024XQggx+J1P96jOGhsbu5wzKiqK+vp6kpKS\n+Nvf/sZzzz3Ho48+ytSpU/nd735HSkpKj9uF6A/SkyREAMTHx3e5cVitVtRqNbGxsURFRXUZ493Y\n2AhAYmIi4eHhvP/++/5/n3/+OQsWLOjxPC+++CInTpwgLy+PzZs3c+utt/qfi42NxWw2+x/X1dVh\nt9t73C6EEOLi0F/3qM4SEhK6nNNisZCQkADA9OnTefrpp9m+fTspKSk88cQTp90uRH+QJEmIAJgx\nYwZ79+71D3179dVXmTFjBlqtlokTJ/Lhhx8CUFFRwb59+wBIS0sjOTmZ999/H/DdmH7xi19gs9l6\nPE9DQwNZWVmEh4dTXV3N1q1b/e3nzZvHm2++iaIoGI1GlixZgtls7nG7EEKIi0N/3aM6mzNnDuvX\nr/fv+8EHHzBnzhw+//xzfve73+H1egkLC2PUqFGoVKoetwvRX2S4nRABkJyczB/+8AfuuusuXC4X\n6enpPProowD88Ic/5N5772XevHlkZ2dz5ZVXAqBSqXjqqad45JFH+Mtf/oJarea73/1ul6ESX7V0\n6VJ+9rOfsXDhQkaOHMkDDzzAT3/6U1544QVuv/12ysvLmTt3LiEhIdx///2kpqb2uF0IIcTFob/u\nUZ39/Oc/55FHHmHRokWo1WruvPNOxo8fT1tbG++++y4LFy5Ep9MRFxfHypUrSUxM7Ha7EP1FpSiK\nMtBBCCGEEEIIIcT5QobbCSGEEEIIIUQnkiQJIYQQQgghRCeSJAkhhBBCCCFEJ5IkCSGEEEIIIUQn\nkiQJIYQQQgghRCcXZAlwo7H5zI3OIDY2DLO5d7X/z0eDPX4Y/Ncw2OMHuYbzwWCPX6+PHOgQzlvn\neq8a7H8bgz1+kGs4Hwz2+GHwX8Ngj7+n+5T0JPVAq9UMdAjnZLDHD4P/GgZ7/CDXcD4Y7PGLwBns\nfxuDPX6QazgfDPb4YfBfw2CPvyeSJAkhhBBCCCFEJ5IkCSGEEEIIIUQnkiQJIYQQQgghRCeSJAkh\nhBBCCCFEJ5IkCSGEEEIIIUQnkiQJIYQQQgghRCeSJAkhhBBCCCFEJ5Ik9bNPP/2oV+3++tcnqamp\nDnA0QgghRFdynxJCCEmS+lVtbQ0ffri5V23vued/SE1NC3BEQgghxElynxJCCB/tQAdwMXnqqT9R\nVFTIrFlTufLKq6itreEvf/kHf/zj7zEa67Hb7dxxx53MmDGLu+++k1/84pd88slHtLa2UFFRTnV1\nFT/72f9w2WUzBvpShBD9wGSxc+BYI+OHxaJWqc75eHWNNmIjgwkOujBXR79YKIrCZ1/WMHtKBuf+\nV9GV3KeEEMJHepL60be+tZyJEydz++3fx+128Y9/PENrawvTpk1n9eqn+f3v/8izz/77lP3q6w08\n8cQq7rnnPjZuzBuAyIUQgeRVFMrrmk/Z/vYXJ1i14QBb8899SFNtQysPPbOLVz4sOW27plYn7++q\noM3lOedzisCoN9t58f2jvLWtrM+PLfcpIYTwuSh7kjZ8XMqeI/WnbaPRqPB4lF4fc+qoRG6ZN7zX\n7UePzgUgMjKKoqJCNm7MQ6VS09RkPaXt+PETAUhMTKSlpaXX5xBCDA6fHajhpc1H+dmN45mYk+Df\nXtdoA+C1T8uYmKMnNjL4a59jR2EdHq/C7iIDt83PQddNb5K9zc1T6w9QUd+CV1FYPH3o1z6fOHc9\n3as8Xi8AW3aWs/1AzVkd82zuVXKfEkJczKQnaYAEBQUB8MEH79PU1MTf//4MK1c+0W1bjebkhxlF\n6X3iJoQYHPaXGAH4sszUZXu92Y5KBQ6nh3VbjnZ5zuvt/XuBoijsLDQAvmMVlDWc0sbt8fKPNw9R\nUe/7gPvZlzXyfnOeUrUPvQz0b0fuU0KIi9lF2ZN0y7zhZ/wmTa+PxGg8dfjLuVCr1Xg8XYewWCwW\nUlJSUavVbN36MS6Xq0/PKYQ4v7ncHoorLAAcPtHo3+5wurG2OpmYo6fF5iS/xITJaichOpTnNxVR\nXGnh99+bRpD2zPOLjtU0YbI6yEyO5ERdMzsPG8hOi+bNbceYNSGV4WnR5H12jMLjjUzIjic0WMvO\nwwaOVFgYPTQ2YNcuTq+ne1Wzzck9qz5nyqhEvr94dJ+eU+5TQgjhIz1J/Wjo0GEcPXqE1taTQxHm\nzJnHF19s4557fkxoaCiJiYk8//yaAYxSCNGfiiutON2+4VNGiwOjxQ74epEAUvThjMuKA6DG1ArA\noeONGMx29h019uocHb1IS2Zlka4Pp6DMxJ/+s59tBbU8tf4A7++q4P1dFSTGhnLndbnMmeSrWLb1\ngJR3Ph8FaX23bqfL2+fHlvuUEEL4XJQ9SQMlNjaWvLx3u2xLSUnlxRdf9T++8sqrAPjud38AQFbW\nyW8Rs7KGs3r10/0QqRCDn6IoHDzWSPNhAxarnVnjU4kK1w10WKc4eMw39G1STgL5JSaKys3oY0L9\nSVJqQjjBat/wqhqTjeFpbszNbQB8ml/N9NzkU47Z2ORAUSA+OoQqYwu7igxEhAYxJjOWS8ck8cbW\nY9Sb7UzKSeBAiYkNn5SiUav44XW5hAZryUmPJiU+jP3FRpptTiLDzr/XLdCKi4u56667uP322/n2\nt7/dbZsnn3ySAwcOsHbtWgBWrlzJl19+iUql4sEHH2T8+PEBia0jSXK5+764htynhBDCR5IkIcQF\nweP18sKmI4zJjOOysclsK6jlhfeO+J93ub0smZWFoihUGVtJ14f753YMpMLjjei0aq6fOYz8EhOH\nTzQye0Iq9e09Sinx4YRo2pOkhlZqGlr9+xZXWak2tpCmj/Bva7Y5eejZ3djb3KTEh1HXaENRYMnM\nYWg1ai4fm8LH+6uZOS6FJbOGsa2glnVbirl5TjbDUqIA35yXhdMyWLv5KDaH+6JLkmw2G48++iiX\nXXZZj21KS0vZs2ePf97O7t27KS8vZ/369ZSVlfHggw+yfv36gMSnUatRq1QB6UkSQgjhI8PthBAX\nhKMVFrYfquPZd4vY9mUN6z8uITRYw09umgCcrBSXX2Li4ed2n7HCZX9obHJQbWplREYMQxIjiI0M\npqjcjFdRqDf74k1JCCcxNhSNWkVtQ6t/yN347HgA3t1RTlm1FXubG4Ateyqxt7lJ04dTb7aTHBfG\nz2+ewHUzhwEQGxnMkz+ZwQ2zs1CpVMyekMrf753FgqlDusTm2z6bpLiw/no5zhs6nY41a9aQmJjY\nY5vHH3+ce++91/94x44dzJ8/H4Ds7GysVmtAq7wFadUB6UkSQgjhIz1JQoh+pygKlhbnOZW0/qr8\nYl9lOK+i8Hx7D9LtV43iykuH8u//HsTQPnzteG0T4JvXM210Up+d/2y53F7/mkXjsuJRqVSMGRrL\n9kN1VBpafJXtgOT4cKwWG4mxodSYbFQbfUnS4ulDqTD4ijDsPGwgOlzHj67P5cN9VUSH63hoxSWo\n1So0atUZe8x6Kv7QXZnwi4FWq0Wr7fn2mJeXx7Rp00hLS/NvM5lM5Obm+h/HxcVhNBqJiIjo7hDn\nLEir9s9lE0II0fckSRLiAtNid/H6p2VcNyOTuKiQgQ6nWx/srWL9RyX85juX+Id4dfbx/ip2FhpI\n04czLiueySP0pz2eoijklxoJC9ay9IocnttURO6wOGaNT0GtVpEY65vjoygKdQ2+HpqSqq5rvRSV\nm3l7+3HGZcczfUxyjwncF4dqabG7ufIrPS81plZCg7X+/VxuL1pN9wmKy+3hb3kHOXSskZFDYpg9\nPhWA8cMT2H6ojt1FBuotdmKjgv2JSmp8OLUNNg6X+yrgDUmM4J6bJnDwWAMNTQ62HqjhT//JB+CG\nmcMu2gQn0CwWC3l5eTz//PMYDIYe2/WmDHZsbBjaXlQn7E6wToPL5UWvj/xa+58vBnv8INdwPhjs\n8cPgv4bBHn93JEkS4gLzxcFaPvuyhrioYK6bMWygwwF8a/oUlZsZPTQWlQo+ya9GwTf07atJkrWl\njQ0fl+J0eymttrL1QA1XTc/gxm9ko+6hR6TC0EJjUxvTc5OYOT6FrNQo9DGh/gQlMSaUGlMrrQ43\nte3D7gyNNppsTqLa59t8kl/NkQoLRyosvPZJGXFRwQxPi2bRpRlkJvtidHu8vPxBCQ6nm5njUggL\n8b2Fmpvb+P0Le8hIjuTBb0/B5nDzwL93kJEUwV1Lxvnbddi4/QSHjjUyPjueu5aM9Sc0E4fHExas\n5YtDdTS1OhmZEePfJyUhHIqNVBtbiY0MJjRYy9DkSIYm+25Mw9OieW5TEZFhOr4xKQ0RGDt37qSx\nsZFly5bhdDqpqKhg5cqVJCYmYjKdXOeqvr4evf70yb25fUjl16FRqXC6PX2+VEV/CsRSG/1NrmHg\nDfb4YfBfw4UQf3dkTpIQF5gj7WvuGBrtAxzJSZ8V1PDk+gO8/cUJjtU0YWhPVIo6rQvU4d2d5Tjd\nXm6bn8OvV0whKS6M93ZW8NL7R09pW17XTIWhmf3FvlLYk3N8H0pTE8L9FcAAEmNDAahtaPWfG6C0\nvTdJURRKqyxEhetYtmAE47Pj8XgVdhfV8/sX9rLm7UK8iuKf+6MoUFp9sifqnS9O4HR7OVbdhL3N\nzdFKMy12F4dPmHn85X00Njn8bWsbWnl/VwVxUcH8+PqxXXp8grQapo1OxNrqRAESY0/OB0qNP/lz\nWkL4Ka/FjHEpPHz7VH61bDLB0osUMIsWLWLTpk1s2LCB1atXk5uby4MPPsiMGTPYvHkzAIWFhSQm\nJgZsqB20D7dzyZwkIYQIFOlJOs/cdNO1vPTSesLCLr7J0uLcebxejlaaATCcw7fUfW1fe5GE93aW\nU2HwfdsUotNwvLYZm8Pt72lpbHLwaX41CdEhzJmUhlaj5tfLp/Cnl/fz2Zc1XD9zmH84m7WljcfW\n7sPt8Q1r02rU5A6L6/b8Se1J0uETZjxehfioEBqaHJRUWZg8Qk9jUxuWFidTRui5Yko6V0xJR1EU\nDpeb2fBxKTsKDVyWm8zhcrP/mEcrzYzPjsdktfPZlzWAbz5USZWVovZ2o4fGUlRu5rG1+7j35gkk\nxYWxbksxHq/CbfNHEKw7NZm5fGwKnx6o6RI3+BK/7n7uLCPpwhvuMBAOHTrEn/70J6qrq9FqtWze\nvJl58+aRnp7OggULut1n8uTJ5ObmsnTpUlQqFQ8//HBAY9QO8JwkuVcJIS50kiQJcQGpMLRgb/N9\nu1zXYENRlHMuc110ohGHy8PE4Qn+Y7k9Xn7/wl7GZcdx85yTa6R4FYUKQzNpCRH+nhybw8WRCot/\nonl+iYnocB0zx6fw7o5yjlaamZSjx9zcxpq3D+P2KFx7eSZajW//iNAgZk9M5ZUPSzhQYmTu5HQA\nPj1Qg9vjJSU+jNoGG5eM0hMa3P1bWkePzJelvuFQl41N4r2dFf55SR29Qtlp0f59VCoVuZlx3H7V\nKB59cS8f76+m3mInSKvG61UorvT12L29/QQer8JlucnsKKzjaKWZI+VmgrRqfn7zBD7cW8lrn5bx\n2Lp9qACH08P47Hgm5SR0G2t2WpR/DlVipyQpOS4MFaDQc5Ik+sbYsWP9ax+dTnp6epd29913XyDD\n6iJIq8bl8vTJ/3EhhBCnCmiS1JuF9Tovxvfaa6+xceNG/3OHDh0iPz+f5cuXY7PZ/N9Y3X///Ywd\nOzaQofe5O+5YxsqVT5KcnExdXS2/+tX/oNcnYrfbcTgc3Hvv/2PMmMF1TeL809GDodWosbW5abG7\nzmmNG4fTzaq8g7Q5PVw+NpnlV44kWKehrtFGlbEFS0sbN87ORq1WUVBm4o2tx6isb+HKqUNYekUO\nAAXHGvB4Fa65PJP8YiMV9S1clpvM2GFxvLujnMMnzGjUKta8fZhWh5sJ2fFcPq7rAqlTRuh55cMS\n9hf7kiS3x8un+dWEBmt56DuXYG/zENZDggQnh9udqPP1YmUkRpKRFEl5XTNtLo8/SRqeHn3KvsNS\nohiWEsmXpSYUYEJ2PM12Fydqm6msb2H7wTpS4sNYtmAEu4sM7C82YWi0MXpoLEFaNVdNH0psZDDP\nbTpCTISO2RNSuebyzB4/2KpUKq6cOoTXPinrMl9LF6QhISYEo8XR7XA7cXEJ0qjxKuDxKmg1fZck\nyb1KCCF8ApYk9WZhva8uxnfzzTdz8803+/d/7733/G3/+Mc/MmLEiECFG3CzZ89l+/bPuPHGW9i2\nbSuzZ88lOzuH2bPnsG/fHl5++UUee+x/BzpMMcgdaU+Spo7Ss6PQgKHRftZJkqWlDZvDTWpCOHuL\nDLQ5PQQHafjiUB02h5uf3TSeqnrf+i8tdhfHa5sI0qr562sFgO8b7u0Ha7nxG9kEadUcKPH13kwZ\noWd8djxvbC3jiinpREfo0AWp2VlYx6f51WjUKpZfOYI5k9JOSSDiokLITI7kSIWFVoeLg2UNWFud\nXDl1CCE6LSG607+VxUUFo1Gr8Hh9FceS48PISY/meG0TReVmSqutaDUqhvYwXG3e5HSefbcI8FWg\nM1rsHKtp4h9vHsKrKFw/cxhhIVoykyMpq/GVGB81NNa///TcZCaP0BOkVffqW/95k9OZMyntlEIV\nw1KiaGp1SU+S8PfU+qoo9t30YrlXCSGET8CSpJ4W1us8kbVjMb7Vq1efsv/f//53nnjiiYDEllf6\nDvn1B0/bpvMHqt6YlDiObw6/psfnZ8+ey+rVf+HGG2/h88+3cvfd9/Lqq2t55ZW1uFwuQkLOz1LN\nYvBwe7wUV1lIiQ9jxJAYdhQaqGu0dds7cjp/zztIRX0Lj33/Uj7Lrwbg/mWTWPP2YQ4db8Tt8VLV\nvlYPQEFZAy0OFwpw15KxlNVY2by7ki9LTUwYnkBBWQP6mBDS9OGoVCruWzrJv++IITEcOtZIaLCG\ne26awIghMV8Nx2/yCD0n6pr5eF8Vu4rqUQHzJveuiptGrSYhOgSD2Y5K5Zvrc+mYJD7YW8m6LUex\nNDvJSo3qUuyhs6mjEnn1oxJaHW7GZ8VTaWzh/V0VGBptpOvDuWSUb9HRkRmx/iRpdKckCc5+zaHu\nKvktXziSG2a5ehxWKC4sp7tXNcc6CZ7g4Q97dqJW974nSe5VQgjROwGrbmcymYiNPfkhoWNhvQ7d\nLcbXoaCggJSUlC7lU1etWsWyZcv47W9/i8PhOGWf811WVjYNDUYMhjqam5vZtu1TEhIS+ec/n+W+\n+x4Y6PBEH6ttaOWJdfuwtrT12zlP1DXjdHkZNTSW5Djf0NSzLd7Q2OSgrKYJl9vLug+K2VtkIDUh\nnKFJkYwcEoPb46WyvoUqo68nSa1Ssb/EyM5CA9EROiaNSGDGuBQAPj9Yy8f7q3A4PUzK0XfbgzJn\nYhoZSRH88luTT5sgAf61kv677Tg1plZmT0ztUv3tTDraJkSHEKTVMCwlimsvz6SxqQ2vojA8redk\nUhek4XtXj+FbV+QQHx3CiPRoOq7m+plZ/oSmo2R3cJCGzOS+L6IQHhJEUpxMlBeBI/cqIYTw6bev\nIzsvrHemxfhef/11brjhBv/jFStWMHLkSDIyMnj44Yd5+eWX+d73vtfjuc60QN8P9d8CvvX1LuQc\nXHHFPF56aQ0LFy7AbDYzatRI9PpI1q37ApVKQa+PRKNRk5AQQXj4uQ+nuRAW9jqfr6HS0ExqQjia\nboa6rPuwhK35VURH6PjBknFnPNbrH5dQXtvEL26b3CWZeHf7cb4sMWJzuJgzOZ3504b2eIy97cPa\nJoxIZEyOr2fD0urq8TUsOt7I5wXVXD4ulTHD4lCpVOw+6vsiI0irpqCsAYC5lwwhMTGKCSMT+fRA\nDcamNmobbcRGBjMkKZKC9mII18zMITkpmuSkaIYPieHgsQYKyhqIiQzm5gUj0cef+je9UB/JwhlZ\nZ3x9wPe3MDozjlpTK9+/fiyzuxmWd7p9h6ZGcfBYAxnJUf7X5I7rx1FS3UTRiUYmj0k+7d/bgq88\nd/n4VOxONwtnDPPHMT0yhPCNhUwckUhK8tn14J0pfnHx+ebwa3rs9Xl+UxHbCmr52Z3T+zxxvuyy\nmTz99D+YNesbWCxmsrN98wu3bv0Et9vdp+cSQojzVcCSpNMtrNfTYnwPPvggALt27eI3v/mNf9/O\nJVfnzZvHpk2bTnvuc1mgr0MgFsaaOnUGP/rRHbzwwis4HHb+8IeH2bjxHW688RbeeuttXnjhZTwe\nLyZTCzbbuZV2HewLe8H5fQ3VxhYeenY38yan8e0rR3Z5rs3lYXuBr4Tz5p3lLJiSRnhIUI/HUhSF\nNz4uocXuYs6EFH8Z59qGVv6dV0DH1wuVhmbGZ8b6P5C32F2szjvIvMlpTBudxNH2NYcigzW4HU6C\ndRrKa5u6fQ3L65p5/D/7aXN62PjZMTISI/h/t03is/wqAL5/zRj++eYhAHIzYjAam0loL72961At\nRrOd3GFxjBoS40+SJg+P959r+uhESistRIUFcd+tE9F4vX3yu7z35vGoVSrUahUmU0uv9un4O4ps\nLzMeHxncJZYfXjeG/UeNZCWGn1WM31s8CuCUOH7/vUsJ0Wn67G/3fP5/0BuS4AVG5zlJfe0b35h7\nyr3qk08+5MYbb+HDD7fw7rsbz3wQIYQY5AKWJM2YMYO//e1vLF269JSF9RYtWsSiRYsAqKqq4le/\n+pU/QTIYDISHh6PT+SabK4rCd7/7XVatWkVUVBS7du0iJycnUGEH1OjRuWzdusv/+OWXX/f/PHPm\nNwC4+urr+j0ucfaOtc872XqghgVTh5DUadhXfomRNqeHhJhQTBY7n+ZXc/VlmT0eq6bBRovdBcD+\nYqM/SfpgTyUK8INrxrCv2Mj+YiNGq4PEGF+ltk/2V1FcaUGnVTNtdBI1Jt88oZT4MFQqFcmxYdQ2\ntOJVlC7zWwxmG/+34QBOp4eb5mRTVm0lv8TEs+8UcbTCQmZyJFNHJVJ5+VC8qPxD91LiwwgN1vh7\nmNL14UwYHs+GT0oZlRHT5TWYNT6VNqeHSSP0/v37wrlMUB85JAaNWsWYzK5rKUWF6ZgzqXdzm3qj\nYx0nIQJJ1z5awuXp+yRJ7lVCCBHAOUmdF9b7wx/+wMMPP0xeXh4ffPDBafczGo3ExZ38EKNSqbjl\nllu4/fbbWbZsGXV1dSxbtixQYQvRK9XtCYnHq/DmtuNdnttZ6BtC+sCKSwjRafhwX9Vpv+0trji5\nQOn+Yt9wt6ZWJ9sP1ZEQHcKlY5L8RQA6qte53B4+2ufr9Tle24SiKFSbWkmIDvFXekuKC8Xp9mKy\nOqi32FEUBWtLG0+tP0CTzcW3F45k8fSh3HXDWLJTozhQasLjVZjUPvfnm7Oz+fGNE/yxqVUqMpOj\n/AVN0vURpMSHc89N47nj6tFdrqmj9HVfJkjnKiMpkjW/nMv47PiBDkWIc6YNYE+SEEKIAM9J+urC\neqNGjTqlzVcX4xs7dizPPPNMlzaLFy9m8eLFgQlSDDrm5jaeXH+AZQtGnFJBrDuKouD2KD1WLvs6\nOpKk1IRwdh02MGt8CmMy47C0tHHoWCOZyZGMHBrHrPGpfLC3ksMnGpkwvPvFQ4+2L0qaEh9GlbGV\nerONLw7V4XJ7WTgtA7Vaxaj2ggBHK8zMnpDKjkIDTTYXKhW0OtycqGumqdXZJQHoSFB+s2YXbo+X\ndH0EiqJgtDi4bkYmc9t7TzRJfme8AAAgAElEQVRqNd+/ZgwPP78bp8vb4yKnAFmpUf61mNL1vp7h\nnq5LCBE4gRxuJ4QQIoA9SUIEypFyMzWmVn+vy5m8/cUJfrZqG4Xtc3b6QrWxhbioYG6bn4MKeOLV\nAzzxaj6/XrMTr6Jw+VjfYqhjs3y9oh1loTs0tToprbaiKApHKy1EhetYOC0DgLWbj7JpZznhIVpm\ntleKS00IJzIsiCMVFrxehc27K9CoVSy4ZAgAXxys87frkJPuS6yiwoMYmxVHjamValMrcyamcv3M\nYV3iSYoL40fXj+WG2VmnXag0q31xU5XKl9QJIQZGkEaSJCGECCRZbEMMOkaLHeh9eev8EhNtTg+r\nXi/gnpvG++ek7Cyso95s57pOCYOiKDy5/gBxUSHcsXh0t8drdbiwtDgZlxXPmMw47l82mVc+KuHw\nCTPR4ToWXTqUue3r9wxrTyqO11i7HOO5TUUUlDVw4zeysLY4mToqkYk5Cajeh8ITZsJDtNy1ZCzB\nOt+8A5VKxciMWPYeqefZd4uobbAxc1wKk3IS2LKnkp2HfUlS5wQnd1gcq+6ZRXiIFpVKRYPVwYm6\nZiblJHRbFW7i8AQmnqFXKCvVdz1JsWFnve6PEKLv+HuSAjAnSQghhCRJ4jxRWd9CcJC6V+vedCRJ\n9Y32LttNFjsf7qviuhnDCGuvZNbm9FBpaCEmQkeL3cXf8g7y1E9mEBqs5e0vTlDbYGPOpDSiwn2F\nQk7UNXP4hBkVsGTmMOKiTl04sbp9IdWOhGTEkBge+s4l1DbYSIwJ7TKsLyLUt67NsdomfwGFBquD\ng+3FD97Yesx/jKgwHTPGpfhKXF87pkshBIBRGTHsPVLPjkLfXKVbrxiOWqVChW/IHXTtSeo4f4f4\n6BDio89tIcjoiGCum5F5Xs01EuJidHK4nWeAIxFCiAuTDLcTA86rKPzvK/n8863CXrXvSJJMVgfu\n9m9RFUXhmXcOs2VPJdvay2+Dr6iBV1G4dEwScyam0eb0UNPQisfrpd7sO05Zp16eXYd9RRcUYOfh\nU9fwgpPzkdL0JxMStUpFWkJ4t/OeslOjsLd5qGvw9XxtP1iLAkwZcXKx5JHtC6nesXg0v15xySkJ\nEsCoDN/8K41axY+XjCU8JIjQYC3JnYa99ccQuCWzspiemxzw8wgheiZzkoQQIrAkSRIDztzURovd\nRVV9S69u+EarA/AlVw3tP39xqI7iKl+ys/dovb9tabVv2/C0aH8vS12DjQarw1+lraONV1HYc6Se\n0GAtWo3al8woCoeONWCynOy1qjGemiSdTscQtbIaK15FYVtBLcFBGu64ejS3zB3OtNGJpPbiWCnx\nYVw1PYM7r8v1D+ODk0P6Ole2E0Jc2GROkhBCBJZ8ohIDrrbhZDntGlMrQ5N7XnzS5fZgaW7zPzaY\nbUSEBbHhk1J0QWoSY8Ioq26isclBXFSIPwHKTovG0GhrP5+NyDCd/xhl1b6iCiWVFszNbcwcn4LD\n6WHvkXpW5x0kv8REQnQIj37/UoKDNFSbWlABKfG9S5KyU6MBOF7TRFxkCA1NDmaOTyE0WMuiSzN6\n/TqpVCpunjP8lO3DUqL44lDdKUPthBAXLulJEkKIwJKeJDEgjBa7PzmqbThZgKHK2HLa/UxWBwoQ\nGuzL7w2NdrYX1NJsc3Ht5ZnMnZQKwL6jRryKQlm1lYToEGIigkluT2pqG1r9CRPAidom3B4vu4t8\nPVCXjk7yV6fLLzGhC1JjsjrY+Pnx9hhb0ceEEtzLwgVpet8wvMITjazbchSA2eNTe7Vvb4wYEoMK\n37A+IcTFQZIkIYQILEmSxDnxKgp1jTa8itLrfVrsLh57aS9/+k8+XkXxJ0vgK+BwOkaLb3jdmEzf\n/ByD2cbB477S3pePTWHyCD0qYM/RegyNNlodboan+3pyosKCCAvWUtdoo669Mt6wlCicbi9F5WZ2\nHjYQFRbEqKExjB0Wx7CUKMZnx/PHOy8jITqEzbsrefbdw7TYXb0eageg1ajJTI7EaHFgMNtZPH2o\nP6a+MCQxgke/fymLLh3aZ8cUQpzfpLqdEEIEliRJ4pzsLKzjwad38rvn93CgxNSrfTZ8XEqTzUVT\nq5MaYyu1DTY6ClKfOUnyzQ3KHeYr411lbKW40kK6PoLYyGCiI4IZMSSG0iorL7x3BPDNRwLfcLXk\n+DDqzXb/vKKOHqNn3zmMvc3N1ZdnolGr0WrUPPSdS/j5zROIjQzmO4tG4VUUth+sIzhIw7TRSWf1\nOnUsenvjN7K4aU72We3bG6k9FI0QQlyYpCdJCCECS+YkiXNyoq4Z8CU3q94o4H+WTiS3fR2i7hSd\naOTzg7VoNSrcHoWSKgu1Da0kxIT4j6MoSrfr+MDJJGlIYgRR4TqKKy3AyUVbAeZOTqO02kpJlRUV\nJxMUgJS4MI7VNFFabSUmQudPtppsLtISwpk7Ka3b8+YOi+P+2yah1agZmhyJVnN2Cck1l2cyPTdZ\nSmcLIfqEFG4QQojAkiRJsO+okbio4C4V03qro7rcXUvG8o83D/Hhnsoek6TGJgfPvFuESgXfXTya\nNW8fJr/ERJPNxfiUKLQaNfuLjVhanMRGBvv3q6pv4b/bjvGNiamY2s+njwklKTaUplYnAGOHnTzn\ntNFJTB6hp7HJgaJAUqfEpKNctserkBwXRlJsKBGhQbTYXdw2P+e0yc/IjNgenzsTrUYtCZIQos9I\nT5IQQgSWJEkXOZfbwz/fPERmSiS/WXHJWe9vtDgI1mmYMlLPsJQoCsoaMFrs6GNCu7SzOdz85bUv\nMTe3cfPcbKaPSeLVj0oobJ9PlBofTrBOw/5iI1XGFmIjg1EUhfd3VZD3WRluj8KxmiZCdBqCdRoi\nQ4NIig2jpMqKLkhNTnpMl/NpNd0vTNu5Il1SXBgqlYpb5w2n2eZi9Gl6wIQQ4nwSpPUVjpE5SUII\nERgyieEiZ25x4lV8pbeVsyi+AL4FXBua7CREh6BSqZg3OQ0F+DS/2t9m12EDj764h3tXf06VsZV5\nk9NYNC0DlUpFTnoMHWdMjg8jXR8BnJyX9MqWo2z4pJSwkCAmj9BjbXViMNvRt58vKc6XiI3KiO31\nfJzOi612LNg6Y1zKWZXiFkKIgSY9SUIIEVjSk3SRMzf5hq85nJ5Thrl1x6soFJWbGTkkhjaXB3ub\nh4Qo33yiaaMTWf9xKdsKapmYk8CJ2mZe+agEjVpFmj6ccVnx3DAryz/faER6NPuLjYCvJykqwrd2\n0Y5DdZgsdj49UIM+JoQHlk0hRKehpMpCs83l76XKTPYND5w8Qt/r69XHhKJRq/zD7YQQYjCSOUlC\nCBFYkiRd5MwtJxdmrWloPWOS9N7Oct7YeozlC0eS1T6HKSHal7QEaTUsuCSd/247zh/X7QcgOlzH\nfUsnktbeS9RZzpCTQ+SS48MIC9GSrg+nythKtamVuKhg7ls6yR/TdTOG8fIHxf4eoDGZsfz+jmln\nXY47ISYUQ6PN3xMlhBCDzcmeJM8ARyKEEBcmSZIucpZmp//nugbbaSvTmZvbeOeLcgBKq6xEhfl6\nfjoq04GviltWajSfH6ylocnB964e7U9qviojKYLgIN8co4jQIAAe+e40DGYbtQ02po1Pxe1w+dvP\nnZSGLkjNuKx4wFfSOz3x1OTrTMZkxuJ2e0+ZNyWEEIOFWq1Cq1HJnCQhhAgQSZIuco3NDv/PNZ0W\nde3Oa5+U0ubyfWt5oq6JoUm+BCUh+mSSpFKpyB0W5y+tfToatZrvXzMGjfpkuW+1WkVKfDgp8eHE\nRoZg7JQkqdUqZo1P7d2FncayBSO4bb6CRi1T8oQQg1eQViPD7YQQIkAkSbrIWZpPDrera7D12K7w\nRCM7DxvITI4kOEhDcaWFSqOvwELHcLuvY8rI3s8n6itqlQp6WIdJCCEGC12QWpIkIYQIEPkq/SJn\nbmlDrVIRFxXcY09Ss83JM+8cRqNWsXzhSIalRqEAB0pMAMR36kkSQgjRP6QnSQghAkeSpIucpbmN\n6AgdqQnhWFuc2BzuLs8risLzm45gbXFyw+wshqVEkZkcCUCrw01osIbwEOmQFEKI/qbTqmVOkhBC\nBIgkSRcxr6JgaXESFxlMSpyvQlxtY9fepMPlZg6UmhiVEeNfSyizvaodQHxUqL+ktxBCiP5R2VxN\nkE7BLT1JQggREJIkXcSaW514vAoxkcH+RVb3HzXy5//s5/OCWgB2HqoDYMmsLN9cHkAfHeLvPUqQ\noXZCCNGvzA4Lj+/5K/boIzLcTgghAkSSpAuIubmNgrKG3rdvXyMpNuJkkvTergqOVFhY/3EJTTYn\n+4qNxEcFMzw92r+fSqXyD7mTJEkIIfqXy9s+LDqoDafbi6IoAxuQEEJcgAKaJK1cuZJbb72VpUuX\nUlBQ0G2bJ598kuXLlwOwa9cupk+fzvLly1m+fDmPPvooALW1tSxfvpzbbruNe+65B6fT2e2xLmbV\nplZ+/+Ie/vLal9SYfEPmPi+o5YX3ivB6u7+Bmtsr28VGBpOSEI4K3xj38dnxtDrc/D3vIA6nh2lj\nkvy9SB0y/QvJSpIkhBD9SafxrSuH2pcsuT2SJAkhRF8L2Iz73bt3U15ezvr16ykrK+PBBx9k/fr1\nXdqUlpayZ88egoKC/NumTZvGqlWrurRbtWoVt912G1dddRVPPfUUr7/+OrfddlugQh90qupb+PMr\n+bTYfWsKFZWbGT8qibc+P05Dk4NRGbFMz02moMyErc3N9DHJQNckKSpMx09vHI8+NpTYCB3/759f\nUFJlBfC372zqqET2HTUytn1hVyGEEP0jWONbyBu1b6idy+0lSCsDQ4QQoi8F7F11x44dzJ8/H4Ds\n7GysVistLS1d2jz++OPce++9ZzzWrl27uOKKKwCYO3cuO3bs6PuAB7H/bjtGi93F1ZcNBeBohZna\nhlYamnwLxb71+XGOVphZ9fpBnt54mDe2lqEoSpckCWBiTgJpCeGEhQQxb3I6AGn6cIYkRpxyzoyk\nSFbeOZ3UhPD+uEQhhBDtdOqOJMnXkyQV7oQQou8FrCfJZDKRm5vrfxwXF4fRaCQiwveBOy8vj2nT\nppGWltZlv9LSUn70ox9htVq5++67mTFjBna7HZ3Od1OIj4/HaDSe9tyxsWFotZpzvga9PvKcjxFo\nTa1ODh5rYFhqFD+8cQI7DxsoqbaSf9T3GsVGBmMw23lqw5eAgj42lHd3lKMN0mJ3+W6sWUPj0Cd0\nTYS+tWg0pTVNXDMza0Bfh8HwOzidwR4/yDWcDwZ7/KJvadQaNCoNisoDgMvtGeCIhBDiwtNvC9x0\nnlhqsVjIy8vj+eefx2Aw+LdnZmZy9913c9VVV1FZWcmKFSvYsmVLj8fpidlsO+d49fpIjMbmcz5O\noH2yvwq3R2HqyERMphZy0qLZedjAu9uPA3DntWN4cv0BXG4vN34ji5njUnji1QO89VkZocG+RFJx\nuru91vu/NQlgwF6HwfI76Mlgjx/kGs4HF0L8ou/pNDoUpb0nSSrcCSFEnwvYcLvExERMJpP/cX19\nPXq9HoCdO3fS2NjIsmXLuPvuuyksLGTlypUkJSWxePFiVCoVGRkZJCQkYDAYCAsLw+HwDR0zGAwk\nJiYGKuxBZ0ehARVw6ZgkAEZmxABQaWgmITqEkRmx3DZ/BAunDeGq6UOJjgjmpzeNJzxEi73NQ3iI\nFl3Qufe6CSGE6D86dRBeJEkSQohACViSNGPGDDZv3gxAYWEhiYmJ/qF2ixYtYtOmTWzYsIHVq1eT\nm5vLgw8+yMaNG3n22WcBMBqNNDQ0kJSUxOWXX+4/1pYtW5g1a1agwh40FEWhqr6F0morozNj/fOK\nRmbE+tvkDosDYM6kNG6dl+OvUJcYE8oPr8tFBSREh/Z77EIIIc5NsEZ3MkmSOUlCCNHnAjbcbvLk\nyeTm5rJ06VJUKhUPP/wweXl5REZGsmDBgm73mTdvHvfddx8fffQRLpeLRx55BJ1Ox09/+lPuv/9+\n1q9fT2pqKkuWLAlU2IPCkXIz/3jzkL+aXefqc0mxoURH6LC2OMnNjOvxGGOz4rlv6UTCQ4N6bCOE\nEOL8pNPo8OCrQOqWniQhhOhzAZ2TdN9993V5PGrUqFPapKens3btWgAiIiL417/+dUqbxMREnn/+\n+cAEOQi4PV52FhqYMlJPiE7D+o9LabW7mDg8gTR9uH+oHfgWep2co2f3kXpGZ8ae5qgw+jRJlBBC\niPOXThOEBzegyHA7IYQIgH4r3CC+vk/yq3nlwxIOlJqYOymNckMzl4xK5K4lY7tt/635Odx54wTs\nLY5+jlQIIUR/0Kl1KCigkiRJCCECQZKkQeDzgloA9hcbKa2yALB4ekaP7bUaNRGhQZIkCSHEBUqn\nOblWksxJEkKIvidLdJ9nakytOF0n17yoMDRTWd9CTno04SFammwuRg+NJTM5agCjFEIIMZB0mvb5\npGqv9CQJIUQASJJ0HtlWUMNvntnFG1uP+bd19CItmpbB964eQ3SEjhtmZQ1UiEIIcVEoLi5m/vz5\nrFu37pTnNmzYwC233MLSpUt55JFHUBSFXbt2MX36dJYvX87y5ct59NFHAxpfcHtPkkrjliRJCCEC\nQIbbnScOHWvgxfeOArCvuJ6lVwzH41XYedhAZFgQ47Lj0WrUTMyZOcCRCiHEhc1ms/Hoo49y2WWX\nnfKc3W7n3Xff5eWXXyYoKIgVK1aQn58PwLRp01i1alW/xKhTdwy3k54kIYQIBOlJGmBF5Wb+9dYh\nVr1RgFqtYmhyJI1NbVTWt7C/2EiL3cVlucloNfKrEkKI/qDT6VizZk23C5eHhoby4osvEhQUhN1u\np6Wlxb9Qer/G2NGTpPbInCQhhAgA6UkaIDWmVl79qIRDxxsBSI4LY+kVw7G3efj3xkK+LDX5n5sz\nKW0gQxVCiIuKVqtFqz397fHpp5/mpZdeYsWKFQwZMoSamhpKS0v50Y9+hNVq5e6772bGjBkBi9E/\nJ0mG2wkhREBIkjQAmlqdPLZ2L/Y2D2MyY1kyM4vstChUKhU2hwu1SsXH+dVYW5yMzYojOS5soEMW\nQgjRyZ133smKFSv4wQ9+wJQpU8jMzOTuu+/mqquuorKykhUrVrBlyxZ0Ol2Px4iNDUOr1Xyt88c1\nRvp+UHvRaDXo9ZFf6zgDbbDG3Zlcw8Ab7PHD4L+GwR5/dyRJGgCbdpZjb/Nw4zeyWDx9KCqVyv9c\nWEgQI4ZEc6TCV+p7/pQhAxWmEEKIr7BYLJSUlDB16lRCQkKYPXs2+/fvZ8qUKSxevBiAjIwMEhIS\nMBgMDBnS83u42Wz72nG47ArgG25nbrJjNDZ/7WMNFL0+clDG3Zlcw8Ab7PHD4L+GCyH+7shEl35m\nbm7jk/xq4qKCuXJqRpcEqcOE4QkAJMWGMjYrrr9DFEII0QO3280DDzxAa2srAAcPHmTYsGFs3LiR\nZ599FgCj0UhDQwNJSUkBiyPIXwLcQ5vTc/rGQgghzpr0JPWzTTvKcbm9XHt5JkHa7nPUaaOT+GR/\nNUtmZaHuJokSQggROIcOHeJPf/oT1dXVaLVaNm/ezLx580hPT2fBggX85Cc/YcWKFWi1WkaOHMkV\nV1xBa2sr9913Hx999BEul4tHHnnktEPtzlWwfzFZSZKEECIQJEnqRy63l20FNcRHhTBjXEqP7WIj\ng3n8R6eWnhVCCBF4Y8eOZe3atT0+/81vfpNvfvObXbZFRETwr3/9K9Ch+XWUAFdpPDhckiQJIURf\nk+F2/ehEXRNOt5eJwxOkpLcQQoivrXMJ8DZJkoQQos/JJ/V+VFzpK8YwIiNmgCMRQggxmHUkSdog\nrwy3E0KIAJAkqR8VV1oBGJEePcCRCCGEGMw61klSayVJEkKIQJA5SQFmbXXi9SpEh+sorbaQFBtK\ndETwQIclhBBiEOso3KDWemVOkhBCBIAkSQHk9So8vm4frQ43P7h2DPY2D1NGylA7IYQQ50anbu9J\n0nhxSE+SEEL0OUmSAii/xIjBbAfgX28VAjByiCRJQgghzo2/cIPGV7jBqyiyZIQQQvQhmZMUQFv2\nVAKQpg/H3uYGYIQkSUKIAWRz2fjTnr/ypfHQQIcizoFWrUWjUqNS+3qRnDLkTggh+pQkSQFyvLaJ\nkiorY7PiuPub49Bp1cRHhZAQHTLQoQkhLmJl1hNUNFfzpbFwoEMR50in1UF7kiTFG4QQom/JcLsA\n8Hi9vPX5cQCuvGQISbFh/OrbU9CoVahkOIQQYgAZbEYAGhyNAxyJOFchmmBsKt8oBYfLg9RNFUKI\nviM9SX3M5fbyrzcLKShrYER6NLnD4nzbQ0w4gxsGODohBofqllry6w8OdBjnrbzSd/i//f9EUZSz\n3tfQ2p4k2c19HZboZzqtDkUlPUlCCBEIAU2SVq5cya233srSpUspKCjots2TTz7J8uXL/Y///Oc/\nc+utt3LjjTeyZcsWAB544AGuvfZali9fzvLly/n0008DGfY5efWjEvYVGxmVEcM9N0/w9xw9d+hl\nni548Wt9qBEXDpO9kV21+y6Iv4P99QVUt9R22WZorefX2x9jd93+LtudHhefVm7H6XGd8bitLht/\nO7CGZw+to9Vl69OY+1KxuYxic1m/n9ereNlZu5dSy3Esbdaz3r+jJ8nqbMLjlQ/Wg1mIRoe3oydJ\nkiQhhOhTAUuSdu/eTXl5OevXr+exxx7jscceO6VNaWkpe/bs8T/euXMnJSUlrF+/nmeeeYaVK1f6\nn/vFL37B2rVrWbt2LXPmzAlU2OdEURT2Ha0nOkLHz2+eQGiwbzSjx+vB6myi2dVCna1+gKMc/Opt\nJu7f9jsOmg4PdChnbUv5J7xUtJ4TTZUDHco5abCbefbQOl4rfqvL9jdK38HSZj0lefis+gteK3mL\nvYb8Mx77teKNNDtbUFCob/9Af75xed38u+BFni/8T7+fu95m9CePNa11X2t/8CVbXyfJEuePYG0w\nHnxJkhRuEEKIvhWwJGnHjh3Mnz8fgOzsbKxWKy0tLV3aPP7449x7773+x1OnTuWvf/0rAFFRUdjt\ndjyewfPGX9doo8nmYuSQGHRBGv/2Jmez/+eSAfjm+UJT1FhMi6uVI40lvd7nfOm5aXI2AXDU3PvY\nz0eHG48AUNlcjVfxAlDUUExhg2/7Vz98FzYcBaDBfvp5MAdNh9lj2I9G5fv/U28z9WncfeVIYzEO\nj4MmZzN2t/2M7b2Kl48qPsN0huvvjTLrCf/PNS1nlyTZXHaaXSffhxscMuRuMAvWBqHgBZWslSSE\nEH0tYIUbTCYTubm5/sdxcXEYjUYiIiIAyMvLY9q0aaSlpfnbaDQawsLCAHj99deZPXs2Go3vw9K6\ndet4/vnniY+P56GHHiIuLq7Hc8fGhqHVanp8vrf0+sizar+/zPcBaMqY5C77WhtOzkWqsFee9XG/\nrv46TyB1dw2mE75vwpu9Tb26xrePfMhbRzbz1KLfEhXSv6/JV+NzKm0AHGs5fl79ftweN1pN17eD\nL+sOs7XwOKMShpMalYQKFVHBEWjUGkqP+JJ9h6cNT4gdfUQib+19FxUqNGoNLe5m//U5XA7KrL5C\nJjZae7xuRVHYtO8D1Co1yyd+kxfyX6NF1bvf8Zn09Wt9uKzI/7Mr2E5GXOJp2xfUFZFX+g5Hm4p5\neO69p23bnc7xVx+r9v/c6Gk447UpisKW0s/IiR+Gp70SWlhQKDaXHVeQHb0+ErfXg1Z97u+Zon8F\na4J9P6h9ayUJIYToO/1W3a7zN/kWi4W8vDyef/55DAbDKW0//PBDXn/9dZ577jkArr/+emJiYhg9\nejRPP/00q1ev5re//W2P5zKbz30eg14fidHYfOaGnew77PtWNyUmpMu+FaaT13jIcJT6+qaAV7n7\nOvGfb3q6hlJjOQC1VmOvrnFPRQFNbS1sK9nPtOTJfR5nT7qL32L3fYt/1FhGdV2Df0HIgbSx7H0+\nr97Jzyf/iNSIZP/2f+xci7nN0qVtdnQmP534AwoMR/zbDpQXEx1cR2VTLZcmT6GiuQqTzeK/9gJj\noX/uy+l+Z0caS6iwVjMlcQLDQ3MAOGGqxmhs5qDpMAVG3/DKUXE5TEma0OP1KIrCx5XbyI0fSXJ4\nUp/9X/AqXlSocCse9lR/6d9+tLqcKM+pX9q4PC40ag1qlZp95b5y24X1xXxR/CU5sVm9Pu9X4y80\nlBCqDcXtdXHcVNntte2q3Ud0cBSj4nKoaq7h2f2vkhGZzpz0GQCMiMnmgPEQJ4y17PMW8b/7VvOd\n0bdySfKkXsd1NvGLwNBp298/1B7pSRJCiD4WsOF2iYmJmEwnh8rU19ej1+sB39yjxsZGli1bxt13\n301hYaF//tG2bdv417/+xZo1a4iM9N1cL7vsMkaPHg3AvHnzKC4uDlTY56SkykJ4iJbUhPAu261t\nviFWQWotzc4WDDYjTo/zvJw0rSgKH5R/Svl5OmfG4/X452GYHI29GkbXMQ/saGNpQGPrjVZXKwBu\nxUOZ5UTAz+f0OM/Y5pj1BK1uG2sOvoTN5Rs61uxswdxmYVjsEOYNmcWUxAmkRaRQZj3BhuI3cXqc\nDIn09QJXNFdxuH043eTE8cQER2N322lrP/fhxpP/XxsdFnrySeU2AOYOmUV0cBRB6iCMNhOKovDK\nkTy+qN3NF7W7+c+R1/1D/LpT22ogr/Qd3ix774zX3tPr8VrxW+yty6fZeXJo2n+OvMEDn/+ejWXv\nYXc7SA33JZQdc3y21+zyDzdscjbz6y8eI6/0HQDKrOX+42w68SHlTZW8evS/vHR4Pa8e/W+X85yO\nta0Zk72B7OihJIcnUWurP+W1aPM4WVu0gbVFG/AqXv/vpqK5ioL2eXyj40YA0Gg386XxEF7Fy3sn\nPjpvhqWK3glp/5JFJT1JQgjR5wKWJM2YMYPNmzcDUFhYSGJion+o3aJFi9i0aRMbNmxg9erV5Obm\n8uCDD9Lc3Myf//xn/veGHVAAACAASURBVP3vfxMTE+M/1k9/+lMqK30f2nft2kVOTk6gwv7aGpsc\nmKwOctJjUH+ll8jaPidpbLwv0Xv3+BZ+9fkfWHfktX6P80wMtnreLNvE2qIN5+UHJoPNiNvbPlHZ\n4zxj9TOH2+GfH3PEXDKg16QoCja33T/f5kgv5iW1eZy4elERrjuHG45y79bfnLECW2P7vJR6u4mX\nitajKAoVzb4hXZNTxnFjzrXcMXYZPxx3O1q1li9qfcVWrhw6FxUqKpurOdx4FK1ay4jYbGKCfau1\nWNqsKIrC4YYjhGpDyYhMw9Jm7TbBMbTWc6jhCMOihjIsOgO1Sk1iWAIGu4kGRyNWZxNj40cxJXEC\nDk8b1aeZi9Px+y4xHzvrLyKKzf+fvfMMjKs80/Z1pmmq+qh3y7Zsy93YYGPA1NiBhEASSGCdDbtp\nm+yGZJNswrdZFgghjfTNBrKBQICEmoRisOnFvVu2bKv3MhppZjS9nu/HmTkaWSNZsiRsk3P90syc\n8p4zRe/9Ps9zP8388uDveKtrOw/X/4k7d/4AX9iHKIocHjiKJ+zljbiYu6Z8g3zfvGEfT5x4loeP\nPYEv7Ofdrp14wz529e4nFA3R5mqn0JTPgux5NDia+NG+X/Fu90529+3n3e6d7Ozdm3I8ne4enjr6\ngnzPWuL1SFUZFRSZCojEIgz4R7cW6PH0ISLiDLpoG+6gfuik/NqhAclWvSYukgYDQzQ6WwBpMSEh\naL1h34RCVOHcIE0TT7dTK5EkBQUFhZlm1tLtVqxYwaJFi7j55psRBIE777yT5557DovFwlVXXZVy\nny1btuBwOLj99tvl5374wx9yyy23cPvtt2MwGDAajdx3332zNewzpqFTWiGfV5o55rXheCRpVf4y\nDg7UccAm2aEn+pWcS/TFV8V7vf2ccDTKK87nCp3xyXuaWkcwGmIwMIRZZxp3+/4kdzRn0IXNN0C+\naeL6kdnCHwkQE2PMz6qm2dnKySFJtI2XehmNRfn+np9h0Oj5xsovo1FN7eu6r/8QAE3OFuZlzUm5\nTUyM4Qi6KE8vRSOoqbPXY/MNyPe5KrtM3jbHkMXlpevZ1v4mWpWG2pwF5BlzaRvuIByLsCB7Hjq1\nbkQkBVwgigwGHCzPWwJAh7sbd8hDRlr6qHG817MbgA2lF8vPWQ25dHt65e/L/Oy5GNR69tsO0+xq\npdRSlPKaEpHbQDRAh7uLgvyx38lUdAx38b+HHyImxvjU/Bs4MdTIwYE6ml1t5Bvz8EX8VGVU4A37\n0Kk0LM9bwh9PPI3NZ5fFiz8S4LWOt9kevx5/xM/rHe8QioWZk1HBRUUX0OhsocxSwsaKKzBpjfxo\n36/k/U/lqYa/0uJqo0RXytysOXJt15zMStTxGqJeTx/5Rqu8T0+SLfvOnr00u9ooMhUwGBgiGA2R\nptaRo88iQ2fB5hvAE/Zh0Zpxhz282v4mu3v3sd92GLPWxMKc+dw073r0Gv2k7qHC+0uaJimSpIgk\nBQUFhRllVmuSvvGNb4x6XFNTM2abkpIS/vjHPwJw0003cdNNN43ZpqioiGeffXZ2BjlDnOiYQCTF\nU2mqs6oos5QQFaMM+ofwR0/vinUmhGMRhgNnVoNhSxJub3S+e86JpC5PDwALc2o4aDuC3T9ERlo6\nL7e+xhVll5JnzB21fZ9XSrUrNRfR6enhpKNp2iLJE/Ji1BpQCVMLxPoiUtQrMy2DqowKGpzNfHfH\nfazMX8r1czaNEUtHB49jj0cJXml7g2urrp70uZLTrCZyiHMFh4mJMXL12dRkz6XZ1Ub9UAOd7i4A\nKrNKEb0j219dvoED/YepzChHp9ZSaimWhejCnPnx65MEkDPoYjAgmZnMz6pmID6OoYATi86MJ+wl\nXSel1NYPnkSn0rLEOmL2kngvd/XuA6R6KEN8st7ibJPra8ZcU5Kb5ImhJlZTe9r7BbCjdy+hWJjP\nLvwUqwqWk2vIkUSSs41ARDLcWJ63mA0lFxMTY6hVanINOQz47XLqpIDA1vY3AKjOrKTJ2cq2jrek\n8WdWUpFexk8uuRuNoJbf7xx9Ni3OdmJijAGfnbe6tvPhqqtxhzyyeGp2tTM3aw5NjhY0Kg1llhI5\nnbHH28cyFsvX0e3tlceys3cfIiJLrbUMBRzs7ttPvtGKIAhk67NpHZbSAFcXrKDd3SlHlQpM+QQi\nAfb3H2ZTxVWKSDpHSVOP1CQFw5GzOxgFBQWFDxiz2kz274VINMaBhgEsRi3lBeYxr7tCw2gENSaN\nkW+u+gp3rP4a6TqLPPGaaZ448QxffvE/p2wPDCORl6y0TOoHT9LnHWuscTbp8vQiILAkdyEgpYrt\n6NnDez27+cXBB8YIgkQ90qXxCfUJx5nXJcXEGC+1bOPb793NX5pemvL+idRAk9bI9dWbWGatJRCV\nIg82/1ghk4iumLUmtra/Qae7Z9Ln6vL0yFbPqY6dIFEjlK3PkgXxscETdLq7MWtN5BiyRm1v0Oj5\nrwu/yWcW3gxAmaVEfm1hdkIkjaTbJZrNllqKyNJLCwiOoJPtPXu4473v0eJqwxl00eezUZ1ZhTYp\nWpZnkERSv28AnVpHibkIqyEXi9ZMs6tt3NTJhM06TM1qPSFIF+VKabEV6VLaX7OrjXa3lO5bbilF\nEAQ5ipNvyMUfCXDYfhQBgavKLwNAJaj4zMKbydBZ5LqwqowKQKpNTBbEVRkVeCM++n0DbGl7jXe6\nd/J0w9/kaBRAs6sVT8hLp6eHqowKdGotRaZ8AHpO+Y52x78jy/IWIyLdo0U582XTknyjtF/yezs3\nq4qNFVeSptZxRdkl3HHB7Xxv7R387NLvYTXmTPoeKry/pCnGDQoKCgqzhiKSZoC6lkE8/jBrFuaj\nVo29pcNBNxadBUEQ5OiDXpOGPxKY8bG4Qx729x8mGA3x2Imnp1yT0e8bQCWouL56EwDb2t+a8TGe\nKaIo0uXuxmrIodhcCEjmDY1OKQXJGXTxi4MPyOlWINW6ACzKrSFHn02Do+mMai1iYowHjvyBLW2v\nISLybvcuPGHv6XdMwiOLJBPl6aV8bvFmrp8j3edTTSWGAg6ODzZQmV7GZxbeTEyM8fMDv+XFlm0E\n4p+baCzKMw3P05JkCpAgEUUCKZI0nqBI1CNl6zPJ0mdSZCqg0dHMYMBBmaUkZSqgWjUSBUmYN+To\ns+SUrxGRNCxP2AtNBbJIGgo4OGo/jojIjp698rXPz64edZ68pBSyyvQy+bxVmRU4g65xTSAS739W\nWiatrvZJL0YM+ocwa01ytEqvSaPEXEjHcCfNzlZUgmpMil9ijDafnRJLEVeXbyBXn826ojVk67NY\nlidFeDJ06eToRwvOBHMyKwDpPTs8ILng7es/xLvdu7BozeSZcmh1tXMiXi9UkyXdp8y0DAwaPW2u\nDrn/lCiKdHv6yDPmsiYuikwaI+XppczPquZT829gU+UVgCSMQYo4zcmopCZ7Lj+55G5uqL5Wvtdq\nxRb8nCZhAS6olXQ7BQUFhZlGEUkzwI46KWKzrrZwzGuiKDIcco+pwdBrDIRj4Rl3uNvVu4+oGCVT\nn077cKdcZJ4KT8g7phGmzTdAriGbFXlLKDYXsqfvAB3x1KtUxMSYLESmwjtdO3h1igLMEXTii/gp\nthTJE06bd4DWeM3FtZXX4Ay6eKXtdXmfPt8ABo0Bi9bMvKw5+CMBes8gOtaXZCywseIKwrEw27t3\nn37HJBLOdiatUX6uJlsyITk14rGzZy8iImuL1rAwZz6fmn8DGpWal9te46mGvwHQOtzBm13vsTXp\nehMcGzwZn/xW4I/4xzW4GBFJ0v1cmDOfiCh9JsssxSn3SaY8vZQ8Yy5ri9bIwilTPxJJ6vH0YTXk\nkKbWkZ02IpISaWQHbXUcHZR6DtVkjTZkSU6dnBOPwiT/PV4djyvoRiWoWJG/hIgY5cRAauOKPq+N\nPxz7E+6Qh5gYYyjgIMcw2sp7TkYlETFKh7ubQlP+GMv25ChLIh3wrrXf5ub5HwNguVWqxarOrBy3\n9ixxPVvb3iAcC7MqfxkalYZILMKFhatYmDcPfyTAm13bgRExKQgCS3IX4Qg6+e9dP+Kl1ldxBl34\nI36KzIXUZM+jyFTAhUWrUAkqBEHg4uILZWGXeM9LzIUYtQaAKaeQKpxdkiNJirudgoKCwsyi/Eec\nJh5/mENNdoqtJsryx6baeSM+omKUDN3oXiGG+AqgPzoSTYrGojx2/Gma4pGRqSKKIjt69qBVabj7\n8n/HojXzUus2fONMkO/f/z/8X91jI9cS8uKN+Mg3WlEJKm6ovhYRkWcbXxg3EvFe927u3v0TXu94\nZ9LjdAZdPN34PH9rfnnU5N0X9vHEiWfkGpRTSbi0VaSXotfoMWmNNLlaCccizM2aw9Xll5Grz2ZH\nzx4cASfRWJQBv50CYx6CIFCeXgowaXvzQb9DtsROpCEuy6vlirJLSFPreLtrh+y0Nx4xMSanLCaO\nlSyScg055OizaHA0yxEuURTZ3bcfvTpN7gd0cfGF3L32O6TrLJyMpwwmzBWanG2jomO+sI9WVzsV\n6WVUZEjGCzb/SK1Zq6uDP9Y/RSgakkVSIsqzKF5XBCNRoolIU+u488Jv8aGKy+XnTBojGpWG9uEO\nvBEfRfGoX+Icxwcb8EX8qAQVgWiAA7YjmLWmUT2agFFRnap4tAVG0taaxxNJoWHSdRYWZMXTB20n\nx2wjiiKPn3iGvf0HOTRwFFdwmIgYJVc/WiQln7ci/vlJJs8wEu2ak1k55vXqzEo2L7iJj87ZmHKs\nAAWmPIwaA954zdq1ldfwseoPk66zcHHxhdTkSqYbbcMdGDT6USmOty74RDytL50tra+yt+8gAMWm\nQrQqDf9vzde5ofralOdNpDPOHcfUQ+HcJyGSNJqYIpIUFBQUZhhFJE2TPcf7icZE1tUWplwpHg5K\nReSWtNEiKVEInZwK1OPtZ2fvXv7WvOWMxtLobMHmt7M8bwkFljwuL1tPOBbhoK1uzLbRWBSb307r\ncLssgBJCILHSXJM9l8W5C2hytvLDfb/klwcfHCMwDg8cBeAvTS+l7EMUiATGRJre7dpJTIwhIsop\nRO3Dndy39xds79kzbopfXbzHy+K4lXqOPlsWB3Mzq1Cr1FxTcQURMcqrHW8x4LcTE2MUxI0aEpPc\nttOIpF5vPw/WPcp/7byPR4//edS9yTdaMWgMrC1cjSs0zPMtr0yYzvVK2+vcs/t+OtxdI5EkjXHU\nNvOz5uKL+GXRM+C3MxhwsCB73khhNpIgKbOU4Ay6GA65ZROLQDRAV1K90ltd2xERWZgzL6muZ6Qu\n6c3Od9nVt49jgycZCo7UJIEkQBLnLE2ajE8FQRDITMuQDRSK4+LHrDWhVWnkGqlLi9cm3YPqMVEM\nQRAoMOajFtRUpo+47JVaitCpdRywHR4TFRRFkeHgMBm6dCoypPe7xTE2HfHgQJ0cierx9Mr1SLmG\n0fU3yRGscksKkZSUEliVUZ7yXqwpXDkmQpWMSlDJwq8yvRyrMYfLStZx38XfJdeQLYskkJrAJt8n\nlaBidcEKOXL1Uus2YOSeT8S8rDncWvMJNlZccdptFc5NEul2Gp2o1CQpKCgozDCKSJoGvkCYF3a0\noVGruHBRfsptXPEi8lMjSQmRlFyXlCjwbnG1M+Ab3ftkMuztOwDAuqI1AFyQvxyAPf0HxmybWLUO\nRkNyX5lkIZDghuprSddZ6PX0cdLRxO+PPiaLgmA0RJOzhQxdOipBxUPHHscTGl2n81zTi9y9+yf8\n4sADNDlbCUXDvNuzC028V1D9YAOBSIBfH/o/HAEnJo0Rm29Adu5KEI6GqR9qIM+YK7vTJdd4VMdX\n8dcUrCBHn8X2nj1yelLieopMBWhVGjomEEnukIf79/+GwwNSIX6TsxVRFOWGoYljbShdj1lr4vWO\nd7hz5w/kJqLJRGJR3uveBUCXu0e+58mRJICaePpUQmQmIkXzs8f2A0ukwHUMd9GdJIwSrmTHBk+w\npfU1stIyuaR4rTyJH0gSSW3DHfF9mhkKODFoDHLERqPScEnxWhZkzyNbPznr7FRkJqWXFpulOh5B\nEORoEsC64jWUxl87tR4pwadrbuQry/5plLuaRqXh49XX4Q37+MXBB0aZi/gifiJilIy0dAwaA1ZD\nDi2OzlGR0HA0zF+bXkItqBEQ6Pb0Yo/X9JxqVJGRli5Hl8pTRJLSdWZMGiNWQ45ci3UmJD6/qwuW\nj3mt0JKPWSvZ3Kf6TAAsyqmhzFIip0omavYmQhAELiq6AOMpn0eF8wd9vE+SWqOk2ykoKCjMNIpI\nmgbPvNWMyxPiunUVZJrTUm6TiCRl6EbXJBnkSFKSSIqNCIO9KYTN6WhytmLQ6OUV7Sx9JnMzq2hy\ntmL3D/Jc04v8tUmKUiWnuSUc4EaEwIhFdp7Ryn0Xf5dfbLiPq8ouYzDg4PmWVwBocDQREaOsKVzJ\nVeWX4Ql7aYpP1hMkmpI2OJv52YH/5b49P8Mb9nF52SVYtGbqh06ys3cfvoifjZVXsrpgBSLiqF4v\nif1D0RCL4652gLw6X2jKx6KTUh3VKjXXV3+YmBiTBUoikqRWqSkxF9Ht7SM0ToPWF1u34Y/4+WjV\nRlbkLcEfCTAYcNDvG0AtqMmJT5hzDFnceeG32FhxJcFokAePPDJGKB3oqZOjKQP+wVHudsnMixfi\nJ8TRiYSRQdZY4VCWLkV32oY76PH2kxWv82l0NjPod/DwsT+hVqn5/OLNmHUmua4n8d66gm4G4yl2\njY4WhgKOMWLo+upNfGXZP49bQzMZkgVD8oQ9O00SISaNkXyjlasrLqfQlM+S3EVjjgFQZC6Q708y\n64rXcNO863GHPDx2/Bn5+YRpQ3o8cltmKcEb8slW5NFYlD8ef4rBgIPLStaRb7TS7enDHn89Vz/W\nye3i4guZlzmHQtPYhRBBEPjS0s/yucWbJ74hp+HSkrV8ZuHN8gLHqeeozqwCRmrYUm2zqfJKAPRq\nvRwZVPhgk2mQ/q+odCHFuEFBQUFhhlFE0hnyRsMhdnheosiqZ+OasnG3S0SS0k9Nt4unSQSiYyNJ\nAHv6DoxbB5SK4ZAbm99OZUb5qHSc1QUrAfj5gQd4veMd3o5HV0aJpHg6XKpIUjIfrryKfGMeb3dt\np8HRRP2glCq3MHs+5fHUrGQLblEUsfsHKTDl8+8rv8yC7HnY/HbUgppLS9ayIGcewyE3L7Vui0cw\nLqI47h6WSCXrdPfgDLo4Ek+1S55MJwTL3MzRNRUr8pbw3TXfYE3BSsotpaNTptJLiYkx+fjJ9Hj6\n2N69m3xjHleUXUJJYizubvp9A+Qacka5fRm1Bq6tupovLvksgiDwYN2jcgNXgNdbtst/jxZJo5vf\nWnRmSsxFNDlbsPuHaHA0ka3PwmoYO2FP1Ant7TtIVIyyMGceuYYcmpxtPH7iafwRPzfNu14WUxm6\ndHQqrZzi1jY8knrW4+0jFA1NK2I0HgmRpFenjTp+IpJUlSl9TlfkLeE/1/y7LHKnwiUla5mXOYfW\n4XZZHJ0auU3crw53NzExxqPHn2S/7TBVGRVsqryKYnMhgWhArnfLTZEWd1X5ZXx1xRfGdXqrzCif\nVORmInRqHasLVox7jo/PvY6vLP3ncb+bALU5C1huXczaogumJXAVzh+y4yYpojagpNspKCgozDCz\n2kz2g8w7HftQZ/dz2SIjGvX4WnM4HklIP9W4IUW6XSLFTCOoGfAPcnTwOItyaiblOJWwgU4WBADL\n82p5suEvOOK1J6FYmGA0NMq+OhFJ6o87wZlPmcQn0Kq1/MOCT/LTA7/h90cfR6PSoFdLkauBeE1H\nck8eb8SHPxKgOrOSqoxyvrLsn2kb7kAURTLTMliUPZ89fQfwRwKsLVwtiwWQ0tP63DZ+tO+XqBBQ\nqdSYtMZRdR8Lc+ZRYi7iosJVY8aaZ8xl88KxjYmTzRuSjxUTYzzd+DwiIjdUf1iOOgEcdzTij/iZ\nG1/NP5Wa7Ll8cclneaDuER4+9gQNjibKLaUc6j1GeXqpXPMiIKBVadGptWOOcXnpeh49/iS/q3sU\nX8TPUmttyoluhi4di84sRz5KzMXERJGdvXs56WiiNqeGiwovkLcXBAGrMZeBuA14q0tKtZubWSWn\n6M1G1CEhkorMBaM+v7JIOuVzeqYsti6kwdnMUftx1hWvkcVSwk0yYXLQ6e4mHA2zr/8QVRnlfHnp\nbeg1aRSZC9lvO0yrqx2VoJpWytxskhW3aJ8IQRD458X/8D6NSOFcQKfRYdQYCGv8RGMikWhswv9H\nCgoKCgqTR/k1PUO8EalRp8EysbuZnG43xgJ8bLpdOJ4CtjJ/GQC/PfIH7njve7Sm6INzKs1xR7xT\nRZJBY2BjxZUssy6WU9XcIY9sIgBSL6FoLIrdP0i+0TrhKnRlRhk3zr0OT9iLM+iiJrsatUpNriEb\nAUFO6wJSFsNXpJdRGRcnNdnzEJDOtaH0YgAKTXmoBTWdnh52dO4nJsbQqLSEoiGW5C4aNeHONeTw\nndW3y1GTyZCIeDU6mnn8+DP85vBDDAUcbGl9jQZHE7U5C1iUUwMgR5IOxY0vJlrFr8mey3+s+jeK\nTAVs79nDEyefRURkfdGF5BhysMcjSaem2iW4oGA5ZZYSOcJVkyLFDKSJcLK7WYmlSBZvBo2eT9Xc\nOOb9yzNaCcXCuELDtA63IyBwZdml8uuzK5JGR1iW5C6kIr2MlXlLZ+Q8i3Okz3Qi0nhqemuir1HH\ncBfv9exCQOAzCz8lf/8SBgciItn6LKUvkMJ5R2ZaBlG15JypRJMUFBQUZg4lknSGBEUpdcobdU+4\nnSs0jICARTs6nciQwt0uGK9JWmJdxBLrIg7ZjrK3/wCvtL3Ol5beNuF5ml1tqAV1yuLyhD3zc00v\nAgmRNLomqcHZTFSMptz/VC4tXkvHcBe7+/ZTG3ea06g05OizRqXb2X2pHcMSmHUmLitZBwKy/bNG\npaHQlE+Pp5edHTHUgpr/vuhbdLq7qUgfP61xsliNuRg0eg7bj8nPfX/Pz/BHAuTos9m88CZZZKTr\nLGToLHJdUd4EIgmk2qdvrvoKRwaOEYqFsWZlMkdfzaGBo/R5+wlGQynrWkByKbtx7nX87MD/AjBv\nHCMDkFLIjg2eQECg2FxIvtFKVUYFl5euTxkJSTjcNTpaaB/uotCUz/zsuWhVGsKxyKyIpLmZVVRn\nVrImnu6ZPPZvrvrKjJ3Hasyh0JTPSUcjwWhoTHqrUWsk35RLk6uVSCzCgux5o1LqktPkTrX/VlA4\nH8hIS6fH2weqCMFQFLNhbKRaQUFBQWHqKCLpDBBFkYgQQADZGW48HAEXZp1pzAq1Xh1PtxtVkyRF\nktJUOhbkzGOZtZZ+Xz/1Qw24Q55x6zZC0RCd7m7KLSVjml0mkxBqnrBnlImAO+Rhe88eAJZbF094\nPSBFM26p+Tir8peNKiTPM1qpHzqJP+LHoDEwEHcMS1Vbk+Dj8z4y5rkScxFdnh7aXd0syqnBojOz\nMKl/z3RQCSqqMyupsx/n8tL1WA05PNv0IlqVls8v3jwm0lNsKcI1KPXZmSiSlECn1rEq7lBmtVoY\nGHDL1x8TY2Psv5Opzqzk6vINBCLBMemZySQc7vKMuaSpdaSpdfz7yn8Zd/vFuQt5reNtHj3+JDEx\nRmVGGVqVhor0MhqdLbNSk2TWmfjaii/N+HFTsSR3EVvb3+DEUONIup1uRCxWZpfR3ymJ91ONEbLS\nMjFo9JJInsCmW0HhXCWxMCLoggQUhzsFBQWFGUMRSWeANxAGjRQBcgbGF0n+iJ/BwFBKl7KU7nbx\nmiRtUs3KBQUr6Gh8gf22w1LUJQVtw53ExNhp6zwSIis5klSVUU6d/TiHbHVYdGbmJDXPnAi1Sj1G\nuFiNuTB0EpvPTnl66bi9Z05HiaUI+qS/JyPapsotNZ/AHfLI0asF2fOJilHZBW/UWMxF1E9BJKUi\n+frHS7dLMFHT0QQV6WWoBfWk63oqM8r4p9pb+L+jUuPgynQp3fGKskswaY2yRff5yuLchWxtf4MD\ntsO4Qm4pcqsbqauryipjV+cBLDozS5LcEUES/EWmQppdrSlNGxQUznUSdvuCNqA43CkoKCjMIEpN\n0hnQPeREUEnOc44JIkndHmmmX5JiEprob5GqT1JyA9GVecsQENjTN74leMKZ63QCJ1kkJYwb5mRI\n/VlERJZZF0/KJGI8EnbTiZ48A3GzgpwppnOVxFOg1IKKJdbU1tDTwaIzywIJpJStVAIJRtzRTBoj\nZl1qQ4vTkTz5Pp1ImgwZael8+4KvckP1tZPeZ6m1ls/V/gMLsudRmyulSC7OXcjnFm9Gqzq/10rK\n00soMhWwr/8Qne5u0nXmUZ/jBVZpkWJd0ZqUNUclFunzNlUxr6BwLpCRFEkKhiaukVVQUFBQmDzn\n9+zoLNHtGmn0OlG6XWe8R1DCACAZOZIUHdsnSacaiSRlpFmoyZ7L8aEG+n0DY6IZoWiI97p3oVen\njbHCPpVEup07nm4nIIyq81mRN72oTaL2pT/ucGf3D5Klz0QzxUl4iaUInVrHkoIFMyIqpkNCsJ2u\nHmkiktMNZ6pxZ7LImyyJWrcPGipBxa0LPsGP9/2acCxMRtpowTs/dw7fWvWvKRcrANYWrsYZHKYm\nK3UPIgWFcxk5kqQLEAzHzvJoFBQUFD44KJGkM8Dmdsp/u0MeIrHUq3cJp7JUk7M09dhIUsIC/NS6\nolVxt7tTm5UCvNe9C3fYw2Ul6zBqDROO+9R0O5PWSKFZMhIwa01yVOlMSQgJm2+AUFRyUzuT1XmD\nxsAdF3yNr6z+zLTGMxNYDblsKLmYy8vWn/ExsuPOfzAzkSSFsZSnl3JV+WUApJ/SuDnx+njOdSWW\nIj6/ePNpvz8KCuciGUnpdoGwEklSUFBQmCmUSNIZYPeORI9ERFzBYbno+2/NL3NiqIGvr/wy3e4e\ntCpNyloWlaBC9wN28AAAIABJREFUr04b5W6XsABPO0UkJVK+Ek1fE4SiYV7teJs0tY4Nk5jEJ/of\neUJevGEvJq0Rs9bENeWXk2+0Ttv+OFufKfV48g3K9UjWM6zzsBpzMOoMeJnYPXC2EQQhpbnEVNCq\nNGSmZeAIOsc0klWYOTZVXIk37B3VcFhB4YNO5qh0O6UmSUFBQWGmUETSGeAIDIMecvU52AODOIIu\nWSTt6TuAM+jioO0Ivd5+is1F44oPfdxVK0FQNm4YLZLyDLkICGNE0ttd2xkOubm6fMO4DWCT0aq1\n6NV6hkNufBG/XEP0kTkfmvzFT4BKUJFryMHmHzhj04YPKlZDDo6gE7MSSZo1tGotn675+NkehoLC\n+4pZa0KFipguoPRJUlBQUJhBlHS7M8ATkhrJVmRIPYUSdUmOgFP++/nmV4iIUbkoPBV6jf6UmqQw\nKkGFRhgtqrRqLTmGbPp9IyKp3zfAS63bMGtNXFF2yaTHnq4zM+AflOyoZ2HCnme04o8E2G87DCgi\nKYHVKN0HJd1OQUFhJlEJKkwaM2iDODwB2oY7iIlKbZKCgoLCdFFE0hngi0nOcOWWEmBEJLUOd8jb\nOIJS3dJ4xeIABnUa/kgAUZSc8kLREDqVVm5mmkyBMQ9P2Isn5CUmxnj8+NOEYxFumv+xSUWREph1\nZsIxKa3PpJn51K9ED599/YeAiXsk/T1xack6Li9dT7nl9M16FRQUFKZClj4DQRuk0XuUH+/7NXv7\nDp7tISkoKCic9yjpdlMkEo0Rxo8aKEuXJryOgCSIWl3tAKwrWi03Zy2Ji4ZU6DV6YmKMcCyCTq2V\nRNI4zWDzTVaODh6nz2dj0D9Es6uNZdbFrMhbMqXxJzeknY2oxjUVl1ORUUajo4WoGKXYPH4k7e+J\nYnMhN8697mwPQ0FB4QNItiGDDk8ntjQpgt823MmawpVneVQKCgoK5zeKSJoiQ+4ggjaIShwxZEhE\nktqGO1AJKj4yZyMHbXX4IwGKTONbNevjNuD+SEASSbHwuCKpwCi50PV7bbLL3UfPoJbIkhR1mg2R\npBJULMiex4LseTN+bAUFBQWFsSTMG2IaPwC93r6zORwFBQWFDwSzKpK+//3vc/jwYQRB4I477mDJ\nkrFRj/vvv59Dhw7xxz/+cdx9ent7+da3vkU0GsVqtfLjH/8YnS61mJhthlwBBG2INMGISWtEI6hx\nBF1EYhE63N0Umwsxa038w4JPMhxyy01jU2FQj/RKysBCMBrCqEltQ1xgkgRZt7eXk44mrIacM+rd\nkxxJmkqanoKCgoLCuUlCJAFoVVp6vH2IopgydVtBQUFBYXLMWk3Snj17aG9v58knn+Tee+/l3nvv\nHbNNU1MTe/fuPe0+v/zlL/n0pz/NE088QXl5Oc8888xsDfu0DLh8oA1h0phQCSoy0jJwBlx0eXqI\nxCJUxpuzLrEu4uLiCyc8VkJABeIOd+FoaIz9d4ICo9Qgc1//IQLRIAtzas5o/OZZTrdTUFBQUHh/\nSfRKinnSKTVU4A37cIc9Z3lUCgoKCuc3syaSdu7cyZVXXgnAnDlzcLlceDyjf7R/8IMf8LWvfe20\n++zevZsrrrgCgA0bNrBz587ZGvZp6XM5EQSRdJ0FkFbwhkNuDg8cA6AiLpImQ3K6XTQWJSJGx9h/\nJzBqjVh0ZrxhHwALzzCdzaJVRJKCgoLCB4nqzErMqkzC3XPRx7IA6PEoKXcKCgoK02HWRJLdbicr\nK0t+nJ2dzcDAgPz4ueeeY/Xq1RQXF592H7/fL6fX5eTkjDrO+02nYwiAXFOmNEZ9JiIi29rfBKAy\no3zSxzJoEul2QUIxqUdSmlo77vaJaJJGpWFe1pypD55TjRuUdDsFBQWF851sfRa3VX2JmMuKEJAW\n8Hq9/Wd5VAoKCgrnN++bcUPC5hrA6XTy3HPP8fDDD9PfP/4PefI+Ez13KllZRjSa1A1cp4LVahnz\nnM3tBDNU5OVhtVq4efm1WJsziYkiRen5LCqvnPzx3ZLQ0hrAkiml3lkMxpTnBajIKabR2cKivHkU\nF5zeWjvVcYK6fPnvsoI8sgypz3WuMN69OF8438cPyjWcC5zv4z8faWho4F/+5V/4x3/8R2699dZR\nrz311FM888wzqFQqampquPPOOxEEYVJ1uLNFXpaUGRB0G8GsmDcoKCgoTJdZE0l5eXnY7Xb5sc1m\nw2qVjAZ27drF0NAQt9xyC6FQiI6ODr7//e+Pu4/RaCQQCKDX6+nv7ycvL2/CczscvmmP32q1MDDg\nHvVcMBxl0O9CB2giaQwMuNFj4cOlG+VtTt1nIiKSERE2h5MelRShEiOqcY+Rqc4GYK6l+rTnSTV+\ngHBopJA3MBxjwDP58b7fjHcN5wvn+/hBuYZzgQ/C+M83fD4f99xzDxdddNGY1/x+Py+99BKPP/44\nWq2WzZs3c/DgQSKRiFxT29zczB133MGTTz75vo05w6xDq1HhGtShsqjo8SiRJAUFBYXpMGvpduvW\nrWPr1q0AHDt2jLy8PMxmKdXrQx/6EFu2bOGpp57i17/+NYsWLeKOO+4Yd5+1a9fKz2/bto3169fP\n1rAnpMfuRdAEgdFpa2dKsnFDosHreBbgABcVXsDH536Ei4vWnPE5jVoDAgJpah0aleIAr6CgoHAq\nOp2O3/3udykX5AwGA4888gharRa/34/H48FqtU6qDnc2UQkCeZkGBpwB8gy59MYd7hQUFBQUzoxZ\nmyWvWLGCRYsWcfPNNyMIAnfeeSfPPfccFouFq666atL7APzrv/4r//Ef/8GTTz5JUVER119//WwN\ne0K6bB4EneREl3ATmg6JmiR/NEAwKtUk6VTj1ySlqXVsKL14WudUCSoy0zLQqhWBpKCgoJAKjUaD\nRjPxb+SDDz7Io48+yubNmyktLcVut7No0SL59URNbWJx8P0gL8tAt92LVZ9Hn8+GI+gkW591+h0V\nFBQUFMYwqzPlb3zjG6Me19SMta0uKSmReySl2gek1L2HH3545gc4RToHRkRSZlrmtI+nT/RJigQI\nRRPGDbPf/+m22ltQKf0zFBQU/s4IhUIMDg5SWFg47WN9/vOfZ/PmzXzuc59j5cqVY15/v+pnk9MZ\ny4syONhoJ9eYD0NH8aqHmW+dvOPqRPhCfl5v2c7cnEpqrGdmHJSK8zEd81SUazj7nO/jh/P/Gs73\n8adCCSdMgS6bB8EcQC2oseim7wwnu9tFgrJI0k7gbjdTVE3BgU9BQUHhfOaBBx7AaDTy8Y9/nBtv\nvBGTycS6deu4/fbbz+h4TqeTxsZGLrjgAvR6PZdccgkHDhyYsA53PKZbP3tqvZolTRJckWHpf0tj\nbzul2un/3u/vP8xTDX/FE/YyN7OK21d8cdrHhPO/3g6UazgXON/HD+f/NXwQxp+KSdUkNTU1cf/9\n98uPv/Od79DQ0DAzIztPEEWRTpsHtT5IZloGKmH65VzGeJ8id9j7vkaSFBQUFP5eePPNN7n11lt5\n5ZVX2LBhA08//TQHDhw44+NFIhG+/e1v4/V6Aairq6OysnLCOtz3i4LsuMOdxwBAv2/67TJEUeSJ\nE88QiobQCGo8Ye+0j6mgoKBwPjCpSNJdd93FV7/6VfnxjTfeyD333DMqTe6DjtMTwhsMYVAHyNJP\nP1UDQKvSYNIaGQ4OE0oYN6gUkaSgoKAwU2g0GgRB4J133mHz5s0AxGKxCfc5evQoP/zhD+nu7kaj\n0bB161Yuv/xySkpKuOqqq/jyl7/M5s2b0Wg0zJ8/nyuuuAJBEFLW1L6fFFklUeYc1EAW2Hz20+xx\negLRIIFokNqcBfT5bPjC03ePVVBQUDgfmJRIikajrFq1Sn68atWqvzvXnE6bB0EbBAGyZqAeKUGG\nLh1H0Dli3KBEkhQUFBRmDIvFwuc//3n6+vpYvnw5b775JsJpajJra2snXAS84YYbuOGGG8Y8n6qm\n9v0k3ajFbNDSaw+SVZCJzT99keQOSSk0Fp0Zd8hDT9A17WMqKChMDX/Ez7b2t7i6/DIMGsNZHUuf\nt59H6p/kHxZ8kiJzwbSPJ4riaX+TzxaTyhmzWCw88cQTNDc309jYyEMPPYTJNP2anPOJriTThiz9\nDIqktHT8kYCcwqCIJAUFBYWZ4/777+eTn/wkf/jDHwBIS0vjhz/84dkd1CwhCAJFuSYGHH6shlyc\nQReBSHBax3SHpP9NFp0Zo9ZAOBYhFA1P+TjHBk/yjXfupN9rm9Z4FP6+cAXdvNbxNtFY9GwP5ayy\ns3cf29rfZFfv/lk/lyiKvNO1gwO2Iynv+67e/XS4u9jVt2/a54qJMX6075c8WPfotI81G0xKJN13\n330cO3aM22+/na9//eu0t7dz3333zfbYzim6B7wIOqn7a1ZaxowdN2ElPhBPi5jIAlxBQUFBYWoM\nDQ2RlZVFdnY2Tz31FC+++CJ+v/9sD2vWKM41IQImQVrMG5hmNMkdlno9WXRmjPEVbF9k6il3h2xH\n8Ef8NLvapjUehanR6mqnbbjjvBUZzze/zF+aXuLY4ImzPZSzSpe7B5iZOsPT0ePt48mGv/L7o4/x\n3R330T7cOer1BkczAI2Olmmfq37wJB3uburs9XJt/kQ4Ak7u3vUTjtqPy88FIoFpj2M8JiWSsrOz\n+dznPscLL7zACy+8wE033UR2dvasDepcpMfuRW2QVuRmMpKUqYuLpPg/MsW4QUFBQWHm+M53voNW\nq6W+vp6nn36aa665hu9973tne1izRlGulOWhDkv1SbZpTqrkdDutWTYb8oWnLjLb4hOtoYBzWuNR\nmDyOgJP79/+GH+/7Nf/x3l0csB2ZcHtX0M1fm7bI6f9nm2A0xMEBacwd7u6zPJqzS5dHEkl93v5Z\nP1dCiJWYi3CFhnmj8135NX/ET4e7C4BOdzf+yPQWnN7p3glIEaW2U8RYKo4OnqDfZ+O1jrcBqLPX\n84137qTJ2TqtcYzHpETSz372Mx544AH58YMPPshPfvKTWRnQuUgsJtIz6MVkiQAz0yMpQSKSZPMN\nAqB7HyzAFRQUFP5eEASBJUuW8Oqrr3LLLbdw6aWXfqBraovjIinklaI+0zVvcIdGIkkmOZI0tYlR\nIBKgNz65cwQVkfR+0enuRkSk1FyEPxJg92lStd7t3sGrHW9x8DRi6v3ikK1OFmydHyCR5I/4CU8h\nZTUSi9AXT1M9NZIUioZoH+6c0d+0xMLKdVXXoFfrR937JmcrIiJ6dRoiIk3OVvb1HeSWZ/6Nf3/7\nu9yz+36GQ6mtwEVRZDjk5sRQIx3DXdj9g9QPnpQzqJonIXTaXB3yOJxBF693vIOIiEU7OyVAkzJu\n2L17N3/+85/lxz//+c/51Kc+NSsDOhcZcPkJR2LojCECQJZ+5tPtAlEpXKjUJCkoKCjMHD6fjyNH\njrB161Yee+wxQqEQw8PDZ3tYs0aRVZosuB1pkClNqsLRMG3DnczJrJhy+4pkkWTQSiLJO0WHu8Rk\nHaToxpkwHHIjIGDRvb+26ucz3Z4+AK6tuoY/nXzutEKjM57S1TvJaIU/4icYDZF5BiUIgUiAvzRv\nYV3hasrSS1Jus6dPsurXq9PkSMr5jM03wCttb7C3/yC1OQv4wpLPTGq/Pq+NqCilSw6H3PjCfozx\n7+ILLVt5o/Ndrp+ziavKLxuzb6e7G1dwmNrcBVMYp7SwkmfMpcxSTKOzBX8kgEGjl1PtNpRezMtt\nr1M/eJIj9noQRSxpZvq8/ezu3T9qLM6gi7e7dnDIVjfKTMaiMyMict2cD/Fs4wuTSsVtHW4HQERk\nS+trNDpbmJdVTb4pb9LXNxUm9WsZDocJhUbCr16vl0gkMisDOhfpGYj3hdAG0Kq0mDTGGTt2QiQl\nUESSgoKCwsxx22238d3vfldOE//Vr37Ftddee7aHNWukG3VYjFpsNhGNoMbms/P4iWf4+cHfsrXt\nzVHb+sL+06bvJERSus4i/++bTCTJGXTxavtbeMO+UWk0kxVJvrAPZ9xJLybG+Mm+/+FnB/6XmDix\nffv7iTvkwRc6d+vbuuPCothcSKmlGFdoGFdw/IafCSHS4+077bEjsQg/3f+/3Lfn50RiU58Pbml9\njfe6d/F8yyspX7f7BznpaKIqo5y5WVU4gy6GQ27s/kHe6tp+RudMJibG3teIckyM8YuDD7K7bz+i\nKHJ08PionmOxWIyXWrbx+PGneeLEs/R4Rt6DxPti0EhNovt9NvmY+/sPA/DX5i0pI4UPH/sTD9Q9\nMqWFjQG/HZWgIkefTWl6MSIiXXGB3ehoRiOoubx0PRqVhne7d+EMutg4bwPfXPWvqAU1e/sPjrru\n3xx+iG3tb+IMulicu4APlV/Owuz5uEMeLDoz64svIt+YR4urjXA0zGPHn2Zn71hTCG/YR79vgBJz\nESpBxfae3QCsL75w0tc2VSYVSbr55pvZtGkTtbW1xGIx6urq+MxnJqeAPwh026UPckjwkqXPmFGr\nwgzdKSJJ6ZOkoKCgMGNs2rSJTZs24XQ6cblcfP3rXz9n7WZniuJcEyc7nJQbcujy9NDulkTKS63b\nKEsvZlFODQB/OvksdfZ6vr/uP+V6o1Nxhz0ICJi0Rnn1eqJeSZFYhAO2Izzd8Dd8ET99PhvBuMOe\nRWtmKOiclOXv748+Tqenm7sv+jYd7m4GA0MANMVXjs82kivXryjOyOeLi24728NJSbe3F4PGQGZa\nBqWWYurs9XR5uslIqxmzrTvkkUVprye1cA5EAjiDLgpM+bzW8Y4sptqGO6nOrEyxfRC1So1WNXqq\n2ee18WbXewCcGGpkKOAgW58FgCfk5fETz3Bs8AQiIhcWrMIRdFFnP06nu4fX2t+iwdlMw1ATt9Xe\ngkY1qWnsKLxhH3fv+jEXFV7A9dWbprz/mdA+3Ikz6GJV/jKKTAU83/IKR+3HubBQaq9zsO8YW9pe\nk7cfCjj4yrJ/BqDb0wvA0txadvXto89rozKjnFZXB67QMPOyqulyd/PYiaeZlzVHrpvv89pkQXXS\n0cSKvCWTGqvNZydXn41apabcIkX5OtzdFJkL6fL0Up1ZiVFrpDK9jEZnC3p1Gh+puZrgsMjCnPnU\n2evp8fRRZC5gV+9+uj29rMxbyq0LPjEqENDj6UOn1qJVaajOrGB7zx4eqf8zBwfqOGI/xqq8pWiT\nSlASiy21uQuw6MwcH2ogXWdhae6iM3pPJsOkIkmf+MQnuO+++9i0aRPXXXcdX/3qV3nyySdnbVDn\nGj12LwhRAjHfjPZIAml1TkD6ZyEgjPkxUVBQUFA4c/bv38+VV17Jxo0bufrqq9m4cSN1dXVne1iz\nSlHc4S5dnSWn6dw49zrUKjUPH/sT7pCHaCxK/eBJwrHIhI5Z7pAHk9aISlBhnCCS5I8E+PWh/+Mb\n7/wXj9T/mUgsQmZaBrt793PC0YRZa6Iqs4JILDJqBT0VMTFGi6sNb9jHnr4D7O8/JL82ExbIzqCL\nvzW/PGl79HA0jP8UB61ebz9DAQcN9pZzKrqVIBQNMeAbpMRciCAIlFmKAegYTp1yl5zO5gg6x1wv\nwBMnnuWe3ffzwJFHeKXtNXnucmKoccy2kViEe3b/hN8efnhUxEYURZ5tfIGYGGNJ7iJERHb3HpBf\nf63jbY7Yj1FgyuOG6mu5qOgCSuNjP2Q7QoOzGQGBw/ZjPHTsiTO690ftUhRnV+8+YmIMT8jLrw7+\njuODDYD0+fjN4YdGRXOmS/3gSQCWWxezzFoLwOGBY/Lrb7buAOBLSz5LVUY5x4ca5O9ll7sHAYEV\n+ZLISTyfMLW4suwSNlZcQUyMyelwAEeSjp/qPUqFL+zDE/ZiNeYCUCqLpC5ZuM7NmgPA/Ky5AGwo\nXU96mpQGe0H+cgD29h8kEAnyYssraFVabph77ZhMqSJzAbmGHADmZFTGr0n6bfaGffLfHcNdBCIB\n2lxSql1lehmrC1YAsK5oNWqVelLXdiZMakZ+77338t5772G32ykrK6Ozs5Pbbjs3V05mg267F51R\nSjecaZGkVqkx60y4Qx60au0HfoVTQUFB4f3kpz/9Kb/5zW+YN28eAPX19dx77708/vjjZ3lks0ex\nVZqwaMIWAJZZa7m8dD2RWIS/Nb/MQdsRSixFBKKSSLD57FRmlKc8ljvkkWtOJookNTqaOT7UQK4+\nm4U5NWwoXUePt5/f1T2KP+KnNmcB2fEV7qGAY8LaIpvPTigmFba/3bUDT9iLRWcmTaXjoO0In5z3\nUfTx1KMz4fnmV9jdt58MXTqXla477fYPHn2Ubncv/3XhN9Fr0qTrdUr2x8FoiKGAg1xDDt2eXjLS\n0jHPUhE5SJNdR9DFRfEIxHj0ePsQESkyFwLIQqPTM45IitcjZeuzGAo46PX2U5X0mQhEghyxH0NA\n4IhdmnzfNO9jPNXwV046mriWq0cdL1FY7wy6OD7UwMKc+QAcsB2hfugkNVlz2bzwJu547x529e7l\nmooNgDS5Nmj0fHPlV+QoQqmlCEBOwbp5/sfY33+YwwNHeaPzXT6VN7X02cT43WEPbcMdNDvbOOFo\nRBAEFuTMY3vPHo4NniAzLYNP19x42uOFYxECkcCEn+n6oQZUgor52dUYNAYKjHkcH2ogFA0RiobZ\n31NHkamARTk1BKMhWlztvNO1g4/P/Qjdnl6shhzKLaUA9Pn6iYkxDtrqMGgMzM+qls/d5GxhTeFK\nAA7bj6ESVOhU2kmLpETNUJ5BEklWQw4GjZ6O4S56vf0ICKzOlwTKZaXrMGkNXFR4gbz/4tyF6NVp\n7OjZw4mhRlwhNxsrrjht3dqczAr57xvnXsezjS/wXvduYmKMPx5/ilJzETq19N2rSC+TItsaAzXZ\ncyd1XWfKpCJJR44c4eWXX6ampoZnn32Whx566APdZyKZaCxG76CXXOnzMqOmDQkSNuBpSqqdgoKC\nwoyiUqlkgQSwcOFC1OrZW3k8F6gslMSR2l3Mwuz53FAtTSITq7wHbXWjJk0D/sGUx4nEIvgifszx\nCZhJO34kKeG+dcPc67hp/vXkGa0szV1EWXwluiK9VF5kdMTTusYjEdXQqjT0+Wx4wl5W5C3lwsJV\nhGLh01pZT4Q75JEjU4cHjp52e2fQxfHBBlyhYXb07pGfT7Yc7vX24w37+NG+X/HgkUemVesiiuK4\nfV+isSh/qP8Tjx1/io7hrgmPk0jRKomLpAxdOhadeVzzhsTzic9I7yl1SccGTxCORbim4nI+s/Bm\nbqi+lvXFF1KeXkrbcMeYMR8dHOlj80LLK7Kz2ZMNf0Gr0nLT/I9h0OhZnrcEe2CIJmeLLKyWWxeP\nSrPKSsvEpDXGXdX0rC5YwT/V3opFZ+aF5lfocE5sSCE1R93J8cEGQtGwLFhAiuYk3tcGRzP+iJ+6\neATmqL0+ZaSqzl7Pzw/8Vl4seK7xRe7c+YNx6708YS/tw51UppdjiDtELrXWEo5JY9nXf4hoLMqF\nhasQBIFl1loydOns6t1H63A73oiPYnMhZp0Jk9ZIv3eAtnj63pLchWhUGkrMRejVaTS5pM+lM+ii\nbbiD6swqarLnMhgYYiDuopwQHs83vyLfn7e6ttPoaBll2gCSO2ippQSb3y6lzeUvxWqUoj8GjZ5L\nStaOeq90ai0r85fiCXvpcHcxN7OKK8sum/D9AcjRZ7M0vphzeel65mdV0+xq5YkTzyIg0OnpodnV\nitWQg1lnQhAEanMXnFG65VSYlEjS6aTJezgcRhRFamtrOXDgwGn2+mBgc/iJqn1os6Xw5kxHkgAy\n4gpbsf9WUFBQmFlUKhVbt27F4/Hg8XjYsmXLB14klVjNaNQq+nu0fHnZP5FjkPoaZukzqcoop9HZ\nwv4koTFew9lEWlzCXjfRTDZVEXivT6pjKUxymRIEgU/M+yhFpgKW5S2WayVOZ96QiGpcU36F/Nyq\n/KWsKVyJgMA7XTumlGYViUU4aj9OMBpie88eImIUlaCi0dmCJzRx6t9BW53szPd6xztEYhFEUaTJ\nOdJIs9fTT6urnUgsQrOrLWXPlh5PH99697/Z13dwzGvJvNbxNt989795u2sHvrCfh44+zr27f4o3\n7KN+6KRspPFi6zZASnNMvp/h+PgSznbFcZEkTXaLGQo45Pc1JsbocHcRjUXp8vSgV+tZHHdB6/X0\ns71nN/+14z56vf1y6tOKvCWsLljBFWWXIAgC87OqiYkxmpytxMQY0VgUURSpsx8nTa1jmbWWDnc3\nzzQ+z0NHH8cb9vHRORvlSfi6ojWAlMr3dpeUcnZBPJUqgSAIlJqlSNiqgmXo1DrMOhO31nyCiBjl\nV7v/MObz8Lu6P/LLgw8SioY5OFDHkw1/4YG6P/B213ZC0RDriy9Cp9LybvdObD47GkFNVIyyvWcP\nnXGR7gq5x4jKaCzKMw3P0+hsoX5ISs+rHzxBMBri0EDqNN6TQ42IiCzMGVmsWWqV6miebvgbr3a8\nhVpQySlkapWa9cUXEogGuX//bwAoiUfTCox5DPgHefLkXwBYmb8UAJWgojKjHJvPznDITZ29XjpP\n7iI52nLCIY13W/tb7Ordx9b2N+gY7qLOXs/TDX/jkfo/0x9f7Eik2wFyXRLAVZMQPDdUX8fXVnyJ\nH6+/i9tXfFGOvk6EIAh8fvFmbpx7HQAXx80YYmKMLy75R2riqX0V6WWnPdZMMikJVllZyeOPP86q\nVav47Gc/S2VlJW73+A4pHyTe7dhL2tK3scWz4IrMBTN+joTDneJsp6CgoDCz3HXXXdxzzz1897vf\nRRAEli5dyt133322hzWraNQqygvMtPa4CYajpGlHROFy62JaXO30efsps5TQ4+0bt5dSot9Juk6K\nTGlVWjQqjRxJSkRMBEGgz2tDo9KQox/daL4qo5z/t+brAATj6X1DAceE409Eki4tuYj6oRP4IwEq\n0stQCSpW5i9lX/8htvfsGdfVShRFTjqayErLQK/R839HH6PF1UauIYdQNESaWsflpet5ue116uz1\nXFQ0ki4zV//sAAAgAElEQVTkCXl54uSz9Hr6+OqKL3DQdkSqB8lbwn7bYfb2H6Iqoxx3yEOZpYQO\ndxc93n45dRFgW/ubzM2qGjWmd7p34g37+EvzFpZaa0etvidTZz9OTIzxVMNfebFlq3yvt7a9wWD8\nvuXqszk2eIIXW7bxZud7BKIBMtMyUAtqhgIOSi1FhGMRBAQKTfnyscvMxdQPnqTT3c38rGr+fPI5\ntvfsYVnnQmw+O3MyK+TtW4bb2dN3AG/Ex++PPsagf4g8Yy5FptFzoPlZ1Wxtf4PXO97hTyefw6w1\ncUvNx7H7B1lmreUjczZyxF7PW13bAZibWcWlJWvl/edkVnB1+Qa2tb/JgH+QzLSMlCYQ87OqaXA2\nc3HRyHtem7uA1QUr2NN3gKP24yyJCw9v2MfhgaOIiDxS/yeanW2oBBXhWIS/Nm8BYGXeUpxBlxxN\n/MicjTzX9CIvtb4qn++ko4kj9nryjXmccDSyKKeGIwNHscdNRJqdbczPqpYfH7Qd4dKStezpO8Cg\nf4iNlVcCUB+vdVqYPX/kvbCUcGnJWnb27CUUC7O6ZNmodL1LStbKVuyZ+gxZTOYb82h2tdHl6WFd\n0epRx6zOrOL4UANNzlZ29EjRsSXWhURjsfj46jBpTbzUug2DRo8/bsPuiH+uHEEn7/bsAkbS7WAk\nVXNhznxZrE2EXpOW8j2cCktyF3JB/nLmZ8+lNncBczIreKFl22nTTGeaSYmku+66C5fLRXp6Oi+9\n9BKDg4N84QtfmO2xnXWcQRc7HK9BTM0FWeu5ZsHKUT84M4UikhQUFBRmlk9/+tNyjacoilRXS45o\nHo+Hb3/72x/omiSAqsIMmruH6eh3M7dkJANiWd5inm16EYAF2fOkAn+/fZTjXDQWRUTEHY+yJNLt\nBEHAqDHgD/vxR/zctfPHXFJyER+quII+n418o3XCIuqsNMnB7LSRJE8P2fosjFoj/7b8CyCKcnrU\nDdXXctR+gr81v8wya23KOpCXWrfxctvrgLTCHhNjlKeX0jHchYjIJcVrWV2wkpfbXufQwFFZJLUP\nd/LAkT/giovDB488Sru7k7mZVXys+sMcGjjKltZXWVMg1XysKVxJr6+fPm8frqALAYFSSxH1Q5IQ\nSUwuQ9Ew++Ipfs6gSxZ4nZ5uyiwl8rWFYxE63J3kGnJIU+vo9vRydfkG9vYd5O2u7YhAkamAT867\nnp8f/C0vt72GTq2jNmcB7cOdRIhQYimiIx79yDdaR80rEuN5oXkr2/W7OThQh1al4VCfFHUoNRej\n1+jJ1mfRHncSy9FnyZP1ZdbFY+qmqzLK0ao0NDib5ev7n8O/B2BRzgLyjVb+Y9W/4Qg60QgaqrOq\nxvTquq7qGnq9fdTZj3NB/vKUvbyuKLuENYUrx7RNubLsUvb0HeDNru2ySEo0PNUIag7FRdD1czbR\n57Wxq28fZq2JyowylvgXcnjgKDn6bDaUXsybne/JzY4/Oe967tvzMw4NHKXB0UyLq43qzEp8YT8C\nAipBRYurjZa4mUDivG3DHTx+4hkisQir8peTY8ji2NAJzFrTKIEhCAKfnHc9H5vzYdrdXSwun4Pf\nNRINM2mN3FZ7y5j7UGwp/P/s3WdgXOWV8PH/naaZUS+jXi3JkiVZLnKvuIFtTIkTg4GgkLAmbAiE\nZL0J+M3iZElI3ewu7G4SICEQnMSOMQQwYFPdu2U1W733XkZ9yvthpLElS+6yLPv8vkQzc+fOcwc5\nmjPnPOdAtSPIWx+3dtB/j4HA5O2CnTR1NzMjYKqza6CfwZfc5gJymwtQcGRtdpV8Tk6zo+x2sl8C\nWQ1n6OjrRKOonVlfcAQsy8IWObM714NGpeGRxLPzWA0aA/dNvOe6vb5zHZdykKIoeHk53rC77rpr\nVBd0o7Db7fw15y366KWvLJEpkTMJcjWNymsN7EkamDoshBDi6jz99NNjvYQxFRXsyP4UVbUNCpJ8\n9N5EeoRT0lZGvE8sVR011HTW0dHXiZvOUVb3f+l/xNzXwZKwBQB4nBOIGLVG2nvbqWivor3PzNGa\nk8wOnEGvtZdA44UHOrrrXNEoapp6Rg6SWnvaae81k9zf1ndox1dPFw/umnAHf8//B+8V7TpvY/3e\nioN8WPIpfnofwj1CqTTXMD94FkvDFlLYWsLRmhOsjFyKp4sHwa6B5DTn023pRq/Rsz3/Xdp6zdwb\nvZqi1lLnBv9p/sl4671YGbmUncUf82F/q+aJXtGEugdS0VaNgkKgqz9rJqzk/9L/wBcVB3h40n0A\nZNRn0WXpYl7QTI7XpfNRyafsqThAXVcDCb5xfD3hQYxaA+XtlfTZLCT4TGRt7F209rThZ/AhyDWA\n10//DYA5QTOI9Z7AnKAZNHY18UDc2vMGaZ6oPcVfc3c4u48NmOgdQ5RHOMVtZZS2lxPiFsQTUx5l\nR8l7HK9MJ8rTUcoU7BpAU3czblpXvj/jKV469QoV5qphW0hr1VqWhy+m0lzDmgm387fctynqHwo6\n0Go+1D34ghkIlaLikYQHOVR9jNlDSu0GqFXq8wIkcJQTJvnHkVWXS6W5mhC3IPL7A7avJT7A+0W7\n8dS5szRsIb02xxcCSb6TUCkqkv0SifAIY3HIPFSKisl+CeytPEi4ewiBrv5M9I7hdJOjK523i5ez\njNKRhWqjqLWEM/0ld0m+8WQ15vDb9NecM5zSG7IIdw+lvdfM/ODZwwZ/WrWWGK8o3HSudHHxCq15\nQbPw1XuT4BN33vki3EMd/766m3FR6/hSzJ3Ox/4p6WFON+Zg7usg3D2Uid4xuKhdyDmej1Fj4KuT\n1rHlzHYyGrLxM/oNOrdW7ehOdyuSftMjyK7LI6sxB18llIr6UHSa0athH/iH7yKZJCGEuCZmzZo1\n1ksYUxOCHXtdi6raznvs3uhVpDdkE+0ZSVaDY4N9XVcDbjpXOvo6yW0uwI6drMYcgEHZGqPGQG1H\nHVX92YX6rkay+4+7WKWFSlHh5eJ5wUxSRX/3tQt9qF4YMoePy74grS6D9XFfQqWoOFF7ik/L9lHa\nXo671o1vT93g3GA+IMYralAZ0DT/yews/phD1ceZ6B1NUWspCT5xrIi4jY6+Tn52tJLW3jammiYD\nsDpqBWpFzbtFH+GqNRLo6k+oZxDFLY6sywTPCCb5xOKmdSWnKd+ZnRvoyrY84jbcde7sKv2MDksn\nwa6BnG7M5VcnXuLpaf/sDC6iPSPRqjT49e8lmxEwlc/K91HTUcvMQEdjhYEAbDgpAVNJNiWhUQZ/\nbjFqDWyc8W0auprIbyki2S8BV62Rf5m3geNFp537PYLdgshqzOGOiCW46Vx5cuoGKsxVzkzUUGsm\n3OH8+dGkh/jlsRfxN5rwdHEfcY1D6TUuzqD8cq2euJSsuly+KN/PQ5PWUdBchEalYbLvJGe7bZWi\nwqAy8L2UbzmfZ9Qa+P6MJ523UwKmsLfyICkBUwFHV8jTTblMNSXx9cQH2Vn8MUdrTrIychnHatMo\nbC3mSPVxFBTuiV5NVn8QEuIWRJW5hvT6LGdTlEudUXQxOrWWyX4Jwz6mVWuJ8AinsLWY1VErBnWU\nC3MPdnYJHBDhEcYjCQ/g1d+RcWnYQkf7dePoJATGIwmSRtDU5fg/cT8mUIGCVnNJPS6uyECQpJUg\nSQghxDVg8tTjZtBSXH1+kBTrHe2cdTIQSNR3NjDBM8JZqgRnu7+5ac8GSa5aA3bslLSVOe/b07/f\nJPASytG99V7ktxTR1/9t+1ADTRtC3UYOktQqNYm+cRyoOkppWzkalYY/Zv8FBYUk30ncE73qvABp\nOItC5vFp2V52lXzmfN2BkiJXrZHvTv9nWntbB33YvyNyqbOMTaWoCPM8u84ojwhUioqJ3tGcrMug\nrrMerVpLbnMBEzwjCDCaWBm5FHedG4m+cfgZfNmR/z6fV+xnV+mnNHc7uv5FD9nPoVJUfHvKP2Hu\nMzv3h13MhWYu+hl8nAEYON7PCZ6RzttLwxbio/dmXn9rZzed6yW3WvZy8eTf5vwrauX6NUeZHpyE\nn8GXozUnWRAyxznwdKR9XyOJ8YriudkbnU0L5gTNwM/gS4xXFGqVmnuiV3H3hJUoikJ0dyQAvbY+\nQtyCCHYLdGbpHohbyz8KPyS/pYjqjjrctW7Eek24wCtfO6uilpHdmMOS0EsLOAeCbnBc/9cTHhgx\nGL4VSZA0AovNMYDPZnXUe+q0oxckmQx+eLt4DeogIoQQQlwpRVGYEOxBRmEjbR29eLgO/yWcqX+D\n9kCHu4FSJcDZMWxQuV3/QNmilhLnfTWdjo5Yga4XLrcDnHskjtekEWI1kV1RQJ/NwqLQuWhUGueA\nzQsFSeDYBH+g6iinG3OdTRMeTfoq0/wnX3QNA9x0riwJW8iHJZ9wuOY4Xi6eJPWXiAH4GrzxNXif\n97yp57xGqEeQ8+eBuUJx3jGcrMsgt7mAjr5O7Nids2R0at2gjMmXYu4ko+E0B6qOolVp8HbxGrQf\n5Ny1DpRDjjZ3nduITTEuheEqZlhdCZWi4p7oVfwh601+m/GaY+DpFQYl55YuqlVq4nxiBj0+sAfo\n3LliUf0ZuNSE9TR2NRHlGcEUUxL5LUV0WbpYGDJ3VAeenmuSz0Qm+Uy8+IHDUBSFGecETUKCpBEN\n1JTabY5/ENpRLLfTa1z4yfxNo3Z+IYQQt56BICm9oIGFU4YPOgZaMQ90uMtrLkSr0pDkO8nZ9tlt\n0J4kRxvwhu6mc+YetaBSVJgMF8/eDLzemzl/h5yz939Wvhe9Wk97n5kk33jn4NmRxPnEoFJUZDWe\noaWnDVeN0dm++nIsC1/InooDdFq6mBs087I/zIZ5OoIkV40R//4ypYG9QDnNBVS2V6FTaUcst1Kr\n1NwRuYS/5LyFxWZx7uMRl2eaaTIJPnHOPURDuwtea65aI0GuAVR31DoDJn+jn/P3O9kvke357wKQ\nco1K7cT1N6pB0gsvvEB6ejqKorBp0yaSk8/+omzbto3t27ejUqmIj49n8+bNbN++nXfffdd5TFZW\nFmlpaTz88MN0dnZiNDq+wfrBD35AUlLSaC4da38myTqQSRrFcjshhBDiWpufFMT7B0t472AJc5MC\n0ajP/zvm5eKJRqWhvquRjr5Oqsw1xHpNYGbgdNLqM9GpdYP2yw7MSgLHHiRPFw8OVR/D3+B3SYMd\nbwtdgI/em7bedlwMarwUH9p7zews/pi23nbumbCK5RGLz+uiNpRBY3CWB4KjTO5KBksaNAbujVnN\n7tIvWBAy+7Kfb3L1JcQtiAmekc41+xl88NF7k1GfjR07swNT0F8guzI7MIUPiz+luaeF6HPK3sSl\nG+gW99Oj/4HdbifSI+LiT7pKCT5x1HU2DJu18jV4M9Ermpbe1vPKJ8X4MWpB0tGjRyktLWXr1q0U\nFhayadMmtm7dCkBXVxc7d+5ky5YtaLVaUlNTSUtLY926daxbt875/A8//NB5vp/97GeDpqaPNosz\nSHLc1mlv7uGDQgghbi6+nnpumxbCJ8cr2JtexdLp55d0qxQVfgZf6jrrSavLwI6did7RJPhMxKDR\n46kb3FHMqDU6fw509SfCI4xD1ccuaT8SOConBoZmmkzu1Nc7OnrNDJhGp6X7sjb7J/jEOYOkkbqi\nXYr5wbOdc2gul0pRsWnWdwfdNzBk9VD1MYCLznbRqDSsjV3DOwUfkHQF2TDhYDL6smFyKr3WPnSX\nuR/pStw14Q4Wh85zDmse6ltTvoEN+7Bd7cT4MGr/5Q4dOsTy5Y5BWtHR0bS2tmI2OyZFGwwGXn/9\ndbRaLV1dXZjNZkymwd00/vd//5dvfetb5533ehkot7P27y2VTJIQQojx5s65kbho1bx3oISePuuw\nxyT6xNFt7eGvuTsAR2MHrVrLk1M38Ejig4OOHZpJSvSNJ847xhn4XCmtWntZARI4hlsC+Op9iLoO\nmYPLEeft2Mvip/ch5hL2x0z3T+bf5z3j3LMlrkyib/xl7Uu7Glq1dsQAaeBx6Vo8vo3aJ/+Ghga8\nvc/+Y/fx8aG+vn7QMS+//DIrVqxg5cqVhIWFOe/PyMggKChoUOD04osv8tBDD/Hcc8/R3d09Wst2\nGgiSLJaBPUkSJAkhhBhfPF11LJkeQmtHL1lFTcMec2/MatbGrEGjqDFo9ER4OP4eR3iEndc22HVQ\nJikAg0bPU9MeY0r/IM/rKdQtmNsjlrBu4t0XLc+73hJ94wh1C2Zl1PIbbm1CiEtz3Ro32O328+57\n7LHHSE1NZcOGDaSkpJCS4phivX37dr70pS85j0tNTSUuLo7w8HA2b97Mli1bePTRR0d8LW9vI5qr\nbLRgqXZ846YoalQqhaBAz4s848ZjMl3et3I3ovF+DeN9/SDXcCMY7+sXY2tKtC8fHSkjr7yFlLjz\nZ6CoFBXLwhcxxZSExWa5YPvoczNJFxseO9oUReGe6FVjuoaRGLVGnp11aw80FmK8G7Ugyd/fn4aG\nBufturo6Z2aopaWF/Px8Zs6ciV6vZ9GiRZw8edIZJB05coQf/vCHzueuWLHC+fPSpUv54IMPLvja\nzc2dV71+q90RJHV3W9FqdM666fHi3Frv8Wq8X8N4Xz/INdwIbob1i7E1IdgDjVoht7z5gsf5XaB0\naMDAniRPnbuz050QQtyMRq2GbP78+ezatQuA7Oxs/P39cXNztBG1WCw888wzdHR0AJCZmUlUlKP7\nR21tLa6uruh0jjpOu93OI488QlubYyDekSNHiI29tKFmV8NiHSi3k/1IQgghxi+tRs2EIA/Ka810\ndg8/xPVSDZTbBbkGXoulCSHEDWvUMknTp08nMTGR9evXoygKmzdvZseOHbi7u7NixQqeeOIJUlNT\n0Wg0xMXFsWzZMgDq6+vx8Tn7bZaiKNx333088sgjGAwGAgICePLJJ0dr2U4D3e0sfRIkCSGEGN8m\nhnuTV9FKQWULydF+V3weN60rD8WvI9Q96OIHCyHEODaqe5I2btw46HZ8/NkhaWvXrmXt2rXnPScp\nKYlXX3110H2rV69m9erVo7PIEZxt3ACuozhIVgghhBhtcWFevA/kll9dkAQwL3jmtVmUEELcwK5b\n44bxxtK/J6mvz47OVTJJQgghxq/oEA9UikJ2URNNbdmU15l59qvTcdWP/jwZIYQYj+TT/wgGyu36\nLKDVytskhBBi/NLrNEQGuVNWZ+bI6VqqGjo4mVt/8ScKIcQtSj79j8A5TNaqoJNyOyGEEOPcjDh/\nFAWWTg8B4ESeBElCCDESKbcbwUAmCZtKBskKIYQY9+6YFcaS6SG4aNXkV7SSXdxEZ7cFo14+Cggh\nxFDy6X8E1oEgya5IdzshhBDjnqIouGgdlREpcSasNjvphQ0XeZYQQtya5NP/CAbK7bCr0Eq5nRBC\niJtISpw/ACdkX5IQQgxLgqQRWGxWFBRAQSeNG4QQQtxEQvxcCfI1klnUSFfP1Q2YFUKIm5F8+h+B\nxWZBrTgySLInSQghxM1m1qQA+iw2jufWjfVShBDihiOf/kdgsVlR9QdJ0t1OCCHEzWb+5EAUYH9G\n9VgvRQghbjgSJI3AarOi6n97pHGDEEKIm42fp4FJkd7kV7RS09Q51ssRQogbinz6H4HFZnFmkmSY\nrBBCiJvRguQgQLJJQggxlHz6H4FlUCZJyu2EEELcfKbHmjC6aNifWU2LuWeslyOEEDcMCZJGYLFZ\nUJDGDUIIIW5eOq2albPDaevo5Rd/SaO5XQIlIYQACZJGZLVZUeyyJ0kIIcTN7c65EayaHU5tUye/\n2XYKm90+1ksSQogxJ5/+R+CYk+R4e2SYrBBCiJuVoih85bZoZicEUFnfweniprFekhBCjDkJkkZg\nsVnOZpKkcYMQQoibmKIo3D4zDIA9p6rGeDVCCDH25NP/CCw2K0i5nRBCiFtEZKA7Yf5unCpokCYO\nQohbnnz6H4bNbsOOHVAAKbcTQghx81MUhcVTg7Ha7BzIlJbgQohbmwRJw7DYrI4fbJJJEkKIW1Fe\nXh7Lly/nzTffPO+xw4cPc99997F+/XqeffZZbDYbR44cYc6cOTz88MM8/PDDPP/882Ow6qs3JyEQ\nnUbFZycr6eqxjPVyhBBizGjGegE3Iqvd8YfBbu/PJMmeJCGEuGV0dnby/PPPM3fu3GEff+6553jj\njTcIDAzkqaeeYt++fej1embNmsWLL754nVd7bRn1Gm6fFcb7B0vZ+lk+j6yaNNZLEkKIMSGf/ofh\nzCTZZZisEELcanQ6Ha+88gr+/v7DPr5jxw4CAwMB8PHxobm5+Xoub9TdPT+KMH839qZXcyq/YayX\nI4QQY0KCpGFY7Y4gyW4b2JMkb5MQQtwqNBoNer1+xMfd3NwAqKur48CBAyxevBiAgoICHn/8cR54\n4AEOHDhwXdY6GjRqFRvWJKBRK7y+K0fK7oQQt6RRLbd74YUXSE9PR1EUNm3aRHJysvOxbdu2sX37\ndlQqFfHx8WzevJmjR4/yne98h9jYWAAmTpzIv/3bv1FdXc33v/99rFYrJpOJX/3qV+h0ulFbt9U2\nOEiSPUlCCCHO1djYyOOPP87mzZvx9vYmMjKSb3/726xatYry8nJSU1PZvXv3Bf9WeXsb0VxlpYLJ\n5H5Vz7/Qedctm8hfd+fySVoV37grkZ4+K2qVgkZ97f4mjtb6rye5hrE33tcP4/8axvv6hzNqQdLR\no0cpLS1l69atFBYWsmnTJrZu3QpAV1cXO3fuZMuWLWi1WlJTU0lLSwMYtqb7xRdf5MEHH2TVqlX8\n5je/Yfv27Tz44IOjtXQs/Zkkm2SShBBCDGE2m9mwYQNPP/00CxYsACAgIIDVq1cDEB4ejp+fH7W1\ntYSFhY14nubmzqtah8nkTn19+1Wd40IWTw7k4yOlvLu3kI6OHr44VUlsiCffvX8qKkW56vOP9vqv\nB7mGsTfe1w/j/xpuhvUPZ9Q+/R86dIjly5cDEB0dTWtrK2azGQCDwcDrr7+OVqulq6sLs9mMyWQa\n8VxHjhxh2bJlACxZsoRDhw6N1rKBczJJVgWtRoVyDf4YCCGEuDn8/Oc/52tf+xqLFi1y3vfuu+/y\nhz/8AYD6+noaGxsJCAgYqyVeEzqtmgeWxWK12dl9rJy+PhvZJc3slWGzQohbwKhlkhoaGkhMTHTe\n9vHxob6+3lnLDfDyyy/zxhtvkJqaSlhYGFVVVc6a7tbWVr797W8zf/58urq6nCULvr6+1NfXj9ay\nAbD0d7ez2hQptRNCiFtMVlYWv/jFL6isrESj0bBr1y6WLl1KaGgoCxYs4J133qG0tJTt27cDsGbN\nGu688042btzIp59+Sl9fHz/60Y9GtSz8epka68c9C6Kw2ezMSQzgJ28c5+9fFDB5gi++nnqqGzvI\nKmpi2kQ//DwNY71cIYS4Zq5bC3C73X7efY899hipqals2LCBlJSUEWu6L3aeoa62zrtF1b9h16bC\nRacZt3WW43Xd5xrv1zDe1w9yDTeC8b7+8SYpKYk///nPIz6elZU17P2/+93vRmtJY0ZRFO5ZEOW8\nfd+SGF7/KJd//e1B3AxazF19AOSVt/DE2sljtUwhhLjmRi1I8vf3p6HhbOvQuro6Z0ldS0sL+fn5\nzJw5E71ez6JFizh58iQpKSnD1nQbjUa6u7vR6/XU1taO2JZ1wNXWeTe0OOoqLRYwqpRxWWc53utD\nYfxfw3hfP8g13AhuhvWLm8eiKcGYu/o4U9pMQ0s3MSGelNeZyShqpKvHgsFFxi8KIW4Oo1ZLNn/+\nfHbt2gVAdnY2/v7+zlI7i8XCM888Q0dHBwCZmZlERUWNWNM9b94857l2797NwoULR2vZjvXZ+svt\nLDJIVgghhBigKAp3zo1k4/pp/PzxuTz1lWQWJAfRZ7GRXiAzlYQQN49R+8pn+vTpJCYmsn79ehRF\nYfPmzezYsQN3d3dWrFjBE088QWpqKhqNhri4OJYtW0ZHR8ewNd1PPvkkP/jBD9i6dSvBwcHce++9\no7Vs4OycJKtV9iQJIYQQFzIz3p9/7C/m6Jk65iQGDnrsTEkTL76Vyb/cP5WYUM8xWqEQQly+Uc2L\nb9y4cdDt+Ph4589r165l7dq1gx53c3Mbtqbb39+f1157bXQWOYyB7nY2G2i1VzfDQgghhLiZBfu5\nEmpyJau4kc7uPox6rfOxPelV9PRZ2ZteJUGSEGJckTTJMAbmJNltKskkCSGEEBcxc1IAFqudA5k1\nzvv6LDYyChsBSMuvx2K1jdXyhBDiskkEMIyBTBJ2lQySFUIIIS5iXmIgLjo1f/s0n91Hy7Db7Zwp\nbaK714pGrdDRbSG3vGWslymEEJdMIoBhDGSSsCvopNxOCCGEuCBfTz3PPjQdDzcdf/usgH/sL+Zk\nnmOm4V3zHS3ET+SOPOPQZrdjs118xIcQQlwvEiQNw9rf3U4ySUIIIcSlCQ9w54cPz8DPU8+7B0o4\nmFWDh6uOVbPDcTNoOZlXP2IgtO2zAv7lfw/Q3Wu5zqsWQojhSQQwDKvdUTcte5KEEEKIS+frqedf\n1k/Fw6jFYrUzLdYPjVrF9Il+tHX0kl/hKLkrqmrjdzsy6O2zYrPZOZhVQ2tHL8VVbWN8BUII4SAR\nwDDOZpIUdBoptxNCCCEuVYC3ke/eN5WESG+WTQ8FYE6CozX4nlNV2O123tydy84DxRw5U0tBZSvm\nrj4AiqolSBJC3BhkNPYwzu5JknI7IYQQ4nJFBLqzcf005+24cC+CfI0cy6kjJc5ESU07APsyqokN\nOdsavOicTJLdbudkXgN6nZrEKJ/rt3ghhECCpGGd7W6noNNKkCSEEEJcDUVRWDo9lC0f5/HK+6cB\nMHkbKKhopa6pE51GhV6nprg/k1Tb1MmfPswht7wFnUbFS08vRCuVHUKI60gigGFYB2WS5P+UhRBC\niKs1NzEQnVZFb5+NyEB3vn5nIgBtnX0kRPoQHeJJi7mXxtZu/nt7BrnlLXgYtfRabORVtI7x6oUQ\nt6J+gQoAACAASURBVBoJkoZhsckwWSGEEOJaMuo1zEt07E26fVYYcyYH4qp3FLRMjfVjQrAHAO/s\nL6KmqZP5SYH805oEALKLmsZm0UKIW5ZEAMOwnjMnSfYkCSGEENfGuiUxPLl2MrMnBaDVqFmWEoqr\nXsPUGD8mBDmCpAOZNQDcMTuciWFeaDUqsoolSBJCXF+yJ2kYFps0bhBCCCGuNYOLhmkTTc7b9yyI\n4u4FUagUBa3GAwWwA4lRPoSa3ACYGOZFdnETLeYevNxcBp3PbrejKMp1vAIhxK1CIoBhnJtJkhbg\nQgghxOhQFAVVf5BjcNEQ5OcKwB0zw5zHJEY6OttlFzfR02fFYnXMMmzt6GXj/x3kzd2513nVQohb\ngQRJw3B2t7NJJkkIIYS4Xu6eH8mKGWGDWn4nTXD8/PcvCnnyv/by/OvHMXf1seXjPJrbe/girYq6\nlq6xWrIQ4iYlEcAwLHbHMFm7lNsJIYQQ182sSQE8sDx2UAldiJ8r/t4G2jp6cTfqKK8z8/zrxzie\nU4ebQYvNbuejw6UA1DR10mexjdXyhRA3EdmTNIxz5yRJkCSEEEKMHUVReParKfT0WfHz1POH909z\nKLsWjVrh+w9O43/eymR/ZjXtnX2cyKtn5axw7lsaQ5/FykdHy5mTEIDJyzDWlyGEGGckAhiG1d7/\nLZRkkoQQQogx5+mqw9/LgEpR+PrqSdw5N4J/WpNAqMmN1XMjsFjtnMirB+Bk//8ezq7l7b1F/HzL\nSeqlHE8IcZkkkzSMgXI77DInSQghhLiRaNQqvrw42nl7XlIgpbXtBPu6kl3cxKmCBuqaOzlV0ABA\nc3sPv/xLGpseTsHb3WWk0wohxCASAQzjbOMGBa10txNCCCFuWBq1iodvj2NZSiiTo30BSMtvILu4\niSBfI/csiKKxrZsPDpWO8UqFEOOJBEnDcM5JQvYkCSGEEOPFQFe8nYdK6bXYmBrrx5p5EXi56TiY\nXU13r2WMVyiEGC8kAhiG1W5FsauQIEkIIYQYP/y9DPh7GzB39QEwLcaEWqVi0ZRgunqsHDldO8Yr\nFEKMFxIBDMMxTNbx1kiQJIQQQowfA9kkd6OWCcEeACyeGoJKUfg8rZLO7j5qmjqx2+2DnldY1crh\n0zXXfb1CiBvTqDZueOGFF0hPT0dRFDZt2kRycrLzsW3btrF9+3ZUKhXx8fFs3rwZRVH45S9/yYkT\nJ7BYLHzzm9/k9ttv55lnniE7OxsvLy8AHn30UW677bZRW7fV5sgkadRnJ4ELIYQQ4sY3OcqXz09W\nMiXaD5XK8Tfc292FabF+nMir59v/tQ+AiaGerJ4bSZ/FyrGcOo6eqQMgwNtIVJCH83yZRY00t/ew\naErwoNdpaOlCq9dddD1lte0E+7miUcuXrkKMJ6MWJB09epTS0lK2bt1KYWEhmzZtYuvWrQB0dXWx\nc+dOtmzZglarJTU1lbS0NHp7e8nPz2fr1q00NzfzpS99idtvvx2A733veyxZsmS0ljuIxW6R9t9C\nCCHEOJQc48tXb5/ItFjToPvvnBdBVWMHPh56sNvJLmkm7+/pzse93HS0mHs5XdLkDJK6ey38/h/Z\ndPZYCDW5OTNTRVVt/HzLSUzeBv79GzNRq1Q0t/dg1Gtw0Z5t+FReZ+ZHrx1j0ZQgHlk16TpcvRDi\nWhm1IOnQoUMsX74cgOjoaFpbWzGbzbi5uWEwGHj99dcBR8BkNpsxmUwEBwc7s00eHh50dXVhtVpH\nfI3RYrVZ+wfJSmc7IYQQYjxRKQpLp4eed39koAc/3TDHeTu3rJlTBQ14uroQ6u9KeIA7T7+4n+zi\nJu6cGwnA3vRqOnsczR7e3lvIv6yfRnN7Dy/tyMBitVHd0MGxM3VEBLrz49eOERfuzXfvm+J8jeLq\nNgD2pVezPCWMUH+3UbxyIcS1NGqpkoaGBry9vZ23fXx8qK+vH3TMyy+/zIoVK1i5ciVhYWGo1WqM\nRiMA27dvZ9GiRajVjkDlzTffJDU1le9+97s0NTWN1rIBxzBZu02FVlLjQgghxE0pLtyb+5fGsnJ2\nOElRvngYdYQHuFFQ2UpPnxWL1cbHx8rQaVTEhHiSXdLMzkMl/PIvJ2k197JiRhgqBXYeLuW1D3Po\ntdjILGokt6zZ+RpVDR0A2IFtXxSMzYUKIa7IdRsmO3SDJMBjjz1GamoqGzZsICUlhZSUFAA++eQT\ntm/fzh//+EcA7rnnHry8vJg0aRIvv/wy//M//8Nzzz034mt5exvRXEUWyIYjk6R30WAyuV/xecba\neF77gPF+DeN9/SDXcCMY7+sXYrxIiPShrNZMfkUL7Z19NLb1sGx6KHOTAvnJG8d5a08RCrBydjjr\nboumz2bni5MVAEQFuVNc3c7b+4r5wYNeKIpCZX+QFB3sQVZRE1lFjSRN8B30mlabDbVq8Jeye9Or\nOJBZzdPrpmBwuW4f1YQQ5xi1f3n+/v40NDQ4b9fV1WEyOeqDW1payM/PZ+bMmej1ehYtWsTJkydJ\nSUlh3759/O53v+PVV1/F3d3xwWDu3LnO8yxdupQf/ehHF3zt5ubOq1p7n9WC3aZDrUB9fftVnWus\nmEzu43btA8b7NYz39YNcw43gZli/EONFYqQPHx0pY296NfnlLagUhdtnhWHyMrBydji1TZ3csyCK\n8ADH7/VXlsbyxckKXPUanvrKFF774AwZhY2cLm0mMdKHqoYOvN1dePiOOP79T8d57cMcfvyNWbgZ\ntADsz6jmjV05/DB1hvOctU2dbPk4jz6LjYzCRmYnBIzZ+yHErWzU6snmz5/Prl27AMjOzsbf3x83\nN0ctrsVi4ZlnnqGjw/ENS2ZmJlFRUbS3t/PLX/6S3//+985OdgBPPvkk5eXlABw5coTY2NjRWrZj\nfTYrNpvMSBJCCCFuJbGhnmjUKo7n1NHa0cv6ZTGYvAwA3Lckhie/nOwMZgAigjz453uTeHrdFDxd\ndXxp4QQAPj1eQWd3H83tPYT4OfY73bswiub2Hl774Ax2ux1zVx/bPi/AYrWTW9YCOKpuXv8ohz6L\nDYBTBQ0IIcbGqGWSpk+fTmJiIuvXr0dRFDZv3syOHTtwd3dnxYoVPPHEE6SmpqLRaIiLi2PZsmVs\n27aN5uZmnn76aed5fvGLX/DQQw/x9NNPYzAYMBqN/OxnPxutZQOOOUl2CZKEEEKIW4pOqyY21JMz\npc2smRfB8hlhF33OzHh/588Rge4E+hg5U9pMaa0ZgGA/VwBWz4ngdEkTafkN/OnDHBRFcQ69rW50\nfGl8OLuWnLIWpsb4UV5nJqOwEYvVNmL7cLvdjh1kXIkQo2BUC103btw46HZ8fLzz57Vr17J27dpB\nj99///3cf//9550nODiYt956a3QWOYTNbsNmt/W3AJfudkIIIcSt5MEVEympbmNeUuAVPX/yBF8+\nPl7OnlOVwNkgSaVS2HBXIv/193T2ZVQDYPLS09DSTXWjY5tAWn/m6P6lMXxyooJPT1SQV95CQqTP\nsK/167+dotdi5fsPTLukzyyd3RYMLmoUCaqEuChJlQxhtTtS3I4W4PL2CCGEELeSED9X5k8OuuJA\nIjna0ZjheE6983wDvN1d+GHqDFbODsfoouHhO+Lw89I7M0llNe24GbT4exuYGusHwKn8Bmx2+3kN\nsJraujlT2kxhZRvbPi+86LqqGjr4zov7eHtf0RVdlxC3GmmZMoTV5piHgF2FToIkIYQQQlyGiWFe\n6LQqevscX7oGnxMkAWg1Ku5bEsO626JRFIUgX1cyChupb+mirqWLxEhvFEUhLswLg4uG/ZnVHD5d\ni1aj4pt3JzIxzLFnO6vYMQ5FrVL49EQFCZHe5w3QPdfxnDqsNjsfHCpjZnwAYReY2TRcxz0hbjXy\nL2AIi71/eK1NhUaCJCGEEEJcBq1GxaRwx5xIb3eXEVt4D2Sqgnwd8yEPn64FIDzQ0RhCo1YxM95E\nd68VrUZFW0cvv/prGgcyHaV6A0HSN+9ORKNW8bdP84cdtzIgraABBbDZ7byxKwfbCMfu2FvIE7/Z\n68xuXQs9vVZ++ZeT7E2vumbnFGK0SRQwhNXm+ObHblckkySEEEKIyza5v+QuZEgWaThBvo5jDmfX\nABBxTve8h1bE8etvzePX35rH9+6bgl6n5rUPcqioM3OmpAlfDxdS4kykxJmob+mmpGb4cQFNbd2U\n1rSTEOnNjHh/CivbONIflJ3rg8OlvH+wlF6LjfSCxsu+7pEcz60jp6yFz9Mqr9k5hRhtEgUMYbWf\nLbeTPUlCCCGEuFzTYk0YXDQkRg3fcOFcwf1B0kDzhojAs0GSVqPCx0OPoihMivTh0TsTsNntvLQj\ng45uC4lRviiKwqz+DnvHztTRZ7Hx6vun+bx/yC1AeqEj4Jkaa+LeBVEAZBQODoKO5dSx/YtCPF11\nAOSWNV/p5Z9nIPtVXmumu9dyzc4rxGiSPUlDWGz95XbSuEEIIYQQV8Db3YWXvrMQlerizR+C/IzO\nnw0uaudcpuFMifElIdKb0yWOACapPwhLmuCDwUXNsZxaDHoNB7NqOJhVg6tBy6xJAZzKb3A+39dD\nj4dRS155i7M8r7PbwpaP89BqVPzrA9N4cXsGeRWt2Gz2S7qGC2lo6SKnfw6UzW6nqKptxG59IPuh\nxI1DfguHsJ6zJ0lagAshhBDiSlxqcOGq1+LRn70J93e/4MwjRVFYvywWRXHMRkqIdOx90mrUTI0x\n0djWwzv7inA3anHRqfnjzjP86cMznCltIszfDT9PA4qiEBvqRXN7D42t3QC8va+Ito5e1syLJNjP\nlYnhXnT1WCivM5+3hvcOFPMff0ujz2K9pOs7kOUoI5w+0dFUIr+idcRjTxU08Piv95BTeu2yWEJc\nKQmShrA6M0kqtCMMbxNCCCGEuFaC+5s3hJ+zH2kkoSY3Uu+I476lMRj1Wuf9syY5Su7sdnhgWSzf\nvCuRPouNvenVWK12Fk8Ndh4b298hL6+iheKqVj47WUGAj5GVs8IBiOt/PLe8ZdBrVzd28I/9JWSX\nNPPJiQouxmazcyCzGhetmvVLYwAoqGgZ9li73c6OPUVYbXZnEwshxpKU2w0xkEmy2xV0WgmShBBC\nCDG6gnxdySlrISJw5Lbc51o8NeS8+xKjfPDz1BPs58rshAAUReHHj87CbocAbwM67dnqmNhQTwDy\nylvJLG7GbocHl8c6txnEhfcHSWXNrJgRisVqQ6tR8/fPC7HZ7WjUCu8fLGH+5CA8jLoR17kvo4qG\n1m4WTQnGz8tAkK+Rgqq2YUvq0gsaqah3ZK4yixqx2+0y9FaMKQmShrBIJkkIIYQQ19HcpEDqWrpI\njva74nNo1Cp+9s05KCjO4CLUNHzQFR7ghotOzcm8esxdfUSHeDj3NwH4eRrw9dCTU9bMc384SlVj\nBzEhnuRXtDIxzIuUOBN//SSfd/cX89Xb4wadu7KhAwXwcnNhx94iXLRq7ulvFhEb6sne9Goq6joG\nNaiw2+28f6gEgMhAd0pq2qlq6CBkhPULcT1IFDDE2e520rhBCCFuVXl5eSxfvpw333zzvMcOHz7M\nfffdx/r163n22Wex9Y+OeOGFF7j//vtZv349GRkZ13vJYhyLCfHkX+6fiptBe/GDL0CtUl3SXii1\nSkVMsAfmrj4A7p4fdV7WJj7ci64eK9WNnQT7uZJf0YoC3L80hiXTQgjwNrDnVBVNbd3O5zS39/CT\n14/zw1eP8JM3jtPe2cedcyPwdncBIDbUkaHK7y+56+qx8Or7p3n294cpqmpj+kQTy1JCAcgsarqq\n90KIqyWZpCEG5iRJ4wYhhLg1dXZ28vzzzzN37txhH3/uued44403CAwM5KmnnmLfvn0YDAZKS0vZ\nunUrhYWFbNq0ia1bt17nlQtx6WLDvMguaSY2zGtQFmnAPQuiCPQ1MmtSACYvA7XNnXT1WIgM9ABg\n1ZwI/vRhDp+erGDdbY79Ru/sK6Knz4qPhws1TZ34eui5Y1bY2dfsL/PLr2hl+Yww0vLrHV349Bom\nT/Dl/qUxzhmVmUWNrJwdfk2utc9iQ6VCuuaJyyJB0hABribcFG8azV6SSRJCiFuQTqfjlVde4ZVX\nXhn28R07duDm5igD8vHxobm5mVOnTrF8+XIAoqOjaW1txWw2O48T4kYza1IAx87U8ejdScPu/fHz\nMnDn3Ejn7QBv46DH5yYGsGNPIXvSqrhrXiQNrd3sz6wm2M+VH39jJnllLfh6GQZ94WzyMuDhqiO/\nwtF+vKC/09337p9KVJCH87jwADfyK1ro7rWg113+R9WuHkdVkMFFg9Vm44evHiYqyIPH70m67HOJ\nW5cESUP46L2Zq7mfd9tLnN9mCCGEuHVoNBo0mpH/PA4EPnV1dRw4cIDvfOc7/OY3vyExMdF5jI+P\nD/X19RcMkry9jWiusmLBZLp4N7Qb2XhfP4zfazCZ3PndswFXdY41C6P5y64c3t5fQkFFC3Y7/NM9\nSQQGeBIY4Dnsc5KifTmYUY1do6G4ph0XnZrpiUFoztkHPjspiL9/mk9FUxdzJ5/tyme12cktbeJE\nTh3dPRYeXjXJeS3nHvPkrz9Do1bx39+7jdyyZupbumlu78XNw4DB5cb86Dtef48GjPf1D+fG/E0Z\nY30WR8mdZJKEEEIMp7Gxkccff5zNmzfj7e193uMDQzovpLm586rWYDK5U1/fflXnGEvjff0g1zAr\nzo+/f6ri46NlAMxJDCDCz3jB84Wb3DgIfH60lLKaduLCvWhu6hh0TEJ/C/Jdh0qICXTHarNxKKuW\nnYdLqW06++8mu7CB5/95Pt0dPc77MgobKK91dMlLz6nlRG4dABarjX3Hy5jWP6/pXAWVrYSaXK8o\na3UtjPffo5th/cORIGkYEiQJIYQYidlsZsOGDTz99NMsWLAAAH9/fxoaGpzH1NXVYTKd/2FMiJuJ\nh1HHIyvjqag3MzcpcMRueuca2Je062gZdiAm9PyMU3iAG8F+rqQXNNDR3cc/9hfzyfEK1CqF+UmB\nTJ9o4nhuHYeya/nuf+5heUoocxMDMeo1fHay0nmeU/n1nC45O5g2vbDxvCDp6JlafvePbNbMi2Dt\nougrfCfEzUiCpGH0SpAkhBBiBD//+c/52te+xqJFi5z3zZ8/n5deeon169eTnZ2Nv7+/7EcSt4S5\nSYGXdXyYvxs6rYqGVkdXvJgQr/OOURSFuYkBvLWniHf2FTuH3f7r+qn4eOgBmBLrh5e7Cx8fK2fL\nx3m8e6CYB5bFklnYSIifK1WNHRw5XUd1YwcRAe40tnWfN3+pu9fC1s8KACiuHr+ZEDE6JEgahmSS\nhBDi1pWVlcUvfvELKisr0Wg07Nq1i6VLlxIaGsqCBQt45513KC0tZfv27QCsWbOG+++/n8TERNav\nX4+iKGzevHmMr0KIG5NGrSI62JMzpc0oQEyIx7DHzU0MZMeeIj49UQHAA8tinQESgEpRWHdbDA+s\nnMT2j3N5/2ApL793GoCVs8PZl1FNXrmj1XjSBB+a2ro5lF1LeZ2Z8ABHedXOQ6U0tztK9Sr7B9le\nqR17iyioaOHba5Mx6uXj9c1A/isOo8/iGCirkxbgQghxy0lKSuLPf/7ziI9nZWUNe//GjRtHa0lC\n3FRiQx1BUrDJFaN++NlQPh564sK9yClrITnal+Ro32GP83bXc+/CCcSGevF/72Sh1aiYGe+PuavP\nGSQlRHjT2tnLoexa0gsaCA9wp7SmnV1Hy/D1cMHP00BueQvmrr7zZlXlV7Tw9y8KWTI1hDmJAcN2\nAjycXcP7B0sA2PJxHhvuSriKd+esju4+duwp4s65EYMCRHF9SJA0DMkkCSGEEEKMjtj+xgwTQ88v\ntTvX6rkRdPVaeWB57EXPmRjlw8+/OQeL1Y5Oq2ZqrB9bPytAq1ERE+pJT58NrUbFB4fLCPR1Zdtn\n+Vitdh6+I56csmZyy1uorDcTF362EUtPr5VX3jtNQ2s3BRWt7M+s5okvTR6UKapu7OD1j3LR69SY\nvAwcyq4hyNeIVuMY7JsQ6UOwr3HY4Gqo1o5e/vTBGVbMDCMh0of9GdV8nlaJwUXDV26T/VLXmwRJ\nw+izSpAkhBBCCDEaEiK8+frqeJInDJ8dGpAU5UtS1IWPOZe7Uef8OcDbyNzEQLzcdWg1arQaNY/f\nnchv/5HFb99xZIPvXRhFcrQv7Z29AFQ2dBAX7k1tUycerjreO1BCQ2s3C5ODaDH3klnUyHsHi7l/\n6dmgbcdexwDdx+9JJCLQnR/98Rg79hYNWtfUGD+e+koyAIdP1+Dt5jIoGBuw5eM80gsb6bXYSIj0\nIaOwEYCCipZLfg/EtSNB0jB6LTZUCqhVF4/6hRBCCCHEpVMUhYXJwRc/8CoNLXubNtHEU19J5n/f\nzmJylA9r5kUCOLvyVdZ3kF7QwH9vz2DgE6DJS8+DKyaiUmDTy0f49EQFS6aH4u9lwGqzcbqkCZOX\nnlmTHDOnnvpKMvnlLQT7udLVa2HnoVLSCxpo6+wFO7zy7mncjVp++c/z0GnPbus4mVfP8RxHu/Kc\nsmbqmjud5YLFNe1YrDbnLKnCqlZ2HizlkdXxeJwTGIpra1SDpBdeeIH09HQURWHTpk0kJyc7H9u2\nbRvbt29HpVIRHx/P5s2bURRl2OdUV1fz/e9/H6vVislk4le/+hU63ej9UvRZbGi16ktKjQohhBBC\niPEhKcqX/35yAVqNyvk5L8jXiKJARb2ZxjZH173oUE+a2rr5xupJuPQHM1+5LZrfv5vN9i8K+da9\nSRRXtdPVY2V2wtkOf5MivJkUcTZL1NbRy1t7ijhT0ozFasMOtHX2cTCrhtumhQDQ1WPhz7tz0agV\n5k8OYs+pKv68Ow+rzY5GrdBnsVFa0050iKNd+tZPCyiobOVgZg0rZ4dfj7ftljRq9WRHjx6ltLSU\nrVu38tOf/pSf/vSnzse6urrYuXMnW7Zs4W9/+xtFRUWkpaWN+JwXX3yRBx98kL/85S9EREQ4OwqN\nlj6LTZo2CCGEEELchHRDvgjXadX4exsprW0ns6iRqCB3Nn01hV9/a/6gsrhZk/yZEOzB8Zw6iqra\nyCp2lMMlRvqM+FoJ/Y9llzSRWeQ4XlEcc6JsNsfQ6UOZ1bSae7l9Zjh39We3soubAJyBVH5FKwAF\nFa0UVDp+HhiUO5LSmnYOZlVj6d9GMpTNbsdqG/6xC2nsb99+sxu1IOnQoUMsX74cgOjoaFpbWzGb\nHe0VDQYDr7/+Olqtlq6uLsxmMyaTacTnHDlyhGXLlgGwZMkSDh06NFrLBhzd7XRa2Y8khBBCCHEr\nCDW50ttnw26HBSOUAiqKwpcXOxoovHegmOziJlSKMihzNFREgDtuBi3ZxU1kFTXh6+HCgslB1DZ3\nkZZfD8CeNEeb84XJQfh46Inub4vuZtCyYkYY4OiyB/DR0TIAvNx0FFa10dR2NmDJKmrk758XYLM7\ngq9X3j/Nq++f4cd/OuYMrAY0tHbx/14+zOO/3sO/vXqEI6drnY8NPH84J3Lr+NffHuREbv2Ix9ws\nRi0SaGhowNv77C+Nj48P9fWD39CXX36ZFStWsHLlSsLCwkZ8TldXl7O8ztfX97zzXGt9FtugOlEh\nhBBCCHHzCvFzBRxNu2ZP8h/xuPhwLyaGepJe2EhRVRsTQjwuOBdJpXIEUc3tPXT2WJgc7ecskXt7\nXzFNbd2cyqsnKsidAB8jADPjHK8/eYIPfp56vN1dKKhspbzOTFpePZGB7s79VCfyHJ+JS2va+Z8d\nmXx4pIyCilaa2rqpaujAzaClqr6D/9x2iq4eCwDtnb38x9Z0apu7CPJ1pba5i9c+PEOLuYeqhg6+\n89/7nC3NhzqQWQPgDPBuZtetcYN9mKj0scceIzU1lQ0bNpCSknJJzxnuvqG8vY1orqJczmK1o9Oo\nMJncr/gcN4Lxvn4Y/9cw3tcPcg03gvG+fiGEuNENNG9ImWgacXYTOLJJdy2I4j/+dgo7kHSBUrsB\niVE+HOtvyjB5gg9Bvq7cNi2EL9Iq+c9t6dhs9kH7muYnB1Fc084ds8JRFIWYEE+O5dTx/OvHsQN3\nzo0gJsSTLbvzOJ5TR1KUDy/tyKC3f4TNybx6gvuDvrvmR9LVbeGd/cUczq5h8dQQXnork9qmTlbN\nDmfdkhi+SKvkjV257NhTREW9mY5uC58cL2fl7HBnswhw7J3K6i8DPFPajN1uP2///r70Kj46Wsb3\n7puKr+fFZzt19VhQKQouuhsvOTFqQZK/vz8NDQ3O23V1dZhMJgBaWlrIz89n5syZ6PV6Fi1axMmT\nJ0d8jtFopLu7G71eT21tLf7+I0f4AM3NnVe19t4+K1qtmvr69qs6z1gymdzH9fph/F/DeF8/yDXc\nCG6G9QshxI1uSowfq+aEs2RqyEWPTYjwJibEk4LKVhKjLiFI6g+kNOqzpXlfWRzNqfx6Khs6UCmO\n/U4DXPVavnl3ovP2xDAvjuXU4aJV8Y3V8aT0Z5piQz3Jq2jl/71yBIC75kXy8fFyTubVO8vwkqJ8\nMLpoeO9gCZ+nVaJSKRRUtjIjzuScvbRwShAfHy9nf2Y1AHqdmrbOPjILG5k20eRcR0ZhIxarDZWi\n0NzeQ01TJ0G+roOu9VhOHdWNnby5O5envpJ8wSZoPb1WnvvDUVwNGp772kxUl9hVuqvHwj/2F3P7\nzLBRHbI7auV28+fPZ9euXQBkZ2fj7++Pm5sjSrdYLDzzzDN0dHQAkJmZSVRU1IjPmTdvnvP+3bt3\ns3DhwtFaNna7nV6LzdnJRAghhBBC3Ny0GhXrbovBz8tw0WMVReGxuxJ49M5JTAj2uOjxvp56ZicE\nsHR6KHqdIz9h1Gt4+I44AJJjTXi5uYz4/MVTg3lkVTw/2TCHOYlnM053zAonyNfIzHh/vr4q3jn3\nqaG1m7T8Bnw99AT6GPF0c2HaRBMV9R389ZN8XHRqHlg+0RnAqFUq1i2JAcDHw4Xv9M902pdRL3TN\nTwAAD9JJREFUPWgdx/sbRSxNcQSSp0uaz1treZ2j/0B6YeOgfUulNe0UDtkXtftYGY1t3ZTVmjmQ\nNfi1LuSLU5XsPlbO20PmUV1ro5ZJmj59OomJiaxfvx5FUdi8eTM7duzA3d2dFStW8MQTT5CamopG\noyEuLo5ly5ahKMp5zwF48skn+cEPfsDWrVsJDg7m3nvvHa1lY7E6yvlkkKwQQgghhBiOn5fhkgKq\nAedmhgZMizXx9LopTI7zB4t1xOdq1CoWTTm/mcS0iaZBmZ6Bcx49U4fVZicxyscZCC2dFsLxnDp6\nLTbW3RaNt/vgoGxKtC+P3jmJyEB3QkxuRAS6k1HYSKu5B083F3p6rWQWNhLoY2T5jDA+OV7BmdJm\nlqWEOs/R1tFLa0cvYf5uVDd28qcPc8goaqS9o5f0wkbUKoVfPD4XHw89bZ29fHikDDeDlp4+K+/s\nK2b2pIBL6glw7IwjWDtypo77lsYMGiJ8LY3qnqSNGzcOuh0fH+/8ee3ataxdu/aizwFH6d5rr712\n7Rc4jL7+X1LJJAkhhBBCiNGUHO2Lydt4zcqqk6N9UasUrDY7SeeUAsaFexEV5E6fxc6KmWHnPU9R\nHDOaBixMDuLN3Xnsy6hmzbxIjuc6AqyUOBP+Xgb8PPXklDY725jD2SzS1Bg/lqXo2fpZPvv7s1F+\nnnoaWrv5PK2SLy+O5r39JXT3Wnlw+QSazT18eLiMj4+Xc+fcSOx2O6fyGzB5GQj1dxu0zrqWLkpq\n2tGoVVisNvZnVLNqTsQ1ee+Gum6NG8aLvv5Nb5JJEkIIIYQQ44nBRcOUGD9OlzQxKfJsx2hFUXj2\nq44maec2YxjJnIRAduwpYtfRMm6bFsLOQ6WoVQqLpzoyWpMivNmXUc3naZWsW+EIZMrqHIFemL8b\nM+L9WTA5iPI6Mza7nRA/Vzb+30G+SKskJsSTz05W4O9t4LZpIfT2WdmXXs2OvUX4eRoorm5j97Fy\nFGBeUiDrlsbg0Z8tOnbG0ar8y4sn8Pa+Ij5Pq+SOWeGXvJ/pckgkMMRAkCQtwIUQQgghxHjz6J2T\neP7R2bgO6dKnUasuKUACx56pVXPC6ei28J/bTlHT1Mm8pED8PB0lhguSg9CoFbZ8nMeT//E5reYe\nZyYpLMARNKlUChGB7kQFeaDTqrltWggd3RZeeisTlUphw10JaNQqjHotT6+bgl6n5vfvZrP7WDlB\nvkZCTK4cyKrhtZ1nnOs6llOHWqWwIDmIOQmBNLR2k1veci3etvNIkDRErwRJQgghhBBinDK4aC6p\n/fbFLE8Jw9NVR3F1OypF4c65Z8vaYkO9+Nljc5mTGEB5rZldR8sprzPjolNjGmGv1pJpIahVCja7\nnbWLJxAd7Ol8bEKwB9+9byoGFzURge4889B0fvT1WcT2z6QqqWmjrLadsloziVE+uOq1rJkXwYx4\nf0xeo9PhTsrthnBmkqTcTgghhBBC3KJcdGrWzItky8d5zE4IwN/bOOhxX089X181iZyyFvakV9LT\nayMq2B3VCG2/vd1dWLt4Ak2tPdwxK/y8x2NCPPn1t+bjolU7y+fu7p9J9dYXhTS09QCwdLqju56f\np4Fv3Zt0LS95EAmShpByOyGEEEIIIRzZH6New5Ro32Ef12pUrJkfxZsf5QAQ7n/h2XirZl+4yYLB\nZXBokhDhTXSIB9n97cZXzg4nOdrvUpd/VSRdMsRAdzvJJAkhhBBCiFuZSqUwNzEQ45D9TedaOTfS\n2fAsbEg3uqulKAr3LIgCID7ciy8vnnBNz38hkkkaQtufQfJyH3molxBCCCGEEAI83VxYkBzE5ycr\nL2m47uVKivLlx9+YRaCPEbXq+iUxJEgaIjrYg/+XmkJKYhAtzZ1jvRwhhBBCiP/f3v3HVF39cRx/\nXblc4RoJEtz0D9KYGgtyMSWtb1T0a9WyxZbTvDE2WZbDmtMAmwM3BobXylK38gfl4LpcjD/YsrLm\nylZ4t2KDoJVRa2E14kdxFekHd+f7R+v2Ma6K36/wuVefj//uZ/ey1/kczn3zvucAQFRbXjBX/8mZ\nqQzPuY/b/a8u9g7VeHCm7F8cDocyZ01XvJPfSQIAAADOJ945RXNmXvxdJDvRJAEAAACABU0SAAAA\nAFjQJAEAAACABU0SAAAAAFjQJAEAAACABU0SAAAAAFjQJAEAAACABU0SAAAAAFjQJAEAAACABU0S\nAAAAAFg4jDHG7hAAAAAAEC3YSQIAAAAAC5okAAAAALCgSQIAAAAAC5okAAAAALCgSQIAAAAAC5ok\nAAAAALBw2h0g2tTW1qq9vV0Oh0PPPvusbrjhBrsjjcvWrVv12WefaXR0VKtXr9aRI0fU1dWl5ORk\nSdKqVat0++232xvyHAKBgJ5++mnNnTtXkjRv3jyVlJSorKxMoVBIaWlp8vl8crlcNieN7M0331RL\nS0v4cWdnp7Kzs3X69Gm53W5JUnl5ubKzs+2KeFbHjx/XmjVrVFxcLK/Xq59++inifW9padH+/fs1\nZcoULVu2TI888ojd0cMijWHjxo0aHR2V0+mUz+dTWlqarr/+euXm5oZf9/rrrysuLs7G5P/49xgq\nKioiruFongdMDuqUPahT9qFOUadsYRAWCATM448/bowxpru72yxbtszmROPT2tpqSkpKjDHGDA4O\nmttuu82Ul5ebI0eO2Jxs/I4dO2bWrl17xrWKigpz6NAhY4wxzz//vPH7/XZEu2CBQMBs3rzZeL1e\n89VXX9kd55yGh4eN1+s1mzZtMg0NDcaYyPd9eHjY3HPPPSYYDJqRkRHzwAMPmF9++cXO6GGRxlBW\nVmbeeustY4wxjY2Npq6uzhhjTF5enm05zyXSGCKt4WieB0wO6pR9qFP2oE5Fh8uxTnHczqK1tVV3\n3XWXJCkzM1NDQ0M6deqUzanOb9GiRXrppZckSVdeeaVGRkYUCoVsTvX/CwQCuvPOOyVJd9xxh1pb\nW21OND67du3SmjVr7I4xLi6XS3v27FF6enr4WqT73t7erpycHCUlJSkhIUG5ublqa2uzK/YZIo2h\nqqpK9957ryQpJSVFv/76q13xxiXSGCKJ5nnA5KBORRfq1MSjTkWHy7FO0SRZ9Pf3KyUlJfx4xowZ\n6uvrszHR+MTFxYW3ypuampSfn6+4uDg1NjaqqKhI69at0+DgoM0pz6+7u1tPPPGEVqxYoY8//lgj\nIyPhYwupqakxMRcdHR2aOXOm0tLSJEkvv/yyVq5cqcrKSv322282pxvL6XQqISHhjGuR7nt/f79m\nzJgRfk40rY1IY3C73YqLi1MoFNKBAwf04IMPSpL++OMPrV+/XsuXL9drr71mR9yIIo1B0pg1HM3z\ngMlBnbIXdWryUaeiw+VYp/idpHMwxtgd4YK8//77ampqUn19vTo7O5WcnKysrCzt3r1bO3fuVGVl\npd0Rz2r27NkqLS3Vfffdp56eHhUVFZ3xKWOszEVTU5MefvhhSVJRUZHmz5+vjIwMVVVVye/3a9Wq\nVTYnvDBnu++xMB+hUEhlZWVavHixlixZIkkqKyvT0qVL5XA45PV6tXDhQuXk5NicNLKHHnpozBq+\n8cYbz3hOLMwDJlasfQ9Qp+xHnYoe1Knoxk6SRXp6uvr7+8OPf/755/AnLdHuo48+0iuvvKI9e/Yo\nKSlJS5YsUVZWliSpoKBAx48ftznhuXk8Ht1///1yOBzKyMjQVVddpaGhofCnWr29vefd4o0GgUAg\n/AZx9913KyMjQ1JszMHf3G73mPseaW1E+3xs3LhR11xzjUpLS8PXVqxYoWnTpsntdmvx4sVRPSeR\n1nAszgMuLuqUfahT0YM6FR0u9TpFk2Rxyy236N1335UkdXV1KT09XVdccYXNqc7v5MmT2rp1q159\n9dXwXxhZu3atenp6JP31hvj3X+OJVi0tLdq3b58kqa+vTwMDAyosLAzPx+HDh3XrrbfaGfG8ent7\nNW3aNLlcLhljVFxcrGAwKCk25uBvN99885j7vmDBAn3++ecKBoMaHh5WW1ubFi5caHPSs2tpaVF8\nfLyeeuqp8LVvv/1W69evlzFGo6Ojamtri+o5ibSGY20ecPFRp+xDnYoe1KnocKnXKYeJ5X2wCbBt\n2zZ9+umncjgcqqqq0nXXXWd3pPM6ePCgduzYoTlz5oSvFRYWqrGxUYmJiXK73dqyZYtSU1NtTHlu\np06d0oYNGxQMBvXnn3+qtLRUWVlZKi8v1++//65Zs2Zpy5Ytio+PtzvqWXV2dmr79u3au3evJOnQ\noUPau3evEhMT5fF4VFNTo8TERJtTnqmzs1N1dXX64Ycf5HQ65fF4tG3bNlVUVIy57++884727dsX\nPgKwdOlSu+NLijyGgYEBTZ06NfzDY2ZmpjZv3iyfz6djx45pypQpKigo0JNPPmlz+r9EGoPX69Xu\n3bvHrOFonQdMHuqUPahT9qBOUafsQpMEAAAAABYctwMAAAAAC5okAAAAALCgSQIAAAAAC5okAAAA\nALCgSQIAAAAAC5okIEY1Nzdrw4YNdscAAOCsqFWIVTRJAAAAAGDhtDsAcKlraGjQ22+/rVAopGuv\nvVYlJSVavXq18vPz9eWXX0qSXnzxRXk8Hn3wwQfatWuXEhISlJiYqOrqank8HrW3t6u2tlbx8fGa\nPn266urqJP3zzw2/+eYbzZo1Szt37pTD4bBzuACAGEStAs7EThIwgTo6OvTee+/J7/fr4MGDSkpK\n0ieffKKenh4VFhbqwIEDysvLU319vUZGRrRp0ybt2LFDDQ0Nys/P1/bt2yVJzzzzjKqrq9XY2KhF\nixbpww8/lCR1d3erurpazc3N+vrrr9XV1WXncAEAMYhaBYzFThIwgQKBgL7//nsVFRVJkk6fPq3e\n3l4lJycrOztbkpSbm6v9+/fru+++U2pqqq6++mpJUl5ent544w0NDg4qGAxq3rx5kqTi4mJJf53z\nzsnJUWJioiTJ4/Ho5MmTkzxCAECso1YBY9EkARPI5XKpoKBAlZWV4WsnTpxQYWFh+LExRg6HY8zR\nA+t1Y0zErx8XFzfmNQAAXAhqFTAWx+2ACZSbm6ujR49qeHhYkuT3+9XX16ehoSF98cUXkqS2tjbN\nnz9fs2fP1sDAgH788UdJUmtrqxYsWKCUlBQlJyero6NDklRfXy+/32/PgAAAlxxqFTAWO0nABMrJ\nydHKlSv12GOPaerUqUpPT9dNN90kj8ej5uZmPffcczLG6IUXXlBCQoJqamq0bt06uVwuud1u1dTU\nSJJ8Pp9qa2vldDqVlJQkn8+nw4cP2zw6AMClgFoFjOUw7HkCk+rEiRN69NFHdfToUbujAAAQEbUK\nlzuO2wEAAACABTtJAAAAAGDBThIAAAAAWNAkAQAAAIAFTRIAAAAAWNAkAQAAAIAFTRIAAAAAWNAk\nAQAAAIDFfwFvMvevm6ioOwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1008x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFRl4TKVeGFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('model_shallow_arg.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDCBlNLigeDG",
        "colab_type": "text"
      },
      "source": [
        "## Inter subject comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6oKq1957oIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deepcnn(dropout_rate=0.6):\n",
        "  K.clear_session()\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=25, kernel_size=(1,10), \n",
        "                  strides=(1, 1), input_shape=(22,window,1),\n",
        "                  padding ='valid'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(SpatialDropout2D(dropout_rate-0.3))\n",
        "  model.add(Conv2D(filters=25, kernel_size=(22,1), \n",
        "                  strides=(1, 1),\n",
        "                  padding ='valid', activation = 'elu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(SpatialDropout2D(dropout_rate-0.3))\n",
        "  model.add(MaxPool2D(pool_size=(1, 3), strides=(1, 3)))\n",
        "  model.add(Reshape((-1,25,1)))\n",
        "  model.add(Conv2D(filters=50, kernel_size=(10,25), \n",
        "                  strides=(1, 1),\n",
        "                  padding ='valid', activation = 'elu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(SpatialDropout2D(dropout_rate))\n",
        "  model.add(Reshape((-1,50,1)))\n",
        "  model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "  model.add(Conv2D(filters=100, kernel_size=(10,50), \n",
        "                  strides=(1, 1),\n",
        "                  padding ='valid', activation = 'elu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(SpatialDropout2D(dropout_rate))\n",
        "  model.add(Reshape((-1,100,1)))\n",
        "  model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "  model.add(Conv2D(filters=200, kernel_size=(3,100), \n",
        "                  strides=(1, 1),\n",
        "                  padding ='valid', activation = 'elu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(SpatialDropout2D(dropout_rate))\n",
        "  model.add(Reshape((-1,200,1)))\n",
        "  model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units = 4, activation='softmax'))\n",
        "\n",
        "  myadam = keras.optimizers.Adam(lr=0.001, beta_1=0.95, beta_2=0.9999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer = myadam,\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  # model.summary()\n",
        "  return model\n",
        "\n",
        "def shallowcnn(dropout_rate=0.5):\n",
        "  K.clear_session()\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=40, kernel_size=(1,25), \n",
        "                  strides=(1, 1), input_shape=(22,window,1),\n",
        "                  padding ='valid'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(SpatialDropout2D(dropout_rate-0.3))\n",
        "  model.add(Conv2D(filters=40, kernel_size=(22,1), \n",
        "                  strides=(1, 1),\n",
        "                  padding ='valid'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(SpatialDropout2D(dropout_rate))\n",
        "  model.add(Reshape((-1,40,1)))\n",
        "  model.add(AveragePooling2D(pool_size=(75,1), strides = (15,1)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units = 4, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer = 'adam',\n",
        "                metrics = ['accuracy'])\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziBKsD2LoSnU",
        "colab_type": "text"
      },
      "source": [
        "## intra-subject variability\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgMl86n9oRtO",
        "colab_type": "code",
        "outputId": "fd408479-408e-4a99-9727-e3df26f6b8b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4831
        }
      },
      "source": [
        "dropout_rate = 0.6\n",
        "\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=25, kernel_size=(1,10), \n",
        "                strides=(1, 1), input_shape=(22,window,1),\n",
        "                padding ='valid'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(GaussianNoise(0.1))\n",
        "model.add(Conv2D(filters=25, kernel_size=(22,1), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(GaussianNoise(0.1))\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(MaxPool2D(pool_size=(1, 3), strides=(1, 3)))\n",
        "model.add(Reshape((-1,25,1)))\n",
        "model.add(Conv2D(filters=50, kernel_size=(10,25), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(Reshape((-1,50,1)))\n",
        "model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "model.add(Conv2D(filters=100, kernel_size=(10,50), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(Reshape((-1,100,1)))\n",
        "model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "model.add(Conv2D(filters=200, kernel_size=(3,100), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(Reshape((-1,200,1)))\n",
        "model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "model.add(Flatten())\n",
        "# model.add(Dense(units = 20))\n",
        "# model.add(Activation('elu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(Dense(units = 4, activation='softmax'))\n",
        "\n",
        "myadam = keras.optimizers.Adam(lr=0.001, beta_1=0.95, beta_2=0.9999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = myadam,\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "early_stop = EarlyStopping(monitor='loss', patience=15, verbose=1)\n",
        "\n",
        "history = model.fit(X_train_argment.reshape(-1,22,window,1), Y_train_argment, \n",
        "                    batch_size=256, epochs = 500, validation_split = 0.2,\n",
        "                    callbacks = [early_stop], verbose = 1)\n",
        "\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model_deep_arg_wo_bn.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/500\n",
            "10152/10152 [==============================] - 6s 597us/step - loss: 1.3953 - acc: 0.2673 - val_loss: 1.3734 - val_acc: 0.3006\n",
            "Epoch 2/500\n",
            "10152/10152 [==============================] - 5s 520us/step - loss: 1.3801 - acc: 0.2948 - val_loss: 1.3428 - val_acc: 0.3629\n",
            "Epoch 3/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.3792 - acc: 0.3031 - val_loss: 1.3386 - val_acc: 0.3542\n",
            "Epoch 4/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.3709 - acc: 0.3116 - val_loss: 1.3253 - val_acc: 0.3913\n",
            "Epoch 5/500\n",
            "10152/10152 [==============================] - 5s 514us/step - loss: 1.3620 - acc: 0.3324 - val_loss: 1.3226 - val_acc: 0.3779\n",
            "Epoch 6/500\n",
            "10152/10152 [==============================] - 5s 511us/step - loss: 1.3591 - acc: 0.3320 - val_loss: 1.2942 - val_acc: 0.4086\n",
            "Epoch 7/500\n",
            "10152/10152 [==============================] - 5s 515us/step - loss: 1.3449 - acc: 0.3454 - val_loss: 1.2787 - val_acc: 0.4125\n",
            "Epoch 8/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.3380 - acc: 0.3504 - val_loss: 1.2751 - val_acc: 0.4050\n",
            "Epoch 9/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.3430 - acc: 0.3481 - val_loss: 1.2738 - val_acc: 0.4204\n",
            "Epoch 10/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.3349 - acc: 0.3572 - val_loss: 1.2389 - val_acc: 0.4543\n",
            "Epoch 11/500\n",
            "10152/10152 [==============================] - 5s 515us/step - loss: 1.3188 - acc: 0.3719 - val_loss: 1.2400 - val_acc: 0.4287\n",
            "Epoch 12/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.3107 - acc: 0.3872 - val_loss: 1.2351 - val_acc: 0.4444\n",
            "Epoch 13/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.3079 - acc: 0.3836 - val_loss: 1.2297 - val_acc: 0.4452\n",
            "Epoch 14/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.3043 - acc: 0.3878 - val_loss: 1.2261 - val_acc: 0.4354\n",
            "Epoch 15/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.2945 - acc: 0.3948 - val_loss: 1.2273 - val_acc: 0.4515\n",
            "Epoch 16/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.2793 - acc: 0.4057 - val_loss: 1.1892 - val_acc: 0.4531\n",
            "Epoch 17/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.2773 - acc: 0.4069 - val_loss: 1.1835 - val_acc: 0.4870\n",
            "Epoch 18/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.2854 - acc: 0.4006 - val_loss: 1.1675 - val_acc: 0.5016\n",
            "Epoch 19/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.2594 - acc: 0.4167 - val_loss: 1.1832 - val_acc: 0.4602\n",
            "Epoch 20/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.2541 - acc: 0.4276 - val_loss: 1.1839 - val_acc: 0.4752\n",
            "Epoch 21/500\n",
            "10152/10152 [==============================] - 5s 520us/step - loss: 1.2526 - acc: 0.4293 - val_loss: 1.1917 - val_acc: 0.4653\n",
            "Epoch 22/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.2630 - acc: 0.4274 - val_loss: 1.1415 - val_acc: 0.5134\n",
            "Epoch 23/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.2486 - acc: 0.4348 - val_loss: 1.1272 - val_acc: 0.5087\n",
            "Epoch 24/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.2420 - acc: 0.4363 - val_loss: 1.1573 - val_acc: 0.5146\n",
            "Epoch 25/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.2395 - acc: 0.4403 - val_loss: 1.1790 - val_acc: 0.4748\n",
            "Epoch 26/500\n",
            "10152/10152 [==============================] - 5s 515us/step - loss: 1.2308 - acc: 0.4440 - val_loss: 1.1717 - val_acc: 0.4657\n",
            "Epoch 27/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.2288 - acc: 0.4488 - val_loss: 1.1103 - val_acc: 0.5154\n",
            "Epoch 28/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.2367 - acc: 0.4397 - val_loss: 1.0945 - val_acc: 0.5398\n",
            "Epoch 29/500\n",
            "10152/10152 [==============================] - 5s 515us/step - loss: 1.2061 - acc: 0.4610 - val_loss: 1.1137 - val_acc: 0.5331\n",
            "Epoch 30/500\n",
            "10152/10152 [==============================] - 5s 520us/step - loss: 1.2109 - acc: 0.4674 - val_loss: 1.0877 - val_acc: 0.5292\n",
            "Epoch 31/500\n",
            "10152/10152 [==============================] - 5s 520us/step - loss: 1.2032 - acc: 0.4692 - val_loss: 1.0700 - val_acc: 0.5536\n",
            "Epoch 32/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.1914 - acc: 0.4689 - val_loss: 1.0771 - val_acc: 0.5508\n",
            "Epoch 33/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.1925 - acc: 0.4760 - val_loss: 1.0817 - val_acc: 0.5453\n",
            "Epoch 34/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.1848 - acc: 0.4800 - val_loss: 1.0880 - val_acc: 0.5288\n",
            "Epoch 35/500\n",
            "10152/10152 [==============================] - 5s 515us/step - loss: 1.1839 - acc: 0.4828 - val_loss: 1.0373 - val_acc: 0.5820\n",
            "Epoch 36/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.1680 - acc: 0.4932 - val_loss: 1.0392 - val_acc: 0.5709\n",
            "Epoch 37/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.1801 - acc: 0.4836 - val_loss: 1.0370 - val_acc: 0.5749\n",
            "Epoch 38/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.1569 - acc: 0.4961 - val_loss: 1.0682 - val_acc: 0.5422\n",
            "Epoch 39/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.1597 - acc: 0.4976 - val_loss: 1.0162 - val_acc: 0.5733\n",
            "Epoch 40/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.1489 - acc: 0.5046 - val_loss: 1.0180 - val_acc: 0.5745\n",
            "Epoch 41/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.1707 - acc: 0.4903 - val_loss: 1.0210 - val_acc: 0.5851\n",
            "Epoch 42/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.1527 - acc: 0.5036 - val_loss: 1.0493 - val_acc: 0.5670\n",
            "Epoch 43/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.1614 - acc: 0.4901 - val_loss: 1.0123 - val_acc: 0.5823\n",
            "Epoch 44/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.1430 - acc: 0.5040 - val_loss: 1.0259 - val_acc: 0.5721\n",
            "Epoch 45/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.1494 - acc: 0.5017 - val_loss: 1.0227 - val_acc: 0.5800\n",
            "Epoch 46/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.1371 - acc: 0.5109 - val_loss: 1.0082 - val_acc: 0.5831\n",
            "Epoch 47/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.1444 - acc: 0.5067 - val_loss: 0.9927 - val_acc: 0.5989\n",
            "Epoch 48/500\n",
            "10152/10152 [==============================] - 5s 520us/step - loss: 1.1447 - acc: 0.5093 - val_loss: 1.0114 - val_acc: 0.5823\n",
            "Epoch 49/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.1317 - acc: 0.5103 - val_loss: 0.9823 - val_acc: 0.6009\n",
            "Epoch 50/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.1413 - acc: 0.5090 - val_loss: 0.9947 - val_acc: 0.5898\n",
            "Epoch 51/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.1362 - acc: 0.5093 - val_loss: 0.9733 - val_acc: 0.6052\n",
            "Epoch 52/500\n",
            "10152/10152 [==============================] - 5s 520us/step - loss: 1.1424 - acc: 0.5154 - val_loss: 1.0435 - val_acc: 0.5760\n",
            "Epoch 53/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.1440 - acc: 0.5075 - val_loss: 1.0081 - val_acc: 0.5823\n",
            "Epoch 54/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.1456 - acc: 0.5071 - val_loss: 1.0090 - val_acc: 0.5910\n",
            "Epoch 55/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.1239 - acc: 0.5167 - val_loss: 0.9752 - val_acc: 0.6064\n",
            "Epoch 56/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.1301 - acc: 0.5171 - val_loss: 0.9836 - val_acc: 0.6024\n",
            "Epoch 57/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.1284 - acc: 0.5167 - val_loss: 0.9953 - val_acc: 0.5859\n",
            "Epoch 58/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.1259 - acc: 0.5216 - val_loss: 1.0113 - val_acc: 0.5792\n",
            "Epoch 59/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.1301 - acc: 0.5108 - val_loss: 1.0250 - val_acc: 0.5690\n",
            "Epoch 60/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.1055 - acc: 0.5351 - val_loss: 0.9709 - val_acc: 0.6064\n",
            "Epoch 61/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.1073 - acc: 0.5337 - val_loss: 0.9657 - val_acc: 0.6013\n",
            "Epoch 62/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.1156 - acc: 0.5222 - val_loss: 1.0027 - val_acc: 0.5717\n",
            "Epoch 63/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.1318 - acc: 0.5210 - val_loss: 1.0136 - val_acc: 0.5792\n",
            "Epoch 64/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.1180 - acc: 0.5189 - val_loss: 0.9623 - val_acc: 0.6080\n",
            "Epoch 65/500\n",
            "10152/10152 [==============================] - 5s 513us/step - loss: 1.1144 - acc: 0.5169 - val_loss: 1.0303 - val_acc: 0.5654\n",
            "Epoch 66/500\n",
            "10152/10152 [==============================] - 5s 513us/step - loss: 1.1218 - acc: 0.5214 - val_loss: 1.0125 - val_acc: 0.5717\n",
            "Epoch 67/500\n",
            "10152/10152 [==============================] - 5s 514us/step - loss: 1.0987 - acc: 0.5308 - val_loss: 0.9831 - val_acc: 0.5977\n",
            "Epoch 68/500\n",
            "10152/10152 [==============================] - 5s 523us/step - loss: 1.1115 - acc: 0.5280 - val_loss: 0.9673 - val_acc: 0.5985\n",
            "Epoch 69/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.0936 - acc: 0.5327 - val_loss: 0.9566 - val_acc: 0.6119\n",
            "Epoch 70/500\n",
            "10152/10152 [==============================] - 5s 521us/step - loss: 1.1062 - acc: 0.5273 - val_loss: 0.9833 - val_acc: 0.5890\n",
            "Epoch 71/500\n",
            "10152/10152 [==============================] - 5s 515us/step - loss: 1.1021 - acc: 0.5289 - val_loss: 0.9548 - val_acc: 0.6001\n",
            "Epoch 72/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.0890 - acc: 0.5364 - val_loss: 0.9597 - val_acc: 0.5981\n",
            "Epoch 73/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.0972 - acc: 0.5328 - val_loss: 0.9866 - val_acc: 0.5827\n",
            "Epoch 74/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.1172 - acc: 0.5261 - val_loss: 1.0400 - val_acc: 0.5559\n",
            "Epoch 75/500\n",
            "10152/10152 [==============================] - 5s 520us/step - loss: 1.1152 - acc: 0.5216 - val_loss: 0.9703 - val_acc: 0.5985\n",
            "Epoch 76/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.0918 - acc: 0.5406 - val_loss: 0.9632 - val_acc: 0.6009\n",
            "Epoch 77/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.0923 - acc: 0.5340 - val_loss: 0.9467 - val_acc: 0.6052\n",
            "Epoch 78/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.0963 - acc: 0.5331 - val_loss: 0.9772 - val_acc: 0.5902\n",
            "Epoch 79/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.1051 - acc: 0.5279 - val_loss: 1.0256 - val_acc: 0.5520\n",
            "Epoch 80/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.1030 - acc: 0.5323 - val_loss: 1.0081 - val_acc: 0.5717\n",
            "Epoch 81/500\n",
            "10152/10152 [==============================] - 5s 520us/step - loss: 1.1054 - acc: 0.5305 - val_loss: 0.9941 - val_acc: 0.5764\n",
            "Epoch 82/500\n",
            "10152/10152 [==============================] - 5s 515us/step - loss: 1.0961 - acc: 0.5288 - val_loss: 0.9632 - val_acc: 0.6087\n",
            "Epoch 83/500\n",
            "10152/10152 [==============================] - 5s 521us/step - loss: 1.1006 - acc: 0.5316 - val_loss: 0.9737 - val_acc: 0.5902\n",
            "Epoch 84/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.0835 - acc: 0.5448 - val_loss: 0.9634 - val_acc: 0.6103\n",
            "Epoch 85/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.0925 - acc: 0.5333 - val_loss: 0.9667 - val_acc: 0.5863\n",
            "Epoch 86/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.0808 - acc: 0.5439 - val_loss: 1.0054 - val_acc: 0.5768\n",
            "Epoch 87/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.1122 - acc: 0.5281 - val_loss: 0.9619 - val_acc: 0.6056\n",
            "Epoch 88/500\n",
            "10152/10152 [==============================] - 5s 520us/step - loss: 1.0849 - acc: 0.5356 - val_loss: 0.9334 - val_acc: 0.6147\n",
            "Epoch 89/500\n",
            "10152/10152 [==============================] - 5s 515us/step - loss: 1.0886 - acc: 0.5364 - val_loss: 0.9722 - val_acc: 0.5890\n",
            "Epoch 90/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.0773 - acc: 0.5463 - val_loss: 0.9615 - val_acc: 0.6013\n",
            "Epoch 91/500\n",
            "10152/10152 [==============================] - 5s 516us/step - loss: 1.0781 - acc: 0.5438 - val_loss: 0.9474 - val_acc: 0.6174\n",
            "Epoch 92/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.0963 - acc: 0.5318 - val_loss: 0.9547 - val_acc: 0.6001\n",
            "Epoch 93/500\n",
            "10152/10152 [==============================] - 5s 515us/step - loss: 1.0845 - acc: 0.5381 - val_loss: 0.9477 - val_acc: 0.6170\n",
            "Epoch 94/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.0770 - acc: 0.5439 - val_loss: 0.9256 - val_acc: 0.6221\n",
            "Epoch 95/500\n",
            "10152/10152 [==============================] - 5s 520us/step - loss: 1.0784 - acc: 0.5408 - val_loss: 0.9584 - val_acc: 0.6009\n",
            "Epoch 96/500\n",
            "10152/10152 [==============================] - 5s 520us/step - loss: 1.0771 - acc: 0.5474 - val_loss: 0.9399 - val_acc: 0.6186\n",
            "Epoch 97/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.0804 - acc: 0.5473 - val_loss: 0.9539 - val_acc: 0.6064\n",
            "Epoch 98/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.0889 - acc: 0.5477 - val_loss: 0.9464 - val_acc: 0.6087\n",
            "Epoch 99/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.0841 - acc: 0.5377 - val_loss: 0.9539 - val_acc: 0.5946\n",
            "Epoch 100/500\n",
            "10152/10152 [==============================] - 5s 520us/step - loss: 1.0837 - acc: 0.5426 - val_loss: 0.9510 - val_acc: 0.6028\n",
            "Epoch 101/500\n",
            "10152/10152 [==============================] - 5s 517us/step - loss: 1.0733 - acc: 0.5457 - val_loss: 0.9284 - val_acc: 0.6056\n",
            "Epoch 102/500\n",
            "10152/10152 [==============================] - 5s 522us/step - loss: 1.0799 - acc: 0.5452 - val_loss: 0.9741 - val_acc: 0.5981\n",
            "Epoch 103/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.0570 - acc: 0.5553 - val_loss: 0.9606 - val_acc: 0.6080\n",
            "Epoch 104/500\n",
            "10152/10152 [==============================] - 5s 522us/step - loss: 1.0672 - acc: 0.5541 - val_loss: 0.9304 - val_acc: 0.6064\n",
            "Epoch 105/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.0784 - acc: 0.5496 - val_loss: 0.9435 - val_acc: 0.6056\n",
            "Epoch 106/500\n",
            "10152/10152 [==============================] - 5s 523us/step - loss: 1.0740 - acc: 0.5473 - val_loss: 0.9873 - val_acc: 0.5938\n",
            "Epoch 107/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.0814 - acc: 0.5443 - val_loss: 0.9452 - val_acc: 0.6178\n",
            "Epoch 108/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.0842 - acc: 0.5409 - val_loss: 0.9471 - val_acc: 0.6048\n",
            "Epoch 109/500\n",
            "10152/10152 [==============================] - 5s 520us/step - loss: 1.0655 - acc: 0.5501 - val_loss: 0.9198 - val_acc: 0.6166\n",
            "Epoch 110/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.0655 - acc: 0.5527 - val_loss: 0.9359 - val_acc: 0.6139\n",
            "Epoch 111/500\n",
            "10152/10152 [==============================] - 5s 520us/step - loss: 1.0760 - acc: 0.5452 - val_loss: 1.0050 - val_acc: 0.5772\n",
            "Epoch 112/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.0661 - acc: 0.5547 - val_loss: 0.9846 - val_acc: 0.5898\n",
            "Epoch 113/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.0631 - acc: 0.5495 - val_loss: 0.9327 - val_acc: 0.6103\n",
            "Epoch 114/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.0785 - acc: 0.5447 - val_loss: 0.9626 - val_acc: 0.5950\n",
            "Epoch 115/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.0697 - acc: 0.5526 - val_loss: 0.9144 - val_acc: 0.6399\n",
            "Epoch 116/500\n",
            "10152/10152 [==============================] - 5s 518us/step - loss: 1.0741 - acc: 0.5452 - val_loss: 0.9229 - val_acc: 0.6229\n",
            "Epoch 117/500\n",
            "10152/10152 [==============================] - 5s 519us/step - loss: 1.0613 - acc: 0.5542 - val_loss: 0.9374 - val_acc: 0.6194\n",
            "Epoch 118/500\n",
            "10152/10152 [==============================] - 5s 513us/step - loss: 1.0627 - acc: 0.5501 - val_loss: 0.9110 - val_acc: 0.6261\n",
            "Epoch 00118: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-08d514b73735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# serialize weights to HDF5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_deep_arg_w/o_bn.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved model to disk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproceed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = 'model_deep_arg_w/o_bn.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Uf3uAos7Lr",
        "colab_type": "code",
        "outputId": "b9ba44af-23f0-4cd9-e677-e82589e48ebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "_, accu = model.evaluate(X_test_argment.reshape(-1,22,750,1), Y_test_argment)\n",
        "\n",
        "print(\"training accu is : {:2.2%}\".format(history.history['acc'][-1]))\n",
        "print(\"val accu is : {:2.2%}\".format(history.history['val_acc'][-1]))\n",
        "print(\"test accu is : {:2.2%}\".format(accu))\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model acc')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2658/2658 [==============================] - 1s 320us/step\n",
            "training accu is : 55.01%\n",
            "val accu is : 62.61%\n",
            "test accu is : 60.65%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0QAAAFMCAYAAADr1XxqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiIpKwtCESEREREZGE9QfvgWDHsiVz8QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1008x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utXbPA465Rfi",
        "colab_type": "text"
      },
      "source": [
        "### Train deepcnn on single **subject**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3HftvU94OwB",
        "colab_type": "code",
        "outputId": "3c7f19a2-7108-4b4c-feac-954f0d757fdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62088
        }
      },
      "source": [
        "num_subject = np.unique(person_train_valid).shape[0]\n",
        "cross_subject_accu = np.zeros((num_subject,num_subject))\n",
        "for num_s in  np.arange(num_subject):\n",
        "  idx_s_train = np.where(person_train_idx_argment == num_s)[0]\n",
        "  model_s = deepcnn(0.6)\n",
        "\n",
        "  early_stop = EarlyStopping(monitor='loss', patience=15, verbose=1)\n",
        "  num_train = X_train_argment[idx_s_train].shape[0]\n",
        "\n",
        "  history_s = model_s.fit(X_train_argment[idx_s_train].reshape(num_train,22,window,1),\n",
        "                          Y_train_argment[idx_s_train], \n",
        "                          batch_size=256, epochs = 200, validation_split = 0.2,\n",
        "                          callbacks = [early_stop], verbose = 1)\n",
        "  print(\"Done with one subject\")\n",
        "#   test the accuracy on other subjects\n",
        "  for num_s_j in np.arange(num_subject):\n",
        "    idx_s_test = np.where(person_test_idx_argment == num_s_j)[0]\n",
        "    cross_subject_accu[num_s, num_s_j] = model_s.evaluate(X_test_argment[idx_s_test].reshape(-1,22,window,1), \n",
        "                                                          Y_test_argment[idx_s_test])[1]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1137 samples, validate on 285 samples\n",
            "Epoch 1/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 2.7824 - acc: 0.2515 - val_loss: 1.4646 - val_acc: 0.3719\n",
            "Epoch 2/200\n",
            "1137/1137 [==============================] - 1s 982us/step - loss: 2.7087 - acc: 0.2682 - val_loss: 1.9572 - val_acc: 0.2982\n",
            "Epoch 3/200\n",
            "1137/1137 [==============================] - 1s 995us/step - loss: 2.5169 - acc: 0.2788 - val_loss: 2.2393 - val_acc: 0.2807\n",
            "Epoch 4/200\n",
            "1137/1137 [==============================] - 1s 975us/step - loss: 2.3288 - acc: 0.2797 - val_loss: 2.0002 - val_acc: 0.3018\n",
            "Epoch 5/200\n",
            "1137/1137 [==============================] - 1s 984us/step - loss: 2.3277 - acc: 0.2559 - val_loss: 2.0973 - val_acc: 0.3018\n",
            "Epoch 6/200\n",
            "1137/1137 [==============================] - 1s 980us/step - loss: 2.1592 - acc: 0.2779 - val_loss: 1.9966 - val_acc: 0.3018\n",
            "Epoch 7/200\n",
            "1137/1137 [==============================] - 1s 991us/step - loss: 2.1852 - acc: 0.2806 - val_loss: 2.1265 - val_acc: 0.2912\n",
            "Epoch 8/200\n",
            "1137/1137 [==============================] - 1s 984us/step - loss: 2.0202 - acc: 0.2850 - val_loss: 2.0853 - val_acc: 0.2702\n",
            "Epoch 9/200\n",
            "1137/1137 [==============================] - 1s 982us/step - loss: 2.0050 - acc: 0.2938 - val_loss: 1.9151 - val_acc: 0.2456\n",
            "Epoch 10/200\n",
            "1137/1137 [==============================] - 1s 982us/step - loss: 1.9750 - acc: 0.2876 - val_loss: 1.6809 - val_acc: 0.2772\n",
            "Epoch 11/200\n",
            "1137/1137 [==============================] - 1s 988us/step - loss: 1.9007 - acc: 0.2929 - val_loss: 1.6205 - val_acc: 0.3088\n",
            "Epoch 12/200\n",
            "1137/1137 [==============================] - 1s 994us/step - loss: 1.9692 - acc: 0.2938 - val_loss: 1.5945 - val_acc: 0.3018\n",
            "Epoch 13/200\n",
            "1137/1137 [==============================] - 1s 997us/step - loss: 1.8963 - acc: 0.3069 - val_loss: 1.5452 - val_acc: 0.3263\n",
            "Epoch 14/200\n",
            "1137/1137 [==============================] - 1s 987us/step - loss: 1.7946 - acc: 0.3131 - val_loss: 1.4935 - val_acc: 0.3439\n",
            "Epoch 15/200\n",
            "1137/1137 [==============================] - 1s 983us/step - loss: 1.7963 - acc: 0.2955 - val_loss: 1.4515 - val_acc: 0.3368\n",
            "Epoch 16/200\n",
            "1137/1137 [==============================] - 1s 971us/step - loss: 1.7593 - acc: 0.3008 - val_loss: 1.3973 - val_acc: 0.3825\n",
            "Epoch 17/200\n",
            "1137/1137 [==============================] - 1s 988us/step - loss: 1.6717 - acc: 0.3140 - val_loss: 1.3380 - val_acc: 0.3825\n",
            "Epoch 18/200\n",
            "1137/1137 [==============================] - 1s 981us/step - loss: 1.6344 - acc: 0.3237 - val_loss: 1.2927 - val_acc: 0.3754\n",
            "Epoch 19/200\n",
            "1137/1137 [==============================] - 1s 981us/step - loss: 1.5911 - acc: 0.3237 - val_loss: 1.2766 - val_acc: 0.3719\n",
            "Epoch 20/200\n",
            "1137/1137 [==============================] - 1s 978us/step - loss: 1.5969 - acc: 0.3193 - val_loss: 1.2709 - val_acc: 0.3789\n",
            "Epoch 21/200\n",
            "1137/1137 [==============================] - 1s 975us/step - loss: 1.5286 - acc: 0.3448 - val_loss: 1.2581 - val_acc: 0.3614\n",
            "Epoch 22/200\n",
            "1137/1137 [==============================] - 1s 990us/step - loss: 1.5249 - acc: 0.3395 - val_loss: 1.2776 - val_acc: 0.4070\n",
            "Epoch 23/200\n",
            "1137/1137 [==============================] - 1s 988us/step - loss: 1.5199 - acc: 0.3597 - val_loss: 1.3284 - val_acc: 0.3649\n",
            "Epoch 24/200\n",
            "1137/1137 [==============================] - 1s 986us/step - loss: 1.4933 - acc: 0.3685 - val_loss: 1.2428 - val_acc: 0.4211\n",
            "Epoch 25/200\n",
            "1137/1137 [==============================] - 1s 969us/step - loss: 1.4192 - acc: 0.3536 - val_loss: 1.1367 - val_acc: 0.4912\n",
            "Epoch 26/200\n",
            "1137/1137 [==============================] - 1s 995us/step - loss: 1.3821 - acc: 0.3843 - val_loss: 1.1088 - val_acc: 0.5509\n",
            "Epoch 27/200\n",
            "1137/1137 [==============================] - 1s 987us/step - loss: 1.4083 - acc: 0.3764 - val_loss: 1.1619 - val_acc: 0.4667\n",
            "Epoch 28/200\n",
            "1137/1137 [==============================] - 1s 1ms/step - loss: 1.3440 - acc: 0.3958 - val_loss: 1.1411 - val_acc: 0.4246\n",
            "Epoch 29/200\n",
            "1137/1137 [==============================] - 1s 996us/step - loss: 1.3173 - acc: 0.4055 - val_loss: 1.1253 - val_acc: 0.4947\n",
            "Epoch 30/200\n",
            "1137/1137 [==============================] - 1s 979us/step - loss: 1.3358 - acc: 0.3905 - val_loss: 1.1412 - val_acc: 0.4947\n",
            "Epoch 31/200\n",
            "1137/1137 [==============================] - 1s 993us/step - loss: 1.3385 - acc: 0.3870 - val_loss: 1.1211 - val_acc: 0.4807\n",
            "Epoch 32/200\n",
            "1137/1137 [==============================] - 1s 988us/step - loss: 1.2195 - acc: 0.4503 - val_loss: 1.1143 - val_acc: 0.4596\n",
            "Epoch 33/200\n",
            "1137/1137 [==============================] - 1s 999us/step - loss: 1.2525 - acc: 0.4573 - val_loss: 1.0885 - val_acc: 0.4982\n",
            "Epoch 34/200\n",
            "1137/1137 [==============================] - 1s 984us/step - loss: 1.2578 - acc: 0.4503 - val_loss: 1.0454 - val_acc: 0.5474\n",
            "Epoch 35/200\n",
            "1137/1137 [==============================] - 1s 982us/step - loss: 1.2431 - acc: 0.4424 - val_loss: 0.9957 - val_acc: 0.5614\n",
            "Epoch 36/200\n",
            "1137/1137 [==============================] - 1s 994us/step - loss: 1.1663 - acc: 0.4538 - val_loss: 1.0013 - val_acc: 0.5614\n",
            "Epoch 37/200\n",
            "1137/1137 [==============================] - 1s 989us/step - loss: 1.1922 - acc: 0.4600 - val_loss: 1.0252 - val_acc: 0.5123\n",
            "Epoch 38/200\n",
            "1137/1137 [==============================] - 1s 992us/step - loss: 1.1878 - acc: 0.4758 - val_loss: 0.9989 - val_acc: 0.5368\n",
            "Epoch 39/200\n",
            "1137/1137 [==============================] - 1s 992us/step - loss: 1.1338 - acc: 0.4969 - val_loss: 0.9385 - val_acc: 0.5895\n",
            "Epoch 40/200\n",
            "1137/1137 [==============================] - 1s 987us/step - loss: 1.2054 - acc: 0.4820 - val_loss: 0.9251 - val_acc: 0.5860\n",
            "Epoch 41/200\n",
            "1137/1137 [==============================] - 1s 991us/step - loss: 1.1338 - acc: 0.4855 - val_loss: 0.9312 - val_acc: 0.6070\n",
            "Epoch 42/200\n",
            "1137/1137 [==============================] - 1s 994us/step - loss: 1.1152 - acc: 0.5172 - val_loss: 0.9303 - val_acc: 0.6211\n",
            "Epoch 43/200\n",
            "1137/1137 [==============================] - 1s 1ms/step - loss: 1.0743 - acc: 0.5092 - val_loss: 0.9252 - val_acc: 0.6281\n",
            "Epoch 44/200\n",
            "1137/1137 [==============================] - 1s 985us/step - loss: 1.1014 - acc: 0.5268 - val_loss: 0.9090 - val_acc: 0.6105\n",
            "Epoch 45/200\n",
            "1137/1137 [==============================] - 1s 989us/step - loss: 1.0744 - acc: 0.5154 - val_loss: 0.9087 - val_acc: 0.5930\n",
            "Epoch 46/200\n",
            "1137/1137 [==============================] - 1s 993us/step - loss: 1.1021 - acc: 0.5110 - val_loss: 0.9113 - val_acc: 0.5789\n",
            "Epoch 47/200\n",
            "1137/1137 [==============================] - 1s 988us/step - loss: 1.0666 - acc: 0.5365 - val_loss: 0.9085 - val_acc: 0.5860\n",
            "Epoch 48/200\n",
            "1137/1137 [==============================] - 1s 987us/step - loss: 1.0613 - acc: 0.5277 - val_loss: 0.8959 - val_acc: 0.5895\n",
            "Epoch 49/200\n",
            "1137/1137 [==============================] - 1s 1ms/step - loss: 1.0081 - acc: 0.5655 - val_loss: 0.8997 - val_acc: 0.5930\n",
            "Epoch 50/200\n",
            "1137/1137 [==============================] - 1s 994us/step - loss: 1.0338 - acc: 0.5532 - val_loss: 0.9056 - val_acc: 0.5860\n",
            "Epoch 51/200\n",
            "1137/1137 [==============================] - 1s 994us/step - loss: 1.0295 - acc: 0.5506 - val_loss: 0.9199 - val_acc: 0.5825\n",
            "Epoch 52/200\n",
            "1137/1137 [==============================] - 1s 990us/step - loss: 1.0068 - acc: 0.5506 - val_loss: 0.9417 - val_acc: 0.5754\n",
            "Epoch 53/200\n",
            "1137/1137 [==============================] - 1s 980us/step - loss: 0.9582 - acc: 0.5831 - val_loss: 0.9376 - val_acc: 0.5825\n",
            "Epoch 54/200\n",
            "1137/1137 [==============================] - 1s 983us/step - loss: 1.0418 - acc: 0.5427 - val_loss: 0.9162 - val_acc: 0.6246\n",
            "Epoch 55/200\n",
            "1137/1137 [==============================] - 1s 1ms/step - loss: 0.9636 - acc: 0.5708 - val_loss: 0.9021 - val_acc: 0.6211\n",
            "Epoch 56/200\n",
            "1137/1137 [==============================] - 1s 985us/step - loss: 1.0013 - acc: 0.5602 - val_loss: 0.8854 - val_acc: 0.6070\n",
            "Epoch 57/200\n",
            "1137/1137 [==============================] - 1s 1ms/step - loss: 0.9491 - acc: 0.5937 - val_loss: 0.8860 - val_acc: 0.6070\n",
            "Epoch 58/200\n",
            "1137/1137 [==============================] - 1s 977us/step - loss: 0.9795 - acc: 0.5602 - val_loss: 0.8819 - val_acc: 0.6070\n",
            "Epoch 59/200\n",
            "1137/1137 [==============================] - 1s 980us/step - loss: 0.9427 - acc: 0.5963 - val_loss: 0.8794 - val_acc: 0.6000\n",
            "Epoch 60/200\n",
            "1137/1137 [==============================] - 1s 986us/step - loss: 1.0095 - acc: 0.5374 - val_loss: 0.8796 - val_acc: 0.6070\n",
            "Epoch 61/200\n",
            "1137/1137 [==============================] - 1s 988us/step - loss: 0.9437 - acc: 0.5778 - val_loss: 0.8901 - val_acc: 0.6070\n",
            "Epoch 62/200\n",
            "1137/1137 [==============================] - 1s 982us/step - loss: 0.9080 - acc: 0.6069 - val_loss: 0.9002 - val_acc: 0.6070\n",
            "Epoch 63/200\n",
            "1137/1137 [==============================] - 1s 994us/step - loss: 0.9482 - acc: 0.5787 - val_loss: 0.8871 - val_acc: 0.6105\n",
            "Epoch 64/200\n",
            "1137/1137 [==============================] - 1s 986us/step - loss: 0.9337 - acc: 0.5814 - val_loss: 0.8883 - val_acc: 0.6140\n",
            "Epoch 65/200\n",
            "1137/1137 [==============================] - 1s 990us/step - loss: 0.8810 - acc: 0.6139 - val_loss: 0.8998 - val_acc: 0.6140\n",
            "Epoch 66/200\n",
            "1137/1137 [==============================] - 1s 983us/step - loss: 0.9445 - acc: 0.5884 - val_loss: 0.9145 - val_acc: 0.6211\n",
            "Epoch 67/200\n",
            "1137/1137 [==============================] - 1s 979us/step - loss: 0.9398 - acc: 0.5858 - val_loss: 0.9001 - val_acc: 0.6105\n",
            "Epoch 68/200\n",
            "1137/1137 [==============================] - 1s 989us/step - loss: 0.9228 - acc: 0.6016 - val_loss: 0.9035 - val_acc: 0.6070\n",
            "Epoch 69/200\n",
            "1137/1137 [==============================] - 1s 986us/step - loss: 0.8831 - acc: 0.5981 - val_loss: 0.9227 - val_acc: 0.5930\n",
            "Epoch 70/200\n",
            "1137/1137 [==============================] - 1s 995us/step - loss: 0.8800 - acc: 0.6016 - val_loss: 0.9188 - val_acc: 0.6035\n",
            "Epoch 71/200\n",
            "1137/1137 [==============================] - 1s 980us/step - loss: 0.8831 - acc: 0.6113 - val_loss: 0.9106 - val_acc: 0.6070\n",
            "Epoch 72/200\n",
            "1137/1137 [==============================] - 1s 993us/step - loss: 0.9028 - acc: 0.6077 - val_loss: 0.9387 - val_acc: 0.6000\n",
            "Epoch 73/200\n",
            "1137/1137 [==============================] - 1s 986us/step - loss: 0.8857 - acc: 0.6060 - val_loss: 0.9775 - val_acc: 0.6035\n",
            "Epoch 74/200\n",
            "1137/1137 [==============================] - 1s 987us/step - loss: 0.8838 - acc: 0.6139 - val_loss: 0.9816 - val_acc: 0.6070\n",
            "Epoch 75/200\n",
            "1137/1137 [==============================] - 1s 982us/step - loss: 0.8313 - acc: 0.6394 - val_loss: 0.9881 - val_acc: 0.6000\n",
            "Epoch 76/200\n",
            "1137/1137 [==============================] - 1s 1ms/step - loss: 0.8310 - acc: 0.6288 - val_loss: 0.9977 - val_acc: 0.5789\n",
            "Epoch 77/200\n",
            "1137/1137 [==============================] - 1s 999us/step - loss: 0.8268 - acc: 0.6288 - val_loss: 0.9998 - val_acc: 0.5860\n",
            "Epoch 78/200\n",
            "1137/1137 [==============================] - 1s 993us/step - loss: 0.8283 - acc: 0.6491 - val_loss: 1.0046 - val_acc: 0.6035\n",
            "Epoch 79/200\n",
            "1137/1137 [==============================] - 1s 985us/step - loss: 0.8329 - acc: 0.6561 - val_loss: 1.0009 - val_acc: 0.5895\n",
            "Epoch 80/200\n",
            "1137/1137 [==============================] - 1s 977us/step - loss: 0.8555 - acc: 0.6500 - val_loss: 1.0455 - val_acc: 0.5860\n",
            "Epoch 81/200\n",
            "1137/1137 [==============================] - 1s 983us/step - loss: 0.8227 - acc: 0.6324 - val_loss: 1.0872 - val_acc: 0.5754\n",
            "Epoch 82/200\n",
            "1137/1137 [==============================] - 1s 995us/step - loss: 0.8084 - acc: 0.6790 - val_loss: 1.0909 - val_acc: 0.5684\n",
            "Epoch 83/200\n",
            "1137/1137 [==============================] - 1s 982us/step - loss: 0.7867 - acc: 0.6623 - val_loss: 1.1396 - val_acc: 0.5368\n",
            "Epoch 84/200\n",
            "1137/1137 [==============================] - 1s 993us/step - loss: 0.7885 - acc: 0.6755 - val_loss: 1.1456 - val_acc: 0.5474\n",
            "Epoch 85/200\n",
            "1137/1137 [==============================] - 1s 980us/step - loss: 0.7587 - acc: 0.6763 - val_loss: 1.1331 - val_acc: 0.5579\n",
            "Epoch 86/200\n",
            "1137/1137 [==============================] - 1s 987us/step - loss: 0.7805 - acc: 0.6693 - val_loss: 1.1257 - val_acc: 0.5509\n",
            "Epoch 87/200\n",
            "1137/1137 [==============================] - 1s 982us/step - loss: 0.8037 - acc: 0.6403 - val_loss: 1.0770 - val_acc: 0.5719\n",
            "Epoch 88/200\n",
            "1137/1137 [==============================] - 1s 991us/step - loss: 0.7741 - acc: 0.6588 - val_loss: 1.0889 - val_acc: 0.5789\n",
            "Epoch 89/200\n",
            "1137/1137 [==============================] - 1s 988us/step - loss: 0.7740 - acc: 0.6825 - val_loss: 1.1091 - val_acc: 0.5719\n",
            "Epoch 90/200\n",
            "1137/1137 [==============================] - 1s 983us/step - loss: 0.7958 - acc: 0.6508 - val_loss: 1.1140 - val_acc: 0.5860\n",
            "Epoch 91/200\n",
            "1137/1137 [==============================] - 1s 976us/step - loss: 0.7556 - acc: 0.6719 - val_loss: 1.0606 - val_acc: 0.5965\n",
            "Epoch 92/200\n",
            "1137/1137 [==============================] - 1s 971us/step - loss: 0.7716 - acc: 0.6728 - val_loss: 1.0798 - val_acc: 0.5825\n",
            "Epoch 93/200\n",
            "1137/1137 [==============================] - 1s 970us/step - loss: 0.7448 - acc: 0.6825 - val_loss: 1.1278 - val_acc: 0.5754\n",
            "Epoch 94/200\n",
            "1137/1137 [==============================] - 1s 993us/step - loss: 0.7471 - acc: 0.6807 - val_loss: 1.1112 - val_acc: 0.5754\n",
            "Epoch 95/200\n",
            "1137/1137 [==============================] - 1s 984us/step - loss: 0.7548 - acc: 0.6851 - val_loss: 1.1304 - val_acc: 0.5825\n",
            "Epoch 96/200\n",
            "1137/1137 [==============================] - 1s 999us/step - loss: 0.7608 - acc: 0.6675 - val_loss: 1.1787 - val_acc: 0.5719\n",
            "Epoch 97/200\n",
            "1137/1137 [==============================] - 1s 974us/step - loss: 0.7088 - acc: 0.7027 - val_loss: 1.2106 - val_acc: 0.5544\n",
            "Epoch 98/200\n",
            "1137/1137 [==============================] - 1s 985us/step - loss: 0.7484 - acc: 0.6737 - val_loss: 1.2108 - val_acc: 0.5474\n",
            "Epoch 99/200\n",
            "1137/1137 [==============================] - 1s 991us/step - loss: 0.7455 - acc: 0.6860 - val_loss: 1.2643 - val_acc: 0.5404\n",
            "Epoch 100/200\n",
            "1137/1137 [==============================] - 1s 995us/step - loss: 0.7463 - acc: 0.6983 - val_loss: 1.3336 - val_acc: 0.5439\n",
            "Epoch 101/200\n",
            "1137/1137 [==============================] - 1s 991us/step - loss: 0.7203 - acc: 0.6834 - val_loss: 1.2855 - val_acc: 0.5439\n",
            "Epoch 102/200\n",
            "1137/1137 [==============================] - 1s 984us/step - loss: 0.7176 - acc: 0.6825 - val_loss: 1.2166 - val_acc: 0.5614\n",
            "Epoch 103/200\n",
            "1137/1137 [==============================] - 1s 989us/step - loss: 0.7088 - acc: 0.6772 - val_loss: 1.2815 - val_acc: 0.5649\n",
            "Epoch 104/200\n",
            "1137/1137 [==============================] - 1s 993us/step - loss: 0.7130 - acc: 0.7168 - val_loss: 1.3166 - val_acc: 0.5614\n",
            "Epoch 105/200\n",
            "1137/1137 [==============================] - 1s 987us/step - loss: 0.6819 - acc: 0.7010 - val_loss: 1.2363 - val_acc: 0.5719\n",
            "Epoch 106/200\n",
            "1137/1137 [==============================] - 1s 992us/step - loss: 0.6468 - acc: 0.7142 - val_loss: 1.1393 - val_acc: 0.5825\n",
            "Epoch 107/200\n",
            "1137/1137 [==============================] - 1s 995us/step - loss: 0.6780 - acc: 0.7124 - val_loss: 1.1717 - val_acc: 0.5825\n",
            "Epoch 108/200\n",
            "1137/1137 [==============================] - 1s 982us/step - loss: 0.6938 - acc: 0.7062 - val_loss: 1.2000 - val_acc: 0.5860\n",
            "Epoch 109/200\n",
            "1137/1137 [==============================] - 1s 1ms/step - loss: 0.6910 - acc: 0.7054 - val_loss: 1.1647 - val_acc: 0.5860\n",
            "Epoch 110/200\n",
            "1137/1137 [==============================] - 1s 994us/step - loss: 0.6759 - acc: 0.7282 - val_loss: 1.0925 - val_acc: 0.5965\n",
            "Epoch 111/200\n",
            "1137/1137 [==============================] - 1s 985us/step - loss: 0.6520 - acc: 0.7238 - val_loss: 1.0976 - val_acc: 0.5825\n",
            "Epoch 112/200\n",
            "1137/1137 [==============================] - 1s 986us/step - loss: 0.6779 - acc: 0.6939 - val_loss: 1.1605 - val_acc: 0.5895\n",
            "Epoch 113/200\n",
            "1137/1137 [==============================] - 1s 980us/step - loss: 0.6281 - acc: 0.7335 - val_loss: 1.1495 - val_acc: 0.6035\n",
            "Epoch 114/200\n",
            "1137/1137 [==============================] - 1s 986us/step - loss: 0.6411 - acc: 0.7265 - val_loss: 1.0551 - val_acc: 0.6175\n",
            "Epoch 115/200\n",
            "1137/1137 [==============================] - 1s 992us/step - loss: 0.6332 - acc: 0.7221 - val_loss: 1.0740 - val_acc: 0.6035\n",
            "Epoch 116/200\n",
            "1137/1137 [==============================] - 1s 981us/step - loss: 0.6791 - acc: 0.7203 - val_loss: 1.1308 - val_acc: 0.5860\n",
            "Epoch 117/200\n",
            "1137/1137 [==============================] - 1s 988us/step - loss: 0.6728 - acc: 0.7423 - val_loss: 1.1768 - val_acc: 0.5789\n",
            "Epoch 118/200\n",
            "1137/1137 [==============================] - 1s 985us/step - loss: 0.6868 - acc: 0.7106 - val_loss: 1.2078 - val_acc: 0.5754\n",
            "Epoch 119/200\n",
            "1137/1137 [==============================] - 1s 1ms/step - loss: 0.6727 - acc: 0.7335 - val_loss: 1.1890 - val_acc: 0.5719\n",
            "Epoch 120/200\n",
            "1137/1137 [==============================] - 1s 993us/step - loss: 0.6639 - acc: 0.7247 - val_loss: 1.1865 - val_acc: 0.5965\n",
            "Epoch 121/200\n",
            "1137/1137 [==============================] - 1s 982us/step - loss: 0.6175 - acc: 0.7282 - val_loss: 1.2479 - val_acc: 0.5719\n",
            "Epoch 122/200\n",
            "1137/1137 [==============================] - 1s 975us/step - loss: 0.6506 - acc: 0.7344 - val_loss: 1.2314 - val_acc: 0.5649\n",
            "Epoch 123/200\n",
            "1137/1137 [==============================] - 1s 973us/step - loss: 0.6116 - acc: 0.7449 - val_loss: 1.0969 - val_acc: 0.5930\n",
            "Epoch 124/200\n",
            "1137/1137 [==============================] - 1s 982us/step - loss: 0.5963 - acc: 0.7414 - val_loss: 1.0678 - val_acc: 0.5825\n",
            "Epoch 125/200\n",
            "1137/1137 [==============================] - 1s 992us/step - loss: 0.6134 - acc: 0.7388 - val_loss: 1.0763 - val_acc: 0.5965\n",
            "Epoch 126/200\n",
            "1137/1137 [==============================] - 1s 990us/step - loss: 0.6383 - acc: 0.7370 - val_loss: 1.1052 - val_acc: 0.5754\n",
            "Epoch 127/200\n",
            "1137/1137 [==============================] - 1s 987us/step - loss: 0.6242 - acc: 0.7370 - val_loss: 1.0765 - val_acc: 0.6035\n",
            "Epoch 128/200\n",
            "1137/1137 [==============================] - 1s 991us/step - loss: 0.6242 - acc: 0.7379 - val_loss: 1.1224 - val_acc: 0.6035\n",
            "Epoch 129/200\n",
            "1137/1137 [==============================] - 1s 974us/step - loss: 0.5882 - acc: 0.7511 - val_loss: 1.1441 - val_acc: 0.5965\n",
            "Epoch 130/200\n",
            "1137/1137 [==============================] - 1s 984us/step - loss: 0.6171 - acc: 0.7493 - val_loss: 1.0293 - val_acc: 0.6000\n",
            "Epoch 131/200\n",
            "1137/1137 [==============================] - 1s 991us/step - loss: 0.6153 - acc: 0.7441 - val_loss: 0.9944 - val_acc: 0.6140\n",
            "Epoch 132/200\n",
            "1137/1137 [==============================] - 1s 988us/step - loss: 0.5904 - acc: 0.7414 - val_loss: 0.9903 - val_acc: 0.6386\n",
            "Epoch 133/200\n",
            "1137/1137 [==============================] - 1s 984us/step - loss: 0.5986 - acc: 0.7555 - val_loss: 1.0207 - val_acc: 0.6281\n",
            "Epoch 134/200\n",
            "1137/1137 [==============================] - 1s 994us/step - loss: 0.5985 - acc: 0.7529 - val_loss: 0.9908 - val_acc: 0.6246\n",
            "Epoch 135/200\n",
            "1137/1137 [==============================] - 1s 988us/step - loss: 0.5712 - acc: 0.7634 - val_loss: 0.9954 - val_acc: 0.6140\n",
            "Epoch 136/200\n",
            "1137/1137 [==============================] - 1s 980us/step - loss: 0.5861 - acc: 0.7555 - val_loss: 1.0511 - val_acc: 0.5965\n",
            "Epoch 137/200\n",
            "1137/1137 [==============================] - 1s 990us/step - loss: 0.5974 - acc: 0.7388 - val_loss: 1.1447 - val_acc: 0.5965\n",
            "Epoch 138/200\n",
            "1137/1137 [==============================] - 1s 978us/step - loss: 0.5810 - acc: 0.7573 - val_loss: 1.1025 - val_acc: 0.6140\n",
            "Epoch 139/200\n",
            "1137/1137 [==============================] - 1s 994us/step - loss: 0.5604 - acc: 0.7625 - val_loss: 1.0398 - val_acc: 0.6175\n",
            "Epoch 140/200\n",
            "1137/1137 [==============================] - 1s 984us/step - loss: 0.5713 - acc: 0.7537 - val_loss: 1.1083 - val_acc: 0.6035\n",
            "Epoch 141/200\n",
            "1137/1137 [==============================] - 1s 979us/step - loss: 0.5349 - acc: 0.7784 - val_loss: 1.0910 - val_acc: 0.5965\n",
            "Epoch 142/200\n",
            "1137/1137 [==============================] - 1s 994us/step - loss: 0.5422 - acc: 0.7704 - val_loss: 1.0335 - val_acc: 0.6140\n",
            "Epoch 143/200\n",
            "1137/1137 [==============================] - 1s 987us/step - loss: 0.5402 - acc: 0.7643 - val_loss: 0.9907 - val_acc: 0.6421\n",
            "Epoch 144/200\n",
            "1137/1137 [==============================] - 1s 985us/step - loss: 0.5505 - acc: 0.7599 - val_loss: 0.9046 - val_acc: 0.6561\n",
            "Epoch 145/200\n",
            "1137/1137 [==============================] - 1s 992us/step - loss: 0.5519 - acc: 0.7704 - val_loss: 0.8380 - val_acc: 0.6702\n",
            "Epoch 146/200\n",
            "1137/1137 [==============================] - 1s 984us/step - loss: 0.5237 - acc: 0.7854 - val_loss: 0.8672 - val_acc: 0.6737\n",
            "Epoch 147/200\n",
            "1137/1137 [==============================] - 1s 986us/step - loss: 0.5491 - acc: 0.7573 - val_loss: 0.9203 - val_acc: 0.6632\n",
            "Epoch 148/200\n",
            "1137/1137 [==============================] - 1s 1ms/step - loss: 0.5132 - acc: 0.7863 - val_loss: 0.9092 - val_acc: 0.6316\n",
            "Epoch 149/200\n",
            "1137/1137 [==============================] - 1s 988us/step - loss: 0.5446 - acc: 0.7775 - val_loss: 0.8756 - val_acc: 0.6632\n",
            "Epoch 150/200\n",
            "1137/1137 [==============================] - 1s 989us/step - loss: 0.4838 - acc: 0.8047 - val_loss: 0.8867 - val_acc: 0.6526\n",
            "Epoch 151/200\n",
            "1137/1137 [==============================] - 1s 982us/step - loss: 0.5246 - acc: 0.7810 - val_loss: 0.9127 - val_acc: 0.6702\n",
            "Epoch 152/200\n",
            "1137/1137 [==============================] - 1s 985us/step - loss: 0.5090 - acc: 0.7986 - val_loss: 0.9293 - val_acc: 0.6526\n",
            "Epoch 153/200\n",
            "1137/1137 [==============================] - 1s 991us/step - loss: 0.5040 - acc: 0.7942 - val_loss: 0.8685 - val_acc: 0.6596\n",
            "Epoch 154/200\n",
            "1137/1137 [==============================] - 1s 978us/step - loss: 0.5166 - acc: 0.7889 - val_loss: 0.8696 - val_acc: 0.6596\n",
            "Epoch 155/200\n",
            "1137/1137 [==============================] - 1s 978us/step - loss: 0.5382 - acc: 0.7863 - val_loss: 0.8443 - val_acc: 0.6632\n",
            "Epoch 156/200\n",
            "1137/1137 [==============================] - 1s 978us/step - loss: 0.4867 - acc: 0.7986 - val_loss: 0.8653 - val_acc: 0.6702\n",
            "Epoch 157/200\n",
            "1137/1137 [==============================] - 1s 983us/step - loss: 0.5077 - acc: 0.7880 - val_loss: 0.9596 - val_acc: 0.6421\n",
            "Epoch 158/200\n",
            "1137/1137 [==============================] - 1s 962us/step - loss: 0.4934 - acc: 0.7880 - val_loss: 0.9463 - val_acc: 0.6561\n",
            "Epoch 159/200\n",
            "1137/1137 [==============================] - 1s 977us/step - loss: 0.4870 - acc: 0.8039 - val_loss: 0.8709 - val_acc: 0.6632\n",
            "Epoch 160/200\n",
            "1137/1137 [==============================] - 1s 978us/step - loss: 0.4753 - acc: 0.8047 - val_loss: 0.8855 - val_acc: 0.6596\n",
            "Epoch 161/200\n",
            "1137/1137 [==============================] - 1s 974us/step - loss: 0.4906 - acc: 0.8004 - val_loss: 0.9168 - val_acc: 0.6491\n",
            "Epoch 162/200\n",
            "1137/1137 [==============================] - 1s 983us/step - loss: 0.4490 - acc: 0.8144 - val_loss: 0.9064 - val_acc: 0.6667\n",
            "Epoch 163/200\n",
            "1137/1137 [==============================] - 1s 983us/step - loss: 0.4613 - acc: 0.8171 - val_loss: 0.8542 - val_acc: 0.6912\n",
            "Epoch 164/200\n",
            "1137/1137 [==============================] - 1s 980us/step - loss: 0.4366 - acc: 0.8153 - val_loss: 0.8502 - val_acc: 0.6912\n",
            "Epoch 165/200\n",
            "1137/1137 [==============================] - 1s 990us/step - loss: 0.4432 - acc: 0.8118 - val_loss: 0.8738 - val_acc: 0.6877\n",
            "Epoch 166/200\n",
            "1137/1137 [==============================] - 1s 991us/step - loss: 0.4304 - acc: 0.8223 - val_loss: 0.8642 - val_acc: 0.6807\n",
            "Epoch 167/200\n",
            "1137/1137 [==============================] - 1s 981us/step - loss: 0.4545 - acc: 0.8267 - val_loss: 0.8738 - val_acc: 0.6842\n",
            "Epoch 168/200\n",
            "1137/1137 [==============================] - 1s 985us/step - loss: 0.4563 - acc: 0.8285 - val_loss: 0.9011 - val_acc: 0.6632\n",
            "Epoch 169/200\n",
            "1137/1137 [==============================] - 1s 987us/step - loss: 0.4274 - acc: 0.8259 - val_loss: 0.9173 - val_acc: 0.6667\n",
            "Epoch 170/200\n",
            "1137/1137 [==============================] - 1s 976us/step - loss: 0.4506 - acc: 0.8135 - val_loss: 0.9039 - val_acc: 0.6702\n",
            "Epoch 171/200\n",
            "1137/1137 [==============================] - 1s 983us/step - loss: 0.4200 - acc: 0.8267 - val_loss: 0.8867 - val_acc: 0.6842\n",
            "Epoch 172/200\n",
            "1137/1137 [==============================] - 1s 982us/step - loss: 0.4439 - acc: 0.8206 - val_loss: 0.8824 - val_acc: 0.6807\n",
            "Epoch 173/200\n",
            "1137/1137 [==============================] - 1s 977us/step - loss: 0.4602 - acc: 0.8294 - val_loss: 0.8966 - val_acc: 0.6772\n",
            "Epoch 174/200\n",
            "1137/1137 [==============================] - 1s 980us/step - loss: 0.4042 - acc: 0.8382 - val_loss: 0.8758 - val_acc: 0.6807\n",
            "Epoch 175/200\n",
            "1137/1137 [==============================] - 1s 983us/step - loss: 0.4812 - acc: 0.8135 - val_loss: 0.8942 - val_acc: 0.6737\n",
            "Epoch 176/200\n",
            "1137/1137 [==============================] - 1s 986us/step - loss: 0.4062 - acc: 0.8364 - val_loss: 0.9040 - val_acc: 0.6982\n",
            "Epoch 177/200\n",
            "1137/1137 [==============================] - 1s 994us/step - loss: 0.4600 - acc: 0.8188 - val_loss: 0.8803 - val_acc: 0.6912\n",
            "Epoch 178/200\n",
            "1137/1137 [==============================] - 1s 975us/step - loss: 0.4213 - acc: 0.8188 - val_loss: 0.8511 - val_acc: 0.6807\n",
            "Epoch 179/200\n",
            "1137/1137 [==============================] - 1s 998us/step - loss: 0.3684 - acc: 0.8566 - val_loss: 0.8485 - val_acc: 0.6807\n",
            "Epoch 180/200\n",
            "1137/1137 [==============================] - 1s 995us/step - loss: 0.4400 - acc: 0.8250 - val_loss: 0.8406 - val_acc: 0.6737\n",
            "Epoch 181/200\n",
            "1137/1137 [==============================] - 1s 984us/step - loss: 0.4025 - acc: 0.8241 - val_loss: 0.8457 - val_acc: 0.7088\n",
            "Epoch 182/200\n",
            "1137/1137 [==============================] - 1s 978us/step - loss: 0.4195 - acc: 0.8276 - val_loss: 0.8605 - val_acc: 0.6912\n",
            "Epoch 183/200\n",
            "1137/1137 [==============================] - 1s 987us/step - loss: 0.4111 - acc: 0.8294 - val_loss: 0.8658 - val_acc: 0.6947\n",
            "Epoch 184/200\n",
            "1137/1137 [==============================] - 1s 990us/step - loss: 0.3827 - acc: 0.8487 - val_loss: 0.8436 - val_acc: 0.7053\n",
            "Epoch 185/200\n",
            "1137/1137 [==============================] - 1s 991us/step - loss: 0.4050 - acc: 0.8329 - val_loss: 0.8496 - val_acc: 0.6947\n",
            "Epoch 186/200\n",
            "1137/1137 [==============================] - 1s 994us/step - loss: 0.3868 - acc: 0.8470 - val_loss: 0.8299 - val_acc: 0.6772\n",
            "Epoch 187/200\n",
            "1137/1137 [==============================] - 1s 993us/step - loss: 0.4156 - acc: 0.8267 - val_loss: 0.8424 - val_acc: 0.6702\n",
            "Epoch 188/200\n",
            "1137/1137 [==============================] - 1s 984us/step - loss: 0.4010 - acc: 0.8364 - val_loss: 0.8376 - val_acc: 0.6807\n",
            "Epoch 189/200\n",
            "1137/1137 [==============================] - 1s 984us/step - loss: 0.3437 - acc: 0.8584 - val_loss: 0.8332 - val_acc: 0.6667\n",
            "Epoch 190/200\n",
            "1137/1137 [==============================] - 1s 983us/step - loss: 0.3626 - acc: 0.8566 - val_loss: 0.8464 - val_acc: 0.6561\n",
            "Epoch 191/200\n",
            "1137/1137 [==============================] - 1s 995us/step - loss: 0.3780 - acc: 0.8452 - val_loss: 0.8851 - val_acc: 0.6526\n",
            "Epoch 192/200\n",
            "1137/1137 [==============================] - 1s 993us/step - loss: 0.3610 - acc: 0.8646 - val_loss: 0.8800 - val_acc: 0.6632\n",
            "Epoch 193/200\n",
            "1137/1137 [==============================] - 1s 973us/step - loss: 0.3456 - acc: 0.8558 - val_loss: 0.8987 - val_acc: 0.6807\n",
            "Epoch 194/200\n",
            "1137/1137 [==============================] - 1s 985us/step - loss: 0.3220 - acc: 0.8777 - val_loss: 0.9049 - val_acc: 0.6842\n",
            "Epoch 195/200\n",
            "1137/1137 [==============================] - 1s 987us/step - loss: 0.4036 - acc: 0.8426 - val_loss: 0.8802 - val_acc: 0.6982\n",
            "Epoch 196/200\n",
            "1137/1137 [==============================] - 1s 998us/step - loss: 0.3674 - acc: 0.8628 - val_loss: 0.8639 - val_acc: 0.6947\n",
            "Epoch 197/200\n",
            "1137/1137 [==============================] - 1s 994us/step - loss: 0.3585 - acc: 0.8408 - val_loss: 0.8963 - val_acc: 0.6982\n",
            "Epoch 198/200\n",
            "1137/1137 [==============================] - 1s 986us/step - loss: 0.3392 - acc: 0.8698 - val_loss: 0.9584 - val_acc: 0.6947\n",
            "Epoch 199/200\n",
            "1137/1137 [==============================] - 1s 991us/step - loss: 0.3464 - acc: 0.8663 - val_loss: 0.8961 - val_acc: 0.6877\n",
            "Epoch 200/200\n",
            "1137/1137 [==============================] - 1s 986us/step - loss: 0.3386 - acc: 0.8575 - val_loss: 0.8283 - val_acc: 0.6842\n",
            "Done with one subject\n",
            "300/300 [==============================] - 0s 532us/step\n",
            "300/300 [==============================] - 0s 341us/step\n",
            "300/300 [==============================] - 0s 340us/step\n",
            "300/300 [==============================] - 0s 347us/step\n",
            "282/282 [==============================] - 0s 735us/step\n",
            "294/294 [==============================] - 0s 469us/step\n",
            "300/300 [==============================] - 0s 341us/step\n",
            "300/300 [==============================] - 0s 345us/step\n",
            "282/282 [==============================] - 0s 336us/step\n",
            "Train on 1132 samples, validate on 284 samples\n",
            "Epoch 1/200\n",
            "1132/1132 [==============================] - 4s 3ms/step - loss: 2.8423 - acc: 0.2473 - val_loss: 1.5280 - val_acc: 0.3275\n",
            "Epoch 2/200\n",
            "1132/1132 [==============================] - 1s 974us/step - loss: 2.5736 - acc: 0.2677 - val_loss: 1.7257 - val_acc: 0.3838\n",
            "Epoch 3/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 2.4197 - acc: 0.2933 - val_loss: 1.8311 - val_acc: 0.3732\n",
            "Epoch 4/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 2.1775 - acc: 0.3154 - val_loss: 2.0512 - val_acc: 0.3275\n",
            "Epoch 5/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 2.1524 - acc: 0.2818 - val_loss: 1.9114 - val_acc: 0.3239\n",
            "Epoch 6/200\n",
            "1132/1132 [==============================] - 1s 996us/step - loss: 1.9874 - acc: 0.3057 - val_loss: 1.7314 - val_acc: 0.3099\n",
            "Epoch 7/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 1.9804 - acc: 0.3048 - val_loss: 1.5687 - val_acc: 0.3099\n",
            "Epoch 8/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 1.8050 - acc: 0.3110 - val_loss: 1.5015 - val_acc: 0.3803\n",
            "Epoch 9/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.7464 - acc: 0.2915 - val_loss: 1.4766 - val_acc: 0.3486\n",
            "Epoch 10/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 1.6642 - acc: 0.3136 - val_loss: 1.4478 - val_acc: 0.3415\n",
            "Epoch 11/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 1.6456 - acc: 0.3163 - val_loss: 1.4719 - val_acc: 0.3204\n",
            "Epoch 12/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.6048 - acc: 0.2995 - val_loss: 1.4488 - val_acc: 0.3028\n",
            "Epoch 13/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.5518 - acc: 0.3286 - val_loss: 1.4528 - val_acc: 0.3028\n",
            "Epoch 14/200\n",
            "1132/1132 [==============================] - 1s 996us/step - loss: 1.4828 - acc: 0.3207 - val_loss: 1.4640 - val_acc: 0.2852\n",
            "Epoch 15/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 1.4613 - acc: 0.3419 - val_loss: 1.5199 - val_acc: 0.2641\n",
            "Epoch 16/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 1.5106 - acc: 0.3092 - val_loss: 1.5162 - val_acc: 0.2465\n",
            "Epoch 17/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.4829 - acc: 0.2880 - val_loss: 1.4637 - val_acc: 0.2746\n",
            "Epoch 18/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 1.4537 - acc: 0.3366 - val_loss: 1.4314 - val_acc: 0.3239\n",
            "Epoch 19/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.4721 - acc: 0.3118 - val_loss: 1.4196 - val_acc: 0.3169\n",
            "Epoch 20/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.4036 - acc: 0.3551 - val_loss: 1.4596 - val_acc: 0.3028\n",
            "Epoch 21/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 1.4333 - acc: 0.3419 - val_loss: 1.3885 - val_acc: 0.3310\n",
            "Epoch 22/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 1.3869 - acc: 0.3489 - val_loss: 1.3864 - val_acc: 0.3662\n",
            "Epoch 23/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 1.4213 - acc: 0.3286 - val_loss: 1.4009 - val_acc: 0.3380\n",
            "Epoch 24/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 1.4305 - acc: 0.3322 - val_loss: 1.4246 - val_acc: 0.3099\n",
            "Epoch 25/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.4000 - acc: 0.3171 - val_loss: 1.4082 - val_acc: 0.3662\n",
            "Epoch 26/200\n",
            "1132/1132 [==============================] - 1s 980us/step - loss: 1.3778 - acc: 0.3481 - val_loss: 1.4207 - val_acc: 0.3415\n",
            "Epoch 27/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 1.3446 - acc: 0.3401 - val_loss: 1.4554 - val_acc: 0.2817\n",
            "Epoch 28/200\n",
            "1132/1132 [==============================] - 1s 972us/step - loss: 1.4063 - acc: 0.3348 - val_loss: 1.4402 - val_acc: 0.2923\n",
            "Epoch 29/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 1.3787 - acc: 0.3516 - val_loss: 1.4229 - val_acc: 0.3521\n",
            "Epoch 30/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 1.3538 - acc: 0.3534 - val_loss: 1.4094 - val_acc: 0.3486\n",
            "Epoch 31/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.3998 - acc: 0.3445 - val_loss: 1.4242 - val_acc: 0.2923\n",
            "Epoch 32/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 1.3524 - acc: 0.3754 - val_loss: 1.4330 - val_acc: 0.2923\n",
            "Epoch 33/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 1.3556 - acc: 0.3489 - val_loss: 1.3954 - val_acc: 0.3521\n",
            "Epoch 34/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.3640 - acc: 0.3684 - val_loss: 1.3904 - val_acc: 0.3345\n",
            "Epoch 35/200\n",
            "1132/1132 [==============================] - 1s 977us/step - loss: 1.3865 - acc: 0.3463 - val_loss: 1.4142 - val_acc: 0.3275\n",
            "Epoch 36/200\n",
            "1132/1132 [==============================] - 1s 977us/step - loss: 1.3372 - acc: 0.3852 - val_loss: 1.4234 - val_acc: 0.3451\n",
            "Epoch 37/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 1.3346 - acc: 0.3719 - val_loss: 1.4349 - val_acc: 0.3204\n",
            "Epoch 38/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 1.3390 - acc: 0.3684 - val_loss: 1.4409 - val_acc: 0.3310\n",
            "Epoch 39/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 1.3664 - acc: 0.3613 - val_loss: 1.4573 - val_acc: 0.3486\n",
            "Epoch 40/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 1.3270 - acc: 0.3763 - val_loss: 1.4341 - val_acc: 0.3415\n",
            "Epoch 41/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 1.3345 - acc: 0.3852 - val_loss: 1.4470 - val_acc: 0.2923\n",
            "Epoch 42/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 1.3407 - acc: 0.3878 - val_loss: 1.4693 - val_acc: 0.2676\n",
            "Epoch 43/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 1.3439 - acc: 0.3648 - val_loss: 1.4447 - val_acc: 0.3204\n",
            "Epoch 44/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 1.3411 - acc: 0.3860 - val_loss: 1.4548 - val_acc: 0.3239\n",
            "Epoch 45/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.3466 - acc: 0.3852 - val_loss: 1.4576 - val_acc: 0.2958\n",
            "Epoch 46/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.3180 - acc: 0.3816 - val_loss: 1.4426 - val_acc: 0.2993\n",
            "Epoch 47/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 1.3060 - acc: 0.4072 - val_loss: 1.4403 - val_acc: 0.3134\n",
            "Epoch 48/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.3144 - acc: 0.3975 - val_loss: 1.4638 - val_acc: 0.3204\n",
            "Epoch 49/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 1.3063 - acc: 0.3984 - val_loss: 1.4401 - val_acc: 0.3063\n",
            "Epoch 50/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 1.3095 - acc: 0.4064 - val_loss: 1.4321 - val_acc: 0.3239\n",
            "Epoch 51/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 1.3007 - acc: 0.4214 - val_loss: 1.4506 - val_acc: 0.3275\n",
            "Epoch 52/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.3002 - acc: 0.3807 - val_loss: 1.5124 - val_acc: 0.3239\n",
            "Epoch 53/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.2866 - acc: 0.4037 - val_loss: 1.4432 - val_acc: 0.3451\n",
            "Epoch 54/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.3068 - acc: 0.4055 - val_loss: 1.4329 - val_acc: 0.3451\n",
            "Epoch 55/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 1.2732 - acc: 0.4276 - val_loss: 1.4642 - val_acc: 0.3415\n",
            "Epoch 56/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.2878 - acc: 0.4187 - val_loss: 1.4450 - val_acc: 0.3415\n",
            "Epoch 57/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 1.2861 - acc: 0.4258 - val_loss: 1.4287 - val_acc: 0.3556\n",
            "Epoch 58/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.2685 - acc: 0.4258 - val_loss: 1.4571 - val_acc: 0.3239\n",
            "Epoch 59/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 1.2608 - acc: 0.4443 - val_loss: 1.4665 - val_acc: 0.3380\n",
            "Epoch 60/200\n",
            "1132/1132 [==============================] - 1s 978us/step - loss: 1.2877 - acc: 0.4019 - val_loss: 1.4132 - val_acc: 0.3662\n",
            "Epoch 61/200\n",
            "1132/1132 [==============================] - 1s 979us/step - loss: 1.2811 - acc: 0.4187 - val_loss: 1.4375 - val_acc: 0.3345\n",
            "Epoch 62/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 1.2866 - acc: 0.4267 - val_loss: 1.4960 - val_acc: 0.3169\n",
            "Epoch 63/200\n",
            "1132/1132 [==============================] - 1s 979us/step - loss: 1.2499 - acc: 0.4435 - val_loss: 1.5025 - val_acc: 0.2958\n",
            "Epoch 64/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.2498 - acc: 0.4523 - val_loss: 1.5014 - val_acc: 0.2817\n",
            "Epoch 65/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 1.2298 - acc: 0.4523 - val_loss: 1.5128 - val_acc: 0.2817\n",
            "Epoch 66/200\n",
            "1132/1132 [==============================] - 1s 979us/step - loss: 1.2731 - acc: 0.4267 - val_loss: 1.5156 - val_acc: 0.3028\n",
            "Epoch 67/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.2436 - acc: 0.4620 - val_loss: 1.4942 - val_acc: 0.3169\n",
            "Epoch 68/200\n",
            "1132/1132 [==============================] - 1s 973us/step - loss: 1.2186 - acc: 0.4408 - val_loss: 1.4657 - val_acc: 0.3310\n",
            "Epoch 69/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.2728 - acc: 0.4178 - val_loss: 1.4647 - val_acc: 0.3486\n",
            "Epoch 70/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.2748 - acc: 0.4267 - val_loss: 1.5081 - val_acc: 0.3345\n",
            "Epoch 71/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 1.2013 - acc: 0.4567 - val_loss: 1.4798 - val_acc: 0.3415\n",
            "Epoch 72/200\n",
            "1132/1132 [==============================] - 1s 976us/step - loss: 1.1864 - acc: 0.4664 - val_loss: 1.4965 - val_acc: 0.3275\n",
            "Epoch 73/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.2011 - acc: 0.4549 - val_loss: 1.5110 - val_acc: 0.3134\n",
            "Epoch 74/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 1.2116 - acc: 0.4638 - val_loss: 1.5003 - val_acc: 0.3169\n",
            "Epoch 75/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 1.2085 - acc: 0.4717 - val_loss: 1.5199 - val_acc: 0.3275\n",
            "Epoch 76/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 1.1838 - acc: 0.4885 - val_loss: 1.5105 - val_acc: 0.3275\n",
            "Epoch 77/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.1657 - acc: 0.4841 - val_loss: 1.4913 - val_acc: 0.3415\n",
            "Epoch 78/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.2045 - acc: 0.4761 - val_loss: 1.4867 - val_acc: 0.3239\n",
            "Epoch 79/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.1938 - acc: 0.4761 - val_loss: 1.5085 - val_acc: 0.3310\n",
            "Epoch 80/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.1661 - acc: 0.4991 - val_loss: 1.5454 - val_acc: 0.3099\n",
            "Epoch 81/200\n",
            "1132/1132 [==============================] - 1s 979us/step - loss: 1.1395 - acc: 0.5035 - val_loss: 1.5380 - val_acc: 0.3380\n",
            "Epoch 82/200\n",
            "1132/1132 [==============================] - 1s 978us/step - loss: 1.1410 - acc: 0.5150 - val_loss: 1.5168 - val_acc: 0.3380\n",
            "Epoch 83/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.1228 - acc: 0.5141 - val_loss: 1.5275 - val_acc: 0.3380\n",
            "Epoch 84/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 1.1410 - acc: 0.5035 - val_loss: 1.5425 - val_acc: 0.3310\n",
            "Epoch 85/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 1.1199 - acc: 0.5053 - val_loss: 1.5391 - val_acc: 0.3345\n",
            "Epoch 86/200\n",
            "1132/1132 [==============================] - 1s 980us/step - loss: 1.1343 - acc: 0.5053 - val_loss: 1.5462 - val_acc: 0.3239\n",
            "Epoch 87/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.1272 - acc: 0.5071 - val_loss: 1.5614 - val_acc: 0.3275\n",
            "Epoch 88/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.1356 - acc: 0.4947 - val_loss: 1.5713 - val_acc: 0.3239\n",
            "Epoch 89/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 1.1205 - acc: 0.5203 - val_loss: 1.5507 - val_acc: 0.3063\n",
            "Epoch 90/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.0956 - acc: 0.5292 - val_loss: 1.5518 - val_acc: 0.3099\n",
            "Epoch 91/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 1.0914 - acc: 0.5247 - val_loss: 1.5695 - val_acc: 0.3204\n",
            "Epoch 92/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 1.1247 - acc: 0.5203 - val_loss: 1.5744 - val_acc: 0.3310\n",
            "Epoch 93/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.0889 - acc: 0.5247 - val_loss: 1.5630 - val_acc: 0.3380\n",
            "Epoch 94/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 1.0728 - acc: 0.5468 - val_loss: 1.5692 - val_acc: 0.3345\n",
            "Epoch 95/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 1.1037 - acc: 0.5265 - val_loss: 1.6142 - val_acc: 0.3345\n",
            "Epoch 96/200\n",
            "1132/1132 [==============================] - 1s 978us/step - loss: 1.0788 - acc: 0.5415 - val_loss: 1.6253 - val_acc: 0.3415\n",
            "Epoch 97/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 1.0703 - acc: 0.5521 - val_loss: 1.6174 - val_acc: 0.3521\n",
            "Epoch 98/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 1.0725 - acc: 0.5318 - val_loss: 1.6074 - val_acc: 0.3486\n",
            "Epoch 99/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.0634 - acc: 0.5327 - val_loss: 1.6009 - val_acc: 0.3521\n",
            "Epoch 100/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.0368 - acc: 0.5645 - val_loss: 1.6124 - val_acc: 0.3486\n",
            "Epoch 101/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 1.0567 - acc: 0.5601 - val_loss: 1.6238 - val_acc: 0.3415\n",
            "Epoch 102/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.0199 - acc: 0.5689 - val_loss: 1.6429 - val_acc: 0.3345\n",
            "Epoch 103/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 1.0334 - acc: 0.5866 - val_loss: 1.6286 - val_acc: 0.3310\n",
            "Epoch 104/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.9906 - acc: 0.6007 - val_loss: 1.6051 - val_acc: 0.3380\n",
            "Epoch 105/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.9946 - acc: 0.5848 - val_loss: 1.5997 - val_acc: 0.3310\n",
            "Epoch 106/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.0024 - acc: 0.5945 - val_loss: 1.6083 - val_acc: 0.3486\n",
            "Epoch 107/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 0.9921 - acc: 0.5972 - val_loss: 1.6333 - val_acc: 0.3521\n",
            "Epoch 108/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 0.9666 - acc: 0.5928 - val_loss: 1.6542 - val_acc: 0.3521\n",
            "Epoch 109/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.9895 - acc: 0.5769 - val_loss: 1.6559 - val_acc: 0.3415\n",
            "Epoch 110/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.9710 - acc: 0.5981 - val_loss: 1.6661 - val_acc: 0.3521\n",
            "Epoch 111/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.9366 - acc: 0.5981 - val_loss: 1.6975 - val_acc: 0.3521\n",
            "Epoch 112/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 0.9668 - acc: 0.5866 - val_loss: 1.6958 - val_acc: 0.3556\n",
            "Epoch 113/200\n",
            "1132/1132 [==============================] - 1s 996us/step - loss: 0.9586 - acc: 0.5963 - val_loss: 1.6989 - val_acc: 0.3451\n",
            "Epoch 114/200\n",
            "1132/1132 [==============================] - 1s 977us/step - loss: 0.9412 - acc: 0.6263 - val_loss: 1.7144 - val_acc: 0.3345\n",
            "Epoch 115/200\n",
            "1132/1132 [==============================] - 1s 980us/step - loss: 0.9259 - acc: 0.6148 - val_loss: 1.7154 - val_acc: 0.3521\n",
            "Epoch 116/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.9525 - acc: 0.5954 - val_loss: 1.6849 - val_acc: 0.3380\n",
            "Epoch 117/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 0.9395 - acc: 0.6025 - val_loss: 1.7036 - val_acc: 0.3415\n",
            "Epoch 118/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.9156 - acc: 0.6131 - val_loss: 1.7429 - val_acc: 0.3521\n",
            "Epoch 119/200\n",
            "1132/1132 [==============================] - 1s 972us/step - loss: 0.8962 - acc: 0.6281 - val_loss: 1.7620 - val_acc: 0.3486\n",
            "Epoch 120/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 0.9277 - acc: 0.6122 - val_loss: 1.7414 - val_acc: 0.3451\n",
            "Epoch 121/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 0.8796 - acc: 0.6396 - val_loss: 1.7202 - val_acc: 0.3592\n",
            "Epoch 122/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 0.8677 - acc: 0.6484 - val_loss: 1.7038 - val_acc: 0.3521\n",
            "Epoch 123/200\n",
            "1132/1132 [==============================] - 1s 978us/step - loss: 0.8840 - acc: 0.6325 - val_loss: 1.7189 - val_acc: 0.3521\n",
            "Epoch 124/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.8628 - acc: 0.6519 - val_loss: 1.7594 - val_acc: 0.3592\n",
            "Epoch 125/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.9076 - acc: 0.6210 - val_loss: 1.7902 - val_acc: 0.3697\n",
            "Epoch 126/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.8352 - acc: 0.6502 - val_loss: 1.7909 - val_acc: 0.3275\n",
            "Epoch 127/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 0.8862 - acc: 0.6369 - val_loss: 1.7734 - val_acc: 0.3345\n",
            "Epoch 128/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 0.8349 - acc: 0.6749 - val_loss: 1.7602 - val_acc: 0.3556\n",
            "Epoch 129/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.8152 - acc: 0.6705 - val_loss: 1.7754 - val_acc: 0.3451\n",
            "Epoch 130/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.8801 - acc: 0.6334 - val_loss: 1.8073 - val_acc: 0.3345\n",
            "Epoch 131/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 0.8470 - acc: 0.6458 - val_loss: 1.8439 - val_acc: 0.3063\n",
            "Epoch 132/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.8333 - acc: 0.6475 - val_loss: 1.9353 - val_acc: 0.3239\n",
            "Epoch 133/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.8392 - acc: 0.6625 - val_loss: 1.9673 - val_acc: 0.3134\n",
            "Epoch 134/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.7859 - acc: 0.6899 - val_loss: 1.9016 - val_acc: 0.3275\n",
            "Epoch 135/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.8133 - acc: 0.6767 - val_loss: 1.8695 - val_acc: 0.3063\n",
            "Epoch 136/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.8847 - acc: 0.6422 - val_loss: 1.8913 - val_acc: 0.3063\n",
            "Epoch 137/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.8005 - acc: 0.6581 - val_loss: 1.9816 - val_acc: 0.3310\n",
            "Epoch 138/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.8002 - acc: 0.6714 - val_loss: 1.9696 - val_acc: 0.3310\n",
            "Epoch 139/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 0.7885 - acc: 0.6882 - val_loss: 1.9456 - val_acc: 0.3099\n",
            "Epoch 140/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.7819 - acc: 0.6767 - val_loss: 1.9386 - val_acc: 0.3345\n",
            "Epoch 141/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.7843 - acc: 0.6811 - val_loss: 1.9244 - val_acc: 0.3415\n",
            "Epoch 142/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.7539 - acc: 0.7067 - val_loss: 1.8732 - val_acc: 0.3486\n",
            "Epoch 143/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 0.7301 - acc: 0.7058 - val_loss: 1.8952 - val_acc: 0.3415\n",
            "Epoch 144/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 0.7679 - acc: 0.6996 - val_loss: 1.9642 - val_acc: 0.3169\n",
            "Epoch 145/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.7593 - acc: 0.6882 - val_loss: 1.9961 - val_acc: 0.3099\n",
            "Epoch 146/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.7624 - acc: 0.6970 - val_loss: 1.9856 - val_acc: 0.3063\n",
            "Epoch 147/200\n",
            "1132/1132 [==============================] - 1s 1000us/step - loss: 0.7133 - acc: 0.7244 - val_loss: 1.9961 - val_acc: 0.3204\n",
            "Epoch 148/200\n",
            "1132/1132 [==============================] - 1s 979us/step - loss: 0.7501 - acc: 0.7111 - val_loss: 1.9587 - val_acc: 0.3345\n",
            "Epoch 149/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.6931 - acc: 0.7164 - val_loss: 1.9502 - val_acc: 0.3380\n",
            "Epoch 150/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.7217 - acc: 0.7129 - val_loss: 1.9410 - val_acc: 0.3345\n",
            "Epoch 151/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.7410 - acc: 0.6899 - val_loss: 1.9634 - val_acc: 0.3415\n",
            "Epoch 152/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.7481 - acc: 0.6846 - val_loss: 1.9726 - val_acc: 0.3275\n",
            "Epoch 153/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 0.7087 - acc: 0.7173 - val_loss: 1.9844 - val_acc: 0.3239\n",
            "Epoch 154/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.7128 - acc: 0.7076 - val_loss: 2.0126 - val_acc: 0.3275\n",
            "Epoch 155/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 0.7007 - acc: 0.7306 - val_loss: 2.0403 - val_acc: 0.3275\n",
            "Epoch 156/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.7251 - acc: 0.7111 - val_loss: 2.0672 - val_acc: 0.3134\n",
            "Epoch 157/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.7010 - acc: 0.7023 - val_loss: 2.0671 - val_acc: 0.3169\n",
            "Epoch 158/200\n",
            "1132/1132 [==============================] - 1s 978us/step - loss: 0.6685 - acc: 0.7217 - val_loss: 2.0431 - val_acc: 0.2958\n",
            "Epoch 159/200\n",
            "1132/1132 [==============================] - 1s 996us/step - loss: 0.6929 - acc: 0.7094 - val_loss: 2.0391 - val_acc: 0.3063\n",
            "Epoch 160/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.6687 - acc: 0.7261 - val_loss: 2.0266 - val_acc: 0.3134\n",
            "Epoch 161/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.6740 - acc: 0.7270 - val_loss: 2.0373 - val_acc: 0.3134\n",
            "Epoch 162/200\n",
            "1132/1132 [==============================] - 1s 979us/step - loss: 0.6345 - acc: 0.7473 - val_loss: 2.0774 - val_acc: 0.3134\n",
            "Epoch 163/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 0.6443 - acc: 0.7500 - val_loss: 2.1105 - val_acc: 0.3169\n",
            "Epoch 164/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.6653 - acc: 0.7314 - val_loss: 2.1316 - val_acc: 0.3169\n",
            "Epoch 165/200\n",
            "1132/1132 [==============================] - 1s 973us/step - loss: 0.6750 - acc: 0.7332 - val_loss: 2.1539 - val_acc: 0.2887\n",
            "Epoch 166/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.6587 - acc: 0.7332 - val_loss: 2.1957 - val_acc: 0.3063\n",
            "Epoch 167/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 0.6578 - acc: 0.7244 - val_loss: 2.2126 - val_acc: 0.3239\n",
            "Epoch 168/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.6684 - acc: 0.7297 - val_loss: 2.1595 - val_acc: 0.3275\n",
            "Epoch 169/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.6408 - acc: 0.7456 - val_loss: 2.1503 - val_acc: 0.3099\n",
            "Epoch 170/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 0.6313 - acc: 0.7518 - val_loss: 2.2378 - val_acc: 0.2923\n",
            "Epoch 171/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.6263 - acc: 0.7456 - val_loss: 2.3005 - val_acc: 0.3028\n",
            "Epoch 172/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.6276 - acc: 0.7535 - val_loss: 2.2889 - val_acc: 0.2993\n",
            "Epoch 173/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 0.6414 - acc: 0.7332 - val_loss: 2.2661 - val_acc: 0.3099\n",
            "Epoch 174/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 0.5953 - acc: 0.7580 - val_loss: 2.2770 - val_acc: 0.3063\n",
            "Epoch 175/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.5924 - acc: 0.7721 - val_loss: 2.3000 - val_acc: 0.3063\n",
            "Epoch 176/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 0.6098 - acc: 0.7712 - val_loss: 2.2921 - val_acc: 0.3063\n",
            "Epoch 177/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.6226 - acc: 0.7553 - val_loss: 2.3059 - val_acc: 0.3204\n",
            "Epoch 178/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 0.5844 - acc: 0.7739 - val_loss: 2.3405 - val_acc: 0.3310\n",
            "Epoch 179/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.6096 - acc: 0.7518 - val_loss: 2.3004 - val_acc: 0.3275\n",
            "Epoch 180/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.5796 - acc: 0.7783 - val_loss: 2.2643 - val_acc: 0.3310\n",
            "Epoch 181/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.5299 - acc: 0.7986 - val_loss: 2.3087 - val_acc: 0.3204\n",
            "Epoch 182/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 0.5984 - acc: 0.7597 - val_loss: 2.2780 - val_acc: 0.3063\n",
            "Epoch 183/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.5944 - acc: 0.7686 - val_loss: 2.2601 - val_acc: 0.3134\n",
            "Epoch 184/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.6076 - acc: 0.7650 - val_loss: 2.3111 - val_acc: 0.3169\n",
            "Epoch 185/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.5897 - acc: 0.7677 - val_loss: 2.3580 - val_acc: 0.3239\n",
            "Epoch 186/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 0.5992 - acc: 0.7650 - val_loss: 2.3238 - val_acc: 0.3099\n",
            "Epoch 187/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 0.5553 - acc: 0.7756 - val_loss: 2.3141 - val_acc: 0.3204\n",
            "Epoch 188/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.5465 - acc: 0.7668 - val_loss: 2.3531 - val_acc: 0.3134\n",
            "Epoch 189/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 0.5352 - acc: 0.7933 - val_loss: 2.3325 - val_acc: 0.3380\n",
            "Epoch 190/200\n",
            "1132/1132 [==============================] - 1s 996us/step - loss: 0.5827 - acc: 0.7783 - val_loss: 2.2697 - val_acc: 0.3169\n",
            "Epoch 191/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 0.5568 - acc: 0.7783 - val_loss: 2.3012 - val_acc: 0.3063\n",
            "Epoch 192/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.5192 - acc: 0.8048 - val_loss: 2.4102 - val_acc: 0.3134\n",
            "Epoch 193/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.5398 - acc: 0.7986 - val_loss: 2.4547 - val_acc: 0.3239\n",
            "Epoch 194/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 0.5591 - acc: 0.7933 - val_loss: 2.4007 - val_acc: 0.3169\n",
            "Epoch 195/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.5325 - acc: 0.7871 - val_loss: 2.3659 - val_acc: 0.2852\n",
            "Epoch 196/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.5629 - acc: 0.8004 - val_loss: 2.4879 - val_acc: 0.2923\n",
            "Epoch 197/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.5315 - acc: 0.8004 - val_loss: 2.5844 - val_acc: 0.3134\n",
            "Epoch 198/200\n",
            "1132/1132 [==============================] - 1s 980us/step - loss: 0.5565 - acc: 0.7836 - val_loss: 2.4835 - val_acc: 0.3063\n",
            "Epoch 199/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 0.5260 - acc: 0.7986 - val_loss: 2.4081 - val_acc: 0.2852\n",
            "Epoch 200/200\n",
            "1132/1132 [==============================] - 1s 979us/step - loss: 0.5132 - acc: 0.8083 - val_loss: 2.4641 - val_acc: 0.2852\n",
            "Done with one subject\n",
            "300/300 [==============================] - 0s 328us/step\n",
            "300/300 [==============================] - 0s 351us/step\n",
            "300/300 [==============================] - 0s 334us/step\n",
            "300/300 [==============================] - 0s 333us/step\n",
            "282/282 [==============================] - 0s 354us/step\n",
            "294/294 [==============================] - 0s 334us/step\n",
            "300/300 [==============================] - 0s 343us/step\n",
            "300/300 [==============================] - 0s 340us/step\n",
            "282/282 [==============================] - 0s 334us/step\n",
            "Train on 1132 samples, validate on 284 samples\n",
            "Epoch 1/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 2.7568 - acc: 0.2783 - val_loss: 1.5177 - val_acc: 0.3169\n",
            "Epoch 2/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 2.7040 - acc: 0.2880 - val_loss: 1.8312 - val_acc: 0.2852\n",
            "Epoch 3/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 2.3683 - acc: 0.2933 - val_loss: 2.2674 - val_acc: 0.2852\n",
            "Epoch 4/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 2.2415 - acc: 0.2553 - val_loss: 1.6634 - val_acc: 0.2852\n",
            "Epoch 5/200\n",
            "1132/1132 [==============================] - 1s 980us/step - loss: 1.9281 - acc: 0.2942 - val_loss: 1.9322 - val_acc: 0.2958\n",
            "Epoch 6/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 1.7973 - acc: 0.2986 - val_loss: 2.0062 - val_acc: 0.3099\n",
            "Epoch 7/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 1.7072 - acc: 0.3101 - val_loss: 1.7016 - val_acc: 0.2711\n",
            "Epoch 8/200\n",
            "1132/1132 [==============================] - 1s 980us/step - loss: 1.6406 - acc: 0.2792 - val_loss: 1.8222 - val_acc: 0.2570\n",
            "Epoch 9/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.5968 - acc: 0.3216 - val_loss: 1.5299 - val_acc: 0.2887\n",
            "Epoch 10/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.6135 - acc: 0.2765 - val_loss: 1.5578 - val_acc: 0.2958\n",
            "Epoch 11/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 1.5531 - acc: 0.3277 - val_loss: 1.9450 - val_acc: 0.3063\n",
            "Epoch 12/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 1.5284 - acc: 0.3101 - val_loss: 1.6919 - val_acc: 0.3099\n",
            "Epoch 13/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 1.5620 - acc: 0.2898 - val_loss: 1.4512 - val_acc: 0.3345\n",
            "Epoch 14/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.5128 - acc: 0.2968 - val_loss: 1.4188 - val_acc: 0.3239\n",
            "Epoch 15/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 1.4613 - acc: 0.3428 - val_loss: 1.4213 - val_acc: 0.3345\n",
            "Epoch 16/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.4797 - acc: 0.3127 - val_loss: 1.4299 - val_acc: 0.3239\n",
            "Epoch 17/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 1.4458 - acc: 0.3454 - val_loss: 1.4437 - val_acc: 0.3239\n",
            "Epoch 18/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.4333 - acc: 0.3551 - val_loss: 1.4327 - val_acc: 0.3169\n",
            "Epoch 19/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.4783 - acc: 0.3224 - val_loss: 1.3809 - val_acc: 0.3345\n",
            "Epoch 20/200\n",
            "1132/1132 [==============================] - 1s 972us/step - loss: 1.4227 - acc: 0.3525 - val_loss: 1.3936 - val_acc: 0.3521\n",
            "Epoch 21/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 1.4446 - acc: 0.3295 - val_loss: 1.4047 - val_acc: 0.3521\n",
            "Epoch 22/200\n",
            "1132/1132 [==============================] - 1s 973us/step - loss: 1.4297 - acc: 0.3375 - val_loss: 1.3840 - val_acc: 0.3345\n",
            "Epoch 23/200\n",
            "1132/1132 [==============================] - 1s 971us/step - loss: 1.3935 - acc: 0.3604 - val_loss: 1.3614 - val_acc: 0.3380\n",
            "Epoch 24/200\n",
            "1132/1132 [==============================] - 1s 970us/step - loss: 1.4058 - acc: 0.3419 - val_loss: 1.3898 - val_acc: 0.3556\n",
            "Epoch 25/200\n",
            "1132/1132 [==============================] - 1s 970us/step - loss: 1.3919 - acc: 0.3428 - val_loss: 1.4200 - val_acc: 0.3415\n",
            "Epoch 26/200\n",
            "1132/1132 [==============================] - 1s 970us/step - loss: 1.3983 - acc: 0.3578 - val_loss: 1.3903 - val_acc: 0.3380\n",
            "Epoch 27/200\n",
            "1132/1132 [==============================] - 1s 970us/step - loss: 1.3672 - acc: 0.3940 - val_loss: 1.3526 - val_acc: 0.3486\n",
            "Epoch 28/200\n",
            "1132/1132 [==============================] - 1s 973us/step - loss: 1.3501 - acc: 0.3807 - val_loss: 1.3477 - val_acc: 0.3521\n",
            "Epoch 29/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 1.3801 - acc: 0.3578 - val_loss: 1.3734 - val_acc: 0.3204\n",
            "Epoch 30/200\n",
            "1132/1132 [==============================] - 1s 996us/step - loss: 1.3783 - acc: 0.3816 - val_loss: 1.3938 - val_acc: 0.3134\n",
            "Epoch 31/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 1.3217 - acc: 0.3931 - val_loss: 1.3765 - val_acc: 0.2958\n",
            "Epoch 32/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 1.3125 - acc: 0.4011 - val_loss: 1.3564 - val_acc: 0.3063\n",
            "Epoch 33/200\n",
            "1132/1132 [==============================] - 1s 971us/step - loss: 1.3518 - acc: 0.4002 - val_loss: 1.3654 - val_acc: 0.3380\n",
            "Epoch 34/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 1.3723 - acc: 0.3843 - val_loss: 1.3650 - val_acc: 0.3310\n",
            "Epoch 35/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 1.3095 - acc: 0.4099 - val_loss: 1.3476 - val_acc: 0.3275\n",
            "Epoch 36/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.3132 - acc: 0.4019 - val_loss: 1.3552 - val_acc: 0.3169\n",
            "Epoch 37/200\n",
            "1132/1132 [==============================] - 1s 975us/step - loss: 1.2920 - acc: 0.4267 - val_loss: 1.3573 - val_acc: 0.3239\n",
            "Epoch 38/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 1.3746 - acc: 0.3648 - val_loss: 1.3521 - val_acc: 0.3310\n",
            "Epoch 39/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 1.2786 - acc: 0.4382 - val_loss: 1.3347 - val_acc: 0.3592\n",
            "Epoch 40/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 1.3129 - acc: 0.4152 - val_loss: 1.3267 - val_acc: 0.3768\n",
            "Epoch 41/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 1.2748 - acc: 0.4302 - val_loss: 1.3361 - val_acc: 0.3662\n",
            "Epoch 42/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.2655 - acc: 0.4390 - val_loss: 1.3604 - val_acc: 0.3556\n",
            "Epoch 43/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 1.2782 - acc: 0.4231 - val_loss: 1.3673 - val_acc: 0.3486\n",
            "Epoch 44/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 1.2440 - acc: 0.4505 - val_loss: 1.3428 - val_acc: 0.3521\n",
            "Epoch 45/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.2445 - acc: 0.4611 - val_loss: 1.3407 - val_acc: 0.3521\n",
            "Epoch 46/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.2441 - acc: 0.4373 - val_loss: 1.3708 - val_acc: 0.3415\n",
            "Epoch 47/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 1.2377 - acc: 0.4470 - val_loss: 1.4041 - val_acc: 0.3204\n",
            "Epoch 48/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 1.2281 - acc: 0.4620 - val_loss: 1.4021 - val_acc: 0.3345\n",
            "Epoch 49/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.2703 - acc: 0.4505 - val_loss: 1.3941 - val_acc: 0.3556\n",
            "Epoch 50/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 1.2438 - acc: 0.4514 - val_loss: 1.3996 - val_acc: 0.3486\n",
            "Epoch 51/200\n",
            "1132/1132 [==============================] - 1s 978us/step - loss: 1.2047 - acc: 0.4744 - val_loss: 1.3730 - val_acc: 0.3732\n",
            "Epoch 52/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.1912 - acc: 0.4655 - val_loss: 1.3491 - val_acc: 0.3979\n",
            "Epoch 53/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 1.2098 - acc: 0.4532 - val_loss: 1.3308 - val_acc: 0.3838\n",
            "Epoch 54/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.2399 - acc: 0.4655 - val_loss: 1.3382 - val_acc: 0.3838\n",
            "Epoch 55/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.2203 - acc: 0.4691 - val_loss: 1.3580 - val_acc: 0.3838\n",
            "Epoch 56/200\n",
            "1132/1132 [==============================] - 1s 970us/step - loss: 1.1775 - acc: 0.4850 - val_loss: 1.3475 - val_acc: 0.3732\n",
            "Epoch 57/200\n",
            "1132/1132 [==============================] - 1s 996us/step - loss: 1.1948 - acc: 0.4779 - val_loss: 1.3410 - val_acc: 0.3873\n",
            "Epoch 58/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.1810 - acc: 0.4982 - val_loss: 1.3372 - val_acc: 0.3944\n",
            "Epoch 59/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.1373 - acc: 0.5141 - val_loss: 1.3472 - val_acc: 0.3803\n",
            "Epoch 60/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 1.2026 - acc: 0.4938 - val_loss: 1.3372 - val_acc: 0.3908\n",
            "Epoch 61/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.1641 - acc: 0.4885 - val_loss: 1.3036 - val_acc: 0.4155\n",
            "Epoch 62/200\n",
            "1132/1132 [==============================] - 1s 974us/step - loss: 1.1473 - acc: 0.4912 - val_loss: 1.2804 - val_acc: 0.4085\n",
            "Epoch 63/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.1576 - acc: 0.4973 - val_loss: 1.2809 - val_acc: 0.4155\n",
            "Epoch 64/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 1.1421 - acc: 0.5247 - val_loss: 1.2882 - val_acc: 0.4155\n",
            "Epoch 65/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.1304 - acc: 0.5062 - val_loss: 1.2747 - val_acc: 0.4225\n",
            "Epoch 66/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.0972 - acc: 0.5265 - val_loss: 1.2541 - val_acc: 0.4401\n",
            "Epoch 67/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.1101 - acc: 0.5336 - val_loss: 1.2351 - val_acc: 0.4613\n",
            "Epoch 68/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.0896 - acc: 0.5318 - val_loss: 1.2503 - val_acc: 0.4331\n",
            "Epoch 69/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.1032 - acc: 0.5300 - val_loss: 1.2533 - val_acc: 0.4296\n",
            "Epoch 70/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.0910 - acc: 0.5477 - val_loss: 1.2540 - val_acc: 0.4296\n",
            "Epoch 71/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.0717 - acc: 0.5557 - val_loss: 1.2548 - val_acc: 0.4296\n",
            "Epoch 72/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.0666 - acc: 0.5477 - val_loss: 1.2623 - val_acc: 0.4261\n",
            "Epoch 73/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.0836 - acc: 0.5389 - val_loss: 1.2388 - val_acc: 0.4472\n",
            "Epoch 74/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.0745 - acc: 0.5451 - val_loss: 1.2209 - val_acc: 0.4613\n",
            "Epoch 75/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 1.0441 - acc: 0.5486 - val_loss: 1.2327 - val_acc: 0.4472\n",
            "Epoch 76/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 1.0273 - acc: 0.5548 - val_loss: 1.2464 - val_acc: 0.4331\n",
            "Epoch 77/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 1.0435 - acc: 0.5610 - val_loss: 1.2663 - val_acc: 0.4261\n",
            "Epoch 78/200\n",
            "1132/1132 [==============================] - 1s 978us/step - loss: 1.0396 - acc: 0.5548 - val_loss: 1.2524 - val_acc: 0.4507\n",
            "Epoch 79/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 1.0211 - acc: 0.5671 - val_loss: 1.2171 - val_acc: 0.4859\n",
            "Epoch 80/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.0483 - acc: 0.5671 - val_loss: 1.1943 - val_acc: 0.4789\n",
            "Epoch 81/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 1.0355 - acc: 0.5689 - val_loss: 1.1704 - val_acc: 0.4894\n",
            "Epoch 82/200\n",
            "1132/1132 [==============================] - 1s 980us/step - loss: 0.9889 - acc: 0.5963 - val_loss: 1.1628 - val_acc: 0.4859\n",
            "Epoch 83/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.0516 - acc: 0.5557 - val_loss: 1.1933 - val_acc: 0.4789\n",
            "Epoch 84/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 1.0070 - acc: 0.5786 - val_loss: 1.2037 - val_acc: 0.4965\n",
            "Epoch 85/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.9903 - acc: 0.5928 - val_loss: 1.1846 - val_acc: 0.5035\n",
            "Epoch 86/200\n",
            "1132/1132 [==============================] - 1s 979us/step - loss: 0.9851 - acc: 0.5627 - val_loss: 1.1875 - val_acc: 0.5176\n",
            "Epoch 87/200\n",
            "1132/1132 [==============================] - 1s 978us/step - loss: 1.0177 - acc: 0.5760 - val_loss: 1.1851 - val_acc: 0.4894\n",
            "Epoch 88/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.9497 - acc: 0.6078 - val_loss: 1.1901 - val_acc: 0.4930\n",
            "Epoch 89/200\n",
            "1132/1132 [==============================] - 1s 975us/step - loss: 0.9255 - acc: 0.6051 - val_loss: 1.2145 - val_acc: 0.4859\n",
            "Epoch 90/200\n",
            "1132/1132 [==============================] - 1s 978us/step - loss: 0.9787 - acc: 0.5830 - val_loss: 1.2251 - val_acc: 0.4930\n",
            "Epoch 91/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.9549 - acc: 0.6157 - val_loss: 1.1931 - val_acc: 0.4930\n",
            "Epoch 92/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 0.9070 - acc: 0.6396 - val_loss: 1.1823 - val_acc: 0.5070\n",
            "Epoch 93/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 0.9347 - acc: 0.6051 - val_loss: 1.1980 - val_acc: 0.4894\n",
            "Epoch 94/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.9371 - acc: 0.6148 - val_loss: 1.1997 - val_acc: 0.4894\n",
            "Epoch 95/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.8717 - acc: 0.6272 - val_loss: 1.1879 - val_acc: 0.4859\n",
            "Epoch 96/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.8869 - acc: 0.6290 - val_loss: 1.1831 - val_acc: 0.4930\n",
            "Epoch 97/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.9308 - acc: 0.6095 - val_loss: 1.1799 - val_acc: 0.4930\n",
            "Epoch 98/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.9441 - acc: 0.6175 - val_loss: 1.1766 - val_acc: 0.4930\n",
            "Epoch 99/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.8943 - acc: 0.6422 - val_loss: 1.1870 - val_acc: 0.4754\n",
            "Epoch 100/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.8518 - acc: 0.6652 - val_loss: 1.2004 - val_acc: 0.4894\n",
            "Epoch 101/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.8278 - acc: 0.6511 - val_loss: 1.1971 - val_acc: 0.4894\n",
            "Epoch 102/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 0.8809 - acc: 0.6475 - val_loss: 1.1800 - val_acc: 0.4894\n",
            "Epoch 103/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.8549 - acc: 0.6493 - val_loss: 1.1576 - val_acc: 0.5000\n",
            "Epoch 104/200\n",
            "1132/1132 [==============================] - 1s 975us/step - loss: 0.8345 - acc: 0.6634 - val_loss: 1.1596 - val_acc: 0.4965\n",
            "Epoch 105/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.8140 - acc: 0.6855 - val_loss: 1.1630 - val_acc: 0.4859\n",
            "Epoch 106/200\n",
            "1132/1132 [==============================] - 1s 980us/step - loss: 0.8175 - acc: 0.6643 - val_loss: 1.1567 - val_acc: 0.4930\n",
            "Epoch 107/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.8364 - acc: 0.6687 - val_loss: 1.1591 - val_acc: 0.5387\n",
            "Epoch 108/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 0.7949 - acc: 0.6767 - val_loss: 1.1825 - val_acc: 0.5282\n",
            "Epoch 109/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.8331 - acc: 0.6564 - val_loss: 1.2023 - val_acc: 0.4859\n",
            "Epoch 110/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 0.8256 - acc: 0.6670 - val_loss: 1.2247 - val_acc: 0.5070\n",
            "Epoch 111/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.7961 - acc: 0.6731 - val_loss: 1.2579 - val_acc: 0.5141\n",
            "Epoch 112/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.8240 - acc: 0.6758 - val_loss: 1.2753 - val_acc: 0.5141\n",
            "Epoch 113/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 0.7978 - acc: 0.6652 - val_loss: 1.2603 - val_acc: 0.5176\n",
            "Epoch 114/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.7620 - acc: 0.6935 - val_loss: 1.2434 - val_acc: 0.5070\n",
            "Epoch 115/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.7877 - acc: 0.6811 - val_loss: 1.2308 - val_acc: 0.5035\n",
            "Epoch 116/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 0.7601 - acc: 0.6926 - val_loss: 1.2162 - val_acc: 0.5070\n",
            "Epoch 117/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 0.7812 - acc: 0.6873 - val_loss: 1.1945 - val_acc: 0.5282\n",
            "Epoch 118/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.7920 - acc: 0.7032 - val_loss: 1.1773 - val_acc: 0.5246\n",
            "Epoch 119/200\n",
            "1132/1132 [==============================] - 1s 978us/step - loss: 0.7659 - acc: 0.6846 - val_loss: 1.1951 - val_acc: 0.5317\n",
            "Epoch 120/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.7320 - acc: 0.6961 - val_loss: 1.2108 - val_acc: 0.5317\n",
            "Epoch 121/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.7283 - acc: 0.7253 - val_loss: 1.2225 - val_acc: 0.5246\n",
            "Epoch 122/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.7484 - acc: 0.6979 - val_loss: 1.2674 - val_acc: 0.5070\n",
            "Epoch 123/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 0.6814 - acc: 0.7244 - val_loss: 1.3066 - val_acc: 0.5070\n",
            "Epoch 124/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.7158 - acc: 0.7155 - val_loss: 1.3391 - val_acc: 0.5141\n",
            "Epoch 125/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 0.7116 - acc: 0.7076 - val_loss: 1.3380 - val_acc: 0.5000\n",
            "Epoch 126/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 0.6738 - acc: 0.7244 - val_loss: 1.3078 - val_acc: 0.5070\n",
            "Epoch 127/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.7104 - acc: 0.7200 - val_loss: 1.2680 - val_acc: 0.5211\n",
            "Epoch 128/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 0.6919 - acc: 0.7332 - val_loss: 1.2612 - val_acc: 0.5070\n",
            "Epoch 129/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 0.7350 - acc: 0.6988 - val_loss: 1.2939 - val_acc: 0.5211\n",
            "Epoch 130/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.6705 - acc: 0.7297 - val_loss: 1.2813 - val_acc: 0.5141\n",
            "Epoch 131/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 0.6546 - acc: 0.7341 - val_loss: 1.2612 - val_acc: 0.5141\n",
            "Epoch 132/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.6717 - acc: 0.7385 - val_loss: 1.2335 - val_acc: 0.5211\n",
            "Epoch 133/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 0.7262 - acc: 0.7049 - val_loss: 1.2330 - val_acc: 0.5317\n",
            "Epoch 134/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 0.6361 - acc: 0.7429 - val_loss: 1.2337 - val_acc: 0.5317\n",
            "Epoch 135/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.6714 - acc: 0.7385 - val_loss: 1.2343 - val_acc: 0.5246\n",
            "Epoch 136/200\n",
            "1132/1132 [==============================] - 1s 977us/step - loss: 0.6914 - acc: 0.7217 - val_loss: 1.2229 - val_acc: 0.5246\n",
            "Epoch 137/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 0.6436 - acc: 0.7527 - val_loss: 1.2143 - val_acc: 0.5387\n",
            "Epoch 138/200\n",
            "1132/1132 [==============================] - 1s 979us/step - loss: 0.6638 - acc: 0.7456 - val_loss: 1.2112 - val_acc: 0.5387\n",
            "Epoch 139/200\n",
            "1132/1132 [==============================] - 1s 996us/step - loss: 0.6491 - acc: 0.7359 - val_loss: 1.2033 - val_acc: 0.5352\n",
            "Epoch 140/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 0.6517 - acc: 0.7535 - val_loss: 1.2057 - val_acc: 0.5317\n",
            "Epoch 141/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 0.6546 - acc: 0.7553 - val_loss: 1.2223 - val_acc: 0.5246\n",
            "Epoch 142/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 0.6494 - acc: 0.7482 - val_loss: 1.2505 - val_acc: 0.5176\n",
            "Epoch 143/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 0.6399 - acc: 0.7420 - val_loss: 1.3285 - val_acc: 0.5176\n",
            "Epoch 144/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.6307 - acc: 0.7482 - val_loss: 1.3771 - val_acc: 0.5246\n",
            "Epoch 145/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.6111 - acc: 0.7615 - val_loss: 1.4027 - val_acc: 0.5176\n",
            "Epoch 146/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.6245 - acc: 0.7571 - val_loss: 1.3978 - val_acc: 0.5106\n",
            "Epoch 147/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 0.5644 - acc: 0.7747 - val_loss: 1.3719 - val_acc: 0.5106\n",
            "Epoch 148/200\n",
            "1132/1132 [==============================] - 1s 1000us/step - loss: 0.5876 - acc: 0.7712 - val_loss: 1.3479 - val_acc: 0.5176\n",
            "Epoch 149/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 0.6211 - acc: 0.7473 - val_loss: 1.3166 - val_acc: 0.5387\n",
            "Epoch 150/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 0.5964 - acc: 0.7686 - val_loss: 1.3357 - val_acc: 0.5352\n",
            "Epoch 151/200\n",
            "1132/1132 [==============================] - 1s 978us/step - loss: 0.5618 - acc: 0.7809 - val_loss: 1.3388 - val_acc: 0.5282\n",
            "Epoch 152/200\n",
            "1132/1132 [==============================] - 1s 978us/step - loss: 0.5992 - acc: 0.7800 - val_loss: 1.3095 - val_acc: 0.5423\n",
            "Epoch 153/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.5875 - acc: 0.7686 - val_loss: 1.3050 - val_acc: 0.5211\n",
            "Epoch 154/200\n",
            "1132/1132 [==============================] - 1s 980us/step - loss: 0.5925 - acc: 0.7756 - val_loss: 1.3442 - val_acc: 0.4965\n",
            "Epoch 155/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.5697 - acc: 0.7721 - val_loss: 1.4037 - val_acc: 0.5035\n",
            "Epoch 156/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.5923 - acc: 0.7739 - val_loss: 1.4457 - val_acc: 0.5176\n",
            "Epoch 157/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.5873 - acc: 0.7739 - val_loss: 1.4614 - val_acc: 0.5211\n",
            "Epoch 158/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 0.5673 - acc: 0.7641 - val_loss: 1.4368 - val_acc: 0.5106\n",
            "Epoch 159/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.5885 - acc: 0.7703 - val_loss: 1.3851 - val_acc: 0.5106\n",
            "Epoch 160/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.5645 - acc: 0.7827 - val_loss: 1.3516 - val_acc: 0.5106\n",
            "Epoch 161/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 0.5288 - acc: 0.7871 - val_loss: 1.3546 - val_acc: 0.5141\n",
            "Epoch 162/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 0.5384 - acc: 0.7933 - val_loss: 1.3621 - val_acc: 0.5176\n",
            "Epoch 163/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.5197 - acc: 0.7959 - val_loss: 1.3592 - val_acc: 0.5246\n",
            "Epoch 164/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.4882 - acc: 0.8092 - val_loss: 1.3656 - val_acc: 0.5317\n",
            "Epoch 165/200\n",
            "1132/1132 [==============================] - 1s 978us/step - loss: 0.5506 - acc: 0.7915 - val_loss: 1.3400 - val_acc: 0.5282\n",
            "Epoch 166/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.5227 - acc: 0.8021 - val_loss: 1.2987 - val_acc: 0.5528\n",
            "Epoch 167/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 0.5633 - acc: 0.7827 - val_loss: 1.2973 - val_acc: 0.5387\n",
            "Epoch 168/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.5035 - acc: 0.8127 - val_loss: 1.3136 - val_acc: 0.5387\n",
            "Epoch 169/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.5277 - acc: 0.7933 - val_loss: 1.3359 - val_acc: 0.5352\n",
            "Epoch 170/200\n",
            "1132/1132 [==============================] - 1s 979us/step - loss: 0.5111 - acc: 0.8065 - val_loss: 1.3999 - val_acc: 0.5246\n",
            "Epoch 171/200\n",
            "1132/1132 [==============================] - 1s 980us/step - loss: 0.5193 - acc: 0.8021 - val_loss: 1.3813 - val_acc: 0.5246\n",
            "Epoch 172/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 0.5215 - acc: 0.8092 - val_loss: 1.3794 - val_acc: 0.5211\n",
            "Epoch 173/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.5109 - acc: 0.8021 - val_loss: 1.3729 - val_acc: 0.5317\n",
            "Epoch 174/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.4869 - acc: 0.8198 - val_loss: 1.3648 - val_acc: 0.5176\n",
            "Epoch 175/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.4821 - acc: 0.7986 - val_loss: 1.3870 - val_acc: 0.5282\n",
            "Epoch 176/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.4642 - acc: 0.8136 - val_loss: 1.4005 - val_acc: 0.5176\n",
            "Epoch 177/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.4781 - acc: 0.8127 - val_loss: 1.3388 - val_acc: 0.5282\n",
            "Epoch 178/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 0.4804 - acc: 0.8074 - val_loss: 1.2835 - val_acc: 0.5317\n",
            "Epoch 179/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.5165 - acc: 0.8065 - val_loss: 1.2874 - val_acc: 0.5458\n",
            "Epoch 180/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.5066 - acc: 0.8074 - val_loss: 1.2984 - val_acc: 0.5246\n",
            "Epoch 181/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 0.4687 - acc: 0.8110 - val_loss: 1.3021 - val_acc: 0.5493\n",
            "Epoch 182/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.4973 - acc: 0.8145 - val_loss: 1.2987 - val_acc: 0.5669\n",
            "Epoch 183/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.4759 - acc: 0.8286 - val_loss: 1.2748 - val_acc: 0.5634\n",
            "Epoch 184/200\n",
            "1132/1132 [==============================] - 1s 996us/step - loss: 0.4439 - acc: 0.8339 - val_loss: 1.2381 - val_acc: 0.5528\n",
            "Epoch 185/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 0.4539 - acc: 0.8260 - val_loss: 1.2384 - val_acc: 0.5634\n",
            "Epoch 186/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.4538 - acc: 0.8295 - val_loss: 1.2738 - val_acc: 0.5493\n",
            "Epoch 187/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.4482 - acc: 0.8260 - val_loss: 1.3433 - val_acc: 0.5317\n",
            "Epoch 188/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 0.4708 - acc: 0.8163 - val_loss: 1.4049 - val_acc: 0.5176\n",
            "Epoch 189/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 0.5040 - acc: 0.8012 - val_loss: 1.4365 - val_acc: 0.5317\n",
            "Epoch 190/200\n",
            "1132/1132 [==============================] - 1s 979us/step - loss: 0.4342 - acc: 0.8304 - val_loss: 1.4527 - val_acc: 0.5317\n",
            "Epoch 191/200\n",
            "1132/1132 [==============================] - 1s 980us/step - loss: 0.4415 - acc: 0.8375 - val_loss: 1.4341 - val_acc: 0.5282\n",
            "Epoch 192/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.4552 - acc: 0.8304 - val_loss: 1.3953 - val_acc: 0.5106\n",
            "Epoch 193/200\n",
            "1132/1132 [==============================] - 1s 978us/step - loss: 0.4742 - acc: 0.8295 - val_loss: 1.3810 - val_acc: 0.5141\n",
            "Epoch 194/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.4579 - acc: 0.8189 - val_loss: 1.3696 - val_acc: 0.5106\n",
            "Epoch 195/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.4075 - acc: 0.8366 - val_loss: 1.3712 - val_acc: 0.5106\n",
            "Epoch 196/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 0.4302 - acc: 0.8401 - val_loss: 1.3555 - val_acc: 0.5035\n",
            "Epoch 197/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 0.4332 - acc: 0.8339 - val_loss: 1.3613 - val_acc: 0.5176\n",
            "Epoch 198/200\n",
            "1132/1132 [==============================] - 1s 980us/step - loss: 0.4335 - acc: 0.8277 - val_loss: 1.3455 - val_acc: 0.5141\n",
            "Epoch 199/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.4178 - acc: 0.8375 - val_loss: 1.3246 - val_acc: 0.5035\n",
            "Epoch 200/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.4573 - acc: 0.8198 - val_loss: 1.3204 - val_acc: 0.5211\n",
            "Done with one subject\n",
            "300/300 [==============================] - 0s 342us/step\n",
            "300/300 [==============================] - 0s 335us/step\n",
            "300/300 [==============================] - 0s 336us/step\n",
            "300/300 [==============================] - 0s 374us/step\n",
            "282/282 [==============================] - 0s 365us/step\n",
            "294/294 [==============================] - 0s 353us/step\n",
            "300/300 [==============================] - 0s 337us/step\n",
            "300/300 [==============================] - 0s 354us/step\n",
            "282/282 [==============================] - 0s 336us/step\n",
            "Train on 1123 samples, validate on 281 samples\n",
            "Epoch 1/200\n",
            "1123/1123 [==============================] - 3s 3ms/step - loss: 2.7867 - acc: 0.2306 - val_loss: 1.5710 - val_acc: 0.2171\n",
            "Epoch 2/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 2.6350 - acc: 0.2618 - val_loss: 1.8660 - val_acc: 0.2313\n",
            "Epoch 3/200\n",
            "1123/1123 [==============================] - 1s 999us/step - loss: 2.4403 - acc: 0.2663 - val_loss: 1.9197 - val_acc: 0.2883\n",
            "Epoch 4/200\n",
            "1123/1123 [==============================] - 1s 982us/step - loss: 2.3019 - acc: 0.2769 - val_loss: 1.8834 - val_acc: 0.2883\n",
            "Epoch 5/200\n",
            "1123/1123 [==============================] - 1s 998us/step - loss: 2.2044 - acc: 0.2565 - val_loss: 1.8447 - val_acc: 0.2456\n",
            "Epoch 6/200\n",
            "1123/1123 [==============================] - 1s 984us/step - loss: 2.0118 - acc: 0.2850 - val_loss: 1.8025 - val_acc: 0.1673\n",
            "Epoch 7/200\n",
            "1123/1123 [==============================] - 1s 992us/step - loss: 1.8996 - acc: 0.2778 - val_loss: 1.9099 - val_acc: 0.2135\n",
            "Epoch 8/200\n",
            "1123/1123 [==============================] - 1s 986us/step - loss: 1.9009 - acc: 0.2582 - val_loss: 1.8936 - val_acc: 0.2206\n",
            "Epoch 9/200\n",
            "1123/1123 [==============================] - 1s 990us/step - loss: 1.8074 - acc: 0.2867 - val_loss: 1.6801 - val_acc: 0.2206\n",
            "Epoch 10/200\n",
            "1123/1123 [==============================] - 1s 997us/step - loss: 1.6610 - acc: 0.2947 - val_loss: 1.4823 - val_acc: 0.2384\n",
            "Epoch 11/200\n",
            "1123/1123 [==============================] - 1s 993us/step - loss: 1.6673 - acc: 0.2805 - val_loss: 1.4422 - val_acc: 0.2776\n",
            "Epoch 12/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 1.5845 - acc: 0.2903 - val_loss: 1.4994 - val_acc: 0.2562\n",
            "Epoch 13/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 1.6066 - acc: 0.2939 - val_loss: 1.5423 - val_acc: 0.2384\n",
            "Epoch 14/200\n",
            "1123/1123 [==============================] - 1s 994us/step - loss: 1.5653 - acc: 0.3054 - val_loss: 1.5636 - val_acc: 0.2171\n",
            "Epoch 15/200\n",
            "1123/1123 [==============================] - 1s 989us/step - loss: 1.5543 - acc: 0.2921 - val_loss: 1.5152 - val_acc: 0.2349\n",
            "Epoch 16/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 1.5224 - acc: 0.2769 - val_loss: 1.4506 - val_acc: 0.3060\n",
            "Epoch 17/200\n",
            "1123/1123 [==============================] - 1s 986us/step - loss: 1.5218 - acc: 0.2752 - val_loss: 1.4896 - val_acc: 0.2705\n",
            "Epoch 18/200\n",
            "1123/1123 [==============================] - 1s 985us/step - loss: 1.4981 - acc: 0.2867 - val_loss: 1.5812 - val_acc: 0.2242\n",
            "Epoch 19/200\n",
            "1123/1123 [==============================] - 1s 991us/step - loss: 1.4832 - acc: 0.3215 - val_loss: 1.5447 - val_acc: 0.2028\n",
            "Epoch 20/200\n",
            "1123/1123 [==============================] - 1s 1ms/step - loss: 1.4459 - acc: 0.3081 - val_loss: 1.4962 - val_acc: 0.2527\n",
            "Epoch 21/200\n",
            "1123/1123 [==============================] - 1s 985us/step - loss: 1.4800 - acc: 0.2921 - val_loss: 1.4999 - val_acc: 0.2491\n",
            "Epoch 22/200\n",
            "1123/1123 [==============================] - 1s 994us/step - loss: 1.4504 - acc: 0.3126 - val_loss: 1.5173 - val_acc: 0.2491\n",
            "Epoch 23/200\n",
            "1123/1123 [==============================] - 1s 983us/step - loss: 1.4866 - acc: 0.2992 - val_loss: 1.5083 - val_acc: 0.2420\n",
            "Epoch 24/200\n",
            "1123/1123 [==============================] - 1s 992us/step - loss: 1.4881 - acc: 0.3037 - val_loss: 1.5216 - val_acc: 0.2242\n",
            "Epoch 25/200\n",
            "1123/1123 [==============================] - 1s 999us/step - loss: 1.4397 - acc: 0.3063 - val_loss: 1.5022 - val_acc: 0.2278\n",
            "Epoch 26/200\n",
            "1123/1123 [==============================] - 1s 987us/step - loss: 1.4211 - acc: 0.3170 - val_loss: 1.4986 - val_acc: 0.2313\n",
            "Epoch 27/200\n",
            "1123/1123 [==============================] - 1s 986us/step - loss: 1.3987 - acc: 0.3313 - val_loss: 1.4653 - val_acc: 0.2527\n",
            "Epoch 28/200\n",
            "1123/1123 [==============================] - 1s 987us/step - loss: 1.4138 - acc: 0.3277 - val_loss: 1.4844 - val_acc: 0.2527\n",
            "Epoch 29/200\n",
            "1123/1123 [==============================] - 1s 983us/step - loss: 1.4154 - acc: 0.3517 - val_loss: 1.5002 - val_acc: 0.2527\n",
            "Epoch 30/200\n",
            "1123/1123 [==============================] - 1s 986us/step - loss: 1.3842 - acc: 0.3500 - val_loss: 1.4825 - val_acc: 0.2420\n",
            "Epoch 31/200\n",
            "1123/1123 [==============================] - 1s 995us/step - loss: 1.4013 - acc: 0.3384 - val_loss: 1.4666 - val_acc: 0.2562\n",
            "Epoch 32/200\n",
            "1123/1123 [==============================] - 1s 991us/step - loss: 1.4054 - acc: 0.3446 - val_loss: 1.4778 - val_acc: 0.2349\n",
            "Epoch 33/200\n",
            "1123/1123 [==============================] - 1s 985us/step - loss: 1.3803 - acc: 0.3535 - val_loss: 1.4963 - val_acc: 0.2527\n",
            "Epoch 34/200\n",
            "1123/1123 [==============================] - 1s 986us/step - loss: 1.3845 - acc: 0.3455 - val_loss: 1.4962 - val_acc: 0.2420\n",
            "Epoch 35/200\n",
            "1123/1123 [==============================] - 1s 987us/step - loss: 1.3997 - acc: 0.3517 - val_loss: 1.4690 - val_acc: 0.2776\n",
            "Epoch 36/200\n",
            "1123/1123 [==============================] - 1s 992us/step - loss: 1.3948 - acc: 0.3419 - val_loss: 1.4526 - val_acc: 0.2740\n",
            "Epoch 37/200\n",
            "1123/1123 [==============================] - 1s 992us/step - loss: 1.3568 - acc: 0.3669 - val_loss: 1.4718 - val_acc: 0.2420\n",
            "Epoch 38/200\n",
            "1123/1123 [==============================] - 1s 982us/step - loss: 1.3787 - acc: 0.3642 - val_loss: 1.4728 - val_acc: 0.2669\n",
            "Epoch 39/200\n",
            "1123/1123 [==============================] - 1s 991us/step - loss: 1.3406 - acc: 0.3695 - val_loss: 1.4528 - val_acc: 0.2633\n",
            "Epoch 40/200\n",
            "1123/1123 [==============================] - 1s 990us/step - loss: 1.3492 - acc: 0.3838 - val_loss: 1.4571 - val_acc: 0.2705\n",
            "Epoch 41/200\n",
            "1123/1123 [==============================] - 1s 987us/step - loss: 1.3614 - acc: 0.3695 - val_loss: 1.4607 - val_acc: 0.2562\n",
            "Epoch 42/200\n",
            "1123/1123 [==============================] - 1s 996us/step - loss: 1.3344 - acc: 0.3847 - val_loss: 1.4596 - val_acc: 0.2669\n",
            "Epoch 43/200\n",
            "1123/1123 [==============================] - 1s 992us/step - loss: 1.2975 - acc: 0.4221 - val_loss: 1.4481 - val_acc: 0.2740\n",
            "Epoch 44/200\n",
            "1123/1123 [==============================] - 1s 1ms/step - loss: 1.3268 - acc: 0.3793 - val_loss: 1.4497 - val_acc: 0.2740\n",
            "Epoch 45/200\n",
            "1123/1123 [==============================] - 1s 983us/step - loss: 1.3259 - acc: 0.3963 - val_loss: 1.4661 - val_acc: 0.2562\n",
            "Epoch 46/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 1.3498 - acc: 0.3847 - val_loss: 1.4670 - val_acc: 0.2669\n",
            "Epoch 47/200\n",
            "1123/1123 [==============================] - 1s 996us/step - loss: 1.2994 - acc: 0.4105 - val_loss: 1.4612 - val_acc: 0.2847\n",
            "Epoch 48/200\n",
            "1123/1123 [==============================] - 1s 981us/step - loss: 1.3154 - acc: 0.3980 - val_loss: 1.4771 - val_acc: 0.2740\n",
            "Epoch 49/200\n",
            "1123/1123 [==============================] - 1s 990us/step - loss: 1.2881 - acc: 0.4212 - val_loss: 1.4855 - val_acc: 0.2527\n",
            "Epoch 50/200\n",
            "1123/1123 [==============================] - 1s 978us/step - loss: 1.2849 - acc: 0.4150 - val_loss: 1.4808 - val_acc: 0.2740\n",
            "Epoch 51/200\n",
            "1123/1123 [==============================] - 1s 994us/step - loss: 1.2629 - acc: 0.4301 - val_loss: 1.4801 - val_acc: 0.2847\n",
            "Epoch 52/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 1.2565 - acc: 0.4256 - val_loss: 1.4892 - val_acc: 0.2740\n",
            "Epoch 53/200\n",
            "1123/1123 [==============================] - 1s 987us/step - loss: 1.2605 - acc: 0.4292 - val_loss: 1.4918 - val_acc: 0.2669\n",
            "Epoch 54/200\n",
            "1123/1123 [==============================] - 1s 999us/step - loss: 1.2451 - acc: 0.4426 - val_loss: 1.4903 - val_acc: 0.2598\n",
            "Epoch 55/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 1.2474 - acc: 0.4310 - val_loss: 1.4943 - val_acc: 0.2491\n",
            "Epoch 56/200\n",
            "1123/1123 [==============================] - 1s 985us/step - loss: 1.2164 - acc: 0.4657 - val_loss: 1.4946 - val_acc: 0.2669\n",
            "Epoch 57/200\n",
            "1123/1123 [==============================] - 1s 983us/step - loss: 1.2384 - acc: 0.4666 - val_loss: 1.5094 - val_acc: 0.2776\n",
            "Epoch 58/200\n",
            "1123/1123 [==============================] - 1s 990us/step - loss: 1.2360 - acc: 0.4533 - val_loss: 1.5452 - val_acc: 0.2527\n",
            "Epoch 59/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 1.2123 - acc: 0.4693 - val_loss: 1.5906 - val_acc: 0.2456\n",
            "Epoch 60/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 1.2104 - acc: 0.4817 - val_loss: 1.5978 - val_acc: 0.2491\n",
            "Epoch 61/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 1.1983 - acc: 0.4613 - val_loss: 1.5936 - val_acc: 0.2598\n",
            "Epoch 62/200\n",
            "1123/1123 [==============================] - 1s 989us/step - loss: 1.1886 - acc: 0.4791 - val_loss: 1.5992 - val_acc: 0.2669\n",
            "Epoch 63/200\n",
            "1123/1123 [==============================] - 1s 987us/step - loss: 1.1495 - acc: 0.5049 - val_loss: 1.6388 - val_acc: 0.2562\n",
            "Epoch 64/200\n",
            "1123/1123 [==============================] - 1s 984us/step - loss: 1.1749 - acc: 0.4737 - val_loss: 1.6830 - val_acc: 0.2456\n",
            "Epoch 65/200\n",
            "1123/1123 [==============================] - 1s 997us/step - loss: 1.1501 - acc: 0.5058 - val_loss: 1.7022 - val_acc: 0.2384\n",
            "Epoch 66/200\n",
            "1123/1123 [==============================] - 1s 981us/step - loss: 1.1522 - acc: 0.5004 - val_loss: 1.6841 - val_acc: 0.2491\n",
            "Epoch 67/200\n",
            "1123/1123 [==============================] - 1s 981us/step - loss: 1.1282 - acc: 0.5013 - val_loss: 1.6572 - val_acc: 0.2669\n",
            "Epoch 68/200\n",
            "1123/1123 [==============================] - 1s 992us/step - loss: 1.1242 - acc: 0.5156 - val_loss: 1.6542 - val_acc: 0.2776\n",
            "Epoch 69/200\n",
            "1123/1123 [==============================] - 1s 997us/step - loss: 1.1247 - acc: 0.5085 - val_loss: 1.6504 - val_acc: 0.2740\n",
            "Epoch 70/200\n",
            "1123/1123 [==============================] - 1s 993us/step - loss: 1.1083 - acc: 0.5165 - val_loss: 1.6480 - val_acc: 0.2811\n",
            "Epoch 71/200\n",
            "1123/1123 [==============================] - 1s 992us/step - loss: 1.1170 - acc: 0.5156 - val_loss: 1.6504 - val_acc: 0.2847\n",
            "Epoch 72/200\n",
            "1123/1123 [==============================] - 1s 999us/step - loss: 1.0813 - acc: 0.5325 - val_loss: 1.6770 - val_acc: 0.2740\n",
            "Epoch 73/200\n",
            "1123/1123 [==============================] - 1s 986us/step - loss: 1.1062 - acc: 0.5174 - val_loss: 1.6898 - val_acc: 0.2705\n",
            "Epoch 74/200\n",
            "1123/1123 [==============================] - 1s 996us/step - loss: 1.0809 - acc: 0.5396 - val_loss: 1.6892 - val_acc: 0.2705\n",
            "Epoch 75/200\n",
            "1123/1123 [==============================] - 1s 995us/step - loss: 1.0692 - acc: 0.5432 - val_loss: 1.7037 - val_acc: 0.2633\n",
            "Epoch 76/200\n",
            "1123/1123 [==============================] - 1s 987us/step - loss: 1.0893 - acc: 0.5343 - val_loss: 1.6923 - val_acc: 0.2669\n",
            "Epoch 77/200\n",
            "1123/1123 [==============================] - 1s 994us/step - loss: 1.0713 - acc: 0.5423 - val_loss: 1.6812 - val_acc: 0.2527\n",
            "Epoch 78/200\n",
            "1123/1123 [==============================] - 1s 995us/step - loss: 1.0522 - acc: 0.5476 - val_loss: 1.6850 - val_acc: 0.2669\n",
            "Epoch 79/200\n",
            "1123/1123 [==============================] - 1s 986us/step - loss: 1.0593 - acc: 0.5503 - val_loss: 1.6978 - val_acc: 0.2705\n",
            "Epoch 80/200\n",
            "1123/1123 [==============================] - 1s 992us/step - loss: 1.0180 - acc: 0.5726 - val_loss: 1.7104 - val_acc: 0.2669\n",
            "Epoch 81/200\n",
            "1123/1123 [==============================] - 1s 979us/step - loss: 1.0379 - acc: 0.5690 - val_loss: 1.7226 - val_acc: 0.2883\n",
            "Epoch 82/200\n",
            "1123/1123 [==============================] - 1s 984us/step - loss: 1.0276 - acc: 0.5459 - val_loss: 1.7321 - val_acc: 0.2918\n",
            "Epoch 83/200\n",
            "1123/1123 [==============================] - 1s 1ms/step - loss: 1.0274 - acc: 0.5610 - val_loss: 1.7291 - val_acc: 0.2954\n",
            "Epoch 84/200\n",
            "1123/1123 [==============================] - 1s 984us/step - loss: 0.9970 - acc: 0.5744 - val_loss: 1.7276 - val_acc: 0.2776\n",
            "Epoch 85/200\n",
            "1123/1123 [==============================] - 1s 990us/step - loss: 0.9874 - acc: 0.5717 - val_loss: 1.7475 - val_acc: 0.2633\n",
            "Epoch 86/200\n",
            "1123/1123 [==============================] - 1s 991us/step - loss: 1.0149 - acc: 0.5797 - val_loss: 1.7677 - val_acc: 0.2527\n",
            "Epoch 87/200\n",
            "1123/1123 [==============================] - 1s 990us/step - loss: 0.9592 - acc: 0.5922 - val_loss: 1.7807 - val_acc: 0.2669\n",
            "Epoch 88/200\n",
            "1123/1123 [==============================] - 1s 993us/step - loss: 0.9719 - acc: 0.5904 - val_loss: 1.8068 - val_acc: 0.2527\n",
            "Epoch 89/200\n",
            "1123/1123 [==============================] - 1s 987us/step - loss: 1.0058 - acc: 0.5735 - val_loss: 1.8019 - val_acc: 0.2669\n",
            "Epoch 90/200\n",
            "1123/1123 [==============================] - 1s 1ms/step - loss: 0.9386 - acc: 0.6109 - val_loss: 1.8074 - val_acc: 0.2705\n",
            "Epoch 91/200\n",
            "1123/1123 [==============================] - 1s 975us/step - loss: 0.9621 - acc: 0.6002 - val_loss: 1.7986 - val_acc: 0.2918\n",
            "Epoch 92/200\n",
            "1123/1123 [==============================] - 1s 982us/step - loss: 0.9312 - acc: 0.6269 - val_loss: 1.8108 - val_acc: 0.2776\n",
            "Epoch 93/200\n",
            "1123/1123 [==============================] - 1s 972us/step - loss: 0.9129 - acc: 0.6438 - val_loss: 1.8130 - val_acc: 0.2705\n",
            "Epoch 94/200\n",
            "1123/1123 [==============================] - 1s 970us/step - loss: 0.9371 - acc: 0.6135 - val_loss: 1.8136 - val_acc: 0.2562\n",
            "Epoch 95/200\n",
            "1123/1123 [==============================] - 1s 966us/step - loss: 0.8876 - acc: 0.6340 - val_loss: 1.8140 - val_acc: 0.2562\n",
            "Epoch 96/200\n",
            "1123/1123 [==============================] - 1s 967us/step - loss: 0.9183 - acc: 0.6037 - val_loss: 1.8112 - val_acc: 0.2847\n",
            "Epoch 97/200\n",
            "1123/1123 [==============================] - 1s 973us/step - loss: 0.8913 - acc: 0.6500 - val_loss: 1.8359 - val_acc: 0.2776\n",
            "Epoch 98/200\n",
            "1123/1123 [==============================] - 1s 970us/step - loss: 0.8781 - acc: 0.6340 - val_loss: 1.8456 - val_acc: 0.2562\n",
            "Epoch 99/200\n",
            "1123/1123 [==============================] - 1s 974us/step - loss: 0.8676 - acc: 0.6233 - val_loss: 1.8652 - val_acc: 0.2633\n",
            "Epoch 100/200\n",
            "1123/1123 [==============================] - 1s 971us/step - loss: 0.8540 - acc: 0.6527 - val_loss: 1.8788 - val_acc: 0.2527\n",
            "Epoch 101/200\n",
            "1123/1123 [==============================] - 1s 970us/step - loss: 0.8495 - acc: 0.6652 - val_loss: 1.9049 - val_acc: 0.2384\n",
            "Epoch 102/200\n",
            "1123/1123 [==============================] - 1s 975us/step - loss: 0.8710 - acc: 0.6456 - val_loss: 1.8971 - val_acc: 0.2491\n",
            "Epoch 103/200\n",
            "1123/1123 [==============================] - 1s 975us/step - loss: 0.8385 - acc: 0.6411 - val_loss: 1.9004 - val_acc: 0.2562\n",
            "Epoch 104/200\n",
            "1123/1123 [==============================] - 1s 981us/step - loss: 0.8394 - acc: 0.6563 - val_loss: 1.8976 - val_acc: 0.2847\n",
            "Epoch 105/200\n",
            "1123/1123 [==============================] - 1s 962us/step - loss: 0.8024 - acc: 0.6643 - val_loss: 1.9007 - val_acc: 0.2776\n",
            "Epoch 106/200\n",
            "1123/1123 [==============================] - 1s 973us/step - loss: 0.8410 - acc: 0.6340 - val_loss: 1.9169 - val_acc: 0.2776\n",
            "Epoch 107/200\n",
            "1123/1123 [==============================] - 1s 986us/step - loss: 0.8208 - acc: 0.6652 - val_loss: 1.9111 - val_acc: 0.2776\n",
            "Epoch 108/200\n",
            "1123/1123 [==============================] - 1s 984us/step - loss: 0.8294 - acc: 0.6759 - val_loss: 1.9054 - val_acc: 0.2705\n",
            "Epoch 109/200\n",
            "1123/1123 [==============================] - 1s 987us/step - loss: 0.7900 - acc: 0.6714 - val_loss: 1.9114 - val_acc: 0.2776\n",
            "Epoch 110/200\n",
            "1123/1123 [==============================] - 1s 975us/step - loss: 0.7926 - acc: 0.6874 - val_loss: 1.9217 - val_acc: 0.2918\n",
            "Epoch 111/200\n",
            "1123/1123 [==============================] - 1s 977us/step - loss: 0.7743 - acc: 0.6857 - val_loss: 1.9350 - val_acc: 0.2740\n",
            "Epoch 112/200\n",
            "1123/1123 [==============================] - 1s 983us/step - loss: 0.7794 - acc: 0.6741 - val_loss: 1.9380 - val_acc: 0.2811\n",
            "Epoch 113/200\n",
            "1123/1123 [==============================] - 1s 994us/step - loss: 0.7493 - acc: 0.6955 - val_loss: 1.9688 - val_acc: 0.2776\n",
            "Epoch 114/200\n",
            "1123/1123 [==============================] - 1s 977us/step - loss: 0.7555 - acc: 0.6928 - val_loss: 1.9794 - val_acc: 0.2598\n",
            "Epoch 115/200\n",
            "1123/1123 [==============================] - 1s 996us/step - loss: 0.7837 - acc: 0.6848 - val_loss: 1.9976 - val_acc: 0.2562\n",
            "Epoch 116/200\n",
            "1123/1123 [==============================] - 1s 984us/step - loss: 0.7735 - acc: 0.6732 - val_loss: 2.0197 - val_acc: 0.2705\n",
            "Epoch 117/200\n",
            "1123/1123 [==============================] - 1s 990us/step - loss: 0.7693 - acc: 0.7008 - val_loss: 2.0128 - val_acc: 0.2705\n",
            "Epoch 118/200\n",
            "1123/1123 [==============================] - 1s 979us/step - loss: 0.7380 - acc: 0.7008 - val_loss: 2.0044 - val_acc: 0.2847\n",
            "Epoch 119/200\n",
            "1123/1123 [==============================] - 1s 1ms/step - loss: 0.7315 - acc: 0.7168 - val_loss: 2.0101 - val_acc: 0.2918\n",
            "Epoch 120/200\n",
            "1123/1123 [==============================] - 1s 975us/step - loss: 0.7332 - acc: 0.7044 - val_loss: 2.0191 - val_acc: 0.2811\n",
            "Epoch 121/200\n",
            "1123/1123 [==============================] - 1s 991us/step - loss: 0.7052 - acc: 0.7097 - val_loss: 2.0436 - val_acc: 0.2740\n",
            "Epoch 122/200\n",
            "1123/1123 [==============================] - 1s 975us/step - loss: 0.7030 - acc: 0.7186 - val_loss: 2.0720 - val_acc: 0.2776\n",
            "Epoch 123/200\n",
            "1123/1123 [==============================] - 1s 977us/step - loss: 0.7150 - acc: 0.7124 - val_loss: 2.1154 - val_acc: 0.2954\n",
            "Epoch 124/200\n",
            "1123/1123 [==============================] - 1s 986us/step - loss: 0.7323 - acc: 0.7044 - val_loss: 2.1300 - val_acc: 0.2811\n",
            "Epoch 125/200\n",
            "1123/1123 [==============================] - 1s 993us/step - loss: 0.7176 - acc: 0.7053 - val_loss: 2.1688 - val_acc: 0.2669\n",
            "Epoch 126/200\n",
            "1123/1123 [==============================] - 1s 993us/step - loss: 0.6819 - acc: 0.7391 - val_loss: 2.1948 - val_acc: 0.2776\n",
            "Epoch 127/200\n",
            "1123/1123 [==============================] - 1s 990us/step - loss: 0.6812 - acc: 0.7240 - val_loss: 2.2354 - val_acc: 0.2776\n",
            "Epoch 128/200\n",
            "1123/1123 [==============================] - 1s 978us/step - loss: 0.6730 - acc: 0.7275 - val_loss: 2.2557 - val_acc: 0.2562\n",
            "Epoch 129/200\n",
            "1123/1123 [==============================] - 1s 995us/step - loss: 0.6717 - acc: 0.7346 - val_loss: 2.2358 - val_acc: 0.2598\n",
            "Epoch 130/200\n",
            "1123/1123 [==============================] - 1s 983us/step - loss: 0.6779 - acc: 0.7311 - val_loss: 2.1691 - val_acc: 0.2776\n",
            "Epoch 131/200\n",
            "1123/1123 [==============================] - 1s 987us/step - loss: 0.6540 - acc: 0.7382 - val_loss: 2.1646 - val_acc: 0.2847\n",
            "Epoch 132/200\n",
            "1123/1123 [==============================] - 1s 995us/step - loss: 0.6880 - acc: 0.7346 - val_loss: 2.1864 - val_acc: 0.2811\n",
            "Epoch 133/200\n",
            "1123/1123 [==============================] - 1s 986us/step - loss: 0.6372 - acc: 0.7631 - val_loss: 2.1873 - val_acc: 0.2740\n",
            "Epoch 134/200\n",
            "1123/1123 [==============================] - 1s 972us/step - loss: 0.6579 - acc: 0.7329 - val_loss: 2.2002 - val_acc: 0.2740\n",
            "Epoch 135/200\n",
            "1123/1123 [==============================] - 1s 993us/step - loss: 0.6475 - acc: 0.7471 - val_loss: 2.2141 - val_acc: 0.2811\n",
            "Epoch 136/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 0.5956 - acc: 0.7631 - val_loss: 2.2232 - val_acc: 0.2847\n",
            "Epoch 137/200\n",
            "1123/1123 [==============================] - 1s 990us/step - loss: 0.5948 - acc: 0.7756 - val_loss: 2.2618 - val_acc: 0.2705\n",
            "Epoch 138/200\n",
            "1123/1123 [==============================] - 1s 994us/step - loss: 0.6120 - acc: 0.7640 - val_loss: 2.2818 - val_acc: 0.2705\n",
            "Epoch 139/200\n",
            "1123/1123 [==============================] - 1s 993us/step - loss: 0.6222 - acc: 0.7622 - val_loss: 2.2882 - val_acc: 0.2776\n",
            "Epoch 140/200\n",
            "1123/1123 [==============================] - 1s 985us/step - loss: 0.6095 - acc: 0.7560 - val_loss: 2.2524 - val_acc: 0.2598\n",
            "Epoch 141/200\n",
            "1123/1123 [==============================] - 1s 997us/step - loss: 0.5745 - acc: 0.7738 - val_loss: 2.2526 - val_acc: 0.2633\n",
            "Epoch 142/200\n",
            "1123/1123 [==============================] - 1s 993us/step - loss: 0.5931 - acc: 0.7738 - val_loss: 2.2468 - val_acc: 0.2633\n",
            "Epoch 143/200\n",
            "1123/1123 [==============================] - 1s 984us/step - loss: 0.5939 - acc: 0.7631 - val_loss: 2.2756 - val_acc: 0.2633\n",
            "Epoch 144/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 0.5862 - acc: 0.7658 - val_loss: 2.3462 - val_acc: 0.2491\n",
            "Epoch 145/200\n",
            "1123/1123 [==============================] - 1s 990us/step - loss: 0.5296 - acc: 0.8068 - val_loss: 2.3892 - val_acc: 0.2562\n",
            "Epoch 146/200\n",
            "1123/1123 [==============================] - 1s 982us/step - loss: 0.6152 - acc: 0.7524 - val_loss: 2.3957 - val_acc: 0.2562\n",
            "Epoch 147/200\n",
            "1123/1123 [==============================] - 1s 984us/step - loss: 0.5490 - acc: 0.7845 - val_loss: 2.4172 - val_acc: 0.2527\n",
            "Epoch 148/200\n",
            "1123/1123 [==============================] - 1s 992us/step - loss: 0.5765 - acc: 0.7916 - val_loss: 2.4421 - val_acc: 0.2527\n",
            "Epoch 149/200\n",
            "1123/1123 [==============================] - 1s 998us/step - loss: 0.5559 - acc: 0.7943 - val_loss: 2.4842 - val_acc: 0.2633\n",
            "Epoch 150/200\n",
            "1123/1123 [==============================] - 1s 1ms/step - loss: 0.5523 - acc: 0.7774 - val_loss: 2.5219 - val_acc: 0.2491\n",
            "Epoch 151/200\n",
            "1123/1123 [==============================] - 1s 992us/step - loss: 0.5314 - acc: 0.7827 - val_loss: 2.5689 - val_acc: 0.2456\n",
            "Epoch 152/200\n",
            "1123/1123 [==============================] - 1s 1ms/step - loss: 0.5470 - acc: 0.7872 - val_loss: 2.5492 - val_acc: 0.2527\n",
            "Epoch 153/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 0.5331 - acc: 0.7952 - val_loss: 2.5727 - val_acc: 0.2633\n",
            "Epoch 154/200\n",
            "1123/1123 [==============================] - 1s 990us/step - loss: 0.5276 - acc: 0.7916 - val_loss: 2.5660 - val_acc: 0.2669\n",
            "Epoch 155/200\n",
            "1123/1123 [==============================] - 1s 968us/step - loss: 0.5054 - acc: 0.8032 - val_loss: 2.5357 - val_acc: 0.2598\n",
            "Epoch 156/200\n",
            "1123/1123 [==============================] - 1s 989us/step - loss: 0.5389 - acc: 0.7952 - val_loss: 2.5121 - val_acc: 0.2456\n",
            "Epoch 157/200\n",
            "1123/1123 [==============================] - 1s 987us/step - loss: 0.5233 - acc: 0.7996 - val_loss: 2.5083 - val_acc: 0.2456\n",
            "Epoch 158/200\n",
            "1123/1123 [==============================] - 1s 985us/step - loss: 0.5180 - acc: 0.7934 - val_loss: 2.5018 - val_acc: 0.2562\n",
            "Epoch 159/200\n",
            "1123/1123 [==============================] - 1s 995us/step - loss: 0.5638 - acc: 0.7747 - val_loss: 2.4809 - val_acc: 0.2705\n",
            "Epoch 160/200\n",
            "1123/1123 [==============================] - 1s 997us/step - loss: 0.4949 - acc: 0.8059 - val_loss: 2.5272 - val_acc: 0.2811\n",
            "Epoch 161/200\n",
            "1123/1123 [==============================] - 1s 975us/step - loss: 0.4935 - acc: 0.8032 - val_loss: 2.5870 - val_acc: 0.2776\n",
            "Epoch 162/200\n",
            "1123/1123 [==============================] - 1s 987us/step - loss: 0.5232 - acc: 0.8157 - val_loss: 2.6612 - val_acc: 0.2633\n",
            "Epoch 163/200\n",
            "1123/1123 [==============================] - 1s 979us/step - loss: 0.4955 - acc: 0.8059 - val_loss: 2.7330 - val_acc: 0.2633\n",
            "Epoch 164/200\n",
            "1123/1123 [==============================] - 1s 979us/step - loss: 0.5048 - acc: 0.8077 - val_loss: 2.7700 - val_acc: 0.2527\n",
            "Epoch 165/200\n",
            "1123/1123 [==============================] - 1s 992us/step - loss: 0.4669 - acc: 0.8264 - val_loss: 2.8009 - val_acc: 0.2384\n",
            "Epoch 166/200\n",
            "1123/1123 [==============================] - 1s 983us/step - loss: 0.5129 - acc: 0.8014 - val_loss: 2.8192 - val_acc: 0.2491\n",
            "Epoch 167/200\n",
            "1123/1123 [==============================] - 1s 993us/step - loss: 0.4878 - acc: 0.8094 - val_loss: 2.8530 - val_acc: 0.2456\n",
            "Epoch 168/200\n",
            "1123/1123 [==============================] - 1s 982us/step - loss: 0.4869 - acc: 0.8130 - val_loss: 2.8360 - val_acc: 0.2491\n",
            "Epoch 169/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 0.5234 - acc: 0.7890 - val_loss: 2.8120 - val_acc: 0.2527\n",
            "Epoch 170/200\n",
            "1123/1123 [==============================] - 1s 984us/step - loss: 0.4860 - acc: 0.8130 - val_loss: 2.7723 - val_acc: 0.2633\n",
            "Epoch 171/200\n",
            "1123/1123 [==============================] - 1s 981us/step - loss: 0.4975 - acc: 0.8094 - val_loss: 2.7090 - val_acc: 0.2598\n",
            "Epoch 172/200\n",
            "1123/1123 [==============================] - 1s 989us/step - loss: 0.4660 - acc: 0.8281 - val_loss: 2.6830 - val_acc: 0.2598\n",
            "Epoch 173/200\n",
            "1123/1123 [==============================] - 1s 1ms/step - loss: 0.4597 - acc: 0.8183 - val_loss: 2.7140 - val_acc: 0.2456\n",
            "Epoch 174/200\n",
            "1123/1123 [==============================] - 1s 982us/step - loss: 0.4606 - acc: 0.8290 - val_loss: 2.7706 - val_acc: 0.2384\n",
            "Epoch 175/200\n",
            "1123/1123 [==============================] - 1s 983us/step - loss: 0.4687 - acc: 0.8272 - val_loss: 2.7707 - val_acc: 0.2456\n",
            "Epoch 176/200\n",
            "1123/1123 [==============================] - 1s 980us/step - loss: 0.4634 - acc: 0.8290 - val_loss: 2.7383 - val_acc: 0.2527\n",
            "Epoch 177/200\n",
            "1123/1123 [==============================] - 1s 978us/step - loss: 0.4595 - acc: 0.8290 - val_loss: 2.7521 - val_acc: 0.2633\n",
            "Epoch 178/200\n",
            "1123/1123 [==============================] - 1s 985us/step - loss: 0.4715 - acc: 0.8112 - val_loss: 2.7714 - val_acc: 0.2562\n",
            "Epoch 179/200\n",
            "1123/1123 [==============================] - 1s 991us/step - loss: 0.4536 - acc: 0.8264 - val_loss: 2.8230 - val_acc: 0.2384\n",
            "Epoch 180/200\n",
            "1123/1123 [==============================] - 1s 973us/step - loss: 0.4666 - acc: 0.8335 - val_loss: 2.8802 - val_acc: 0.2384\n",
            "Epoch 181/200\n",
            "1123/1123 [==============================] - 1s 992us/step - loss: 0.4692 - acc: 0.8228 - val_loss: 2.8931 - val_acc: 0.2349\n",
            "Epoch 182/200\n",
            "1123/1123 [==============================] - 1s 984us/step - loss: 0.4806 - acc: 0.8166 - val_loss: 2.9166 - val_acc: 0.2349\n",
            "Epoch 183/200\n",
            "1123/1123 [==============================] - 1s 994us/step - loss: 0.4512 - acc: 0.8344 - val_loss: 2.9020 - val_acc: 0.2491\n",
            "Epoch 184/200\n",
            "1123/1123 [==============================] - 1s 992us/step - loss: 0.4467 - acc: 0.8317 - val_loss: 2.8959 - val_acc: 0.2491\n",
            "Epoch 185/200\n",
            "1123/1123 [==============================] - 1s 982us/step - loss: 0.4715 - acc: 0.8237 - val_loss: 2.8847 - val_acc: 0.2491\n",
            "Epoch 186/200\n",
            "1123/1123 [==============================] - 1s 993us/step - loss: 0.4528 - acc: 0.8246 - val_loss: 2.8923 - val_acc: 0.2456\n",
            "Epoch 187/200\n",
            "1123/1123 [==============================] - 1s 984us/step - loss: 0.4413 - acc: 0.8228 - val_loss: 2.9376 - val_acc: 0.2527\n",
            "Epoch 188/200\n",
            "1123/1123 [==============================] - 1s 992us/step - loss: 0.5022 - acc: 0.8121 - val_loss: 2.9741 - val_acc: 0.2456\n",
            "Epoch 189/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 0.4476 - acc: 0.8415 - val_loss: 2.9827 - val_acc: 0.2456\n",
            "Epoch 190/200\n",
            "1123/1123 [==============================] - 1s 983us/step - loss: 0.4217 - acc: 0.8344 - val_loss: 2.9520 - val_acc: 0.2598\n",
            "Epoch 191/200\n",
            "1123/1123 [==============================] - 1s 983us/step - loss: 0.4143 - acc: 0.8477 - val_loss: 2.9242 - val_acc: 0.2669\n",
            "Epoch 192/200\n",
            "1123/1123 [==============================] - 1s 983us/step - loss: 0.4481 - acc: 0.8272 - val_loss: 2.9259 - val_acc: 0.2491\n",
            "Epoch 193/200\n",
            "1123/1123 [==============================] - 1s 995us/step - loss: 0.4317 - acc: 0.8326 - val_loss: 2.9597 - val_acc: 0.2420\n",
            "Epoch 194/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 0.4667 - acc: 0.8166 - val_loss: 2.9626 - val_acc: 0.2420\n",
            "Epoch 195/200\n",
            "1123/1123 [==============================] - 1s 989us/step - loss: 0.4654 - acc: 0.8121 - val_loss: 2.9756 - val_acc: 0.2384\n",
            "Epoch 196/200\n",
            "1123/1123 [==============================] - 1s 990us/step - loss: 0.4356 - acc: 0.8264 - val_loss: 3.0024 - val_acc: 0.2456\n",
            "Epoch 197/200\n",
            "1123/1123 [==============================] - 1s 982us/step - loss: 0.4357 - acc: 0.8317 - val_loss: 3.0278 - val_acc: 0.2562\n",
            "Epoch 198/200\n",
            "1123/1123 [==============================] - 1s 988us/step - loss: 0.4392 - acc: 0.8317 - val_loss: 3.0604 - val_acc: 0.2527\n",
            "Epoch 199/200\n",
            "1123/1123 [==============================] - 1s 1ms/step - loss: 0.3808 - acc: 0.8531 - val_loss: 3.0386 - val_acc: 0.2669\n",
            "Epoch 200/200\n",
            "1123/1123 [==============================] - 1s 986us/step - loss: 0.4308 - acc: 0.8281 - val_loss: 3.0031 - val_acc: 0.2598\n",
            "Done with one subject\n",
            "300/300 [==============================] - 0s 376us/step\n",
            "300/300 [==============================] - 0s 341us/step\n",
            "300/300 [==============================] - 0s 332us/step\n",
            "300/300 [==============================] - 0s 336us/step\n",
            "282/282 [==============================] - 0s 333us/step\n",
            "294/294 [==============================] - 0s 349us/step\n",
            "300/300 [==============================] - 0s 341us/step\n",
            "300/300 [==============================] - 0s 332us/step\n",
            "282/282 [==============================] - 0s 348us/step\n",
            "Train on 1128 samples, validate on 282 samples\n",
            "Epoch 1/200\n",
            "1128/1128 [==============================] - 3s 3ms/step - loss: 2.9006 - acc: 0.2518 - val_loss: 1.4968 - val_acc: 0.3262\n",
            "Epoch 2/200\n",
            "1128/1128 [==============================] - 1s 985us/step - loss: 2.9607 - acc: 0.2535 - val_loss: 2.1389 - val_acc: 0.3014\n",
            "Epoch 3/200\n",
            "1128/1128 [==============================] - 1s 984us/step - loss: 2.4766 - acc: 0.2846 - val_loss: 2.5712 - val_acc: 0.3298\n",
            "Epoch 4/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 2.3598 - acc: 0.2846 - val_loss: 2.3535 - val_acc: 0.3227\n",
            "Epoch 5/200\n",
            "1128/1128 [==============================] - 1s 996us/step - loss: 2.3162 - acc: 0.2996 - val_loss: 2.0822 - val_acc: 0.3227\n",
            "Epoch 6/200\n",
            "1128/1128 [==============================] - 1s 992us/step - loss: 2.2060 - acc: 0.2580 - val_loss: 2.0683 - val_acc: 0.3582\n",
            "Epoch 7/200\n",
            "1128/1128 [==============================] - 1s 991us/step - loss: 2.0411 - acc: 0.2988 - val_loss: 1.9333 - val_acc: 0.2979\n",
            "Epoch 8/200\n",
            "1128/1128 [==============================] - 1s 992us/step - loss: 2.0953 - acc: 0.2739 - val_loss: 1.8695 - val_acc: 0.3085\n",
            "Epoch 9/200\n",
            "1128/1128 [==============================] - 1s 988us/step - loss: 1.9561 - acc: 0.3041 - val_loss: 1.8040 - val_acc: 0.2766\n",
            "Epoch 10/200\n",
            "1128/1128 [==============================] - 1s 992us/step - loss: 1.8819 - acc: 0.2996 - val_loss: 1.6940 - val_acc: 0.3369\n",
            "Epoch 11/200\n",
            "1128/1128 [==============================] - 1s 999us/step - loss: 1.8240 - acc: 0.3094 - val_loss: 1.6400 - val_acc: 0.3369\n",
            "Epoch 12/200\n",
            "1128/1128 [==============================] - 1s 978us/step - loss: 1.8501 - acc: 0.3067 - val_loss: 1.5413 - val_acc: 0.3582\n",
            "Epoch 13/200\n",
            "1128/1128 [==============================] - 1s 1000us/step - loss: 1.8020 - acc: 0.3147 - val_loss: 1.4604 - val_acc: 0.3546\n",
            "Epoch 14/200\n",
            "1128/1128 [==============================] - 1s 985us/step - loss: 1.7422 - acc: 0.3085 - val_loss: 1.4025 - val_acc: 0.3546\n",
            "Epoch 15/200\n",
            "1128/1128 [==============================] - 1s 1ms/step - loss: 1.6570 - acc: 0.3165 - val_loss: 1.3532 - val_acc: 0.3440\n",
            "Epoch 16/200\n",
            "1128/1128 [==============================] - 1s 987us/step - loss: 1.6549 - acc: 0.3059 - val_loss: 1.3841 - val_acc: 0.3475\n",
            "Epoch 17/200\n",
            "1128/1128 [==============================] - 1s 985us/step - loss: 1.5775 - acc: 0.3262 - val_loss: 1.4631 - val_acc: 0.3369\n",
            "Epoch 18/200\n",
            "1128/1128 [==============================] - 1s 988us/step - loss: 1.6427 - acc: 0.3404 - val_loss: 1.5155 - val_acc: 0.3262\n",
            "Epoch 19/200\n",
            "1128/1128 [==============================] - 1s 1ms/step - loss: 1.5329 - acc: 0.3431 - val_loss: 1.4508 - val_acc: 0.3156\n",
            "Epoch 20/200\n",
            "1128/1128 [==============================] - 1s 987us/step - loss: 1.5359 - acc: 0.3271 - val_loss: 1.4253 - val_acc: 0.3546\n",
            "Epoch 21/200\n",
            "1128/1128 [==============================] - 1s 990us/step - loss: 1.4949 - acc: 0.3466 - val_loss: 1.3551 - val_acc: 0.4043\n",
            "Epoch 22/200\n",
            "1128/1128 [==============================] - 1s 994us/step - loss: 1.4540 - acc: 0.3369 - val_loss: 1.3509 - val_acc: 0.3652\n",
            "Epoch 23/200\n",
            "1128/1128 [==============================] - 1s 993us/step - loss: 1.4558 - acc: 0.3200 - val_loss: 1.3528 - val_acc: 0.3404\n",
            "Epoch 24/200\n",
            "1128/1128 [==============================] - 1s 1ms/step - loss: 1.4351 - acc: 0.3351 - val_loss: 1.3367 - val_acc: 0.3511\n",
            "Epoch 25/200\n",
            "1128/1128 [==============================] - 1s 992us/step - loss: 1.4037 - acc: 0.3679 - val_loss: 1.3342 - val_acc: 0.3617\n",
            "Epoch 26/200\n",
            "1128/1128 [==============================] - 1s 983us/step - loss: 1.3767 - acc: 0.3679 - val_loss: 1.3791 - val_acc: 0.3440\n",
            "Epoch 27/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 1.3936 - acc: 0.3608 - val_loss: 1.4135 - val_acc: 0.3333\n",
            "Epoch 28/200\n",
            "1128/1128 [==============================] - 1s 998us/step - loss: 1.3720 - acc: 0.3599 - val_loss: 1.3253 - val_acc: 0.3723\n",
            "Epoch 29/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 1.3750 - acc: 0.3608 - val_loss: 1.3157 - val_acc: 0.3901\n",
            "Epoch 30/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 1.3289 - acc: 0.3963 - val_loss: 1.3387 - val_acc: 0.3936\n",
            "Epoch 31/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 1.3181 - acc: 0.3936 - val_loss: 1.3986 - val_acc: 0.3546\n",
            "Epoch 32/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 1.3350 - acc: 0.3980 - val_loss: 1.3272 - val_acc: 0.4078\n",
            "Epoch 33/200\n",
            "1128/1128 [==============================] - 1s 999us/step - loss: 1.3438 - acc: 0.3892 - val_loss: 1.3206 - val_acc: 0.4078\n",
            "Epoch 34/200\n",
            "1128/1128 [==============================] - 1s 981us/step - loss: 1.3014 - acc: 0.4158 - val_loss: 1.3556 - val_acc: 0.3972\n",
            "Epoch 35/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 1.3275 - acc: 0.4211 - val_loss: 1.3796 - val_acc: 0.3759\n",
            "Epoch 36/200\n",
            "1128/1128 [==============================] - 1s 994us/step - loss: 1.3087 - acc: 0.4087 - val_loss: 1.3785 - val_acc: 0.3759\n",
            "Epoch 37/200\n",
            "1128/1128 [==============================] - 1s 991us/step - loss: 1.2615 - acc: 0.4433 - val_loss: 1.3881 - val_acc: 0.3688\n",
            "Epoch 38/200\n",
            "1128/1128 [==============================] - 1s 997us/step - loss: 1.3200 - acc: 0.4113 - val_loss: 1.3439 - val_acc: 0.3936\n",
            "Epoch 39/200\n",
            "1128/1128 [==============================] - 1s 997us/step - loss: 1.2577 - acc: 0.4300 - val_loss: 1.3318 - val_acc: 0.4007\n",
            "Epoch 40/200\n",
            "1128/1128 [==============================] - 1s 987us/step - loss: 1.2961 - acc: 0.4043 - val_loss: 1.3831 - val_acc: 0.3759\n",
            "Epoch 41/200\n",
            "1128/1128 [==============================] - 1s 993us/step - loss: 1.2867 - acc: 0.4167 - val_loss: 1.3746 - val_acc: 0.3688\n",
            "Epoch 42/200\n",
            "1128/1128 [==============================] - 1s 991us/step - loss: 1.2657 - acc: 0.4246 - val_loss: 1.3428 - val_acc: 0.3830\n",
            "Epoch 43/200\n",
            "1128/1128 [==============================] - 1s 990us/step - loss: 1.2526 - acc: 0.4450 - val_loss: 1.3440 - val_acc: 0.3936\n",
            "Epoch 44/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 1.2396 - acc: 0.4681 - val_loss: 1.4267 - val_acc: 0.3546\n",
            "Epoch 45/200\n",
            "1128/1128 [==============================] - 1s 991us/step - loss: 1.2381 - acc: 0.4628 - val_loss: 1.3839 - val_acc: 0.3475\n",
            "Epoch 46/200\n",
            "1128/1128 [==============================] - 1s 997us/step - loss: 1.2284 - acc: 0.4583 - val_loss: 1.3115 - val_acc: 0.3901\n",
            "Epoch 47/200\n",
            "1128/1128 [==============================] - 1s 985us/step - loss: 1.2328 - acc: 0.4388 - val_loss: 1.3171 - val_acc: 0.4043\n",
            "Epoch 48/200\n",
            "1128/1128 [==============================] - 1s 986us/step - loss: 1.2232 - acc: 0.4539 - val_loss: 1.4058 - val_acc: 0.3582\n",
            "Epoch 49/200\n",
            "1128/1128 [==============================] - 1s 982us/step - loss: 1.2087 - acc: 0.4734 - val_loss: 1.4108 - val_acc: 0.3582\n",
            "Epoch 50/200\n",
            "1128/1128 [==============================] - 1s 986us/step - loss: 1.1962 - acc: 0.4707 - val_loss: 1.3537 - val_acc: 0.4078\n",
            "Epoch 51/200\n",
            "1128/1128 [==============================] - 1s 983us/step - loss: 1.2104 - acc: 0.4690 - val_loss: 1.3446 - val_acc: 0.4184\n",
            "Epoch 52/200\n",
            "1128/1128 [==============================] - 1s 976us/step - loss: 1.2028 - acc: 0.4601 - val_loss: 1.3786 - val_acc: 0.3901\n",
            "Epoch 53/200\n",
            "1128/1128 [==============================] - 1s 997us/step - loss: 1.1916 - acc: 0.4885 - val_loss: 1.3812 - val_acc: 0.3830\n",
            "Epoch 54/200\n",
            "1128/1128 [==============================] - 1s 988us/step - loss: 1.1898 - acc: 0.4778 - val_loss: 1.3520 - val_acc: 0.4113\n",
            "Epoch 55/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 1.1558 - acc: 0.4956 - val_loss: 1.3359 - val_acc: 0.4255\n",
            "Epoch 56/200\n",
            "1128/1128 [==============================] - 1s 993us/step - loss: 1.1454 - acc: 0.5009 - val_loss: 1.3664 - val_acc: 0.3972\n",
            "Epoch 57/200\n",
            "1128/1128 [==============================] - 1s 991us/step - loss: 1.1586 - acc: 0.4973 - val_loss: 1.3886 - val_acc: 0.3546\n",
            "Epoch 58/200\n",
            "1128/1128 [==============================] - 1s 992us/step - loss: 1.1517 - acc: 0.5142 - val_loss: 1.3828 - val_acc: 0.3617\n",
            "Epoch 59/200\n",
            "1128/1128 [==============================] - 1s 986us/step - loss: 1.1046 - acc: 0.5089 - val_loss: 1.3509 - val_acc: 0.3901\n",
            "Epoch 60/200\n",
            "1128/1128 [==============================] - 1s 990us/step - loss: 1.1430 - acc: 0.5044 - val_loss: 1.3233 - val_acc: 0.4220\n",
            "Epoch 61/200\n",
            "1128/1128 [==============================] - 1s 997us/step - loss: 1.1655 - acc: 0.4947 - val_loss: 1.3704 - val_acc: 0.3688\n",
            "Epoch 62/200\n",
            "1128/1128 [==============================] - 1s 987us/step - loss: 1.1398 - acc: 0.5160 - val_loss: 1.3743 - val_acc: 0.3723\n",
            "Epoch 63/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 1.1407 - acc: 0.5266 - val_loss: 1.3391 - val_acc: 0.4043\n",
            "Epoch 64/200\n",
            "1128/1128 [==============================] - 1s 988us/step - loss: 1.1335 - acc: 0.5089 - val_loss: 1.3157 - val_acc: 0.4397\n",
            "Epoch 65/200\n",
            "1128/1128 [==============================] - 1s 997us/step - loss: 1.1054 - acc: 0.5301 - val_loss: 1.3261 - val_acc: 0.4255\n",
            "Epoch 66/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 1.1267 - acc: 0.5319 - val_loss: 1.3508 - val_acc: 0.4078\n",
            "Epoch 67/200\n",
            "1128/1128 [==============================] - 1s 982us/step - loss: 1.0959 - acc: 0.5346 - val_loss: 1.3915 - val_acc: 0.3830\n",
            "Epoch 68/200\n",
            "1128/1128 [==============================] - 1s 990us/step - loss: 1.1238 - acc: 0.5363 - val_loss: 1.3802 - val_acc: 0.3936\n",
            "Epoch 69/200\n",
            "1128/1128 [==============================] - 1s 982us/step - loss: 1.0633 - acc: 0.5656 - val_loss: 1.3563 - val_acc: 0.4291\n",
            "Epoch 70/200\n",
            "1128/1128 [==============================] - 1s 985us/step - loss: 1.0955 - acc: 0.5434 - val_loss: 1.3358 - val_acc: 0.4291\n",
            "Epoch 71/200\n",
            "1128/1128 [==============================] - 1s 984us/step - loss: 1.0532 - acc: 0.5532 - val_loss: 1.3969 - val_acc: 0.4184\n",
            "Epoch 72/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 1.0981 - acc: 0.5426 - val_loss: 1.4168 - val_acc: 0.3972\n",
            "Epoch 73/200\n",
            "1128/1128 [==============================] - 1s 1ms/step - loss: 1.0250 - acc: 0.5709 - val_loss: 1.3717 - val_acc: 0.4291\n",
            "Epoch 74/200\n",
            "1128/1128 [==============================] - 1s 990us/step - loss: 1.0670 - acc: 0.5408 - val_loss: 1.3683 - val_acc: 0.4220\n",
            "Epoch 75/200\n",
            "1128/1128 [==============================] - 1s 994us/step - loss: 1.0245 - acc: 0.5691 - val_loss: 1.3767 - val_acc: 0.4113\n",
            "Epoch 76/200\n",
            "1128/1128 [==============================] - 1s 983us/step - loss: 1.0196 - acc: 0.5727 - val_loss: 1.3987 - val_acc: 0.4184\n",
            "Epoch 77/200\n",
            "1128/1128 [==============================] - 1s 987us/step - loss: 1.0126 - acc: 0.5824 - val_loss: 1.3937 - val_acc: 0.4078\n",
            "Epoch 78/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 0.9829 - acc: 0.6064 - val_loss: 1.3553 - val_acc: 0.4362\n",
            "Epoch 79/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 0.9898 - acc: 0.5807 - val_loss: 1.3394 - val_acc: 0.4397\n",
            "Epoch 80/200\n",
            "1128/1128 [==============================] - 1s 1ms/step - loss: 1.0134 - acc: 0.5895 - val_loss: 1.3513 - val_acc: 0.4291\n",
            "Epoch 81/200\n",
            "1128/1128 [==============================] - 1s 994us/step - loss: 1.0044 - acc: 0.5851 - val_loss: 1.3677 - val_acc: 0.4149\n",
            "Epoch 82/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 0.9897 - acc: 0.5904 - val_loss: 1.3867 - val_acc: 0.4184\n",
            "Epoch 83/200\n",
            "1128/1128 [==============================] - 1s 993us/step - loss: 1.0121 - acc: 0.5754 - val_loss: 1.4143 - val_acc: 0.4291\n",
            "Epoch 84/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 0.9584 - acc: 0.6144 - val_loss: 1.4107 - val_acc: 0.4255\n",
            "Epoch 85/200\n",
            "1128/1128 [==============================] - 1s 992us/step - loss: 0.9838 - acc: 0.5984 - val_loss: 1.3897 - val_acc: 0.4326\n",
            "Epoch 86/200\n",
            "1128/1128 [==============================] - 1s 985us/step - loss: 0.9363 - acc: 0.6126 - val_loss: 1.4248 - val_acc: 0.4362\n",
            "Epoch 87/200\n",
            "1128/1128 [==============================] - 1s 992us/step - loss: 0.9363 - acc: 0.6259 - val_loss: 1.4469 - val_acc: 0.4184\n",
            "Epoch 88/200\n",
            "1128/1128 [==============================] - 1s 998us/step - loss: 0.9540 - acc: 0.6144 - val_loss: 1.4318 - val_acc: 0.4078\n",
            "Epoch 89/200\n",
            "1128/1128 [==============================] - 1s 985us/step - loss: 0.9235 - acc: 0.6268 - val_loss: 1.4205 - val_acc: 0.4149\n",
            "Epoch 90/200\n",
            "1128/1128 [==============================] - 1s 982us/step - loss: 0.9408 - acc: 0.6126 - val_loss: 1.4186 - val_acc: 0.4184\n",
            "Epoch 91/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 0.9102 - acc: 0.6445 - val_loss: 1.4272 - val_acc: 0.4326\n",
            "Epoch 92/200\n",
            "1128/1128 [==============================] - 1s 986us/step - loss: 0.9226 - acc: 0.6374 - val_loss: 1.4295 - val_acc: 0.4362\n",
            "Epoch 93/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 0.9260 - acc: 0.6223 - val_loss: 1.4128 - val_acc: 0.4291\n",
            "Epoch 94/200\n",
            "1128/1128 [==============================] - 1s 984us/step - loss: 0.9024 - acc: 0.6277 - val_loss: 1.4195 - val_acc: 0.4220\n",
            "Epoch 95/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 0.8853 - acc: 0.6410 - val_loss: 1.4276 - val_acc: 0.4291\n",
            "Epoch 96/200\n",
            "1128/1128 [==============================] - 1s 986us/step - loss: 0.8815 - acc: 0.6436 - val_loss: 1.4494 - val_acc: 0.4113\n",
            "Epoch 97/200\n",
            "1128/1128 [==============================] - 1s 990us/step - loss: 0.8776 - acc: 0.6410 - val_loss: 1.4430 - val_acc: 0.4007\n",
            "Epoch 98/200\n",
            "1128/1128 [==============================] - 1s 982us/step - loss: 0.8887 - acc: 0.6489 - val_loss: 1.4237 - val_acc: 0.4291\n",
            "Epoch 99/200\n",
            "1128/1128 [==============================] - 1s 996us/step - loss: 0.8426 - acc: 0.6667 - val_loss: 1.4029 - val_acc: 0.4326\n",
            "Epoch 100/200\n",
            "1128/1128 [==============================] - 1s 991us/step - loss: 0.8663 - acc: 0.6578 - val_loss: 1.4336 - val_acc: 0.4043\n",
            "Epoch 101/200\n",
            "1128/1128 [==============================] - 1s 993us/step - loss: 0.8141 - acc: 0.6817 - val_loss: 1.4649 - val_acc: 0.4220\n",
            "Epoch 102/200\n",
            "1128/1128 [==============================] - 1s 999us/step - loss: 0.8228 - acc: 0.6667 - val_loss: 1.4484 - val_acc: 0.4220\n",
            "Epoch 103/200\n",
            "1128/1128 [==============================] - 1s 981us/step - loss: 0.8409 - acc: 0.6534 - val_loss: 1.4274 - val_acc: 0.4255\n",
            "Epoch 104/200\n",
            "1128/1128 [==============================] - 1s 992us/step - loss: 0.8011 - acc: 0.6915 - val_loss: 1.4505 - val_acc: 0.4468\n",
            "Epoch 105/200\n",
            "1128/1128 [==============================] - 1s 988us/step - loss: 0.8164 - acc: 0.6826 - val_loss: 1.4907 - val_acc: 0.4255\n",
            "Epoch 106/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 0.7634 - acc: 0.6950 - val_loss: 1.4934 - val_acc: 0.4326\n",
            "Epoch 107/200\n",
            "1128/1128 [==============================] - 1s 997us/step - loss: 0.8058 - acc: 0.6879 - val_loss: 1.4623 - val_acc: 0.4433\n",
            "Epoch 108/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 0.7602 - acc: 0.6941 - val_loss: 1.4447 - val_acc: 0.4574\n",
            "Epoch 109/200\n",
            "1128/1128 [==============================] - 1s 988us/step - loss: 0.7992 - acc: 0.6924 - val_loss: 1.4240 - val_acc: 0.4610\n",
            "Epoch 110/200\n",
            "1128/1128 [==============================] - 1s 992us/step - loss: 0.7578 - acc: 0.7119 - val_loss: 1.4169 - val_acc: 0.4397\n",
            "Epoch 111/200\n",
            "1128/1128 [==============================] - 1s 993us/step - loss: 0.7723 - acc: 0.6941 - val_loss: 1.4230 - val_acc: 0.4397\n",
            "Epoch 112/200\n",
            "1128/1128 [==============================] - 1s 987us/step - loss: 0.7578 - acc: 0.6835 - val_loss: 1.4586 - val_acc: 0.4291\n",
            "Epoch 113/200\n",
            "1128/1128 [==============================] - 1s 987us/step - loss: 0.7962 - acc: 0.6826 - val_loss: 1.5110 - val_acc: 0.4255\n",
            "Epoch 114/200\n",
            "1128/1128 [==============================] - 1s 981us/step - loss: 0.7431 - acc: 0.7057 - val_loss: 1.5063 - val_acc: 0.4220\n",
            "Epoch 115/200\n",
            "1128/1128 [==============================] - 1s 993us/step - loss: 0.7331 - acc: 0.7083 - val_loss: 1.5034 - val_acc: 0.4362\n",
            "Epoch 116/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 0.7571 - acc: 0.7048 - val_loss: 1.4907 - val_acc: 0.4433\n",
            "Epoch 117/200\n",
            "1128/1128 [==============================] - 1s 991us/step - loss: 0.7849 - acc: 0.6968 - val_loss: 1.4706 - val_acc: 0.4362\n",
            "Epoch 118/200\n",
            "1128/1128 [==============================] - 1s 996us/step - loss: 0.7227 - acc: 0.7066 - val_loss: 1.4808 - val_acc: 0.4149\n",
            "Epoch 119/200\n",
            "1128/1128 [==============================] - 1s 985us/step - loss: 0.7020 - acc: 0.7216 - val_loss: 1.5098 - val_acc: 0.4326\n",
            "Epoch 120/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 0.6838 - acc: 0.7429 - val_loss: 1.5093 - val_acc: 0.4468\n",
            "Epoch 121/200\n",
            "1128/1128 [==============================] - 1s 991us/step - loss: 0.7102 - acc: 0.7225 - val_loss: 1.4693 - val_acc: 0.4539\n",
            "Epoch 122/200\n",
            "1128/1128 [==============================] - 1s 990us/step - loss: 0.6931 - acc: 0.7243 - val_loss: 1.4494 - val_acc: 0.4504\n",
            "Epoch 123/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 0.6846 - acc: 0.7385 - val_loss: 1.4526 - val_acc: 0.4220\n",
            "Epoch 124/200\n",
            "1128/1128 [==============================] - 1s 984us/step - loss: 0.6820 - acc: 0.7207 - val_loss: 1.4756 - val_acc: 0.4078\n",
            "Epoch 125/200\n",
            "1128/1128 [==============================] - 1s 993us/step - loss: 0.6846 - acc: 0.7296 - val_loss: 1.4855 - val_acc: 0.4113\n",
            "Epoch 126/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 0.6821 - acc: 0.7243 - val_loss: 1.4549 - val_acc: 0.4184\n",
            "Epoch 127/200\n",
            "1128/1128 [==============================] - 1s 991us/step - loss: 0.6732 - acc: 0.7252 - val_loss: 1.4338 - val_acc: 0.4113\n",
            "Epoch 128/200\n",
            "1128/1128 [==============================] - 1s 990us/step - loss: 0.6596 - acc: 0.7491 - val_loss: 1.4481 - val_acc: 0.4184\n",
            "Epoch 129/200\n",
            "1128/1128 [==============================] - 1s 981us/step - loss: 0.6327 - acc: 0.7589 - val_loss: 1.4755 - val_acc: 0.4184\n",
            "Epoch 130/200\n",
            "1128/1128 [==============================] - 1s 976us/step - loss: 0.6260 - acc: 0.7535 - val_loss: 1.5054 - val_acc: 0.4362\n",
            "Epoch 131/200\n",
            "1128/1128 [==============================] - 1s 993us/step - loss: 0.6389 - acc: 0.7571 - val_loss: 1.5192 - val_acc: 0.4220\n",
            "Epoch 132/200\n",
            "1128/1128 [==============================] - 1s 988us/step - loss: 0.6195 - acc: 0.7615 - val_loss: 1.5388 - val_acc: 0.4149\n",
            "Epoch 133/200\n",
            "1128/1128 [==============================] - 1s 1ms/step - loss: 0.6099 - acc: 0.7730 - val_loss: 1.5473 - val_acc: 0.4255\n",
            "Epoch 134/200\n",
            "1128/1128 [==============================] - 1s 983us/step - loss: 0.6073 - acc: 0.7589 - val_loss: 1.5517 - val_acc: 0.4291\n",
            "Epoch 135/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 0.6494 - acc: 0.7456 - val_loss: 1.5556 - val_acc: 0.4149\n",
            "Epoch 136/200\n",
            "1128/1128 [==============================] - 1s 985us/step - loss: 0.5806 - acc: 0.7748 - val_loss: 1.5792 - val_acc: 0.4184\n",
            "Epoch 137/200\n",
            "1128/1128 [==============================] - 1s 1ms/step - loss: 0.6230 - acc: 0.7535 - val_loss: 1.6240 - val_acc: 0.4113\n",
            "Epoch 138/200\n",
            "1128/1128 [==============================] - 1s 990us/step - loss: 0.5675 - acc: 0.7775 - val_loss: 1.6338 - val_acc: 0.4362\n",
            "Epoch 139/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 0.5959 - acc: 0.7660 - val_loss: 1.5971 - val_acc: 0.4149\n",
            "Epoch 140/200\n",
            "1128/1128 [==============================] - 1s 990us/step - loss: 0.5903 - acc: 0.7535 - val_loss: 1.5751 - val_acc: 0.4291\n",
            "Epoch 141/200\n",
            "1128/1128 [==============================] - 1s 991us/step - loss: 0.5568 - acc: 0.7890 - val_loss: 1.5801 - val_acc: 0.4397\n",
            "Epoch 142/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 0.5697 - acc: 0.7819 - val_loss: 1.6111 - val_acc: 0.4397\n",
            "Epoch 143/200\n",
            "1128/1128 [==============================] - 1s 995us/step - loss: 0.5353 - acc: 0.7863 - val_loss: 1.6105 - val_acc: 0.4149\n",
            "Epoch 144/200\n",
            "1128/1128 [==============================] - 1s 990us/step - loss: 0.6103 - acc: 0.7677 - val_loss: 1.6167 - val_acc: 0.4184\n",
            "Epoch 145/200\n",
            "1128/1128 [==============================] - 1s 997us/step - loss: 0.5547 - acc: 0.7872 - val_loss: 1.6306 - val_acc: 0.4149\n",
            "Epoch 146/200\n",
            "1128/1128 [==============================] - 1s 985us/step - loss: 0.5890 - acc: 0.7757 - val_loss: 1.6289 - val_acc: 0.4113\n",
            "Epoch 147/200\n",
            "1128/1128 [==============================] - 1s 996us/step - loss: 0.5487 - acc: 0.7961 - val_loss: 1.6288 - val_acc: 0.4291\n",
            "Epoch 148/200\n",
            "1128/1128 [==============================] - 1s 994us/step - loss: 0.5781 - acc: 0.7872 - val_loss: 1.6340 - val_acc: 0.4149\n",
            "Epoch 149/200\n",
            "1128/1128 [==============================] - 1s 997us/step - loss: 0.5410 - acc: 0.8023 - val_loss: 1.6306 - val_acc: 0.4220\n",
            "Epoch 150/200\n",
            "1128/1128 [==============================] - 1s 993us/step - loss: 0.5363 - acc: 0.8005 - val_loss: 1.6437 - val_acc: 0.4220\n",
            "Epoch 151/200\n",
            "1128/1128 [==============================] - 1s 991us/step - loss: 0.5523 - acc: 0.7881 - val_loss: 1.6480 - val_acc: 0.4184\n",
            "Epoch 152/200\n",
            "1128/1128 [==============================] - 1s 984us/step - loss: 0.5292 - acc: 0.7970 - val_loss: 1.6339 - val_acc: 0.4362\n",
            "Epoch 153/200\n",
            "1128/1128 [==============================] - 1s 1000us/step - loss: 0.4981 - acc: 0.8032 - val_loss: 1.6485 - val_acc: 0.4539\n",
            "Epoch 154/200\n",
            "1128/1128 [==============================] - 1s 980us/step - loss: 0.5330 - acc: 0.8023 - val_loss: 1.6748 - val_acc: 0.4433\n",
            "Epoch 155/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 0.5589 - acc: 0.7793 - val_loss: 1.7101 - val_acc: 0.4504\n",
            "Epoch 156/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 0.5357 - acc: 0.7961 - val_loss: 1.7415 - val_acc: 0.4433\n",
            "Epoch 157/200\n",
            "1128/1128 [==============================] - 1s 993us/step - loss: 0.4586 - acc: 0.8271 - val_loss: 1.8022 - val_acc: 0.4220\n",
            "Epoch 158/200\n",
            "1128/1128 [==============================] - 1s 989us/step - loss: 0.5139 - acc: 0.8103 - val_loss: 1.8082 - val_acc: 0.4149\n",
            "Epoch 159/200\n",
            "1128/1128 [==============================] - 1s 994us/step - loss: 0.5085 - acc: 0.8183 - val_loss: 1.7963 - val_acc: 0.4043\n",
            "Epoch 160/200\n",
            "1128/1128 [==============================] - 1s 980us/step - loss: 0.5340 - acc: 0.7908 - val_loss: 1.7616 - val_acc: 0.4043\n",
            "Epoch 161/200\n",
            "1128/1128 [==============================] - 1s 986us/step - loss: 0.5015 - acc: 0.8085 - val_loss: 1.7294 - val_acc: 0.4220\n",
            "Epoch 162/200\n",
            "1128/1128 [==============================] - 1s 985us/step - loss: 0.5064 - acc: 0.8023 - val_loss: 1.7024 - val_acc: 0.4291\n",
            "Epoch 163/200\n",
            "1128/1128 [==============================] - 1s 984us/step - loss: 0.4955 - acc: 0.8067 - val_loss: 1.7113 - val_acc: 0.4326\n",
            "Epoch 164/200\n",
            "1128/1128 [==============================] - 1s 994us/step - loss: 0.5033 - acc: 0.8085 - val_loss: 1.7165 - val_acc: 0.4326\n",
            "Epoch 165/200\n",
            "1128/1128 [==============================] - 1s 994us/step - loss: 0.5080 - acc: 0.8147 - val_loss: 1.6821 - val_acc: 0.4326\n",
            "Epoch 166/200\n",
            "1128/1128 [==============================] - 1s 974us/step - loss: 0.4682 - acc: 0.8165 - val_loss: 1.6753 - val_acc: 0.4255\n",
            "Epoch 167/200\n",
            "1128/1128 [==============================] - 1s 986us/step - loss: 0.4678 - acc: 0.8103 - val_loss: 1.6794 - val_acc: 0.4326\n",
            "Epoch 168/200\n",
            "1128/1128 [==============================] - 1s 975us/step - loss: 0.5162 - acc: 0.8059 - val_loss: 1.7075 - val_acc: 0.4362\n",
            "Epoch 169/200\n",
            "1128/1128 [==============================] - 1s 977us/step - loss: 0.4711 - acc: 0.8200 - val_loss: 1.7370 - val_acc: 0.4433\n",
            "Epoch 170/200\n",
            "1128/1128 [==============================] - 1s 969us/step - loss: 0.4662 - acc: 0.8351 - val_loss: 1.7451 - val_acc: 0.4397\n",
            "Epoch 171/200\n",
            "1128/1128 [==============================] - 1s 980us/step - loss: 0.4802 - acc: 0.8289 - val_loss: 1.7743 - val_acc: 0.4362\n",
            "Epoch 172/200\n",
            "1128/1128 [==============================] - 1s 987us/step - loss: 0.4953 - acc: 0.8076 - val_loss: 1.8027 - val_acc: 0.4291\n",
            "Epoch 00172: early stopping\n",
            "Done with one subject\n",
            "300/300 [==============================] - 0s 353us/step\n",
            "300/300 [==============================] - 0s 351us/step\n",
            "300/300 [==============================] - 0s 341us/step\n",
            "300/300 [==============================] - 0s 354us/step\n",
            "282/282 [==============================] - 0s 351us/step\n",
            "294/294 [==============================] - 0s 381us/step\n",
            "300/300 [==============================] - 0s 341us/step\n",
            "300/300 [==============================] - 0s 346us/step\n",
            "282/282 [==============================] - 0s 348us/step\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 2.6435 - acc: 0.2562 - val_loss: 1.5609 - val_acc: 0.2535\n",
            "Epoch 2/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 2.6261 - acc: 0.2739 - val_loss: 1.6541 - val_acc: 0.3169\n",
            "Epoch 3/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 2.4088 - acc: 0.2668 - val_loss: 1.7537 - val_acc: 0.2782\n",
            "Epoch 4/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 2.2400 - acc: 0.2747 - val_loss: 1.8206 - val_acc: 0.2993\n",
            "Epoch 5/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 2.0238 - acc: 0.2977 - val_loss: 2.0595 - val_acc: 0.2606\n",
            "Epoch 6/200\n",
            "1132/1132 [==============================] - 1s 979us/step - loss: 2.0367 - acc: 0.2659 - val_loss: 1.9791 - val_acc: 0.2606\n",
            "Epoch 7/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.8353 - acc: 0.2774 - val_loss: 1.6588 - val_acc: 0.3134\n",
            "Epoch 8/200\n",
            "1132/1132 [==============================] - 1s 996us/step - loss: 1.8229 - acc: 0.2906 - val_loss: 1.6318 - val_acc: 0.3275\n",
            "Epoch 9/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 1.7366 - acc: 0.2809 - val_loss: 1.5774 - val_acc: 0.3204\n",
            "Epoch 10/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.6691 - acc: 0.3030 - val_loss: 1.7997 - val_acc: 0.2324\n",
            "Epoch 11/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.5916 - acc: 0.3154 - val_loss: 1.6536 - val_acc: 0.2254\n",
            "Epoch 12/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 1.5756 - acc: 0.3004 - val_loss: 1.4716 - val_acc: 0.2887\n",
            "Epoch 13/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 1.5356 - acc: 0.3110 - val_loss: 1.5401 - val_acc: 0.2782\n",
            "Epoch 14/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.5968 - acc: 0.2686 - val_loss: 1.5343 - val_acc: 0.2606\n",
            "Epoch 15/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.4875 - acc: 0.3083 - val_loss: 1.5033 - val_acc: 0.2852\n",
            "Epoch 16/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 1.5013 - acc: 0.3127 - val_loss: 1.5287 - val_acc: 0.2782\n",
            "Epoch 17/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.5067 - acc: 0.3057 - val_loss: 1.4458 - val_acc: 0.2993\n",
            "Epoch 18/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 1.4890 - acc: 0.3136 - val_loss: 1.3953 - val_acc: 0.2958\n",
            "Epoch 19/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 1.4767 - acc: 0.2915 - val_loss: 1.4932 - val_acc: 0.2923\n",
            "Epoch 20/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.4757 - acc: 0.3021 - val_loss: 1.5706 - val_acc: 0.2817\n",
            "Epoch 21/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 1.4533 - acc: 0.3065 - val_loss: 1.4263 - val_acc: 0.2852\n",
            "Epoch 22/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 1.4584 - acc: 0.3074 - val_loss: 1.4735 - val_acc: 0.2746\n",
            "Epoch 23/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 1.4482 - acc: 0.3092 - val_loss: 1.5760 - val_acc: 0.2711\n",
            "Epoch 24/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 1.4624 - acc: 0.3136 - val_loss: 1.4608 - val_acc: 0.2782\n",
            "Epoch 25/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 1.4545 - acc: 0.3154 - val_loss: 1.4395 - val_acc: 0.2993\n",
            "Epoch 26/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.4338 - acc: 0.3233 - val_loss: 1.4621 - val_acc: 0.2641\n",
            "Epoch 27/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 1.3910 - acc: 0.3525 - val_loss: 1.4847 - val_acc: 0.2746\n",
            "Epoch 28/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 1.4185 - acc: 0.3242 - val_loss: 1.4459 - val_acc: 0.2465\n",
            "Epoch 29/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 1.3929 - acc: 0.3330 - val_loss: 1.4404 - val_acc: 0.2676\n",
            "Epoch 30/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 1.4168 - acc: 0.3083 - val_loss: 1.4326 - val_acc: 0.3028\n",
            "Epoch 31/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.3635 - acc: 0.3472 - val_loss: 1.4738 - val_acc: 0.2817\n",
            "Epoch 32/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 1.4249 - acc: 0.3357 - val_loss: 1.4949 - val_acc: 0.2817\n",
            "Epoch 33/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.3822 - acc: 0.3436 - val_loss: 1.4682 - val_acc: 0.2782\n",
            "Epoch 34/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 1.3753 - acc: 0.3277 - val_loss: 1.4000 - val_acc: 0.3662\n",
            "Epoch 35/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 1.3727 - acc: 0.3693 - val_loss: 1.4969 - val_acc: 0.2887\n",
            "Epoch 36/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 1.3702 - acc: 0.3640 - val_loss: 1.4587 - val_acc: 0.2746\n",
            "Epoch 37/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 1.3721 - acc: 0.3489 - val_loss: 1.3754 - val_acc: 0.3662\n",
            "Epoch 38/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.3524 - acc: 0.3754 - val_loss: 1.4261 - val_acc: 0.3521\n",
            "Epoch 39/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.4140 - acc: 0.3357 - val_loss: 1.4726 - val_acc: 0.2606\n",
            "Epoch 40/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 1.3639 - acc: 0.3746 - val_loss: 1.4066 - val_acc: 0.3063\n",
            "Epoch 41/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.3731 - acc: 0.3542 - val_loss: 1.3804 - val_acc: 0.3944\n",
            "Epoch 42/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 1.3797 - acc: 0.3763 - val_loss: 1.4749 - val_acc: 0.2500\n",
            "Epoch 43/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 1.3627 - acc: 0.3498 - val_loss: 1.4205 - val_acc: 0.3345\n",
            "Epoch 44/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 1.3685 - acc: 0.3754 - val_loss: 1.3823 - val_acc: 0.3873\n",
            "Epoch 45/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.3539 - acc: 0.3887 - val_loss: 1.4264 - val_acc: 0.2958\n",
            "Epoch 46/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 1.3488 - acc: 0.3860 - val_loss: 1.4783 - val_acc: 0.2711\n",
            "Epoch 47/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.3272 - acc: 0.3975 - val_loss: 1.3924 - val_acc: 0.3451\n",
            "Epoch 48/200\n",
            "1132/1132 [==============================] - 1s 986us/step - loss: 1.3104 - acc: 0.3905 - val_loss: 1.3844 - val_acc: 0.3768\n",
            "Epoch 49/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 1.2904 - acc: 0.4249 - val_loss: 1.4911 - val_acc: 0.2817\n",
            "Epoch 50/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 1.3460 - acc: 0.3631 - val_loss: 1.4459 - val_acc: 0.3099\n",
            "Epoch 51/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.3137 - acc: 0.4002 - val_loss: 1.4163 - val_acc: 0.3486\n",
            "Epoch 52/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 1.2945 - acc: 0.3966 - val_loss: 1.4375 - val_acc: 0.3204\n",
            "Epoch 53/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 1.2867 - acc: 0.4214 - val_loss: 1.4488 - val_acc: 0.3134\n",
            "Epoch 54/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 1.2514 - acc: 0.4302 - val_loss: 1.3994 - val_acc: 0.3380\n",
            "Epoch 55/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 1.2659 - acc: 0.4170 - val_loss: 1.3757 - val_acc: 0.3803\n",
            "Epoch 56/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 1.2379 - acc: 0.4399 - val_loss: 1.4056 - val_acc: 0.3451\n",
            "Epoch 57/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 1.2191 - acc: 0.4373 - val_loss: 1.4280 - val_acc: 0.3310\n",
            "Epoch 58/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.2523 - acc: 0.4355 - val_loss: 1.4018 - val_acc: 0.3556\n",
            "Epoch 59/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.2461 - acc: 0.4479 - val_loss: 1.3784 - val_acc: 0.3486\n",
            "Epoch 60/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.2232 - acc: 0.4311 - val_loss: 1.3853 - val_acc: 0.3556\n",
            "Epoch 61/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 1.2045 - acc: 0.4682 - val_loss: 1.4181 - val_acc: 0.3204\n",
            "Epoch 62/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 1.1731 - acc: 0.4859 - val_loss: 1.4143 - val_acc: 0.3345\n",
            "Epoch 63/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.2121 - acc: 0.4585 - val_loss: 1.4010 - val_acc: 0.3521\n",
            "Epoch 64/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.1863 - acc: 0.4779 - val_loss: 1.4103 - val_acc: 0.3310\n",
            "Epoch 65/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 1.1756 - acc: 0.4982 - val_loss: 1.4183 - val_acc: 0.3239\n",
            "Epoch 66/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.1992 - acc: 0.4770 - val_loss: 1.4263 - val_acc: 0.3204\n",
            "Epoch 67/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 1.1738 - acc: 0.4956 - val_loss: 1.4335 - val_acc: 0.3451\n",
            "Epoch 68/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 1.1691 - acc: 0.5035 - val_loss: 1.4342 - val_acc: 0.3310\n",
            "Epoch 69/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 1.1804 - acc: 0.4832 - val_loss: 1.4354 - val_acc: 0.3310\n",
            "Epoch 70/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 1.1656 - acc: 0.4885 - val_loss: 1.4424 - val_acc: 0.3310\n",
            "Epoch 71/200\n",
            "1132/1132 [==============================] - 1s 996us/step - loss: 1.1418 - acc: 0.5044 - val_loss: 1.4602 - val_acc: 0.3486\n",
            "Epoch 72/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.1338 - acc: 0.5141 - val_loss: 1.4740 - val_acc: 0.3204\n",
            "Epoch 73/200\n",
            "1132/1132 [==============================] - 1s 1000us/step - loss: 1.1154 - acc: 0.5186 - val_loss: 1.4487 - val_acc: 0.3345\n",
            "Epoch 74/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.1454 - acc: 0.5018 - val_loss: 1.4382 - val_acc: 0.3451\n",
            "Epoch 75/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.1381 - acc: 0.5212 - val_loss: 1.4534 - val_acc: 0.3521\n",
            "Epoch 76/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 1.0991 - acc: 0.5415 - val_loss: 1.4909 - val_acc: 0.3275\n",
            "Epoch 77/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.0871 - acc: 0.5345 - val_loss: 1.5222 - val_acc: 0.3204\n",
            "Epoch 78/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 1.0805 - acc: 0.5468 - val_loss: 1.5106 - val_acc: 0.3134\n",
            "Epoch 79/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 1.0943 - acc: 0.5292 - val_loss: 1.5107 - val_acc: 0.3028\n",
            "Epoch 80/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 1.0968 - acc: 0.5159 - val_loss: 1.5229 - val_acc: 0.3239\n",
            "Epoch 81/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 1.0444 - acc: 0.5539 - val_loss: 1.5277 - val_acc: 0.3486\n",
            "Epoch 82/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 1.0903 - acc: 0.5292 - val_loss: 1.5118 - val_acc: 0.3592\n",
            "Epoch 83/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 1.0902 - acc: 0.5221 - val_loss: 1.5131 - val_acc: 0.3310\n",
            "Epoch 84/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 1.0546 - acc: 0.5592 - val_loss: 1.5273 - val_acc: 0.3380\n",
            "Epoch 85/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 1.0534 - acc: 0.5601 - val_loss: 1.5561 - val_acc: 0.3239\n",
            "Epoch 86/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 1.0128 - acc: 0.5760 - val_loss: 1.5474 - val_acc: 0.3310\n",
            "Epoch 87/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 1.0083 - acc: 0.5698 - val_loss: 1.5387 - val_acc: 0.3415\n",
            "Epoch 88/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 1.0248 - acc: 0.5698 - val_loss: 1.5454 - val_acc: 0.3380\n",
            "Epoch 89/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 1.0564 - acc: 0.5565 - val_loss: 1.5470 - val_acc: 0.3310\n",
            "Epoch 90/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 1.0069 - acc: 0.5857 - val_loss: 1.5477 - val_acc: 0.3380\n",
            "Epoch 91/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 1.0191 - acc: 0.5936 - val_loss: 1.5410 - val_acc: 0.3345\n",
            "Epoch 92/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 1.0137 - acc: 0.5716 - val_loss: 1.5546 - val_acc: 0.3345\n",
            "Epoch 93/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 0.9813 - acc: 0.5804 - val_loss: 1.5755 - val_acc: 0.3204\n",
            "Epoch 94/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 0.9486 - acc: 0.6113 - val_loss: 1.5902 - val_acc: 0.3239\n",
            "Epoch 95/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.9861 - acc: 0.5892 - val_loss: 1.5918 - val_acc: 0.3239\n",
            "Epoch 96/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 0.9194 - acc: 0.6325 - val_loss: 1.5807 - val_acc: 0.3134\n",
            "Epoch 97/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.9587 - acc: 0.6051 - val_loss: 1.5630 - val_acc: 0.3239\n",
            "Epoch 98/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.9345 - acc: 0.6193 - val_loss: 1.5883 - val_acc: 0.3063\n",
            "Epoch 99/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.9629 - acc: 0.5998 - val_loss: 1.6134 - val_acc: 0.3380\n",
            "Epoch 100/200\n",
            "1132/1132 [==============================] - 1s 996us/step - loss: 0.9215 - acc: 0.6113 - val_loss: 1.5569 - val_acc: 0.3239\n",
            "Epoch 101/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.9319 - acc: 0.6122 - val_loss: 1.5737 - val_acc: 0.3169\n",
            "Epoch 102/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.9154 - acc: 0.6131 - val_loss: 1.6492 - val_acc: 0.2993\n",
            "Epoch 103/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.9433 - acc: 0.6131 - val_loss: 1.6834 - val_acc: 0.3099\n",
            "Epoch 104/200\n",
            "1132/1132 [==============================] - 1s 985us/step - loss: 0.9211 - acc: 0.6422 - val_loss: 1.6344 - val_acc: 0.3275\n",
            "Epoch 105/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.8781 - acc: 0.6369 - val_loss: 1.6247 - val_acc: 0.3380\n",
            "Epoch 106/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.9138 - acc: 0.6201 - val_loss: 1.6530 - val_acc: 0.3099\n",
            "Epoch 107/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.9136 - acc: 0.6210 - val_loss: 1.7049 - val_acc: 0.3063\n",
            "Epoch 108/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.8559 - acc: 0.6466 - val_loss: 1.6700 - val_acc: 0.3239\n",
            "Epoch 109/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 0.8569 - acc: 0.6493 - val_loss: 1.6815 - val_acc: 0.3275\n",
            "Epoch 110/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 0.8702 - acc: 0.6546 - val_loss: 1.7228 - val_acc: 0.3239\n",
            "Epoch 111/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.8814 - acc: 0.6511 - val_loss: 1.7459 - val_acc: 0.3239\n",
            "Epoch 112/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.8763 - acc: 0.6360 - val_loss: 1.7223 - val_acc: 0.3169\n",
            "Epoch 113/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 0.7906 - acc: 0.6820 - val_loss: 1.7222 - val_acc: 0.3239\n",
            "Epoch 114/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.8549 - acc: 0.6652 - val_loss: 1.7253 - val_acc: 0.3204\n",
            "Epoch 115/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.8663 - acc: 0.6387 - val_loss: 1.7336 - val_acc: 0.3099\n",
            "Epoch 116/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 0.8623 - acc: 0.6422 - val_loss: 1.7474 - val_acc: 0.3063\n",
            "Epoch 117/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 0.8044 - acc: 0.6846 - val_loss: 1.7445 - val_acc: 0.2958\n",
            "Epoch 118/200\n",
            "1132/1132 [==============================] - 1s 996us/step - loss: 0.7767 - acc: 0.7032 - val_loss: 1.7610 - val_acc: 0.2923\n",
            "Epoch 119/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.8029 - acc: 0.6846 - val_loss: 1.7850 - val_acc: 0.2993\n",
            "Epoch 120/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.8244 - acc: 0.6758 - val_loss: 1.8260 - val_acc: 0.2852\n",
            "Epoch 121/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.7681 - acc: 0.6864 - val_loss: 1.8509 - val_acc: 0.2852\n",
            "Epoch 122/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.7932 - acc: 0.6846 - val_loss: 1.8191 - val_acc: 0.2887\n",
            "Epoch 123/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 0.7952 - acc: 0.6917 - val_loss: 1.8224 - val_acc: 0.2923\n",
            "Epoch 124/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.7436 - acc: 0.7023 - val_loss: 1.8501 - val_acc: 0.2711\n",
            "Epoch 125/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.7409 - acc: 0.7085 - val_loss: 1.8724 - val_acc: 0.2746\n",
            "Epoch 126/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.7395 - acc: 0.6908 - val_loss: 1.8628 - val_acc: 0.2923\n",
            "Epoch 127/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.7243 - acc: 0.7102 - val_loss: 1.8777 - val_acc: 0.2958\n",
            "Epoch 128/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 0.6972 - acc: 0.7288 - val_loss: 1.9080 - val_acc: 0.2958\n",
            "Epoch 129/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.7310 - acc: 0.7058 - val_loss: 1.8725 - val_acc: 0.3063\n",
            "Epoch 130/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 0.7236 - acc: 0.7120 - val_loss: 1.8759 - val_acc: 0.2817\n",
            "Epoch 131/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.7214 - acc: 0.7155 - val_loss: 1.9205 - val_acc: 0.2782\n",
            "Epoch 132/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.7374 - acc: 0.7173 - val_loss: 1.9354 - val_acc: 0.2817\n",
            "Epoch 133/200\n",
            "1132/1132 [==============================] - 1s 996us/step - loss: 0.7165 - acc: 0.7102 - val_loss: 1.9035 - val_acc: 0.2958\n",
            "Epoch 134/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.6799 - acc: 0.7376 - val_loss: 1.9171 - val_acc: 0.2887\n",
            "Epoch 135/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.7616 - acc: 0.6899 - val_loss: 1.9667 - val_acc: 0.2993\n",
            "Epoch 136/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 0.6772 - acc: 0.7314 - val_loss: 1.9660 - val_acc: 0.3239\n",
            "Epoch 137/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.6967 - acc: 0.7253 - val_loss: 1.9549 - val_acc: 0.3169\n",
            "Epoch 138/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 0.7227 - acc: 0.7235 - val_loss: 1.9517 - val_acc: 0.3169\n",
            "Epoch 139/200\n",
            "1132/1132 [==============================] - 1s 999us/step - loss: 0.7073 - acc: 0.7076 - val_loss: 1.9425 - val_acc: 0.3099\n",
            "Epoch 140/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 0.7084 - acc: 0.7111 - val_loss: 1.9507 - val_acc: 0.2993\n",
            "Epoch 141/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.6464 - acc: 0.7597 - val_loss: 1.9457 - val_acc: 0.2958\n",
            "Epoch 142/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 0.6511 - acc: 0.7270 - val_loss: 1.9633 - val_acc: 0.2852\n",
            "Epoch 143/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 0.6538 - acc: 0.7403 - val_loss: 1.9972 - val_acc: 0.2993\n",
            "Epoch 144/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.6127 - acc: 0.7615 - val_loss: 2.0236 - val_acc: 0.2993\n",
            "Epoch 145/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.6525 - acc: 0.7323 - val_loss: 2.0362 - val_acc: 0.3099\n",
            "Epoch 146/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.6785 - acc: 0.7314 - val_loss: 2.0184 - val_acc: 0.3063\n",
            "Epoch 147/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.7281 - acc: 0.7226 - val_loss: 1.9647 - val_acc: 0.2958\n",
            "Epoch 148/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.6415 - acc: 0.7588 - val_loss: 1.9521 - val_acc: 0.2993\n",
            "Epoch 149/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.6439 - acc: 0.7509 - val_loss: 1.9738 - val_acc: 0.2887\n",
            "Epoch 150/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.6358 - acc: 0.7394 - val_loss: 2.0183 - val_acc: 0.3028\n",
            "Epoch 151/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 0.6162 - acc: 0.7650 - val_loss: 2.0577 - val_acc: 0.2958\n",
            "Epoch 152/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.5966 - acc: 0.7659 - val_loss: 2.0631 - val_acc: 0.2993\n",
            "Epoch 153/200\n",
            "1132/1132 [==============================] - 1s 995us/step - loss: 0.5997 - acc: 0.7721 - val_loss: 2.0468 - val_acc: 0.3204\n",
            "Epoch 154/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.6303 - acc: 0.7659 - val_loss: 2.0422 - val_acc: 0.3345\n",
            "Epoch 155/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.5834 - acc: 0.7756 - val_loss: 2.0835 - val_acc: 0.3415\n",
            "Epoch 156/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.5775 - acc: 0.7712 - val_loss: 2.1151 - val_acc: 0.3134\n",
            "Epoch 157/200\n",
            "1132/1132 [==============================] - 1s 979us/step - loss: 0.5731 - acc: 0.7836 - val_loss: 2.1364 - val_acc: 0.3028\n",
            "Epoch 158/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.5864 - acc: 0.7730 - val_loss: 2.1575 - val_acc: 0.2993\n",
            "Epoch 159/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 0.5911 - acc: 0.7862 - val_loss: 2.1847 - val_acc: 0.2958\n",
            "Epoch 160/200\n",
            "1132/1132 [==============================] - 1s 981us/step - loss: 0.6056 - acc: 0.7677 - val_loss: 2.2160 - val_acc: 0.3063\n",
            "Epoch 161/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.5927 - acc: 0.7686 - val_loss: 2.2100 - val_acc: 0.3099\n",
            "Epoch 162/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.5521 - acc: 0.7933 - val_loss: 2.1220 - val_acc: 0.3169\n",
            "Epoch 163/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 0.6198 - acc: 0.7650 - val_loss: 2.1001 - val_acc: 0.3134\n",
            "Epoch 164/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 0.5786 - acc: 0.7783 - val_loss: 2.1255 - val_acc: 0.3028\n",
            "Epoch 165/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.5306 - acc: 0.7765 - val_loss: 2.1077 - val_acc: 0.3028\n",
            "Epoch 166/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.5849 - acc: 0.7818 - val_loss: 2.1325 - val_acc: 0.2887\n",
            "Epoch 167/200\n",
            "1132/1132 [==============================] - 1s 982us/step - loss: 0.5916 - acc: 0.7765 - val_loss: 2.1937 - val_acc: 0.2923\n",
            "Epoch 168/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.6056 - acc: 0.7500 - val_loss: 2.1693 - val_acc: 0.2993\n",
            "Epoch 169/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.5385 - acc: 0.7862 - val_loss: 2.1572 - val_acc: 0.3169\n",
            "Epoch 170/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.5653 - acc: 0.7889 - val_loss: 2.1870 - val_acc: 0.3099\n",
            "Epoch 171/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.5631 - acc: 0.7836 - val_loss: 2.1997 - val_acc: 0.2993\n",
            "Epoch 172/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.5338 - acc: 0.8057 - val_loss: 2.1797 - val_acc: 0.3028\n",
            "Epoch 173/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.5467 - acc: 0.7809 - val_loss: 2.2001 - val_acc: 0.3099\n",
            "Epoch 174/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.5223 - acc: 0.7915 - val_loss: 2.2333 - val_acc: 0.3028\n",
            "Epoch 175/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 0.5484 - acc: 0.7836 - val_loss: 2.2300 - val_acc: 0.3063\n",
            "Epoch 176/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.5420 - acc: 0.7853 - val_loss: 2.2457 - val_acc: 0.2958\n",
            "Epoch 177/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 0.5192 - acc: 0.7977 - val_loss: 2.3067 - val_acc: 0.3134\n",
            "Epoch 178/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.5491 - acc: 0.7836 - val_loss: 2.3449 - val_acc: 0.3028\n",
            "Epoch 179/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 0.5288 - acc: 0.8004 - val_loss: 2.3513 - val_acc: 0.3099\n",
            "Epoch 180/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.5351 - acc: 0.7906 - val_loss: 2.3221 - val_acc: 0.2887\n",
            "Epoch 181/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.4888 - acc: 0.8083 - val_loss: 2.3189 - val_acc: 0.2993\n",
            "Epoch 182/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 0.5139 - acc: 0.7986 - val_loss: 2.3190 - val_acc: 0.2852\n",
            "Epoch 183/200\n",
            "1132/1132 [==============================] - 1s 993us/step - loss: 0.5599 - acc: 0.7809 - val_loss: 2.3058 - val_acc: 0.2958\n",
            "Epoch 184/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.5107 - acc: 0.8039 - val_loss: 2.2835 - val_acc: 0.2923\n",
            "Epoch 185/200\n",
            "1132/1132 [==============================] - 1s 984us/step - loss: 0.5338 - acc: 0.7968 - val_loss: 2.3302 - val_acc: 0.2923\n",
            "Epoch 186/200\n",
            "1132/1132 [==============================] - 1s 998us/step - loss: 0.5165 - acc: 0.7995 - val_loss: 2.3832 - val_acc: 0.2852\n",
            "Epoch 187/200\n",
            "1132/1132 [==============================] - 1s 988us/step - loss: 0.5284 - acc: 0.7915 - val_loss: 2.4208 - val_acc: 0.2746\n",
            "Epoch 188/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.4863 - acc: 0.8083 - val_loss: 2.4476 - val_acc: 0.2782\n",
            "Epoch 189/200\n",
            "1132/1132 [==============================] - 1s 994us/step - loss: 0.4796 - acc: 0.8216 - val_loss: 2.4432 - val_acc: 0.2887\n",
            "Epoch 190/200\n",
            "1132/1132 [==============================] - 1s 992us/step - loss: 0.5263 - acc: 0.7906 - val_loss: 2.4324 - val_acc: 0.2993\n",
            "Epoch 191/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 0.5033 - acc: 0.7968 - val_loss: 2.4537 - val_acc: 0.2993\n",
            "Epoch 192/200\n",
            "1132/1132 [==============================] - 1s 997us/step - loss: 0.4920 - acc: 0.8171 - val_loss: 2.4711 - val_acc: 0.3169\n",
            "Epoch 193/200\n",
            "1132/1132 [==============================] - 1s 977us/step - loss: 0.4912 - acc: 0.8136 - val_loss: 2.4571 - val_acc: 0.2993\n",
            "Epoch 194/200\n",
            "1132/1132 [==============================] - 1s 989us/step - loss: 0.4921 - acc: 0.8118 - val_loss: 2.3962 - val_acc: 0.3028\n",
            "Epoch 195/200\n",
            "1132/1132 [==============================] - 1s 990us/step - loss: 0.4542 - acc: 0.8251 - val_loss: 2.3529 - val_acc: 0.3028\n",
            "Epoch 196/200\n",
            "1132/1132 [==============================] - 1s 991us/step - loss: 0.4793 - acc: 0.8163 - val_loss: 2.3903 - val_acc: 0.3063\n",
            "Epoch 197/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.5050 - acc: 0.8065 - val_loss: 2.3826 - val_acc: 0.2993\n",
            "Epoch 198/200\n",
            "1132/1132 [==============================] - 1s 1ms/step - loss: 0.5062 - acc: 0.8145 - val_loss: 2.3417 - val_acc: 0.3063\n",
            "Epoch 199/200\n",
            "1132/1132 [==============================] - 1s 987us/step - loss: 0.4908 - acc: 0.8163 - val_loss: 2.3952 - val_acc: 0.3028\n",
            "Epoch 200/200\n",
            "1132/1132 [==============================] - 1s 983us/step - loss: 0.5037 - acc: 0.8101 - val_loss: 2.4244 - val_acc: 0.2958\n",
            "Done with one subject\n",
            "300/300 [==============================] - 0s 349us/step\n",
            "300/300 [==============================] - 0s 335us/step\n",
            "300/300 [==============================] - 0s 337us/step\n",
            "300/300 [==============================] - 0s 331us/step\n",
            "282/282 [==============================] - 0s 334us/step\n",
            "294/294 [==============================] - 0s 348us/step\n",
            "300/300 [==============================] - 0s 336us/step\n",
            "300/300 [==============================] - 0s 344us/step\n",
            "282/282 [==============================] - 0s 331us/step\n",
            "Train on 1142 samples, validate on 286 samples\n",
            "Epoch 1/200\n",
            "1142/1142 [==============================] - 3s 3ms/step - loss: 2.7182 - acc: 0.2627 - val_loss: 1.8788 - val_acc: 0.1923\n",
            "Epoch 2/200\n",
            "1142/1142 [==============================] - 1s 995us/step - loss: 2.5705 - acc: 0.2872 - val_loss: 2.0358 - val_acc: 0.2343\n",
            "Epoch 3/200\n",
            "1142/1142 [==============================] - 1s 993us/step - loss: 2.3908 - acc: 0.2846 - val_loss: 1.8393 - val_acc: 0.2867\n",
            "Epoch 4/200\n",
            "1142/1142 [==============================] - 1s 996us/step - loss: 2.0879 - acc: 0.3196 - val_loss: 1.9435 - val_acc: 0.3357\n",
            "Epoch 5/200\n",
            "1142/1142 [==============================] - 1s 995us/step - loss: 2.0209 - acc: 0.3371 - val_loss: 1.9077 - val_acc: 0.3007\n",
            "Epoch 6/200\n",
            "1142/1142 [==============================] - 1s 982us/step - loss: 1.8787 - acc: 0.3415 - val_loss: 1.7142 - val_acc: 0.3042\n",
            "Epoch 7/200\n",
            "1142/1142 [==============================] - 1s 989us/step - loss: 1.8502 - acc: 0.3319 - val_loss: 1.5849 - val_acc: 0.3392\n",
            "Epoch 8/200\n",
            "1142/1142 [==============================] - 1s 1ms/step - loss: 1.8511 - acc: 0.3214 - val_loss: 1.5559 - val_acc: 0.3357\n",
            "Epoch 9/200\n",
            "1142/1142 [==============================] - 1s 988us/step - loss: 1.7989 - acc: 0.3520 - val_loss: 1.5395 - val_acc: 0.3531\n",
            "Epoch 10/200\n",
            "1142/1142 [==============================] - 1s 1ms/step - loss: 1.7483 - acc: 0.3389 - val_loss: 1.4953 - val_acc: 0.3427\n",
            "Epoch 11/200\n",
            "1142/1142 [==============================] - 1s 993us/step - loss: 1.7147 - acc: 0.3284 - val_loss: 1.4523 - val_acc: 0.3601\n",
            "Epoch 12/200\n",
            "1142/1142 [==============================] - 1s 993us/step - loss: 1.6316 - acc: 0.3503 - val_loss: 1.3877 - val_acc: 0.3811\n",
            "Epoch 13/200\n",
            "1142/1142 [==============================] - 1s 981us/step - loss: 1.5681 - acc: 0.3424 - val_loss: 1.3523 - val_acc: 0.3986\n",
            "Epoch 14/200\n",
            "1142/1142 [==============================] - 1s 995us/step - loss: 1.5864 - acc: 0.3319 - val_loss: 1.3115 - val_acc: 0.4266\n",
            "Epoch 15/200\n",
            "1142/1142 [==============================] - 1s 988us/step - loss: 1.5463 - acc: 0.3380 - val_loss: 1.2949 - val_acc: 0.3881\n",
            "Epoch 16/200\n",
            "1142/1142 [==============================] - 1s 985us/step - loss: 1.4540 - acc: 0.3625 - val_loss: 1.3099 - val_acc: 0.3846\n",
            "Epoch 17/200\n",
            "1142/1142 [==============================] - 1s 988us/step - loss: 1.4457 - acc: 0.3485 - val_loss: 1.3231 - val_acc: 0.3881\n",
            "Epoch 18/200\n",
            "1142/1142 [==============================] - 1s 995us/step - loss: 1.4270 - acc: 0.3669 - val_loss: 1.3043 - val_acc: 0.3986\n",
            "Epoch 19/200\n",
            "1142/1142 [==============================] - 1s 1ms/step - loss: 1.4390 - acc: 0.3529 - val_loss: 1.2716 - val_acc: 0.4021\n",
            "Epoch 20/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 1.3929 - acc: 0.3695 - val_loss: 1.2621 - val_acc: 0.3916\n",
            "Epoch 21/200\n",
            "1142/1142 [==============================] - 1s 1ms/step - loss: 1.3793 - acc: 0.3809 - val_loss: 1.2485 - val_acc: 0.3986\n",
            "Epoch 22/200\n",
            "1142/1142 [==============================] - 1s 987us/step - loss: 1.3799 - acc: 0.3660 - val_loss: 1.2466 - val_acc: 0.4266\n",
            "Epoch 23/200\n",
            "1142/1142 [==============================] - 1s 1000us/step - loss: 1.3430 - acc: 0.3975 - val_loss: 1.2652 - val_acc: 0.4266\n",
            "Epoch 24/200\n",
            "1142/1142 [==============================] - 1s 984us/step - loss: 1.3308 - acc: 0.3993 - val_loss: 1.2863 - val_acc: 0.4021\n",
            "Epoch 25/200\n",
            "1142/1142 [==============================] - 1s 984us/step - loss: 1.3597 - acc: 0.4247 - val_loss: 1.2851 - val_acc: 0.4126\n",
            "Epoch 26/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 1.3522 - acc: 0.4081 - val_loss: 1.2542 - val_acc: 0.4161\n",
            "Epoch 27/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 1.3126 - acc: 0.4186 - val_loss: 1.2584 - val_acc: 0.3951\n",
            "Epoch 28/200\n",
            "1142/1142 [==============================] - 1s 1ms/step - loss: 1.2882 - acc: 0.4229 - val_loss: 1.2676 - val_acc: 0.3951\n",
            "Epoch 29/200\n",
            "1142/1142 [==============================] - 1s 979us/step - loss: 1.2565 - acc: 0.4492 - val_loss: 1.2726 - val_acc: 0.4091\n",
            "Epoch 30/200\n",
            "1142/1142 [==============================] - 1s 998us/step - loss: 1.2925 - acc: 0.4326 - val_loss: 1.2857 - val_acc: 0.4126\n",
            "Epoch 31/200\n",
            "1142/1142 [==============================] - 1s 1ms/step - loss: 1.2786 - acc: 0.4553 - val_loss: 1.2861 - val_acc: 0.4301\n",
            "Epoch 32/200\n",
            "1142/1142 [==============================] - 1s 988us/step - loss: 1.2508 - acc: 0.4606 - val_loss: 1.2922 - val_acc: 0.4336\n",
            "Epoch 33/200\n",
            "1142/1142 [==============================] - 1s 980us/step - loss: 1.2299 - acc: 0.4527 - val_loss: 1.2867 - val_acc: 0.4441\n",
            "Epoch 34/200\n",
            "1142/1142 [==============================] - 1s 983us/step - loss: 1.2422 - acc: 0.4597 - val_loss: 1.2672 - val_acc: 0.4545\n",
            "Epoch 35/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 1.2620 - acc: 0.4475 - val_loss: 1.2695 - val_acc: 0.4615\n",
            "Epoch 36/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 1.2003 - acc: 0.4886 - val_loss: 1.2728 - val_acc: 0.4371\n",
            "Epoch 37/200\n",
            "1142/1142 [==============================] - 1s 992us/step - loss: 1.2088 - acc: 0.4825 - val_loss: 1.2834 - val_acc: 0.4196\n",
            "Epoch 38/200\n",
            "1142/1142 [==============================] - 1s 990us/step - loss: 1.2277 - acc: 0.4623 - val_loss: 1.2987 - val_acc: 0.4336\n",
            "Epoch 39/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 1.1997 - acc: 0.4939 - val_loss: 1.2858 - val_acc: 0.4231\n",
            "Epoch 40/200\n",
            "1142/1142 [==============================] - 1s 996us/step - loss: 1.1862 - acc: 0.5114 - val_loss: 1.2782 - val_acc: 0.4615\n",
            "Epoch 41/200\n",
            "1142/1142 [==============================] - 1s 990us/step - loss: 1.1743 - acc: 0.4799 - val_loss: 1.3130 - val_acc: 0.4371\n",
            "Epoch 42/200\n",
            "1142/1142 [==============================] - 1s 990us/step - loss: 1.1769 - acc: 0.4965 - val_loss: 1.3436 - val_acc: 0.4056\n",
            "Epoch 43/200\n",
            "1142/1142 [==============================] - 1s 985us/step - loss: 1.1787 - acc: 0.5079 - val_loss: 1.3649 - val_acc: 0.4196\n",
            "Epoch 44/200\n",
            "1142/1142 [==============================] - 1s 985us/step - loss: 1.1516 - acc: 0.4956 - val_loss: 1.3524 - val_acc: 0.4091\n",
            "Epoch 45/200\n",
            "1142/1142 [==============================] - 1s 989us/step - loss: 1.1541 - acc: 0.5026 - val_loss: 1.3355 - val_acc: 0.4336\n",
            "Epoch 46/200\n",
            "1142/1142 [==============================] - 1s 980us/step - loss: 1.1246 - acc: 0.5377 - val_loss: 1.3379 - val_acc: 0.4231\n",
            "Epoch 47/200\n",
            "1142/1142 [==============================] - 1s 991us/step - loss: 1.1292 - acc: 0.5158 - val_loss: 1.3474 - val_acc: 0.4301\n",
            "Epoch 48/200\n",
            "1142/1142 [==============================] - 1s 993us/step - loss: 1.1094 - acc: 0.5166 - val_loss: 1.3374 - val_acc: 0.4301\n",
            "Epoch 49/200\n",
            "1142/1142 [==============================] - 1s 980us/step - loss: 1.1715 - acc: 0.5044 - val_loss: 1.3119 - val_acc: 0.4545\n",
            "Epoch 50/200\n",
            "1142/1142 [==============================] - 1s 992us/step - loss: 1.1277 - acc: 0.5228 - val_loss: 1.3270 - val_acc: 0.4371\n",
            "Epoch 51/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 1.1045 - acc: 0.5412 - val_loss: 1.3473 - val_acc: 0.4161\n",
            "Epoch 52/200\n",
            "1142/1142 [==============================] - 1s 999us/step - loss: 1.1074 - acc: 0.5429 - val_loss: 1.3466 - val_acc: 0.4126\n",
            "Epoch 53/200\n",
            "1142/1142 [==============================] - 1s 980us/step - loss: 1.0649 - acc: 0.5490 - val_loss: 1.3441 - val_acc: 0.4196\n",
            "Epoch 54/200\n",
            "1142/1142 [==============================] - 1s 987us/step - loss: 1.0845 - acc: 0.5368 - val_loss: 1.3542 - val_acc: 0.4126\n",
            "Epoch 55/200\n",
            "1142/1142 [==============================] - 1s 983us/step - loss: 1.1102 - acc: 0.5438 - val_loss: 1.3671 - val_acc: 0.4056\n",
            "Epoch 56/200\n",
            "1142/1142 [==============================] - 1s 991us/step - loss: 1.0825 - acc: 0.5622 - val_loss: 1.3402 - val_acc: 0.4336\n",
            "Epoch 57/200\n",
            "1142/1142 [==============================] - 1s 993us/step - loss: 1.1000 - acc: 0.5350 - val_loss: 1.3310 - val_acc: 0.4336\n",
            "Epoch 58/200\n",
            "1142/1142 [==============================] - 1s 998us/step - loss: 1.0626 - acc: 0.5394 - val_loss: 1.3335 - val_acc: 0.4301\n",
            "Epoch 59/200\n",
            "1142/1142 [==============================] - 1s 985us/step - loss: 1.0667 - acc: 0.5254 - val_loss: 1.3510 - val_acc: 0.4266\n",
            "Epoch 60/200\n",
            "1142/1142 [==============================] - 1s 982us/step - loss: 1.0638 - acc: 0.5543 - val_loss: 1.3540 - val_acc: 0.4196\n",
            "Epoch 61/200\n",
            "1142/1142 [==============================] - 1s 980us/step - loss: 1.0796 - acc: 0.5385 - val_loss: 1.3610 - val_acc: 0.4441\n",
            "Epoch 62/200\n",
            "1142/1142 [==============================] - 1s 981us/step - loss: 1.0410 - acc: 0.5657 - val_loss: 1.3644 - val_acc: 0.4336\n",
            "Epoch 63/200\n",
            "1142/1142 [==============================] - 1s 982us/step - loss: 1.0015 - acc: 0.5849 - val_loss: 1.3702 - val_acc: 0.4406\n",
            "Epoch 64/200\n",
            "1142/1142 [==============================] - 1s 989us/step - loss: 1.0529 - acc: 0.5639 - val_loss: 1.3722 - val_acc: 0.4231\n",
            "Epoch 65/200\n",
            "1142/1142 [==============================] - 1s 984us/step - loss: 0.9994 - acc: 0.5893 - val_loss: 1.3637 - val_acc: 0.4231\n",
            "Epoch 66/200\n",
            "1142/1142 [==============================] - 1s 976us/step - loss: 0.9956 - acc: 0.5893 - val_loss: 1.3524 - val_acc: 0.4441\n",
            "Epoch 67/200\n",
            "1142/1142 [==============================] - 1s 981us/step - loss: 1.0182 - acc: 0.5622 - val_loss: 1.3436 - val_acc: 0.4406\n",
            "Epoch 68/200\n",
            "1142/1142 [==============================] - 1s 977us/step - loss: 0.9845 - acc: 0.6051 - val_loss: 1.3501 - val_acc: 0.4266\n",
            "Epoch 69/200\n",
            "1142/1142 [==============================] - 1s 990us/step - loss: 1.0417 - acc: 0.5560 - val_loss: 1.3472 - val_acc: 0.4371\n",
            "Epoch 70/200\n",
            "1142/1142 [==============================] - 1s 988us/step - loss: 0.9690 - acc: 0.5762 - val_loss: 1.3473 - val_acc: 0.4476\n",
            "Epoch 71/200\n",
            "1142/1142 [==============================] - 1s 993us/step - loss: 0.9693 - acc: 0.6033 - val_loss: 1.3508 - val_acc: 0.4510\n",
            "Epoch 72/200\n",
            "1142/1142 [==============================] - 1s 990us/step - loss: 0.9833 - acc: 0.5806 - val_loss: 1.3615 - val_acc: 0.4510\n",
            "Epoch 73/200\n",
            "1142/1142 [==============================] - 1s 984us/step - loss: 0.9387 - acc: 0.6182 - val_loss: 1.3616 - val_acc: 0.4510\n",
            "Epoch 74/200\n",
            "1142/1142 [==============================] - 1s 983us/step - loss: 0.9581 - acc: 0.5981 - val_loss: 1.3749 - val_acc: 0.4336\n",
            "Epoch 75/200\n",
            "1142/1142 [==============================] - 1s 966us/step - loss: 0.9442 - acc: 0.6060 - val_loss: 1.3887 - val_acc: 0.4371\n",
            "Epoch 76/200\n",
            "1142/1142 [==============================] - 1s 997us/step - loss: 0.9186 - acc: 0.6208 - val_loss: 1.3820 - val_acc: 0.4545\n",
            "Epoch 77/200\n",
            "1142/1142 [==============================] - 1s 983us/step - loss: 0.9683 - acc: 0.5876 - val_loss: 1.3516 - val_acc: 0.4720\n",
            "Epoch 78/200\n",
            "1142/1142 [==============================] - 1s 983us/step - loss: 0.9372 - acc: 0.6138 - val_loss: 1.3512 - val_acc: 0.4650\n",
            "Epoch 79/200\n",
            "1142/1142 [==============================] - 1s 981us/step - loss: 0.8769 - acc: 0.6436 - val_loss: 1.3714 - val_acc: 0.4615\n",
            "Epoch 80/200\n",
            "1142/1142 [==============================] - 1s 990us/step - loss: 0.9213 - acc: 0.6173 - val_loss: 1.3779 - val_acc: 0.4441\n",
            "Epoch 81/200\n",
            "1142/1142 [==============================] - 1s 987us/step - loss: 0.8678 - acc: 0.6497 - val_loss: 1.3731 - val_acc: 0.4476\n",
            "Epoch 82/200\n",
            "1142/1142 [==============================] - 1s 984us/step - loss: 0.9248 - acc: 0.6331 - val_loss: 1.3705 - val_acc: 0.4336\n",
            "Epoch 83/200\n",
            "1142/1142 [==============================] - 1s 996us/step - loss: 0.8975 - acc: 0.6103 - val_loss: 1.3733 - val_acc: 0.4650\n",
            "Epoch 84/200\n",
            "1142/1142 [==============================] - 1s 990us/step - loss: 0.8962 - acc: 0.6147 - val_loss: 1.3826 - val_acc: 0.4615\n",
            "Epoch 85/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 0.8813 - acc: 0.6506 - val_loss: 1.3606 - val_acc: 0.4476\n",
            "Epoch 86/200\n",
            "1142/1142 [==============================] - 1s 997us/step - loss: 0.8622 - acc: 0.6489 - val_loss: 1.3696 - val_acc: 0.4545\n",
            "Epoch 87/200\n",
            "1142/1142 [==============================] - 1s 1ms/step - loss: 0.8462 - acc: 0.6515 - val_loss: 1.3820 - val_acc: 0.4301\n",
            "Epoch 88/200\n",
            "1142/1142 [==============================] - 1s 988us/step - loss: 0.8600 - acc: 0.6699 - val_loss: 1.4049 - val_acc: 0.4441\n",
            "Epoch 89/200\n",
            "1142/1142 [==============================] - 1s 992us/step - loss: 0.8252 - acc: 0.6856 - val_loss: 1.3694 - val_acc: 0.4371\n",
            "Epoch 90/200\n",
            "1142/1142 [==============================] - 1s 993us/step - loss: 0.8176 - acc: 0.6664 - val_loss: 1.3652 - val_acc: 0.4441\n",
            "Epoch 91/200\n",
            "1142/1142 [==============================] - 1s 985us/step - loss: 0.8049 - acc: 0.6830 - val_loss: 1.3663 - val_acc: 0.4406\n",
            "Epoch 92/200\n",
            "1142/1142 [==============================] - 1s 989us/step - loss: 0.7978 - acc: 0.6848 - val_loss: 1.3920 - val_acc: 0.4510\n",
            "Epoch 93/200\n",
            "1142/1142 [==============================] - 1s 996us/step - loss: 0.8417 - acc: 0.6673 - val_loss: 1.3832 - val_acc: 0.4545\n",
            "Epoch 94/200\n",
            "1142/1142 [==============================] - 1s 975us/step - loss: 0.7980 - acc: 0.6751 - val_loss: 1.4102 - val_acc: 0.4510\n",
            "Epoch 95/200\n",
            "1142/1142 [==============================] - 1s 979us/step - loss: 0.8183 - acc: 0.6804 - val_loss: 1.4223 - val_acc: 0.4580\n",
            "Epoch 96/200\n",
            "1142/1142 [==============================] - 1s 992us/step - loss: 0.7660 - acc: 0.7032 - val_loss: 1.4031 - val_acc: 0.4720\n",
            "Epoch 97/200\n",
            "1142/1142 [==============================] - 1s 991us/step - loss: 0.7843 - acc: 0.6988 - val_loss: 1.3962 - val_acc: 0.4895\n",
            "Epoch 98/200\n",
            "1142/1142 [==============================] - 1s 999us/step - loss: 0.7701 - acc: 0.6909 - val_loss: 1.4057 - val_acc: 0.4580\n",
            "Epoch 99/200\n",
            "1142/1142 [==============================] - 1s 991us/step - loss: 0.7633 - acc: 0.7067 - val_loss: 1.4347 - val_acc: 0.4580\n",
            "Epoch 100/200\n",
            "1142/1142 [==============================] - 1s 984us/step - loss: 0.7649 - acc: 0.6839 - val_loss: 1.4613 - val_acc: 0.4441\n",
            "Epoch 101/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 0.7584 - acc: 0.6891 - val_loss: 1.4677 - val_acc: 0.4476\n",
            "Epoch 102/200\n",
            "1142/1142 [==============================] - 1s 983us/step - loss: 0.7260 - acc: 0.7224 - val_loss: 1.4513 - val_acc: 0.4510\n",
            "Epoch 103/200\n",
            "1142/1142 [==============================] - 1s 983us/step - loss: 0.7021 - acc: 0.7399 - val_loss: 1.4345 - val_acc: 0.4720\n",
            "Epoch 104/200\n",
            "1142/1142 [==============================] - 1s 998us/step - loss: 0.7425 - acc: 0.7145 - val_loss: 1.4377 - val_acc: 0.4755\n",
            "Epoch 105/200\n",
            "1142/1142 [==============================] - 1s 992us/step - loss: 0.7353 - acc: 0.7137 - val_loss: 1.4392 - val_acc: 0.4720\n",
            "Epoch 106/200\n",
            "1142/1142 [==============================] - 1s 989us/step - loss: 0.7095 - acc: 0.7189 - val_loss: 1.4337 - val_acc: 0.4825\n",
            "Epoch 107/200\n",
            "1142/1142 [==============================] - 1s 993us/step - loss: 0.7169 - acc: 0.7163 - val_loss: 1.4382 - val_acc: 0.4755\n",
            "Epoch 108/200\n",
            "1142/1142 [==============================] - 1s 996us/step - loss: 0.7028 - acc: 0.7443 - val_loss: 1.4320 - val_acc: 0.4720\n",
            "Epoch 109/200\n",
            "1142/1142 [==============================] - 1s 982us/step - loss: 0.7199 - acc: 0.7180 - val_loss: 1.4259 - val_acc: 0.4650\n",
            "Epoch 110/200\n",
            "1142/1142 [==============================] - 1s 995us/step - loss: 0.6973 - acc: 0.7198 - val_loss: 1.4255 - val_acc: 0.4755\n",
            "Epoch 111/200\n",
            "1142/1142 [==============================] - 1s 991us/step - loss: 0.6734 - acc: 0.7417 - val_loss: 1.4425 - val_acc: 0.4790\n",
            "Epoch 112/200\n",
            "1142/1142 [==============================] - 1s 996us/step - loss: 0.7103 - acc: 0.7198 - val_loss: 1.4611 - val_acc: 0.4685\n",
            "Epoch 113/200\n",
            "1142/1142 [==============================] - 1s 978us/step - loss: 0.6630 - acc: 0.7356 - val_loss: 1.4658 - val_acc: 0.4580\n",
            "Epoch 114/200\n",
            "1142/1142 [==============================] - 1s 995us/step - loss: 0.6452 - acc: 0.7557 - val_loss: 1.4584 - val_acc: 0.4895\n",
            "Epoch 115/200\n",
            "1142/1142 [==============================] - 1s 973us/step - loss: 0.6574 - acc: 0.7539 - val_loss: 1.4536 - val_acc: 0.4860\n",
            "Epoch 116/200\n",
            "1142/1142 [==============================] - 1s 981us/step - loss: 0.6879 - acc: 0.7338 - val_loss: 1.4764 - val_acc: 0.4650\n",
            "Epoch 117/200\n",
            "1142/1142 [==============================] - 1s 987us/step - loss: 0.6564 - acc: 0.7417 - val_loss: 1.5111 - val_acc: 0.4615\n",
            "Epoch 118/200\n",
            "1142/1142 [==============================] - 1s 990us/step - loss: 0.6633 - acc: 0.7382 - val_loss: 1.5235 - val_acc: 0.4615\n",
            "Epoch 119/200\n",
            "1142/1142 [==============================] - 1s 988us/step - loss: 0.6562 - acc: 0.7426 - val_loss: 1.5204 - val_acc: 0.4685\n",
            "Epoch 120/200\n",
            "1142/1142 [==============================] - 1s 987us/step - loss: 0.6437 - acc: 0.7391 - val_loss: 1.5108 - val_acc: 0.4755\n",
            "Epoch 121/200\n",
            "1142/1142 [==============================] - 1s 1ms/step - loss: 0.6296 - acc: 0.7583 - val_loss: 1.5140 - val_acc: 0.4790\n",
            "Epoch 122/200\n",
            "1142/1142 [==============================] - 1s 983us/step - loss: 0.6400 - acc: 0.7583 - val_loss: 1.5159 - val_acc: 0.4720\n",
            "Epoch 123/200\n",
            "1142/1142 [==============================] - 1s 982us/step - loss: 0.6225 - acc: 0.7548 - val_loss: 1.5160 - val_acc: 0.4755\n",
            "Epoch 124/200\n",
            "1142/1142 [==============================] - 1s 990us/step - loss: 0.6249 - acc: 0.7627 - val_loss: 1.5059 - val_acc: 0.4860\n",
            "Epoch 125/200\n",
            "1142/1142 [==============================] - 1s 998us/step - loss: 0.6113 - acc: 0.7601 - val_loss: 1.4990 - val_acc: 0.4930\n",
            "Epoch 126/200\n",
            "1142/1142 [==============================] - 1s 1ms/step - loss: 0.6031 - acc: 0.7636 - val_loss: 1.5071 - val_acc: 0.5070\n",
            "Epoch 127/200\n",
            "1142/1142 [==============================] - 1s 994us/step - loss: 0.5913 - acc: 0.7855 - val_loss: 1.5005 - val_acc: 0.5035\n",
            "Epoch 128/200\n",
            "1142/1142 [==============================] - 1s 996us/step - loss: 0.6378 - acc: 0.7408 - val_loss: 1.5040 - val_acc: 0.5000\n",
            "Epoch 129/200\n",
            "1142/1142 [==============================] - 1s 988us/step - loss: 0.5965 - acc: 0.7732 - val_loss: 1.5104 - val_acc: 0.4720\n",
            "Epoch 130/200\n",
            "1142/1142 [==============================] - 1s 989us/step - loss: 0.5739 - acc: 0.7767 - val_loss: 1.5106 - val_acc: 0.5035\n",
            "Epoch 131/200\n",
            "1142/1142 [==============================] - 1s 981us/step - loss: 0.6007 - acc: 0.7706 - val_loss: 1.5021 - val_acc: 0.5035\n",
            "Epoch 132/200\n",
            "1142/1142 [==============================] - 1s 994us/step - loss: 0.5690 - acc: 0.7671 - val_loss: 1.5094 - val_acc: 0.5035\n",
            "Epoch 133/200\n",
            "1142/1142 [==============================] - 1s 989us/step - loss: 0.5519 - acc: 0.7820 - val_loss: 1.5517 - val_acc: 0.4615\n",
            "Epoch 134/200\n",
            "1142/1142 [==============================] - 1s 989us/step - loss: 0.5804 - acc: 0.7680 - val_loss: 1.5753 - val_acc: 0.4685\n",
            "Epoch 135/200\n",
            "1142/1142 [==============================] - 1s 995us/step - loss: 0.5615 - acc: 0.7960 - val_loss: 1.5915 - val_acc: 0.4825\n",
            "Epoch 136/200\n",
            "1142/1142 [==============================] - 1s 988us/step - loss: 0.5685 - acc: 0.7942 - val_loss: 1.5949 - val_acc: 0.4685\n",
            "Epoch 137/200\n",
            "1142/1142 [==============================] - 1s 1ms/step - loss: 0.5662 - acc: 0.7793 - val_loss: 1.5990 - val_acc: 0.5035\n",
            "Epoch 138/200\n",
            "1142/1142 [==============================] - 1s 993us/step - loss: 0.5816 - acc: 0.7776 - val_loss: 1.6269 - val_acc: 0.5035\n",
            "Epoch 139/200\n",
            "1142/1142 [==============================] - 1s 989us/step - loss: 0.5539 - acc: 0.7968 - val_loss: 1.6249 - val_acc: 0.4755\n",
            "Epoch 140/200\n",
            "1142/1142 [==============================] - 1s 994us/step - loss: 0.5299 - acc: 0.7898 - val_loss: 1.6527 - val_acc: 0.4580\n",
            "Epoch 141/200\n",
            "1142/1142 [==============================] - 1s 990us/step - loss: 0.5503 - acc: 0.7802 - val_loss: 1.6374 - val_acc: 0.4685\n",
            "Epoch 142/200\n",
            "1142/1142 [==============================] - 1s 993us/step - loss: 0.5654 - acc: 0.7758 - val_loss: 1.6148 - val_acc: 0.4895\n",
            "Epoch 143/200\n",
            "1142/1142 [==============================] - 1s 992us/step - loss: 0.5214 - acc: 0.8004 - val_loss: 1.6050 - val_acc: 0.4825\n",
            "Epoch 144/200\n",
            "1142/1142 [==============================] - 1s 989us/step - loss: 0.5312 - acc: 0.7942 - val_loss: 1.6030 - val_acc: 0.4790\n",
            "Epoch 145/200\n",
            "1142/1142 [==============================] - 1s 990us/step - loss: 0.5060 - acc: 0.8091 - val_loss: 1.5983 - val_acc: 0.4895\n",
            "Epoch 146/200\n",
            "1142/1142 [==============================] - 1s 982us/step - loss: 0.5095 - acc: 0.8065 - val_loss: 1.6057 - val_acc: 0.4790\n",
            "Epoch 147/200\n",
            "1142/1142 [==============================] - 1s 992us/step - loss: 0.5018 - acc: 0.8187 - val_loss: 1.6478 - val_acc: 0.4825\n",
            "Epoch 148/200\n",
            "1142/1142 [==============================] - 1s 983us/step - loss: 0.5063 - acc: 0.8091 - val_loss: 1.6706 - val_acc: 0.4720\n",
            "Epoch 149/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 0.4853 - acc: 0.8196 - val_loss: 1.6728 - val_acc: 0.4825\n",
            "Epoch 150/200\n",
            "1142/1142 [==============================] - 1s 992us/step - loss: 0.4821 - acc: 0.8222 - val_loss: 1.6645 - val_acc: 0.5000\n",
            "Epoch 151/200\n",
            "1142/1142 [==============================] - 1s 993us/step - loss: 0.4602 - acc: 0.8292 - val_loss: 1.6731 - val_acc: 0.4930\n",
            "Epoch 152/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 0.4756 - acc: 0.8082 - val_loss: 1.6853 - val_acc: 0.4965\n",
            "Epoch 153/200\n",
            "1142/1142 [==============================] - 1s 991us/step - loss: 0.4909 - acc: 0.8065 - val_loss: 1.7037 - val_acc: 0.4790\n",
            "Epoch 154/200\n",
            "1142/1142 [==============================] - 1s 996us/step - loss: 0.5328 - acc: 0.8082 - val_loss: 1.7079 - val_acc: 0.4615\n",
            "Epoch 155/200\n",
            "1142/1142 [==============================] - 1s 985us/step - loss: 0.4535 - acc: 0.8319 - val_loss: 1.7206 - val_acc: 0.4790\n",
            "Epoch 156/200\n",
            "1142/1142 [==============================] - 1s 989us/step - loss: 0.4794 - acc: 0.8319 - val_loss: 1.7267 - val_acc: 0.4790\n",
            "Epoch 157/200\n",
            "1142/1142 [==============================] - 1s 994us/step - loss: 0.4659 - acc: 0.8275 - val_loss: 1.7166 - val_acc: 0.4755\n",
            "Epoch 158/200\n",
            "1142/1142 [==============================] - 1s 985us/step - loss: 0.4843 - acc: 0.8170 - val_loss: 1.7038 - val_acc: 0.4790\n",
            "Epoch 159/200\n",
            "1142/1142 [==============================] - 1s 998us/step - loss: 0.4719 - acc: 0.8301 - val_loss: 1.6898 - val_acc: 0.4895\n",
            "Epoch 160/200\n",
            "1142/1142 [==============================] - 1s 996us/step - loss: 0.4427 - acc: 0.8389 - val_loss: 1.6917 - val_acc: 0.4965\n",
            "Epoch 161/200\n",
            "1142/1142 [==============================] - 1s 984us/step - loss: 0.5000 - acc: 0.8152 - val_loss: 1.7042 - val_acc: 0.4965\n",
            "Epoch 162/200\n",
            "1142/1142 [==============================] - 1s 990us/step - loss: 0.4409 - acc: 0.8266 - val_loss: 1.7222 - val_acc: 0.4755\n",
            "Epoch 163/200\n",
            "1142/1142 [==============================] - 1s 987us/step - loss: 0.4284 - acc: 0.8301 - val_loss: 1.7655 - val_acc: 0.4860\n",
            "Epoch 164/200\n",
            "1142/1142 [==============================] - 1s 990us/step - loss: 0.4481 - acc: 0.8284 - val_loss: 1.7798 - val_acc: 0.4825\n",
            "Epoch 165/200\n",
            "1142/1142 [==============================] - 1s 993us/step - loss: 0.4455 - acc: 0.8406 - val_loss: 1.7534 - val_acc: 0.4755\n",
            "Epoch 166/200\n",
            "1142/1142 [==============================] - 1s 985us/step - loss: 0.4518 - acc: 0.8450 - val_loss: 1.7472 - val_acc: 0.4755\n",
            "Epoch 167/200\n",
            "1142/1142 [==============================] - 1s 987us/step - loss: 0.4581 - acc: 0.8275 - val_loss: 1.7469 - val_acc: 0.4685\n",
            "Epoch 168/200\n",
            "1142/1142 [==============================] - 1s 989us/step - loss: 0.4241 - acc: 0.8398 - val_loss: 1.7386 - val_acc: 0.4615\n",
            "Epoch 169/200\n",
            "1142/1142 [==============================] - 1s 993us/step - loss: 0.4049 - acc: 0.8459 - val_loss: 1.7510 - val_acc: 0.4685\n",
            "Epoch 170/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 0.4185 - acc: 0.8354 - val_loss: 1.7416 - val_acc: 0.4615\n",
            "Epoch 171/200\n",
            "1142/1142 [==============================] - 1s 991us/step - loss: 0.4516 - acc: 0.8249 - val_loss: 1.7164 - val_acc: 0.4755\n",
            "Epoch 172/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 0.4419 - acc: 0.8363 - val_loss: 1.7053 - val_acc: 0.4895\n",
            "Epoch 173/200\n",
            "1142/1142 [==============================] - 1s 991us/step - loss: 0.3876 - acc: 0.8555 - val_loss: 1.7100 - val_acc: 0.4685\n",
            "Epoch 174/200\n",
            "1142/1142 [==============================] - 1s 985us/step - loss: 0.4178 - acc: 0.8371 - val_loss: 1.7251 - val_acc: 0.4720\n",
            "Epoch 175/200\n",
            "1142/1142 [==============================] - 1s 981us/step - loss: 0.4186 - acc: 0.8441 - val_loss: 1.7283 - val_acc: 0.4930\n",
            "Epoch 176/200\n",
            "1142/1142 [==============================] - 1s 980us/step - loss: 0.4316 - acc: 0.8371 - val_loss: 1.7189 - val_acc: 0.4790\n",
            "Epoch 177/200\n",
            "1142/1142 [==============================] - 1s 979us/step - loss: 0.4292 - acc: 0.8380 - val_loss: 1.7093 - val_acc: 0.4755\n",
            "Epoch 178/200\n",
            "1142/1142 [==============================] - 1s 987us/step - loss: 0.3787 - acc: 0.8555 - val_loss: 1.7308 - val_acc: 0.4930\n",
            "Epoch 179/200\n",
            "1142/1142 [==============================] - 1s 994us/step - loss: 0.4068 - acc: 0.8415 - val_loss: 1.7672 - val_acc: 0.4790\n",
            "Epoch 180/200\n",
            "1142/1142 [==============================] - 1s 995us/step - loss: 0.4144 - acc: 0.8468 - val_loss: 1.8304 - val_acc: 0.4650\n",
            "Epoch 181/200\n",
            "1142/1142 [==============================] - 1s 996us/step - loss: 0.3930 - acc: 0.8546 - val_loss: 1.8866 - val_acc: 0.4685\n",
            "Epoch 182/200\n",
            "1142/1142 [==============================] - 1s 985us/step - loss: 0.4255 - acc: 0.8415 - val_loss: 1.8938 - val_acc: 0.4545\n",
            "Epoch 183/200\n",
            "1142/1142 [==============================] - 1s 993us/step - loss: 0.3923 - acc: 0.8485 - val_loss: 1.8652 - val_acc: 0.4510\n",
            "Epoch 184/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 0.4070 - acc: 0.8406 - val_loss: 1.8495 - val_acc: 0.4510\n",
            "Epoch 185/200\n",
            "1142/1142 [==============================] - 1s 992us/step - loss: 0.4300 - acc: 0.8301 - val_loss: 1.8048 - val_acc: 0.4615\n",
            "Epoch 186/200\n",
            "1142/1142 [==============================] - 1s 992us/step - loss: 0.4109 - acc: 0.8564 - val_loss: 1.7740 - val_acc: 0.4790\n",
            "Epoch 187/200\n",
            "1142/1142 [==============================] - 1s 992us/step - loss: 0.4156 - acc: 0.8459 - val_loss: 1.7574 - val_acc: 0.4965\n",
            "Epoch 188/200\n",
            "1142/1142 [==============================] - 1s 986us/step - loss: 0.3527 - acc: 0.8625 - val_loss: 1.7755 - val_acc: 0.4930\n",
            "Epoch 189/200\n",
            "1142/1142 [==============================] - 1s 989us/step - loss: 0.3880 - acc: 0.8608 - val_loss: 1.7828 - val_acc: 0.4895\n",
            "Epoch 190/200\n",
            "1142/1142 [==============================] - 1s 995us/step - loss: 0.3957 - acc: 0.8529 - val_loss: 1.7969 - val_acc: 0.4930\n",
            "Epoch 191/200\n",
            "1142/1142 [==============================] - 1s 999us/step - loss: 0.3678 - acc: 0.8581 - val_loss: 1.8081 - val_acc: 0.4755\n",
            "Epoch 192/200\n",
            "1142/1142 [==============================] - 1s 992us/step - loss: 0.3790 - acc: 0.8573 - val_loss: 1.8243 - val_acc: 0.4720\n",
            "Epoch 193/200\n",
            "1142/1142 [==============================] - 1s 987us/step - loss: 0.3675 - acc: 0.8590 - val_loss: 1.8361 - val_acc: 0.4825\n",
            "Epoch 194/200\n",
            "1142/1142 [==============================] - 1s 1ms/step - loss: 0.3943 - acc: 0.8459 - val_loss: 1.8719 - val_acc: 0.4930\n",
            "Epoch 195/200\n",
            "1142/1142 [==============================] - 1s 995us/step - loss: 0.4155 - acc: 0.8459 - val_loss: 1.8798 - val_acc: 0.5035\n",
            "Epoch 196/200\n",
            "1142/1142 [==============================] - 1s 989us/step - loss: 0.3811 - acc: 0.8494 - val_loss: 1.8548 - val_acc: 0.4895\n",
            "Epoch 197/200\n",
            "1142/1142 [==============================] - 1s 999us/step - loss: 0.3935 - acc: 0.8389 - val_loss: 1.8632 - val_acc: 0.4825\n",
            "Epoch 198/200\n",
            "1142/1142 [==============================] - 1s 995us/step - loss: 0.3654 - acc: 0.8704 - val_loss: 1.8889 - val_acc: 0.4580\n",
            "Epoch 199/200\n",
            "1142/1142 [==============================] - 1s 988us/step - loss: 0.3821 - acc: 0.8503 - val_loss: 1.8887 - val_acc: 0.4580\n",
            "Epoch 200/200\n",
            "1142/1142 [==============================] - 1s 994us/step - loss: 0.3568 - acc: 0.8520 - val_loss: 1.8849 - val_acc: 0.4615\n",
            "Done with one subject\n",
            "300/300 [==============================] - 0s 364us/step\n",
            "300/300 [==============================] - 0s 347us/step\n",
            "300/300 [==============================] - 0s 341us/step\n",
            "300/300 [==============================] - 0s 343us/step\n",
            "282/282 [==============================] - 0s 356us/step\n",
            "294/294 [==============================] - 0s 340us/step\n",
            "300/300 [==============================] - 0s 352us/step\n",
            "300/300 [==============================] - 0s 353us/step\n",
            "282/282 [==============================] - 0s 348us/step\n",
            "Train on 1113 samples, validate on 279 samples\n",
            "Epoch 1/200\n",
            "1113/1113 [==============================] - 3s 3ms/step - loss: 2.8539 - acc: 0.2498 - val_loss: 1.5148 - val_acc: 0.2832\n",
            "Epoch 2/200\n",
            "1113/1113 [==============================] - 1s 981us/step - loss: 2.6203 - acc: 0.2839 - val_loss: 1.5913 - val_acc: 0.3369\n",
            "Epoch 3/200\n",
            "1113/1113 [==============================] - 1s 993us/step - loss: 2.4113 - acc: 0.3046 - val_loss: 1.5832 - val_acc: 0.3477\n",
            "Epoch 4/200\n",
            "1113/1113 [==============================] - 1s 1000us/step - loss: 2.1195 - acc: 0.3288 - val_loss: 1.6307 - val_acc: 0.3441\n",
            "Epoch 5/200\n",
            "1113/1113 [==============================] - 1s 987us/step - loss: 2.0589 - acc: 0.3199 - val_loss: 1.5955 - val_acc: 0.3477\n",
            "Epoch 6/200\n",
            "1113/1113 [==============================] - 1s 989us/step - loss: 1.9714 - acc: 0.3118 - val_loss: 1.5857 - val_acc: 0.3656\n",
            "Epoch 7/200\n",
            "1113/1113 [==============================] - 1s 994us/step - loss: 1.8012 - acc: 0.3145 - val_loss: 1.6045 - val_acc: 0.3513\n",
            "Epoch 8/200\n",
            "1113/1113 [==============================] - 1s 994us/step - loss: 1.7981 - acc: 0.2875 - val_loss: 1.5697 - val_acc: 0.3584\n",
            "Epoch 9/200\n",
            "1113/1113 [==============================] - 1s 980us/step - loss: 1.7640 - acc: 0.2866 - val_loss: 1.5241 - val_acc: 0.3333\n",
            "Epoch 10/200\n",
            "1113/1113 [==============================] - 1s 979us/step - loss: 1.6972 - acc: 0.3028 - val_loss: 1.5475 - val_acc: 0.2796\n",
            "Epoch 11/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 1.6542 - acc: 0.2893 - val_loss: 1.6805 - val_acc: 0.2760\n",
            "Epoch 12/200\n",
            "1113/1113 [==============================] - 1s 1ms/step - loss: 1.5334 - acc: 0.3414 - val_loss: 1.7937 - val_acc: 0.2724\n",
            "Epoch 13/200\n",
            "1113/1113 [==============================] - 1s 991us/step - loss: 1.5394 - acc: 0.3621 - val_loss: 1.7426 - val_acc: 0.3333\n",
            "Epoch 14/200\n",
            "1113/1113 [==============================] - 1s 987us/step - loss: 1.5344 - acc: 0.3181 - val_loss: 1.7161 - val_acc: 0.3190\n",
            "Epoch 15/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 1.5104 - acc: 0.3531 - val_loss: 1.6768 - val_acc: 0.2796\n",
            "Epoch 16/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 1.4503 - acc: 0.3360 - val_loss: 1.5443 - val_acc: 0.3118\n",
            "Epoch 17/200\n",
            "1113/1113 [==============================] - 1s 988us/step - loss: 1.4450 - acc: 0.3540 - val_loss: 1.4652 - val_acc: 0.3262\n",
            "Epoch 18/200\n",
            "1113/1113 [==============================] - 1s 989us/step - loss: 1.4708 - acc: 0.3621 - val_loss: 1.4558 - val_acc: 0.3441\n",
            "Epoch 19/200\n",
            "1113/1113 [==============================] - 1s 989us/step - loss: 1.3958 - acc: 0.3765 - val_loss: 1.5365 - val_acc: 0.3190\n",
            "Epoch 20/200\n",
            "1113/1113 [==============================] - 1s 989us/step - loss: 1.4374 - acc: 0.3836 - val_loss: 1.5108 - val_acc: 0.3226\n",
            "Epoch 21/200\n",
            "1113/1113 [==============================] - 1s 987us/step - loss: 1.4311 - acc: 0.3513 - val_loss: 1.4291 - val_acc: 0.3369\n",
            "Epoch 22/200\n",
            "1113/1113 [==============================] - 1s 989us/step - loss: 1.4068 - acc: 0.3639 - val_loss: 1.4865 - val_acc: 0.3154\n",
            "Epoch 23/200\n",
            "1113/1113 [==============================] - 1s 987us/step - loss: 1.3862 - acc: 0.3872 - val_loss: 1.4716 - val_acc: 0.3477\n",
            "Epoch 24/200\n",
            "1113/1113 [==============================] - 1s 980us/step - loss: 1.3749 - acc: 0.3863 - val_loss: 1.3860 - val_acc: 0.3584\n",
            "Epoch 25/200\n",
            "1113/1113 [==============================] - 1s 999us/step - loss: 1.3224 - acc: 0.4322 - val_loss: 1.3520 - val_acc: 0.3656\n",
            "Epoch 26/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 1.3457 - acc: 0.4169 - val_loss: 1.3642 - val_acc: 0.3620\n",
            "Epoch 27/200\n",
            "1113/1113 [==============================] - 1s 991us/step - loss: 1.3410 - acc: 0.4016 - val_loss: 1.3836 - val_acc: 0.3477\n",
            "Epoch 28/200\n",
            "1113/1113 [==============================] - 1s 999us/step - loss: 1.3297 - acc: 0.4025 - val_loss: 1.3736 - val_acc: 0.3728\n",
            "Epoch 29/200\n",
            "1113/1113 [==============================] - 1s 981us/step - loss: 1.3586 - acc: 0.3899 - val_loss: 1.3466 - val_acc: 0.3871\n",
            "Epoch 30/200\n",
            "1113/1113 [==============================] - 1s 994us/step - loss: 1.3331 - acc: 0.4034 - val_loss: 1.3318 - val_acc: 0.3943\n",
            "Epoch 31/200\n",
            "1113/1113 [==============================] - 1s 989us/step - loss: 1.3078 - acc: 0.4178 - val_loss: 1.3194 - val_acc: 0.4086\n",
            "Epoch 32/200\n",
            "1113/1113 [==============================] - 1s 977us/step - loss: 1.3102 - acc: 0.4106 - val_loss: 1.3204 - val_acc: 0.4050\n",
            "Epoch 33/200\n",
            "1113/1113 [==============================] - 1s 988us/step - loss: 1.3333 - acc: 0.4043 - val_loss: 1.3597 - val_acc: 0.3692\n",
            "Epoch 34/200\n",
            "1113/1113 [==============================] - 1s 985us/step - loss: 1.2654 - acc: 0.4322 - val_loss: 1.3857 - val_acc: 0.3799\n",
            "Epoch 35/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 1.2929 - acc: 0.4250 - val_loss: 1.3765 - val_acc: 0.3835\n",
            "Epoch 36/200\n",
            "1113/1113 [==============================] - 1s 997us/step - loss: 1.2984 - acc: 0.4385 - val_loss: 1.3514 - val_acc: 0.3907\n",
            "Epoch 37/200\n",
            "1113/1113 [==============================] - 1s 990us/step - loss: 1.2820 - acc: 0.4367 - val_loss: 1.3235 - val_acc: 0.3943\n",
            "Epoch 38/200\n",
            "1113/1113 [==============================] - 1s 988us/step - loss: 1.2855 - acc: 0.4456 - val_loss: 1.3066 - val_acc: 0.4014\n",
            "Epoch 39/200\n",
            "1113/1113 [==============================] - 1s 995us/step - loss: 1.2598 - acc: 0.4654 - val_loss: 1.3215 - val_acc: 0.4122\n",
            "Epoch 40/200\n",
            "1113/1113 [==============================] - 1s 979us/step - loss: 1.2727 - acc: 0.4537 - val_loss: 1.3380 - val_acc: 0.3943\n",
            "Epoch 41/200\n",
            "1113/1113 [==============================] - 1s 990us/step - loss: 1.2611 - acc: 0.4412 - val_loss: 1.3232 - val_acc: 0.4014\n",
            "Epoch 42/200\n",
            "1113/1113 [==============================] - 1s 984us/step - loss: 1.2585 - acc: 0.4483 - val_loss: 1.3170 - val_acc: 0.3943\n",
            "Epoch 43/200\n",
            "1113/1113 [==============================] - 1s 977us/step - loss: 1.2576 - acc: 0.4420 - val_loss: 1.3333 - val_acc: 0.4086\n",
            "Epoch 44/200\n",
            "1113/1113 [==============================] - 1s 977us/step - loss: 1.2207 - acc: 0.4627 - val_loss: 1.3564 - val_acc: 0.4086\n",
            "Epoch 45/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 1.2650 - acc: 0.4555 - val_loss: 1.3830 - val_acc: 0.3835\n",
            "Epoch 46/200\n",
            "1113/1113 [==============================] - 1s 996us/step - loss: 1.2017 - acc: 0.4843 - val_loss: 1.3958 - val_acc: 0.3978\n",
            "Epoch 47/200\n",
            "1113/1113 [==============================] - 1s 989us/step - loss: 1.2223 - acc: 0.4681 - val_loss: 1.3403 - val_acc: 0.4014\n",
            "Epoch 48/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 1.1631 - acc: 0.5157 - val_loss: 1.2876 - val_acc: 0.4337\n",
            "Epoch 49/200\n",
            "1113/1113 [==============================] - 1s 996us/step - loss: 1.1640 - acc: 0.5076 - val_loss: 1.2612 - val_acc: 0.4337\n",
            "Epoch 50/200\n",
            "1113/1113 [==============================] - 1s 984us/step - loss: 1.1801 - acc: 0.4717 - val_loss: 1.2573 - val_acc: 0.4409\n",
            "Epoch 51/200\n",
            "1113/1113 [==============================] - 1s 988us/step - loss: 1.1549 - acc: 0.4897 - val_loss: 1.2852 - val_acc: 0.3907\n",
            "Epoch 52/200\n",
            "1113/1113 [==============================] - 1s 990us/step - loss: 1.2060 - acc: 0.4834 - val_loss: 1.3106 - val_acc: 0.3692\n",
            "Epoch 53/200\n",
            "1113/1113 [==============================] - 1s 990us/step - loss: 1.1663 - acc: 0.5121 - val_loss: 1.3003 - val_acc: 0.3835\n",
            "Epoch 54/200\n",
            "1113/1113 [==============================] - 1s 985us/step - loss: 1.1376 - acc: 0.4978 - val_loss: 1.2675 - val_acc: 0.4301\n",
            "Epoch 55/200\n",
            "1113/1113 [==============================] - 1s 983us/step - loss: 1.1537 - acc: 0.4933 - val_loss: 1.2548 - val_acc: 0.4265\n",
            "Epoch 56/200\n",
            "1113/1113 [==============================] - 1s 994us/step - loss: 1.1617 - acc: 0.5166 - val_loss: 1.2842 - val_acc: 0.4158\n",
            "Epoch 57/200\n",
            "1113/1113 [==============================] - 1s 990us/step - loss: 1.1998 - acc: 0.5148 - val_loss: 1.3350 - val_acc: 0.3978\n",
            "Epoch 58/200\n",
            "1113/1113 [==============================] - 1s 971us/step - loss: 1.1134 - acc: 0.5292 - val_loss: 1.3463 - val_acc: 0.4014\n",
            "Epoch 59/200\n",
            "1113/1113 [==============================] - 1s 983us/step - loss: 1.1340 - acc: 0.5157 - val_loss: 1.3383 - val_acc: 0.4050\n",
            "Epoch 60/200\n",
            "1113/1113 [==============================] - 1s 980us/step - loss: 1.1320 - acc: 0.5238 - val_loss: 1.3352 - val_acc: 0.4122\n",
            "Epoch 61/200\n",
            "1113/1113 [==============================] - 1s 980us/step - loss: 1.1563 - acc: 0.5022 - val_loss: 1.3304 - val_acc: 0.4050\n",
            "Epoch 62/200\n",
            "1113/1113 [==============================] - 1s 977us/step - loss: 1.1509 - acc: 0.5139 - val_loss: 1.3184 - val_acc: 0.4086\n",
            "Epoch 63/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 1.0845 - acc: 0.5418 - val_loss: 1.2996 - val_acc: 0.4158\n",
            "Epoch 64/200\n",
            "1113/1113 [==============================] - 1s 997us/step - loss: 1.1197 - acc: 0.5004 - val_loss: 1.2913 - val_acc: 0.4158\n",
            "Epoch 65/200\n",
            "1113/1113 [==============================] - 1s 977us/step - loss: 1.0761 - acc: 0.5391 - val_loss: 1.3010 - val_acc: 0.4086\n",
            "Epoch 66/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 1.0627 - acc: 0.5588 - val_loss: 1.3011 - val_acc: 0.4229\n",
            "Epoch 67/200\n",
            "1113/1113 [==============================] - 1s 980us/step - loss: 1.0683 - acc: 0.5571 - val_loss: 1.2882 - val_acc: 0.4265\n",
            "Epoch 68/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 1.0882 - acc: 0.5409 - val_loss: 1.2778 - val_acc: 0.4337\n",
            "Epoch 69/200\n",
            "1113/1113 [==============================] - 1s 995us/step - loss: 1.0782 - acc: 0.5418 - val_loss: 1.2801 - val_acc: 0.4122\n",
            "Epoch 70/200\n",
            "1113/1113 [==============================] - 1s 974us/step - loss: 1.0448 - acc: 0.5741 - val_loss: 1.2902 - val_acc: 0.4122\n",
            "Epoch 71/200\n",
            "1113/1113 [==============================] - 1s 983us/step - loss: 1.0268 - acc: 0.5660 - val_loss: 1.2948 - val_acc: 0.3943\n",
            "Epoch 72/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 1.0743 - acc: 0.5517 - val_loss: 1.2850 - val_acc: 0.4014\n",
            "Epoch 73/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 1.0419 - acc: 0.5786 - val_loss: 1.2890 - val_acc: 0.4158\n",
            "Epoch 74/200\n",
            "1113/1113 [==============================] - 1s 990us/step - loss: 1.0392 - acc: 0.5894 - val_loss: 1.3283 - val_acc: 0.4301\n",
            "Epoch 75/200\n",
            "1113/1113 [==============================] - 1s 990us/step - loss: 1.0120 - acc: 0.5849 - val_loss: 1.3557 - val_acc: 0.4373\n",
            "Epoch 76/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 0.9757 - acc: 0.6047 - val_loss: 1.3872 - val_acc: 0.4301\n",
            "Epoch 77/200\n",
            "1113/1113 [==============================] - 1s 990us/step - loss: 1.0129 - acc: 0.5633 - val_loss: 1.3817 - val_acc: 0.4086\n",
            "Epoch 78/200\n",
            "1113/1113 [==============================] - 1s 991us/step - loss: 1.0158 - acc: 0.5759 - val_loss: 1.3012 - val_acc: 0.4265\n",
            "Epoch 79/200\n",
            "1113/1113 [==============================] - 1s 995us/step - loss: 1.0175 - acc: 0.5678 - val_loss: 1.2488 - val_acc: 0.4480\n",
            "Epoch 80/200\n",
            "1113/1113 [==============================] - 1s 997us/step - loss: 0.9869 - acc: 0.6038 - val_loss: 1.2428 - val_acc: 0.4516\n",
            "Epoch 81/200\n",
            "1113/1113 [==============================] - 1s 980us/step - loss: 0.9569 - acc: 0.6011 - val_loss: 1.2756 - val_acc: 0.4444\n",
            "Epoch 82/200\n",
            "1113/1113 [==============================] - 1s 996us/step - loss: 0.9123 - acc: 0.6110 - val_loss: 1.3051 - val_acc: 0.4229\n",
            "Epoch 83/200\n",
            "1113/1113 [==============================] - 1s 988us/step - loss: 0.9713 - acc: 0.5975 - val_loss: 1.2791 - val_acc: 0.4373\n",
            "Epoch 84/200\n",
            "1113/1113 [==============================] - 1s 992us/step - loss: 0.9909 - acc: 0.5822 - val_loss: 1.2444 - val_acc: 0.4444\n",
            "Epoch 85/200\n",
            "1113/1113 [==============================] - 1s 990us/step - loss: 0.9567 - acc: 0.6190 - val_loss: 1.2251 - val_acc: 0.4480\n",
            "Epoch 86/200\n",
            "1113/1113 [==============================] - 1s 996us/step - loss: 0.9922 - acc: 0.6029 - val_loss: 1.2205 - val_acc: 0.4480\n",
            "Epoch 87/200\n",
            "1113/1113 [==============================] - 1s 979us/step - loss: 0.9672 - acc: 0.6101 - val_loss: 1.2386 - val_acc: 0.4731\n",
            "Epoch 88/200\n",
            "1113/1113 [==============================] - 1s 997us/step - loss: 0.9582 - acc: 0.6119 - val_loss: 1.2657 - val_acc: 0.4659\n",
            "Epoch 89/200\n",
            "1113/1113 [==============================] - 1s 992us/step - loss: 0.9189 - acc: 0.6352 - val_loss: 1.2904 - val_acc: 0.4767\n",
            "Epoch 90/200\n",
            "1113/1113 [==============================] - 1s 999us/step - loss: 0.9824 - acc: 0.5795 - val_loss: 1.2627 - val_acc: 0.4695\n",
            "Epoch 91/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 0.9402 - acc: 0.6217 - val_loss: 1.2362 - val_acc: 0.4659\n",
            "Epoch 92/200\n",
            "1113/1113 [==============================] - 1s 979us/step - loss: 0.9285 - acc: 0.6146 - val_loss: 1.2378 - val_acc: 0.4624\n",
            "Epoch 93/200\n",
            "1113/1113 [==============================] - 1s 994us/step - loss: 0.9483 - acc: 0.6208 - val_loss: 1.2619 - val_acc: 0.4552\n",
            "Epoch 94/200\n",
            "1113/1113 [==============================] - 1s 988us/step - loss: 0.9363 - acc: 0.6451 - val_loss: 1.2709 - val_acc: 0.4588\n",
            "Epoch 95/200\n",
            "1113/1113 [==============================] - 1s 974us/step - loss: 0.9114 - acc: 0.6307 - val_loss: 1.2246 - val_acc: 0.4767\n",
            "Epoch 96/200\n",
            "1113/1113 [==============================] - 1s 994us/step - loss: 0.8829 - acc: 0.6370 - val_loss: 1.1816 - val_acc: 0.4695\n",
            "Epoch 97/200\n",
            "1113/1113 [==============================] - 1s 991us/step - loss: 0.8678 - acc: 0.6433 - val_loss: 1.1651 - val_acc: 0.4875\n",
            "Epoch 98/200\n",
            "1113/1113 [==============================] - 1s 978us/step - loss: 0.9336 - acc: 0.6181 - val_loss: 1.1873 - val_acc: 0.4695\n",
            "Epoch 99/200\n",
            "1113/1113 [==============================] - 1s 979us/step - loss: 0.8692 - acc: 0.6514 - val_loss: 1.2204 - val_acc: 0.4624\n",
            "Epoch 100/200\n",
            "1113/1113 [==============================] - 1s 994us/step - loss: 0.8750 - acc: 0.6559 - val_loss: 1.1852 - val_acc: 0.4767\n",
            "Epoch 101/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 0.8714 - acc: 0.6559 - val_loss: 1.1459 - val_acc: 0.4803\n",
            "Epoch 102/200\n",
            "1113/1113 [==============================] - 1s 994us/step - loss: 0.8871 - acc: 0.6262 - val_loss: 1.1430 - val_acc: 0.4839\n",
            "Epoch 103/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 0.8685 - acc: 0.6262 - val_loss: 1.1857 - val_acc: 0.4875\n",
            "Epoch 104/200\n",
            "1113/1113 [==============================] - 1s 983us/step - loss: 0.8841 - acc: 0.6298 - val_loss: 1.2334 - val_acc: 0.4659\n",
            "Epoch 105/200\n",
            "1113/1113 [==============================] - 1s 994us/step - loss: 0.8563 - acc: 0.6577 - val_loss: 1.2605 - val_acc: 0.4695\n",
            "Epoch 106/200\n",
            "1113/1113 [==============================] - 1s 985us/step - loss: 0.8383 - acc: 0.6541 - val_loss: 1.2370 - val_acc: 0.4695\n",
            "Epoch 107/200\n",
            "1113/1113 [==============================] - 1s 985us/step - loss: 0.8313 - acc: 0.6586 - val_loss: 1.1760 - val_acc: 0.4839\n",
            "Epoch 108/200\n",
            "1113/1113 [==============================] - 1s 978us/step - loss: 0.8219 - acc: 0.6667 - val_loss: 1.1889 - val_acc: 0.4803\n",
            "Epoch 109/200\n",
            "1113/1113 [==============================] - 1s 993us/step - loss: 0.8775 - acc: 0.6568 - val_loss: 1.2083 - val_acc: 0.4767\n",
            "Epoch 110/200\n",
            "1113/1113 [==============================] - 1s 975us/step - loss: 0.8462 - acc: 0.6622 - val_loss: 1.2042 - val_acc: 0.4839\n",
            "Epoch 111/200\n",
            "1113/1113 [==============================] - 1s 988us/step - loss: 0.8415 - acc: 0.6631 - val_loss: 1.2098 - val_acc: 0.4803\n",
            "Epoch 112/200\n",
            "1113/1113 [==============================] - 1s 999us/step - loss: 0.7855 - acc: 0.7017 - val_loss: 1.2018 - val_acc: 0.4839\n",
            "Epoch 113/200\n",
            "1113/1113 [==============================] - 1s 990us/step - loss: 0.7981 - acc: 0.6891 - val_loss: 1.2072 - val_acc: 0.4839\n",
            "Epoch 114/200\n",
            "1113/1113 [==============================] - 1s 981us/step - loss: 0.8231 - acc: 0.6667 - val_loss: 1.2004 - val_acc: 0.4910\n",
            "Epoch 115/200\n",
            "1113/1113 [==============================] - 1s 972us/step - loss: 0.8053 - acc: 0.6667 - val_loss: 1.2015 - val_acc: 0.5018\n",
            "Epoch 116/200\n",
            "1113/1113 [==============================] - 1s 978us/step - loss: 0.7774 - acc: 0.6900 - val_loss: 1.2040 - val_acc: 0.4803\n",
            "Epoch 117/200\n",
            "1113/1113 [==============================] - 1s 985us/step - loss: 0.7740 - acc: 0.6900 - val_loss: 1.1927 - val_acc: 0.4875\n",
            "Epoch 118/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 0.8115 - acc: 0.6819 - val_loss: 1.1750 - val_acc: 0.5018\n",
            "Epoch 119/200\n",
            "1113/1113 [==============================] - 1s 992us/step - loss: 0.7233 - acc: 0.7107 - val_loss: 1.1608 - val_acc: 0.5018\n",
            "Epoch 120/200\n",
            "1113/1113 [==============================] - 1s 991us/step - loss: 0.7503 - acc: 0.7017 - val_loss: 1.1727 - val_acc: 0.4982\n",
            "Epoch 121/200\n",
            "1113/1113 [==============================] - 1s 983us/step - loss: 0.7966 - acc: 0.6882 - val_loss: 1.1852 - val_acc: 0.4767\n",
            "Epoch 122/200\n",
            "1113/1113 [==============================] - 1s 983us/step - loss: 0.7490 - acc: 0.7026 - val_loss: 1.2101 - val_acc: 0.4803\n",
            "Epoch 123/200\n",
            "1113/1113 [==============================] - 1s 988us/step - loss: 0.7765 - acc: 0.6963 - val_loss: 1.2093 - val_acc: 0.4982\n",
            "Epoch 124/200\n",
            "1113/1113 [==============================] - 1s 993us/step - loss: 0.7581 - acc: 0.6972 - val_loss: 1.2035 - val_acc: 0.5125\n",
            "Epoch 125/200\n",
            "1113/1113 [==============================] - 1s 988us/step - loss: 0.8004 - acc: 0.6855 - val_loss: 1.2129 - val_acc: 0.4875\n",
            "Epoch 126/200\n",
            "1113/1113 [==============================] - 1s 991us/step - loss: 0.7431 - acc: 0.7044 - val_loss: 1.2191 - val_acc: 0.4803\n",
            "Epoch 127/200\n",
            "1113/1113 [==============================] - 1s 977us/step - loss: 0.7184 - acc: 0.7098 - val_loss: 1.2272 - val_acc: 0.4910\n",
            "Epoch 128/200\n",
            "1113/1113 [==============================] - 1s 991us/step - loss: 0.7765 - acc: 0.6909 - val_loss: 1.2290 - val_acc: 0.4910\n",
            "Epoch 129/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 0.7591 - acc: 0.6918 - val_loss: 1.1893 - val_acc: 0.5018\n",
            "Epoch 130/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 0.7458 - acc: 0.6963 - val_loss: 1.1615 - val_acc: 0.5125\n",
            "Epoch 131/200\n",
            "1113/1113 [==============================] - 1s 975us/step - loss: 0.7365 - acc: 0.7071 - val_loss: 1.1487 - val_acc: 0.5018\n",
            "Epoch 132/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 0.7253 - acc: 0.7053 - val_loss: 1.1553 - val_acc: 0.4875\n",
            "Epoch 133/200\n",
            "1113/1113 [==============================] - 1s 970us/step - loss: 0.6983 - acc: 0.7206 - val_loss: 1.1890 - val_acc: 0.4839\n",
            "Epoch 134/200\n",
            "1113/1113 [==============================] - 1s 977us/step - loss: 0.7249 - acc: 0.7233 - val_loss: 1.2008 - val_acc: 0.4767\n",
            "Epoch 135/200\n",
            "1113/1113 [==============================] - 1s 968us/step - loss: 0.7088 - acc: 0.7233 - val_loss: 1.2013 - val_acc: 0.4588\n",
            "Epoch 136/200\n",
            "1113/1113 [==============================] - 1s 981us/step - loss: 0.7311 - acc: 0.7125 - val_loss: 1.2070 - val_acc: 0.4731\n",
            "Epoch 137/200\n",
            "1113/1113 [==============================] - 1s 971us/step - loss: 0.7025 - acc: 0.7170 - val_loss: 1.2097 - val_acc: 0.4803\n",
            "Epoch 138/200\n",
            "1113/1113 [==============================] - 1s 981us/step - loss: 0.7169 - acc: 0.7197 - val_loss: 1.1827 - val_acc: 0.4910\n",
            "Epoch 139/200\n",
            "1113/1113 [==============================] - 1s 979us/step - loss: 0.6747 - acc: 0.7314 - val_loss: 1.1808 - val_acc: 0.5018\n",
            "Epoch 140/200\n",
            "1113/1113 [==============================] - 1s 973us/step - loss: 0.6843 - acc: 0.7224 - val_loss: 1.1893 - val_acc: 0.4982\n",
            "Epoch 141/200\n",
            "1113/1113 [==============================] - 1s 973us/step - loss: 0.7139 - acc: 0.7224 - val_loss: 1.2112 - val_acc: 0.5125\n",
            "Epoch 142/200\n",
            "1113/1113 [==============================] - 1s 970us/step - loss: 0.6762 - acc: 0.7224 - val_loss: 1.2304 - val_acc: 0.5018\n",
            "Epoch 143/200\n",
            "1113/1113 [==============================] - 1s 988us/step - loss: 0.6647 - acc: 0.7403 - val_loss: 1.2298 - val_acc: 0.4910\n",
            "Epoch 144/200\n",
            "1113/1113 [==============================] - 1s 979us/step - loss: 0.6714 - acc: 0.7323 - val_loss: 1.2363 - val_acc: 0.4659\n",
            "Epoch 145/200\n",
            "1113/1113 [==============================] - 1s 977us/step - loss: 0.6238 - acc: 0.7502 - val_loss: 1.2329 - val_acc: 0.4588\n",
            "Epoch 146/200\n",
            "1113/1113 [==============================] - 1s 983us/step - loss: 0.6438 - acc: 0.7439 - val_loss: 1.2198 - val_acc: 0.4695\n",
            "Epoch 147/200\n",
            "1113/1113 [==============================] - 1s 997us/step - loss: 0.6463 - acc: 0.7412 - val_loss: 1.2225 - val_acc: 0.4803\n",
            "Epoch 148/200\n",
            "1113/1113 [==============================] - 1s 991us/step - loss: 0.6286 - acc: 0.7583 - val_loss: 1.2238 - val_acc: 0.4910\n",
            "Epoch 149/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 0.6408 - acc: 0.7367 - val_loss: 1.2102 - val_acc: 0.5018\n",
            "Epoch 150/200\n",
            "1113/1113 [==============================] - 1s 981us/step - loss: 0.6257 - acc: 0.7493 - val_loss: 1.1953 - val_acc: 0.5054\n",
            "Epoch 151/200\n",
            "1113/1113 [==============================] - 1s 990us/step - loss: 0.6519 - acc: 0.7493 - val_loss: 1.2079 - val_acc: 0.5161\n",
            "Epoch 152/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 0.6268 - acc: 0.7601 - val_loss: 1.2207 - val_acc: 0.5197\n",
            "Epoch 153/200\n",
            "1113/1113 [==============================] - 1s 994us/step - loss: 0.6241 - acc: 0.7520 - val_loss: 1.2358 - val_acc: 0.4875\n",
            "Epoch 154/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 0.6069 - acc: 0.7772 - val_loss: 1.2490 - val_acc: 0.4839\n",
            "Epoch 155/200\n",
            "1113/1113 [==============================] - 1s 1ms/step - loss: 0.6170 - acc: 0.7547 - val_loss: 1.2396 - val_acc: 0.4767\n",
            "Epoch 156/200\n",
            "1113/1113 [==============================] - 1s 984us/step - loss: 0.6138 - acc: 0.7619 - val_loss: 1.2244 - val_acc: 0.4803\n",
            "Epoch 157/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 0.5938 - acc: 0.7583 - val_loss: 1.2304 - val_acc: 0.4910\n",
            "Epoch 158/200\n",
            "1113/1113 [==============================] - 1s 998us/step - loss: 0.5993 - acc: 0.7727 - val_loss: 1.2510 - val_acc: 0.4910\n",
            "Epoch 159/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 0.6255 - acc: 0.7664 - val_loss: 1.2532 - val_acc: 0.4875\n",
            "Epoch 160/200\n",
            "1113/1113 [==============================] - 1s 977us/step - loss: 0.5853 - acc: 0.7574 - val_loss: 1.2487 - val_acc: 0.4875\n",
            "Epoch 161/200\n",
            "1113/1113 [==============================] - 1s 986us/step - loss: 0.5971 - acc: 0.7673 - val_loss: 1.2411 - val_acc: 0.4910\n",
            "Epoch 162/200\n",
            "1113/1113 [==============================] - 1s 975us/step - loss: 0.5607 - acc: 0.7835 - val_loss: 1.2378 - val_acc: 0.4910\n",
            "Epoch 163/200\n",
            "1113/1113 [==============================] - 1s 979us/step - loss: 0.6092 - acc: 0.7673 - val_loss: 1.2340 - val_acc: 0.5090\n",
            "Epoch 164/200\n",
            "1113/1113 [==============================] - 1s 987us/step - loss: 0.6575 - acc: 0.7358 - val_loss: 1.2256 - val_acc: 0.5054\n",
            "Epoch 165/200\n",
            "1113/1113 [==============================] - 1s 993us/step - loss: 0.5422 - acc: 0.7942 - val_loss: 1.2106 - val_acc: 0.5161\n",
            "Epoch 166/200\n",
            "1113/1113 [==============================] - 1s 977us/step - loss: 0.6166 - acc: 0.7745 - val_loss: 1.2244 - val_acc: 0.5018\n",
            "Epoch 167/200\n",
            "1113/1113 [==============================] - 1s 983us/step - loss: 0.5479 - acc: 0.7862 - val_loss: 1.2523 - val_acc: 0.4946\n",
            "Epoch 168/200\n",
            "1113/1113 [==============================] - 1s 981us/step - loss: 0.5397 - acc: 0.7960 - val_loss: 1.2737 - val_acc: 0.4875\n",
            "Epoch 169/200\n",
            "1113/1113 [==============================] - 1s 995us/step - loss: 0.5514 - acc: 0.7942 - val_loss: 1.2820 - val_acc: 0.4910\n",
            "Epoch 170/200\n",
            "1113/1113 [==============================] - 1s 991us/step - loss: 0.5542 - acc: 0.7646 - val_loss: 1.3020 - val_acc: 0.4946\n",
            "Epoch 171/200\n",
            "1113/1113 [==============================] - 1s 992us/step - loss: 0.5602 - acc: 0.7808 - val_loss: 1.3002 - val_acc: 0.5090\n",
            "Epoch 172/200\n",
            "1113/1113 [==============================] - 1s 981us/step - loss: 0.5504 - acc: 0.7951 - val_loss: 1.2913 - val_acc: 0.5125\n",
            "Epoch 173/200\n",
            "1113/1113 [==============================] - 1s 987us/step - loss: 0.5518 - acc: 0.7898 - val_loss: 1.3013 - val_acc: 0.4946\n",
            "Epoch 174/200\n",
            "1113/1113 [==============================] - 1s 988us/step - loss: 0.5201 - acc: 0.7942 - val_loss: 1.2991 - val_acc: 0.5018\n",
            "Epoch 175/200\n",
            "1113/1113 [==============================] - 1s 989us/step - loss: 0.5379 - acc: 0.7826 - val_loss: 1.3014 - val_acc: 0.5018\n",
            "Epoch 176/200\n",
            "1113/1113 [==============================] - 1s 987us/step - loss: 0.6134 - acc: 0.7610 - val_loss: 1.3275 - val_acc: 0.5125\n",
            "Epoch 177/200\n",
            "1113/1113 [==============================] - 1s 985us/step - loss: 0.5357 - acc: 0.7951 - val_loss: 1.3249 - val_acc: 0.5018\n",
            "Epoch 178/200\n",
            "1113/1113 [==============================] - 1s 991us/step - loss: 0.5272 - acc: 0.8077 - val_loss: 1.3194 - val_acc: 0.4982\n",
            "Epoch 179/200\n",
            "1113/1113 [==============================] - 1s 989us/step - loss: 0.5380 - acc: 0.8005 - val_loss: 1.2974 - val_acc: 0.5233\n",
            "Epoch 180/200\n",
            "1113/1113 [==============================] - 1s 984us/step - loss: 0.5119 - acc: 0.7987 - val_loss: 1.3112 - val_acc: 0.4982\n",
            "Epoch 181/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 0.5386 - acc: 0.7817 - val_loss: 1.3390 - val_acc: 0.5018\n",
            "Epoch 182/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 0.5305 - acc: 0.7781 - val_loss: 1.3291 - val_acc: 0.4982\n",
            "Epoch 183/200\n",
            "1113/1113 [==============================] - 1s 1ms/step - loss: 0.5327 - acc: 0.7951 - val_loss: 1.3221 - val_acc: 0.5125\n",
            "Epoch 184/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 0.5013 - acc: 0.7969 - val_loss: 1.3306 - val_acc: 0.4767\n",
            "Epoch 185/200\n",
            "1113/1113 [==============================] - 1s 992us/step - loss: 0.5169 - acc: 0.7951 - val_loss: 1.3323 - val_acc: 0.4875\n",
            "Epoch 186/200\n",
            "1113/1113 [==============================] - 1s 987us/step - loss: 0.4941 - acc: 0.8176 - val_loss: 1.3351 - val_acc: 0.4695\n",
            "Epoch 187/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 0.4814 - acc: 0.8122 - val_loss: 1.3054 - val_acc: 0.5018\n",
            "Epoch 188/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 0.5369 - acc: 0.8023 - val_loss: 1.2835 - val_acc: 0.5125\n",
            "Epoch 189/200\n",
            "1113/1113 [==============================] - 1s 982us/step - loss: 0.5099 - acc: 0.8050 - val_loss: 1.2962 - val_acc: 0.5018\n",
            "Epoch 190/200\n",
            "1113/1113 [==============================] - 1s 980us/step - loss: 0.5032 - acc: 0.8041 - val_loss: 1.3026 - val_acc: 0.5161\n",
            "Epoch 191/200\n",
            "1113/1113 [==============================] - 1s 997us/step - loss: 0.4887 - acc: 0.8095 - val_loss: 1.2983 - val_acc: 0.5233\n",
            "Epoch 192/200\n",
            "1113/1113 [==============================] - 1s 984us/step - loss: 0.4683 - acc: 0.8194 - val_loss: 1.2979 - val_acc: 0.5341\n",
            "Epoch 193/200\n",
            "1113/1113 [==============================] - 1s 994us/step - loss: 0.4571 - acc: 0.8203 - val_loss: 1.2883 - val_acc: 0.5269\n",
            "Epoch 194/200\n",
            "1113/1113 [==============================] - 1s 995us/step - loss: 0.4242 - acc: 0.8446 - val_loss: 1.2941 - val_acc: 0.5269\n",
            "Epoch 195/200\n",
            "1113/1113 [==============================] - 1s 985us/step - loss: 0.5069 - acc: 0.8140 - val_loss: 1.3369 - val_acc: 0.5090\n",
            "Epoch 196/200\n",
            "1113/1113 [==============================] - 1s 989us/step - loss: 0.4807 - acc: 0.8041 - val_loss: 1.3806 - val_acc: 0.4982\n",
            "Epoch 197/200\n",
            "1113/1113 [==============================] - 1s 994us/step - loss: 0.5131 - acc: 0.8140 - val_loss: 1.3602 - val_acc: 0.5090\n",
            "Epoch 198/200\n",
            "1113/1113 [==============================] - 1s 989us/step - loss: 0.5075 - acc: 0.8131 - val_loss: 1.3412 - val_acc: 0.5412\n",
            "Epoch 199/200\n",
            "1113/1113 [==============================] - 1s 988us/step - loss: 0.4509 - acc: 0.8203 - val_loss: 1.3506 - val_acc: 0.5412\n",
            "Epoch 200/200\n",
            "1113/1113 [==============================] - 1s 988us/step - loss: 0.4985 - acc: 0.8086 - val_loss: 1.3210 - val_acc: 0.5412\n",
            "Done with one subject\n",
            "300/300 [==============================] - 0s 336us/step\n",
            "300/300 [==============================] - 0s 332us/step\n",
            "300/300 [==============================] - 0s 339us/step\n",
            "300/300 [==============================] - 0s 330us/step\n",
            "282/282 [==============================] - 0s 338us/step\n",
            "294/294 [==============================] - 0s 346us/step\n",
            "300/300 [==============================] - 0s 361us/step\n",
            "300/300 [==============================] - 0s 333us/step\n",
            "282/282 [==============================] - 0s 349us/step\n",
            "Train on 1108 samples, validate on 278 samples\n",
            "Epoch 1/200\n",
            "1108/1108 [==============================] - 3s 3ms/step - loss: 2.6952 - acc: 0.2653 - val_loss: 1.8221 - val_acc: 0.3525\n",
            "Epoch 2/200\n",
            "1108/1108 [==============================] - 1s 991us/step - loss: 2.4484 - acc: 0.3267 - val_loss: 1.9871 - val_acc: 0.3381\n",
            "Epoch 3/200\n",
            "1108/1108 [==============================] - 1s 993us/step - loss: 2.3078 - acc: 0.3285 - val_loss: 2.2127 - val_acc: 0.3597\n",
            "Epoch 4/200\n",
            "1108/1108 [==============================] - 1s 991us/step - loss: 2.1385 - acc: 0.3150 - val_loss: 2.4587 - val_acc: 0.3417\n",
            "Epoch 5/200\n",
            "1108/1108 [==============================] - 1s 988us/step - loss: 1.9713 - acc: 0.3547 - val_loss: 2.7371 - val_acc: 0.3489\n",
            "Epoch 6/200\n",
            "1108/1108 [==============================] - 1s 985us/step - loss: 1.8515 - acc: 0.3430 - val_loss: 2.7313 - val_acc: 0.2878\n",
            "Epoch 7/200\n",
            "1108/1108 [==============================] - 1s 993us/step - loss: 1.7095 - acc: 0.3718 - val_loss: 2.2663 - val_acc: 0.2698\n",
            "Epoch 8/200\n",
            "1108/1108 [==============================] - 1s 982us/step - loss: 1.6891 - acc: 0.3592 - val_loss: 2.1600 - val_acc: 0.2842\n",
            "Epoch 9/200\n",
            "1108/1108 [==============================] - 1s 990us/step - loss: 1.6506 - acc: 0.3736 - val_loss: 2.0336 - val_acc: 0.2698\n",
            "Epoch 10/200\n",
            "1108/1108 [==============================] - 1s 991us/step - loss: 1.6220 - acc: 0.3574 - val_loss: 1.9489 - val_acc: 0.3165\n",
            "Epoch 11/200\n",
            "1108/1108 [==============================] - 1s 980us/step - loss: 1.5596 - acc: 0.3971 - val_loss: 1.8358 - val_acc: 0.3597\n",
            "Epoch 12/200\n",
            "1108/1108 [==============================] - 1s 1000us/step - loss: 1.5131 - acc: 0.4043 - val_loss: 1.6239 - val_acc: 0.3957\n",
            "Epoch 13/200\n",
            "1108/1108 [==============================] - 1s 1ms/step - loss: 1.4680 - acc: 0.4134 - val_loss: 1.6001 - val_acc: 0.3957\n",
            "Epoch 14/200\n",
            "1108/1108 [==============================] - 1s 985us/step - loss: 1.4182 - acc: 0.4314 - val_loss: 1.6531 - val_acc: 0.3813\n",
            "Epoch 15/200\n",
            "1108/1108 [==============================] - 1s 984us/step - loss: 1.3670 - acc: 0.4468 - val_loss: 1.6788 - val_acc: 0.3669\n",
            "Epoch 16/200\n",
            "1108/1108 [==============================] - 1s 981us/step - loss: 1.4300 - acc: 0.4287 - val_loss: 1.5999 - val_acc: 0.4065\n",
            "Epoch 17/200\n",
            "1108/1108 [==============================] - 1s 995us/step - loss: 1.3240 - acc: 0.4937 - val_loss: 1.4634 - val_acc: 0.4388\n",
            "Epoch 18/200\n",
            "1108/1108 [==============================] - 1s 985us/step - loss: 1.3490 - acc: 0.4449 - val_loss: 1.3761 - val_acc: 0.4748\n",
            "Epoch 19/200\n",
            "1108/1108 [==============================] - 1s 986us/step - loss: 1.2720 - acc: 0.4847 - val_loss: 1.3085 - val_acc: 0.5036\n",
            "Epoch 20/200\n",
            "1108/1108 [==============================] - 1s 985us/step - loss: 1.2985 - acc: 0.4531 - val_loss: 1.2886 - val_acc: 0.5072\n",
            "Epoch 21/200\n",
            "1108/1108 [==============================] - 1s 989us/step - loss: 1.2699 - acc: 0.4937 - val_loss: 1.2848 - val_acc: 0.5108\n",
            "Epoch 22/200\n",
            "1108/1108 [==============================] - 1s 982us/step - loss: 1.2252 - acc: 0.4856 - val_loss: 1.2780 - val_acc: 0.5180\n",
            "Epoch 23/200\n",
            "1108/1108 [==============================] - 1s 988us/step - loss: 1.2472 - acc: 0.4883 - val_loss: 1.2360 - val_acc: 0.5252\n",
            "Epoch 24/200\n",
            "1108/1108 [==============================] - 1s 1ms/step - loss: 1.1838 - acc: 0.5244 - val_loss: 1.2070 - val_acc: 0.5612\n",
            "Epoch 25/200\n",
            "1108/1108 [==============================] - 1s 985us/step - loss: 1.2198 - acc: 0.4928 - val_loss: 1.2061 - val_acc: 0.5719\n",
            "Epoch 26/200\n",
            "1108/1108 [==============================] - 1s 1000us/step - loss: 1.2069 - acc: 0.5027 - val_loss: 1.1876 - val_acc: 0.5719\n",
            "Epoch 27/200\n",
            "1108/1108 [==============================] - 1s 987us/step - loss: 1.1477 - acc: 0.5144 - val_loss: 1.1408 - val_acc: 0.5827\n",
            "Epoch 28/200\n",
            "1108/1108 [==============================] - 1s 991us/step - loss: 1.0838 - acc: 0.5433 - val_loss: 1.1496 - val_acc: 0.5719\n",
            "Epoch 29/200\n",
            "1108/1108 [==============================] - 1s 991us/step - loss: 1.1515 - acc: 0.5262 - val_loss: 1.1477 - val_acc: 0.5683\n",
            "Epoch 30/200\n",
            "1108/1108 [==============================] - 1s 991us/step - loss: 1.0984 - acc: 0.5451 - val_loss: 1.1346 - val_acc: 0.5791\n",
            "Epoch 31/200\n",
            "1108/1108 [==============================] - 1s 980us/step - loss: 1.1026 - acc: 0.5415 - val_loss: 1.1146 - val_acc: 0.5719\n",
            "Epoch 32/200\n",
            "1108/1108 [==============================] - 1s 998us/step - loss: 1.0666 - acc: 0.5623 - val_loss: 1.0845 - val_acc: 0.5899\n",
            "Epoch 33/200\n",
            "1108/1108 [==============================] - 1s 993us/step - loss: 1.0680 - acc: 0.5713 - val_loss: 1.0527 - val_acc: 0.6007\n",
            "Epoch 34/200\n",
            "1108/1108 [==============================] - 1s 990us/step - loss: 1.0335 - acc: 0.5758 - val_loss: 1.0209 - val_acc: 0.6187\n",
            "Epoch 35/200\n",
            "1108/1108 [==============================] - 1s 984us/step - loss: 1.0734 - acc: 0.5848 - val_loss: 0.9976 - val_acc: 0.6223\n",
            "Epoch 36/200\n",
            "1108/1108 [==============================] - 1s 982us/step - loss: 1.0290 - acc: 0.5794 - val_loss: 0.9962 - val_acc: 0.6151\n",
            "Epoch 37/200\n",
            "1108/1108 [==============================] - 1s 994us/step - loss: 0.9953 - acc: 0.5966 - val_loss: 1.0008 - val_acc: 0.6115\n",
            "Epoch 38/200\n",
            "1108/1108 [==============================] - 1s 992us/step - loss: 1.0114 - acc: 0.6110 - val_loss: 0.9996 - val_acc: 0.6187\n",
            "Epoch 39/200\n",
            "1108/1108 [==============================] - 1s 1ms/step - loss: 0.9865 - acc: 0.6056 - val_loss: 1.0089 - val_acc: 0.6331\n",
            "Epoch 40/200\n",
            "1108/1108 [==============================] - 1s 994us/step - loss: 0.9616 - acc: 0.6119 - val_loss: 1.0247 - val_acc: 0.6331\n",
            "Epoch 41/200\n",
            "1108/1108 [==============================] - 1s 1ms/step - loss: 0.9428 - acc: 0.6155 - val_loss: 1.0457 - val_acc: 0.6223\n",
            "Epoch 42/200\n",
            "1108/1108 [==============================] - 1s 988us/step - loss: 0.9232 - acc: 0.6471 - val_loss: 1.0463 - val_acc: 0.6223\n",
            "Epoch 43/200\n",
            "1108/1108 [==============================] - 1s 1ms/step - loss: 0.9008 - acc: 0.6381 - val_loss: 1.0339 - val_acc: 0.6223\n",
            "Epoch 44/200\n",
            "1108/1108 [==============================] - 1s 992us/step - loss: 0.9380 - acc: 0.6309 - val_loss: 0.9965 - val_acc: 0.6295\n",
            "Epoch 45/200\n",
            "1108/1108 [==============================] - 1s 980us/step - loss: 0.9087 - acc: 0.6426 - val_loss: 0.9754 - val_acc: 0.6295\n",
            "Epoch 46/200\n",
            "1108/1108 [==============================] - 1s 995us/step - loss: 0.9277 - acc: 0.6291 - val_loss: 0.9760 - val_acc: 0.6403\n",
            "Epoch 47/200\n",
            "1108/1108 [==============================] - 1s 986us/step - loss: 0.8791 - acc: 0.6552 - val_loss: 0.9887 - val_acc: 0.6331\n",
            "Epoch 48/200\n",
            "1108/1108 [==============================] - 1s 994us/step - loss: 0.8798 - acc: 0.6525 - val_loss: 1.0063 - val_acc: 0.6475\n",
            "Epoch 49/200\n",
            "1108/1108 [==============================] - 1s 980us/step - loss: 0.8905 - acc: 0.6318 - val_loss: 1.0147 - val_acc: 0.6367\n",
            "Epoch 50/200\n",
            "1108/1108 [==============================] - 1s 998us/step - loss: 0.9141 - acc: 0.6300 - val_loss: 0.9952 - val_acc: 0.6151\n",
            "Epoch 51/200\n",
            "1108/1108 [==============================] - 1s 995us/step - loss: 0.8099 - acc: 0.6805 - val_loss: 0.9534 - val_acc: 0.6187\n",
            "Epoch 52/200\n",
            "1108/1108 [==============================] - 1s 994us/step - loss: 0.8593 - acc: 0.6679 - val_loss: 0.8979 - val_acc: 0.6619\n",
            "Epoch 53/200\n",
            "1108/1108 [==============================] - 1s 1ms/step - loss: 0.8602 - acc: 0.6634 - val_loss: 0.8873 - val_acc: 0.6691\n",
            "Epoch 54/200\n",
            "1108/1108 [==============================] - 1s 988us/step - loss: 0.8701 - acc: 0.6516 - val_loss: 0.9453 - val_acc: 0.6475\n",
            "Epoch 55/200\n",
            "1108/1108 [==============================] - 1s 986us/step - loss: 0.8325 - acc: 0.6778 - val_loss: 0.9857 - val_acc: 0.6439\n",
            "Epoch 56/200\n",
            "1108/1108 [==============================] - 1s 991us/step - loss: 0.8141 - acc: 0.6733 - val_loss: 1.0101 - val_acc: 0.6475\n",
            "Epoch 57/200\n",
            "1108/1108 [==============================] - 1s 991us/step - loss: 0.7756 - acc: 0.6931 - val_loss: 1.0007 - val_acc: 0.6475\n",
            "Epoch 58/200\n",
            "1108/1108 [==============================] - 1s 987us/step - loss: 0.7921 - acc: 0.6958 - val_loss: 0.9564 - val_acc: 0.6475\n",
            "Epoch 59/200\n",
            "1108/1108 [==============================] - 1s 986us/step - loss: 0.8291 - acc: 0.6814 - val_loss: 0.9593 - val_acc: 0.6511\n",
            "Epoch 60/200\n",
            "1108/1108 [==============================] - 1s 987us/step - loss: 0.7563 - acc: 0.6940 - val_loss: 1.0065 - val_acc: 0.6403\n",
            "Epoch 61/200\n",
            "1108/1108 [==============================] - 1s 992us/step - loss: 0.7735 - acc: 0.6958 - val_loss: 1.0490 - val_acc: 0.6367\n",
            "Epoch 62/200\n",
            "1108/1108 [==============================] - 1s 988us/step - loss: 0.7725 - acc: 0.6778 - val_loss: 1.0251 - val_acc: 0.6331\n",
            "Epoch 63/200\n",
            "1108/1108 [==============================] - 1s 987us/step - loss: 0.7308 - acc: 0.7067 - val_loss: 0.9869 - val_acc: 0.6511\n",
            "Epoch 64/200\n",
            "1108/1108 [==============================] - 1s 993us/step - loss: 0.7573 - acc: 0.6841 - val_loss: 0.9871 - val_acc: 0.6547\n",
            "Epoch 65/200\n",
            "1108/1108 [==============================] - 1s 988us/step - loss: 0.7549 - acc: 0.6868 - val_loss: 1.0752 - val_acc: 0.6475\n",
            "Epoch 66/200\n",
            "1108/1108 [==============================] - 1s 988us/step - loss: 0.7066 - acc: 0.7157 - val_loss: 1.1373 - val_acc: 0.6403\n",
            "Epoch 67/200\n",
            "1108/1108 [==============================] - 1s 996us/step - loss: 0.7130 - acc: 0.7103 - val_loss: 1.1318 - val_acc: 0.6403\n",
            "Epoch 68/200\n",
            "1108/1108 [==============================] - 1s 994us/step - loss: 0.6897 - acc: 0.7301 - val_loss: 1.0772 - val_acc: 0.6295\n",
            "Epoch 69/200\n",
            "1108/1108 [==============================] - 1s 998us/step - loss: 0.6868 - acc: 0.7265 - val_loss: 1.0414 - val_acc: 0.6403\n",
            "Epoch 70/200\n",
            "1108/1108 [==============================] - 1s 986us/step - loss: 0.6603 - acc: 0.7410 - val_loss: 1.0255 - val_acc: 0.6439\n",
            "Epoch 71/200\n",
            "1108/1108 [==============================] - 1s 990us/step - loss: 0.6998 - acc: 0.7157 - val_loss: 1.0503 - val_acc: 0.6403\n",
            "Epoch 72/200\n",
            "1108/1108 [==============================] - 1s 991us/step - loss: 0.7379 - acc: 0.7211 - val_loss: 1.0948 - val_acc: 0.6475\n",
            "Epoch 73/200\n",
            "1108/1108 [==============================] - 1s 998us/step - loss: 0.6771 - acc: 0.7455 - val_loss: 1.0587 - val_acc: 0.6583\n",
            "Epoch 74/200\n",
            "1108/1108 [==============================] - 1s 995us/step - loss: 0.6850 - acc: 0.7319 - val_loss: 0.9886 - val_acc: 0.6619\n",
            "Epoch 75/200\n",
            "1108/1108 [==============================] - 1s 994us/step - loss: 0.6880 - acc: 0.7392 - val_loss: 0.9677 - val_acc: 0.6583\n",
            "Epoch 76/200\n",
            "1108/1108 [==============================] - 1s 988us/step - loss: 0.6402 - acc: 0.7536 - val_loss: 0.9629 - val_acc: 0.6547\n",
            "Epoch 77/200\n",
            "1108/1108 [==============================] - 1s 992us/step - loss: 0.6488 - acc: 0.7671 - val_loss: 0.9968 - val_acc: 0.6511\n",
            "Epoch 78/200\n",
            "1108/1108 [==============================] - 1s 983us/step - loss: 0.7239 - acc: 0.7193 - val_loss: 1.0201 - val_acc: 0.6547\n",
            "Epoch 79/200\n",
            "1108/1108 [==============================] - 1s 989us/step - loss: 0.6607 - acc: 0.7410 - val_loss: 1.0363 - val_acc: 0.6655\n",
            "Epoch 80/200\n",
            "1108/1108 [==============================] - 1s 982us/step - loss: 0.7013 - acc: 0.7338 - val_loss: 1.0142 - val_acc: 0.6547\n",
            "Epoch 81/200\n",
            "1108/1108 [==============================] - 1s 991us/step - loss: 0.6680 - acc: 0.7473 - val_loss: 1.0005 - val_acc: 0.6547\n",
            "Epoch 82/200\n",
            "1108/1108 [==============================] - 1s 983us/step - loss: 0.6362 - acc: 0.7374 - val_loss: 1.0370 - val_acc: 0.6331\n",
            "Epoch 83/200\n",
            "1108/1108 [==============================] - 1s 989us/step - loss: 0.6191 - acc: 0.7708 - val_loss: 1.0457 - val_acc: 0.6331\n",
            "Epoch 84/200\n",
            "1108/1108 [==============================] - 1s 995us/step - loss: 0.5950 - acc: 0.7717 - val_loss: 1.0496 - val_acc: 0.6439\n",
            "Epoch 85/200\n",
            "1108/1108 [==============================] - 1s 994us/step - loss: 0.6343 - acc: 0.7473 - val_loss: 1.0060 - val_acc: 0.6511\n",
            "Epoch 86/200\n",
            "1108/1108 [==============================] - 1s 981us/step - loss: 0.6320 - acc: 0.7545 - val_loss: 0.9697 - val_acc: 0.6583\n",
            "Epoch 87/200\n",
            "1108/1108 [==============================] - 1s 1ms/step - loss: 0.5890 - acc: 0.7816 - val_loss: 0.9819 - val_acc: 0.6475\n",
            "Epoch 88/200\n",
            "1108/1108 [==============================] - 1s 990us/step - loss: 0.5609 - acc: 0.7870 - val_loss: 1.0286 - val_acc: 0.6439\n",
            "Epoch 89/200\n",
            "1108/1108 [==============================] - 1s 1000us/step - loss: 0.6043 - acc: 0.7717 - val_loss: 1.0520 - val_acc: 0.6367\n",
            "Epoch 90/200\n",
            "1108/1108 [==============================] - 1s 992us/step - loss: 0.6232 - acc: 0.7500 - val_loss: 1.0585 - val_acc: 0.6331\n",
            "Epoch 91/200\n",
            "1108/1108 [==============================] - 1s 985us/step - loss: 0.6050 - acc: 0.7726 - val_loss: 1.0783 - val_acc: 0.6295\n",
            "Epoch 92/200\n",
            "1108/1108 [==============================] - 1s 995us/step - loss: 0.5831 - acc: 0.7527 - val_loss: 1.0813 - val_acc: 0.6511\n",
            "Epoch 93/200\n",
            "1108/1108 [==============================] - 1s 994us/step - loss: 0.5655 - acc: 0.7635 - val_loss: 1.0702 - val_acc: 0.6403\n",
            "Epoch 94/200\n",
            "1108/1108 [==============================] - 1s 997us/step - loss: 0.5831 - acc: 0.7816 - val_loss: 1.0217 - val_acc: 0.6439\n",
            "Epoch 95/200\n",
            "1108/1108 [==============================] - 1s 989us/step - loss: 0.5852 - acc: 0.7635 - val_loss: 1.0163 - val_acc: 0.6691\n",
            "Epoch 96/200\n",
            "1108/1108 [==============================] - 1s 989us/step - loss: 0.5908 - acc: 0.7771 - val_loss: 0.9995 - val_acc: 0.6619\n",
            "Epoch 97/200\n",
            "1108/1108 [==============================] - 1s 992us/step - loss: 0.5612 - acc: 0.7699 - val_loss: 1.0070 - val_acc: 0.6835\n",
            "Epoch 98/200\n",
            "1108/1108 [==============================] - 1s 994us/step - loss: 0.5932 - acc: 0.7608 - val_loss: 0.9491 - val_acc: 0.6978\n",
            "Epoch 99/200\n",
            "1108/1108 [==============================] - 1s 981us/step - loss: 0.5679 - acc: 0.7942 - val_loss: 0.8765 - val_acc: 0.6942\n",
            "Epoch 100/200\n",
            "1108/1108 [==============================] - 1s 987us/step - loss: 0.5415 - acc: 0.7924 - val_loss: 0.8447 - val_acc: 0.6835\n",
            "Epoch 101/200\n",
            "1108/1108 [==============================] - 1s 989us/step - loss: 0.5483 - acc: 0.7834 - val_loss: 0.8525 - val_acc: 0.6727\n",
            "Epoch 102/200\n",
            "1108/1108 [==============================] - 1s 984us/step - loss: 0.5562 - acc: 0.7852 - val_loss: 0.8646 - val_acc: 0.6978\n",
            "Epoch 103/200\n",
            "1108/1108 [==============================] - 1s 990us/step - loss: 0.5351 - acc: 0.7897 - val_loss: 0.9377 - val_acc: 0.6763\n",
            "Epoch 104/200\n",
            "1108/1108 [==============================] - 1s 979us/step - loss: 0.5093 - acc: 0.8014 - val_loss: 0.9537 - val_acc: 0.6691\n",
            "Epoch 105/200\n",
            "1108/1108 [==============================] - 1s 1000us/step - loss: 0.5372 - acc: 0.7861 - val_loss: 0.9444 - val_acc: 0.6655\n",
            "Epoch 106/200\n",
            "1108/1108 [==============================] - 1s 992us/step - loss: 0.5097 - acc: 0.8123 - val_loss: 0.9479 - val_acc: 0.6727\n",
            "Epoch 107/200\n",
            "1108/1108 [==============================] - 1s 989us/step - loss: 0.5074 - acc: 0.8023 - val_loss: 0.9601 - val_acc: 0.6835\n",
            "Epoch 108/200\n",
            "1108/1108 [==============================] - 1s 1ms/step - loss: 0.5422 - acc: 0.7969 - val_loss: 1.0342 - val_acc: 0.6799\n",
            "Epoch 109/200\n",
            "1108/1108 [==============================] - 1s 981us/step - loss: 0.5344 - acc: 0.8060 - val_loss: 1.0335 - val_acc: 0.6799\n",
            "Epoch 110/200\n",
            "1108/1108 [==============================] - 1s 987us/step - loss: 0.4787 - acc: 0.8123 - val_loss: 0.9927 - val_acc: 0.6691\n",
            "Epoch 111/200\n",
            "1108/1108 [==============================] - 1s 996us/step - loss: 0.5361 - acc: 0.7879 - val_loss: 0.9889 - val_acc: 0.6763\n",
            "Epoch 112/200\n",
            "1108/1108 [==============================] - 1s 989us/step - loss: 0.5126 - acc: 0.8132 - val_loss: 0.9695 - val_acc: 0.6763\n",
            "Epoch 113/200\n",
            "1108/1108 [==============================] - 1s 992us/step - loss: 0.5309 - acc: 0.7843 - val_loss: 0.9760 - val_acc: 0.6655\n",
            "Epoch 114/200\n",
            "1108/1108 [==============================] - 1s 989us/step - loss: 0.4761 - acc: 0.8069 - val_loss: 0.9694 - val_acc: 0.6583\n",
            "Epoch 115/200\n",
            "1108/1108 [==============================] - 1s 993us/step - loss: 0.4791 - acc: 0.8132 - val_loss: 0.9159 - val_acc: 0.6727\n",
            "Epoch 116/200\n",
            "1108/1108 [==============================] - 1s 985us/step - loss: 0.5084 - acc: 0.7969 - val_loss: 0.9045 - val_acc: 0.6835\n",
            "Epoch 117/200\n",
            "1108/1108 [==============================] - 1s 990us/step - loss: 0.5138 - acc: 0.8087 - val_loss: 0.8880 - val_acc: 0.6942\n",
            "Epoch 118/200\n",
            "1108/1108 [==============================] - 1s 999us/step - loss: 0.5020 - acc: 0.8105 - val_loss: 0.9051 - val_acc: 0.6942\n",
            "Epoch 119/200\n",
            "1108/1108 [==============================] - 1s 996us/step - loss: 0.4409 - acc: 0.8357 - val_loss: 0.9346 - val_acc: 0.6871\n",
            "Epoch 120/200\n",
            "1108/1108 [==============================] - 1s 995us/step - loss: 0.5046 - acc: 0.8005 - val_loss: 0.9125 - val_acc: 0.6763\n",
            "Epoch 121/200\n",
            "1108/1108 [==============================] - 1s 988us/step - loss: 0.4885 - acc: 0.8195 - val_loss: 0.8695 - val_acc: 0.6978\n",
            "Epoch 122/200\n",
            "1108/1108 [==============================] - 1s 988us/step - loss: 0.4547 - acc: 0.8330 - val_loss: 0.8621 - val_acc: 0.6835\n",
            "Epoch 123/200\n",
            "1108/1108 [==============================] - 1s 996us/step - loss: 0.4833 - acc: 0.8087 - val_loss: 0.8645 - val_acc: 0.6978\n",
            "Epoch 124/200\n",
            "1108/1108 [==============================] - 1s 993us/step - loss: 0.4742 - acc: 0.8042 - val_loss: 0.8494 - val_acc: 0.7086\n",
            "Epoch 125/200\n",
            "1108/1108 [==============================] - 1s 997us/step - loss: 0.4705 - acc: 0.8087 - val_loss: 0.8893 - val_acc: 0.7014\n",
            "Epoch 126/200\n",
            "1108/1108 [==============================] - 1s 999us/step - loss: 0.4476 - acc: 0.8321 - val_loss: 0.9221 - val_acc: 0.6942\n",
            "Epoch 127/200\n",
            "1108/1108 [==============================] - 1s 987us/step - loss: 0.4270 - acc: 0.8348 - val_loss: 0.9360 - val_acc: 0.6942\n",
            "Epoch 128/200\n",
            "1108/1108 [==============================] - 1s 992us/step - loss: 0.4400 - acc: 0.8375 - val_loss: 0.9368 - val_acc: 0.6835\n",
            "Epoch 129/200\n",
            "1108/1108 [==============================] - 1s 982us/step - loss: 0.4332 - acc: 0.8421 - val_loss: 0.8870 - val_acc: 0.6978\n",
            "Epoch 130/200\n",
            "1108/1108 [==============================] - 1s 996us/step - loss: 0.4554 - acc: 0.8231 - val_loss: 0.8305 - val_acc: 0.7086\n",
            "Epoch 131/200\n",
            "1108/1108 [==============================] - 1s 988us/step - loss: 0.4350 - acc: 0.8339 - val_loss: 0.8336 - val_acc: 0.7086\n",
            "Epoch 132/200\n",
            "1108/1108 [==============================] - 1s 988us/step - loss: 0.4382 - acc: 0.8357 - val_loss: 0.8448 - val_acc: 0.7194\n",
            "Epoch 133/200\n",
            "1108/1108 [==============================] - 1s 988us/step - loss: 0.4246 - acc: 0.8303 - val_loss: 0.8131 - val_acc: 0.7302\n",
            "Epoch 134/200\n",
            "1108/1108 [==============================] - 1s 994us/step - loss: 0.4161 - acc: 0.8430 - val_loss: 0.7938 - val_acc: 0.7338\n",
            "Epoch 135/200\n",
            "1108/1108 [==============================] - 1s 986us/step - loss: 0.4104 - acc: 0.8448 - val_loss: 0.8061 - val_acc: 0.7338\n",
            "Epoch 136/200\n",
            "1108/1108 [==============================] - 1s 996us/step - loss: 0.3998 - acc: 0.8547 - val_loss: 0.8029 - val_acc: 0.7230\n",
            "Epoch 137/200\n",
            "1108/1108 [==============================] - 1s 991us/step - loss: 0.3868 - acc: 0.8682 - val_loss: 0.8024 - val_acc: 0.7086\n",
            "Epoch 138/200\n",
            "1108/1108 [==============================] - 1s 995us/step - loss: 0.4097 - acc: 0.8403 - val_loss: 0.8400 - val_acc: 0.7158\n",
            "Epoch 139/200\n",
            "1108/1108 [==============================] - 1s 994us/step - loss: 0.4182 - acc: 0.8366 - val_loss: 0.8319 - val_acc: 0.7230\n",
            "Epoch 140/200\n",
            "1108/1108 [==============================] - 1s 984us/step - loss: 0.4020 - acc: 0.8421 - val_loss: 0.8175 - val_acc: 0.7158\n",
            "Epoch 141/200\n",
            "1108/1108 [==============================] - 1s 994us/step - loss: 0.3791 - acc: 0.8592 - val_loss: 0.7891 - val_acc: 0.7266\n",
            "Epoch 142/200\n",
            "1108/1108 [==============================] - 1s 986us/step - loss: 0.4010 - acc: 0.8375 - val_loss: 0.7843 - val_acc: 0.7482\n",
            "Epoch 143/200\n",
            "1108/1108 [==============================] - 1s 992us/step - loss: 0.4283 - acc: 0.8412 - val_loss: 0.8084 - val_acc: 0.7338\n",
            "Epoch 144/200\n",
            "1108/1108 [==============================] - 1s 988us/step - loss: 0.4101 - acc: 0.8529 - val_loss: 0.8340 - val_acc: 0.7302\n",
            "Epoch 145/200\n",
            "1108/1108 [==============================] - 1s 974us/step - loss: 0.4150 - acc: 0.8448 - val_loss: 0.8622 - val_acc: 0.7338\n",
            "Epoch 146/200\n",
            "1108/1108 [==============================] - 1s 997us/step - loss: 0.4064 - acc: 0.8448 - val_loss: 0.8575 - val_acc: 0.7266\n",
            "Epoch 147/200\n",
            "1108/1108 [==============================] - 1s 985us/step - loss: 0.3868 - acc: 0.8529 - val_loss: 0.8386 - val_acc: 0.7266\n",
            "Epoch 148/200\n",
            "1108/1108 [==============================] - 1s 980us/step - loss: 0.3789 - acc: 0.8619 - val_loss: 0.8013 - val_acc: 0.7266\n",
            "Epoch 149/200\n",
            "1108/1108 [==============================] - 1s 998us/step - loss: 0.4186 - acc: 0.8348 - val_loss: 0.7995 - val_acc: 0.7302\n",
            "Epoch 150/200\n",
            "1108/1108 [==============================] - 1s 996us/step - loss: 0.4053 - acc: 0.8457 - val_loss: 0.8359 - val_acc: 0.7158\n",
            "Epoch 151/200\n",
            "1108/1108 [==============================] - 1s 989us/step - loss: 0.4103 - acc: 0.8448 - val_loss: 0.9121 - val_acc: 0.7122\n",
            "Epoch 152/200\n",
            "1108/1108 [==============================] - 1s 998us/step - loss: 0.3899 - acc: 0.8457 - val_loss: 0.9410 - val_acc: 0.7050\n",
            "Epoch 153/200\n",
            "1108/1108 [==============================] - 1s 973us/step - loss: 0.4145 - acc: 0.8339 - val_loss: 0.8942 - val_acc: 0.7086\n",
            "Epoch 154/200\n",
            "1108/1108 [==============================] - 1s 983us/step - loss: 0.3526 - acc: 0.8718 - val_loss: 0.8625 - val_acc: 0.7410\n",
            "Epoch 155/200\n",
            "1108/1108 [==============================] - 1s 992us/step - loss: 0.3578 - acc: 0.8601 - val_loss: 0.8600 - val_acc: 0.7374\n",
            "Epoch 156/200\n",
            "1108/1108 [==============================] - 1s 993us/step - loss: 0.3933 - acc: 0.8619 - val_loss: 0.8540 - val_acc: 0.7302\n",
            "Epoch 157/200\n",
            "1108/1108 [==============================] - 1s 984us/step - loss: 0.3707 - acc: 0.8547 - val_loss: 0.8625 - val_acc: 0.7302\n",
            "Epoch 158/200\n",
            "1108/1108 [==============================] - 1s 996us/step - loss: 0.3893 - acc: 0.8502 - val_loss: 0.8394 - val_acc: 0.7266\n",
            "Epoch 159/200\n",
            "1108/1108 [==============================] - 1s 996us/step - loss: 0.4002 - acc: 0.8484 - val_loss: 0.7990 - val_acc: 0.7266\n",
            "Epoch 160/200\n",
            "1108/1108 [==============================] - 1s 993us/step - loss: 0.3564 - acc: 0.8637 - val_loss: 0.8303 - val_acc: 0.6978\n",
            "Epoch 161/200\n",
            "1108/1108 [==============================] - 1s 984us/step - loss: 0.3737 - acc: 0.8448 - val_loss: 0.8722 - val_acc: 0.6942\n",
            "Epoch 162/200\n",
            "1108/1108 [==============================] - 1s 979us/step - loss: 0.4067 - acc: 0.8448 - val_loss: 0.8823 - val_acc: 0.6978\n",
            "Epoch 163/200\n",
            "1108/1108 [==============================] - 1s 996us/step - loss: 0.3984 - acc: 0.8484 - val_loss: 0.8487 - val_acc: 0.7194\n",
            "Epoch 164/200\n",
            "1108/1108 [==============================] - 1s 990us/step - loss: 0.3744 - acc: 0.8619 - val_loss: 0.9046 - val_acc: 0.7014\n",
            "Epoch 165/200\n",
            "1108/1108 [==============================] - 1s 987us/step - loss: 0.3703 - acc: 0.8664 - val_loss: 0.9720 - val_acc: 0.6906\n",
            "Epoch 166/200\n",
            "1108/1108 [==============================] - 1s 995us/step - loss: 0.3918 - acc: 0.8466 - val_loss: 0.9263 - val_acc: 0.6942\n",
            "Epoch 167/200\n",
            "1108/1108 [==============================] - 1s 989us/step - loss: 0.3587 - acc: 0.8610 - val_loss: 0.8607 - val_acc: 0.7014\n",
            "Epoch 168/200\n",
            "1108/1108 [==============================] - 1s 984us/step - loss: 0.3573 - acc: 0.8601 - val_loss: 0.7751 - val_acc: 0.7158\n",
            "Epoch 169/200\n",
            "1108/1108 [==============================] - 1s 982us/step - loss: 0.3952 - acc: 0.8511 - val_loss: 0.7671 - val_acc: 0.7158\n",
            "Epoch 00169: early stopping\n",
            "Done with one subject\n",
            "300/300 [==============================] - 0s 356us/step\n",
            "300/300 [==============================] - 0s 337us/step\n",
            "300/300 [==============================] - 0s 344us/step\n",
            "300/300 [==============================] - 0s 339us/step\n",
            "282/282 [==============================] - 0s 342us/step\n",
            "294/294 [==============================] - 0s 338us/step\n",
            "300/300 [==============================] - 0s 337us/step\n",
            "300/300 [==============================] - 0s 335us/step\n",
            "282/282 [==============================] - 0s 330us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THSi-w7zP3X4",
        "colab_type": "code",
        "outputId": "ff50ab02-6d06-4d1a-84ff-c76deba54aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "cross_subject_accu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.59      , 0.34333333, 0.48666667, 0.34333333, 0.20212766,\n",
              "        0.25170068, 0.38333333, 0.36      , 0.32624113],\n",
              "       [0.30333333, 0.21      , 0.23666667, 0.25      , 0.30496454,\n",
              "        0.17346939, 0.23666667, 0.26666667, 0.24113475],\n",
              "       [0.38666667, 0.27666667, 0.46666667, 0.34      , 0.26950355,\n",
              "        0.28231293, 0.35333333, 0.30666667, 0.30141844],\n",
              "       [0.27333333, 0.31666667, 0.2       , 0.31666667, 0.33687943,\n",
              "        0.30272109, 0.26666667, 0.19333333, 0.20567376],\n",
              "       [0.22      , 0.26666667, 0.33666667, 0.31333333, 0.35460993,\n",
              "        0.35034014, 0.31666667, 0.23      , 0.35460993],\n",
              "       [0.23      , 0.29333333, 0.24333333, 0.22333333, 0.25177305,\n",
              "        0.22789116, 0.33      , 0.35      , 0.32269504],\n",
              "       [0.27666667, 0.27333333, 0.31      , 0.34333333, 0.39361702,\n",
              "        0.28231293, 0.44333333, 0.20333333, 0.26595745],\n",
              "       [0.29666667, 0.24333333, 0.36333333, 0.16333333, 0.33687943,\n",
              "        0.24829932, 0.22      , 0.43333333, 0.30851064],\n",
              "       [0.34666667, 0.24666667, 0.38      , 0.17666667, 0.36524823,\n",
              "        0.36394558, 0.32666667, 0.33333333, 0.73758865]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZZjEYCF4QVh",
        "colab_type": "code",
        "outputId": "560bf8d5-ea4e-4818-e853-5f48bfd1d3ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "plt.imshow(cross_subject_accu)\n",
        "plt.grid(False)\n",
        "subject_id = ['subject1','subject2','subject3','subject4','subject5',\n",
        "              'subject6','subject7','subject8','subject9']\n",
        "plt.xticks(range(9), subject_id, rotation='45')\n",
        "plt.yticks(range(9), subject_id, rotation='0')\n",
        "plt.colorbar()\n",
        "\n",
        "# plt.xlabel(subject_id)\n",
        "# plt.ylabel(subject_id)\n",
        "# plt.yticks(rotation='vertical')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7ff28990d128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAFhCAYAAAA2gn5IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8f8BrCmpr1KV7BgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dva0RotBgR9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24RJVitm4eA2",
        "colab_type": "text"
      },
      "source": [
        "### Train shallow on single **subject**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x21KiN3Jghlt",
        "colab_type": "code",
        "outputId": "5a18820e-bd03-452c-958c-60221d4ec522",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11717
        }
      },
      "source": [
        "num_subject = np.unique(person_train_valid).shape[0]\n",
        "cross_subject_accu = np.zeros((num_subject,num_subject))\n",
        "for num_s in  np.arange(num_subject):\n",
        "  idx_s_train = np.where(person_train_idx_argment == num_s)[0]\n",
        "  model_s = shallowcnn(0.6)\n",
        "\n",
        "  early_stop = EarlyStopping(monitor='loss', patience=15, verbose=1)\n",
        "  num_train = X_train_argment[idx_s_train].shape[0]\n",
        "\n",
        "  history_s = model_s.fit(X_train_argment[idx_s_train].reshape(num_train,22,window,1),\n",
        "                          Y_train_argment[idx_s_train], \n",
        "                          batch_size=256, epochs = 200, validation_split = 0.2,\n",
        "                          callbacks = [early_stop], verbose = 1)\n",
        "  print(\"Done with one subject\")\n",
        "#   test the accuracy on other subjects\n",
        "  for num_s_j in np.arange(num_subject):\n",
        "    idx_s_test = np.where(person_test_idx_argment == num_s_j)[0]\n",
        "    cross_subject_accu[num_s, num_s_j] = model_s.evaluate(X_test_argment[idx_s_test].reshape(-1,22,window,1), \n",
        "                                                          Y_test_argment[idx_s_test])[1]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1137 samples, validate on 285 samples\n",
            "Epoch 1/200\n",
            "1137/1137 [==============================] - 3s 3ms/step - loss: 1.6025 - acc: 0.2586 - val_loss: 1.4088 - val_acc: 0.2737\n",
            "Epoch 2/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.4642 - acc: 0.2797 - val_loss: 1.3991 - val_acc: 0.2632\n",
            "Epoch 3/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.4149 - acc: 0.2823 - val_loss: 1.3950 - val_acc: 0.2667\n",
            "Epoch 4/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.4028 - acc: 0.2885 - val_loss: 1.3828 - val_acc: 0.3123\n",
            "Epoch 5/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.3725 - acc: 0.3034 - val_loss: 1.3760 - val_acc: 0.3018\n",
            "Epoch 6/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.3696 - acc: 0.3096 - val_loss: 1.3753 - val_acc: 0.2807\n",
            "Epoch 7/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.3702 - acc: 0.3210 - val_loss: 1.3730 - val_acc: 0.2877\n",
            "Epoch 8/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.3594 - acc: 0.3245 - val_loss: 1.3704 - val_acc: 0.2877\n",
            "Epoch 9/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.3509 - acc: 0.3289 - val_loss: 1.3724 - val_acc: 0.2877\n",
            "Epoch 10/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.3362 - acc: 0.3720 - val_loss: 1.3715 - val_acc: 0.3088\n",
            "Epoch 11/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.3383 - acc: 0.3430 - val_loss: 1.3707 - val_acc: 0.3123\n",
            "Epoch 12/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.3339 - acc: 0.3676 - val_loss: 1.3699 - val_acc: 0.3333\n",
            "Epoch 13/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.3200 - acc: 0.3879 - val_loss: 1.3697 - val_acc: 0.3544\n",
            "Epoch 14/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.2976 - acc: 0.3817 - val_loss: 1.3693 - val_acc: 0.3404\n",
            "Epoch 15/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.2986 - acc: 0.3764 - val_loss: 1.3683 - val_acc: 0.3018\n",
            "Epoch 16/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.2772 - acc: 0.4222 - val_loss: 1.3623 - val_acc: 0.3123\n",
            "Epoch 17/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.2737 - acc: 0.4107 - val_loss: 1.3659 - val_acc: 0.3228\n",
            "Epoch 18/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.2321 - acc: 0.4380 - val_loss: 1.3616 - val_acc: 0.3228\n",
            "Epoch 19/200\n",
            "1137/1137 [==============================] - 2s 1ms/step - loss: 1.2424 - acc: 0.4345 - val_loss: 1.3580 - val_acc: 0.3509\n",
            "Epoch 20/200\n",
            "1137/1137 [==============================] - 2s 1ms/step - loss: 1.2523 - acc: 0.4239 - val_loss: 1.3626 - val_acc: 0.3544\n",
            "Epoch 21/200\n",
            "1137/1137 [==============================] - 2s 1ms/step - loss: 1.2077 - acc: 0.4661 - val_loss: 1.3730 - val_acc: 0.3298\n",
            "Epoch 22/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.1896 - acc: 0.4908 - val_loss: 1.3771 - val_acc: 0.3439\n",
            "Epoch 23/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.1885 - acc: 0.4872 - val_loss: 1.3654 - val_acc: 0.3649\n",
            "Epoch 24/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.1951 - acc: 0.4679 - val_loss: 1.3611 - val_acc: 0.3474\n",
            "Epoch 25/200\n",
            "1137/1137 [==============================] - 2s 1ms/step - loss: 1.1786 - acc: 0.4811 - val_loss: 1.3689 - val_acc: 0.3649\n",
            "Epoch 26/200\n",
            "1137/1137 [==============================] - 2s 1ms/step - loss: 1.1606 - acc: 0.5084 - val_loss: 1.3746 - val_acc: 0.3649\n",
            "Epoch 27/200\n",
            "1137/1137 [==============================] - 2s 1ms/step - loss: 1.1709 - acc: 0.4943 - val_loss: 1.3857 - val_acc: 0.3333\n",
            "Epoch 28/200\n",
            "1137/1137 [==============================] - 2s 1ms/step - loss: 1.1649 - acc: 0.4864 - val_loss: 1.3954 - val_acc: 0.3298\n",
            "Epoch 29/200\n",
            "1137/1137 [==============================] - 2s 1ms/step - loss: 1.1349 - acc: 0.5057 - val_loss: 1.3829 - val_acc: 0.3439\n",
            "Epoch 30/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.1364 - acc: 0.5172 - val_loss: 1.3934 - val_acc: 0.3263\n",
            "Epoch 31/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.1245 - acc: 0.5303 - val_loss: 1.3979 - val_acc: 0.3544\n",
            "Epoch 32/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.1084 - acc: 0.5295 - val_loss: 1.4171 - val_acc: 0.3649\n",
            "Epoch 33/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.1217 - acc: 0.5101 - val_loss: 1.4106 - val_acc: 0.3649\n",
            "Epoch 34/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.1117 - acc: 0.5031 - val_loss: 1.4205 - val_acc: 0.3509\n",
            "Epoch 35/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.1083 - acc: 0.5048 - val_loss: 1.4131 - val_acc: 0.3404\n",
            "Epoch 36/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.1180 - acc: 0.5048 - val_loss: 1.4202 - val_acc: 0.3509\n",
            "Epoch 37/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0829 - acc: 0.5295 - val_loss: 1.4084 - val_acc: 0.3544\n",
            "Epoch 38/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.1147 - acc: 0.5224 - val_loss: 1.4193 - val_acc: 0.3509\n",
            "Epoch 39/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0615 - acc: 0.5558 - val_loss: 1.4350 - val_acc: 0.3509\n",
            "Epoch 40/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0616 - acc: 0.5655 - val_loss: 1.4284 - val_acc: 0.3509\n",
            "Epoch 41/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0897 - acc: 0.5312 - val_loss: 1.4360 - val_acc: 0.3474\n",
            "Epoch 42/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0778 - acc: 0.5268 - val_loss: 1.4331 - val_acc: 0.3368\n",
            "Epoch 43/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0589 - acc: 0.5541 - val_loss: 1.4263 - val_acc: 0.3404\n",
            "Epoch 44/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0488 - acc: 0.5629 - val_loss: 1.4283 - val_acc: 0.3368\n",
            "Epoch 45/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0383 - acc: 0.5866 - val_loss: 1.4411 - val_acc: 0.3614\n",
            "Epoch 46/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0412 - acc: 0.5532 - val_loss: 1.4374 - val_acc: 0.3649\n",
            "Epoch 47/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0349 - acc: 0.5690 - val_loss: 1.4302 - val_acc: 0.3684\n",
            "Epoch 48/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0476 - acc: 0.5541 - val_loss: 1.4560 - val_acc: 0.3684\n",
            "Epoch 49/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0389 - acc: 0.5761 - val_loss: 1.4339 - val_acc: 0.3789\n",
            "Epoch 50/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0533 - acc: 0.5629 - val_loss: 1.4665 - val_acc: 0.3404\n",
            "Epoch 51/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0167 - acc: 0.5761 - val_loss: 1.4279 - val_acc: 0.3614\n",
            "Epoch 52/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0438 - acc: 0.5708 - val_loss: 1.4431 - val_acc: 0.3614\n",
            "Epoch 53/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0164 - acc: 0.5796 - val_loss: 1.4280 - val_acc: 0.3684\n",
            "Epoch 54/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9761 - acc: 0.5893 - val_loss: 1.4592 - val_acc: 0.3754\n",
            "Epoch 55/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0018 - acc: 0.6007 - val_loss: 1.4793 - val_acc: 0.3474\n",
            "Epoch 56/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0203 - acc: 0.5638 - val_loss: 1.4400 - val_acc: 0.3684\n",
            "Epoch 57/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9938 - acc: 0.6051 - val_loss: 1.4844 - val_acc: 0.3368\n",
            "Epoch 58/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9763 - acc: 0.5963 - val_loss: 1.4985 - val_acc: 0.3614\n",
            "Epoch 59/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 1.0229 - acc: 0.5787 - val_loss: 1.4718 - val_acc: 0.3614\n",
            "Epoch 60/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9881 - acc: 0.6077 - val_loss: 1.4582 - val_acc: 0.3614\n",
            "Epoch 61/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9629 - acc: 0.6060 - val_loss: 1.4577 - val_acc: 0.3684\n",
            "Epoch 62/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9547 - acc: 0.5866 - val_loss: 1.4592 - val_acc: 0.3684\n",
            "Epoch 63/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9812 - acc: 0.5770 - val_loss: 1.4546 - val_acc: 0.3789\n",
            "Epoch 64/200\n",
            "1137/1137 [==============================] - 2s 1ms/step - loss: 0.9923 - acc: 0.5726 - val_loss: 1.4396 - val_acc: 0.3719\n",
            "Epoch 65/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9903 - acc: 0.5787 - val_loss: 1.4169 - val_acc: 0.3614\n",
            "Epoch 66/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9460 - acc: 0.6130 - val_loss: 1.4399 - val_acc: 0.3474\n",
            "Epoch 67/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9450 - acc: 0.6095 - val_loss: 1.4487 - val_acc: 0.3544\n",
            "Epoch 68/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9892 - acc: 0.5963 - val_loss: 1.4566 - val_acc: 0.3474\n",
            "Epoch 69/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9465 - acc: 0.6130 - val_loss: 1.4464 - val_acc: 0.3719\n",
            "Epoch 70/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9883 - acc: 0.5910 - val_loss: 1.4467 - val_acc: 0.3649\n",
            "Epoch 71/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9621 - acc: 0.5849 - val_loss: 1.4653 - val_acc: 0.3474\n",
            "Epoch 72/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9574 - acc: 0.6113 - val_loss: 1.4699 - val_acc: 0.3544\n",
            "Epoch 73/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9347 - acc: 0.6280 - val_loss: 1.4475 - val_acc: 0.3614\n",
            "Epoch 74/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9460 - acc: 0.6148 - val_loss: 1.4666 - val_acc: 0.3544\n",
            "Epoch 75/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9311 - acc: 0.6288 - val_loss: 1.4677 - val_acc: 0.3684\n",
            "Epoch 76/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9605 - acc: 0.6033 - val_loss: 1.4518 - val_acc: 0.3860\n",
            "Epoch 77/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9233 - acc: 0.6306 - val_loss: 1.5134 - val_acc: 0.3579\n",
            "Epoch 78/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9283 - acc: 0.6227 - val_loss: 1.4931 - val_acc: 0.3544\n",
            "Epoch 79/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9404 - acc: 0.5937 - val_loss: 1.4891 - val_acc: 0.3684\n",
            "Epoch 80/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9496 - acc: 0.5945 - val_loss: 1.5075 - val_acc: 0.3579\n",
            "Epoch 81/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9173 - acc: 0.6077 - val_loss: 1.4729 - val_acc: 0.3649\n",
            "Epoch 82/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9515 - acc: 0.6060 - val_loss: 1.4848 - val_acc: 0.3649\n",
            "Epoch 83/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9025 - acc: 0.6403 - val_loss: 1.4898 - val_acc: 0.3649\n",
            "Epoch 84/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9297 - acc: 0.6042 - val_loss: 1.4832 - val_acc: 0.3474\n",
            "Epoch 85/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9234 - acc: 0.6271 - val_loss: 1.4807 - val_acc: 0.3509\n",
            "Epoch 86/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9568 - acc: 0.6148 - val_loss: 1.4651 - val_acc: 0.3789\n",
            "Epoch 87/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9176 - acc: 0.6464 - val_loss: 1.4901 - val_acc: 0.3509\n",
            "Epoch 88/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9225 - acc: 0.6271 - val_loss: 1.4274 - val_acc: 0.3860\n",
            "Epoch 89/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8941 - acc: 0.6341 - val_loss: 1.4656 - val_acc: 0.3684\n",
            "Epoch 90/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8999 - acc: 0.6315 - val_loss: 1.4590 - val_acc: 0.3860\n",
            "Epoch 91/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8906 - acc: 0.6482 - val_loss: 1.4901 - val_acc: 0.3895\n",
            "Epoch 92/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8844 - acc: 0.6464 - val_loss: 1.5318 - val_acc: 0.3544\n",
            "Epoch 93/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8873 - acc: 0.6350 - val_loss: 1.5003 - val_acc: 0.3754\n",
            "Epoch 94/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8920 - acc: 0.6262 - val_loss: 1.5014 - val_acc: 0.3825\n",
            "Epoch 95/200\n",
            "1137/1137 [==============================] - 2s 1ms/step - loss: 0.8938 - acc: 0.6332 - val_loss: 1.5052 - val_acc: 0.3579\n",
            "Epoch 96/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8933 - acc: 0.6280 - val_loss: 1.5000 - val_acc: 0.3614\n",
            "Epoch 97/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8592 - acc: 0.6482 - val_loss: 1.4970 - val_acc: 0.3684\n",
            "Epoch 98/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8831 - acc: 0.6464 - val_loss: 1.5221 - val_acc: 0.3719\n",
            "Epoch 99/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8783 - acc: 0.6403 - val_loss: 1.5236 - val_acc: 0.3789\n",
            "Epoch 100/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9091 - acc: 0.6253 - val_loss: 1.4918 - val_acc: 0.4000\n",
            "Epoch 101/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9205 - acc: 0.6201 - val_loss: 1.4808 - val_acc: 0.3754\n",
            "Epoch 102/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8971 - acc: 0.6271 - val_loss: 1.5130 - val_acc: 0.3684\n",
            "Epoch 103/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8733 - acc: 0.6491 - val_loss: 1.4730 - val_acc: 0.3789\n",
            "Epoch 104/200\n",
            "1137/1137 [==============================] - 2s 1ms/step - loss: 0.8729 - acc: 0.6368 - val_loss: 1.4722 - val_acc: 0.3789\n",
            "Epoch 105/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8666 - acc: 0.6315 - val_loss: 1.5105 - val_acc: 0.3719\n",
            "Epoch 106/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8673 - acc: 0.6456 - val_loss: 1.4937 - val_acc: 0.4140\n",
            "Epoch 107/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8802 - acc: 0.6376 - val_loss: 1.4999 - val_acc: 0.3930\n",
            "Epoch 108/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.9067 - acc: 0.6332 - val_loss: 1.4916 - val_acc: 0.3860\n",
            "Epoch 109/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8853 - acc: 0.6271 - val_loss: 1.5181 - val_acc: 0.3789\n",
            "Epoch 110/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8551 - acc: 0.6447 - val_loss: 1.4977 - val_acc: 0.3754\n",
            "Epoch 111/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8813 - acc: 0.6438 - val_loss: 1.5104 - val_acc: 0.3895\n",
            "Epoch 112/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8820 - acc: 0.6218 - val_loss: 1.4811 - val_acc: 0.3930\n",
            "Epoch 113/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8466 - acc: 0.6508 - val_loss: 1.5532 - val_acc: 0.3544\n",
            "Epoch 114/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8792 - acc: 0.6148 - val_loss: 1.4822 - val_acc: 0.3719\n",
            "Epoch 115/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8533 - acc: 0.6579 - val_loss: 1.5003 - val_acc: 0.3930\n",
            "Epoch 116/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8534 - acc: 0.6658 - val_loss: 1.5045 - val_acc: 0.3930\n",
            "Epoch 117/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8570 - acc: 0.6482 - val_loss: 1.5192 - val_acc: 0.3719\n",
            "Epoch 118/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8434 - acc: 0.6596 - val_loss: 1.4971 - val_acc: 0.3825\n",
            "Epoch 119/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8399 - acc: 0.6491 - val_loss: 1.5006 - val_acc: 0.3860\n",
            "Epoch 120/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8177 - acc: 0.6658 - val_loss: 1.5359 - val_acc: 0.3754\n",
            "Epoch 121/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8286 - acc: 0.6807 - val_loss: 1.5220 - val_acc: 0.3860\n",
            "Epoch 122/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8573 - acc: 0.6667 - val_loss: 1.5176 - val_acc: 0.3789\n",
            "Epoch 123/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8483 - acc: 0.6464 - val_loss: 1.5243 - val_acc: 0.3860\n",
            "Epoch 124/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8411 - acc: 0.6702 - val_loss: 1.5050 - val_acc: 0.3754\n",
            "Epoch 125/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8784 - acc: 0.6280 - val_loss: 1.5059 - val_acc: 0.3965\n",
            "Epoch 126/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8847 - acc: 0.6456 - val_loss: 1.5022 - val_acc: 0.4105\n",
            "Epoch 127/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8211 - acc: 0.6728 - val_loss: 1.4999 - val_acc: 0.3930\n",
            "Epoch 128/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8462 - acc: 0.6579 - val_loss: 1.5188 - val_acc: 0.3930\n",
            "Epoch 129/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8372 - acc: 0.6702 - val_loss: 1.4999 - val_acc: 0.4105\n",
            "Epoch 130/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8385 - acc: 0.6605 - val_loss: 1.5214 - val_acc: 0.3825\n",
            "Epoch 131/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8335 - acc: 0.6508 - val_loss: 1.5149 - val_acc: 0.3719\n",
            "Epoch 132/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8370 - acc: 0.6711 - val_loss: 1.5266 - val_acc: 0.4000\n",
            "Epoch 133/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8313 - acc: 0.6588 - val_loss: 1.4975 - val_acc: 0.3860\n",
            "Epoch 134/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8466 - acc: 0.6631 - val_loss: 1.5113 - val_acc: 0.4000\n",
            "Epoch 135/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7979 - acc: 0.6790 - val_loss: 1.5358 - val_acc: 0.3930\n",
            "Epoch 136/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8411 - acc: 0.6464 - val_loss: 1.4953 - val_acc: 0.4000\n",
            "Epoch 137/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8065 - acc: 0.6649 - val_loss: 1.5224 - val_acc: 0.3965\n",
            "Epoch 138/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8200 - acc: 0.6561 - val_loss: 1.5180 - val_acc: 0.3930\n",
            "Epoch 139/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8519 - acc: 0.6482 - val_loss: 1.4908 - val_acc: 0.3965\n",
            "Epoch 140/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8375 - acc: 0.6684 - val_loss: 1.4974 - val_acc: 0.4000\n",
            "Epoch 141/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7923 - acc: 0.6825 - val_loss: 1.4897 - val_acc: 0.4070\n",
            "Epoch 142/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8055 - acc: 0.6781 - val_loss: 1.5074 - val_acc: 0.4000\n",
            "Epoch 143/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8202 - acc: 0.6596 - val_loss: 1.5634 - val_acc: 0.3930\n",
            "Epoch 144/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8010 - acc: 0.6711 - val_loss: 1.5439 - val_acc: 0.4000\n",
            "Epoch 145/200\n",
            "1137/1137 [==============================] - 2s 1ms/step - loss: 0.8175 - acc: 0.6851 - val_loss: 1.5625 - val_acc: 0.4070\n",
            "Epoch 146/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8129 - acc: 0.6623 - val_loss: 1.5319 - val_acc: 0.4035\n",
            "Epoch 147/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7810 - acc: 0.6904 - val_loss: 1.5393 - val_acc: 0.4105\n",
            "Epoch 148/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8197 - acc: 0.6544 - val_loss: 1.5530 - val_acc: 0.3965\n",
            "Epoch 149/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7845 - acc: 0.6790 - val_loss: 1.5433 - val_acc: 0.3965\n",
            "Epoch 150/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8401 - acc: 0.6596 - val_loss: 1.5654 - val_acc: 0.4105\n",
            "Epoch 151/200\n",
            "1137/1137 [==============================] - 2s 1ms/step - loss: 0.8079 - acc: 0.6693 - val_loss: 1.5556 - val_acc: 0.3930\n",
            "Epoch 152/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8087 - acc: 0.6763 - val_loss: 1.5659 - val_acc: 0.3860\n",
            "Epoch 153/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7799 - acc: 0.6834 - val_loss: 1.5605 - val_acc: 0.3965\n",
            "Epoch 154/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8004 - acc: 0.6755 - val_loss: 1.5607 - val_acc: 0.3860\n",
            "Epoch 155/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8175 - acc: 0.6763 - val_loss: 1.5526 - val_acc: 0.3895\n",
            "Epoch 156/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7935 - acc: 0.6728 - val_loss: 1.4984 - val_acc: 0.3965\n",
            "Epoch 157/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8076 - acc: 0.6649 - val_loss: 1.5121 - val_acc: 0.3930\n",
            "Epoch 158/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7827 - acc: 0.6939 - val_loss: 1.5168 - val_acc: 0.3895\n",
            "Epoch 159/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7746 - acc: 0.6913 - val_loss: 1.5208 - val_acc: 0.3860\n",
            "Epoch 160/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7853 - acc: 0.6843 - val_loss: 1.5212 - val_acc: 0.3930\n",
            "Epoch 161/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8078 - acc: 0.6702 - val_loss: 1.5659 - val_acc: 0.3930\n",
            "Epoch 162/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7864 - acc: 0.6851 - val_loss: 1.5132 - val_acc: 0.4105\n",
            "Epoch 163/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8069 - acc: 0.6737 - val_loss: 1.5258 - val_acc: 0.3860\n",
            "Epoch 164/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7915 - acc: 0.6781 - val_loss: 1.5396 - val_acc: 0.3930\n",
            "Epoch 165/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7844 - acc: 0.7001 - val_loss: 1.5114 - val_acc: 0.3965\n",
            "Epoch 166/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7945 - acc: 0.6966 - val_loss: 1.5239 - val_acc: 0.4175\n",
            "Epoch 167/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7515 - acc: 0.7001 - val_loss: 1.5564 - val_acc: 0.4000\n",
            "Epoch 168/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7951 - acc: 0.6869 - val_loss: 1.5498 - val_acc: 0.4035\n",
            "Epoch 169/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.8034 - acc: 0.6807 - val_loss: 1.5164 - val_acc: 0.4070\n",
            "Epoch 170/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7836 - acc: 0.6887 - val_loss: 1.5214 - val_acc: 0.4035\n",
            "Epoch 171/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7594 - acc: 0.6851 - val_loss: 1.5837 - val_acc: 0.4000\n",
            "Epoch 172/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7967 - acc: 0.6816 - val_loss: 1.5585 - val_acc: 0.3930\n",
            "Epoch 173/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7672 - acc: 0.6851 - val_loss: 1.5567 - val_acc: 0.4140\n",
            "Epoch 174/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7393 - acc: 0.7062 - val_loss: 1.5471 - val_acc: 0.4105\n",
            "Epoch 175/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7822 - acc: 0.6922 - val_loss: 1.5485 - val_acc: 0.3860\n",
            "Epoch 176/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7697 - acc: 0.6869 - val_loss: 1.5407 - val_acc: 0.3965\n",
            "Epoch 177/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7620 - acc: 0.7054 - val_loss: 1.5617 - val_acc: 0.3895\n",
            "Epoch 178/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7447 - acc: 0.7054 - val_loss: 1.5548 - val_acc: 0.4000\n",
            "Epoch 179/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7926 - acc: 0.6799 - val_loss: 1.5590 - val_acc: 0.4070\n",
            "Epoch 180/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7807 - acc: 0.6860 - val_loss: 1.5665 - val_acc: 0.3895\n",
            "Epoch 181/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7791 - acc: 0.6728 - val_loss: 1.5517 - val_acc: 0.3860\n",
            "Epoch 182/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7522 - acc: 0.7115 - val_loss: 1.5625 - val_acc: 0.3825\n",
            "Epoch 183/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7688 - acc: 0.6869 - val_loss: 1.5647 - val_acc: 0.3825\n",
            "Epoch 184/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7499 - acc: 0.7010 - val_loss: 1.5704 - val_acc: 0.3754\n",
            "Epoch 185/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7588 - acc: 0.6904 - val_loss: 1.5846 - val_acc: 0.3965\n",
            "Epoch 186/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7701 - acc: 0.6939 - val_loss: 1.5580 - val_acc: 0.4035\n",
            "Epoch 187/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7649 - acc: 0.7062 - val_loss: 1.5810 - val_acc: 0.4070\n",
            "Epoch 188/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7466 - acc: 0.6843 - val_loss: 1.5868 - val_acc: 0.3965\n",
            "Epoch 189/200\n",
            "1137/1137 [==============================] - 2s 2ms/step - loss: 0.7955 - acc: 0.6711 - val_loss: 1.5395 - val_acc: 0.3930\n",
            "Epoch 00189: early stopping\n",
            "Done with one subject\n",
            "300/300 [==============================] - 0s 558us/step\n",
            "300/300 [==============================] - 0s 456us/step\n",
            "300/300 [==============================] - 0s 475us/step\n",
            "300/300 [==============================] - 0s 470us/step\n",
            "282/282 [==============================] - 0s 669us/step\n",
            "294/294 [==============================] - 0s 527us/step\n",
            "300/300 [==============================] - 0s 465us/step\n",
            "300/300 [==============================] - 0s 458us/step\n",
            "282/282 [==============================] - 0s 449us/step\n",
            "Train on 1132 samples, validate on 284 samples\n",
            "Epoch 1/200\n",
            "1132/1132 [==============================] - 3s 2ms/step - loss: 1.5120 - acc: 0.2624 - val_loss: 1.3835 - val_acc: 0.2958\n",
            "Epoch 2/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.3959 - acc: 0.3198 - val_loss: 1.3776 - val_acc: 0.2923\n",
            "Epoch 3/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.3686 - acc: 0.3198 - val_loss: 1.3752 - val_acc: 0.3134\n",
            "Epoch 4/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.3581 - acc: 0.3154 - val_loss: 1.3688 - val_acc: 0.3063\n",
            "Epoch 5/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.3459 - acc: 0.3277 - val_loss: 1.3704 - val_acc: 0.3204\n",
            "Epoch 6/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.3271 - acc: 0.3693 - val_loss: 1.3736 - val_acc: 0.3345\n",
            "Epoch 7/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.3048 - acc: 0.4019 - val_loss: 1.3713 - val_acc: 0.3239\n",
            "Epoch 8/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.3206 - acc: 0.3719 - val_loss: 1.3734 - val_acc: 0.2993\n",
            "Epoch 9/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.2981 - acc: 0.3869 - val_loss: 1.3769 - val_acc: 0.3099\n",
            "Epoch 10/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.2870 - acc: 0.3975 - val_loss: 1.3777 - val_acc: 0.3310\n",
            "Epoch 11/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.2762 - acc: 0.4019 - val_loss: 1.3811 - val_acc: 0.3275\n",
            "Epoch 12/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.2726 - acc: 0.4019 - val_loss: 1.3851 - val_acc: 0.3134\n",
            "Epoch 13/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.2679 - acc: 0.4249 - val_loss: 1.3999 - val_acc: 0.3169\n",
            "Epoch 14/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.2577 - acc: 0.4125 - val_loss: 1.4041 - val_acc: 0.3204\n",
            "Epoch 15/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.2312 - acc: 0.4382 - val_loss: 1.4030 - val_acc: 0.3063\n",
            "Epoch 16/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.2300 - acc: 0.4382 - val_loss: 1.4127 - val_acc: 0.3063\n",
            "Epoch 17/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.2579 - acc: 0.4037 - val_loss: 1.4276 - val_acc: 0.3134\n",
            "Epoch 18/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.2369 - acc: 0.4470 - val_loss: 1.4233 - val_acc: 0.3028\n",
            "Epoch 19/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.2160 - acc: 0.4620 - val_loss: 1.4117 - val_acc: 0.2887\n",
            "Epoch 20/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.2214 - acc: 0.4337 - val_loss: 1.4146 - val_acc: 0.2923\n",
            "Epoch 21/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.2054 - acc: 0.4567 - val_loss: 1.4293 - val_acc: 0.2993\n",
            "Epoch 22/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1959 - acc: 0.4708 - val_loss: 1.4366 - val_acc: 0.2817\n",
            "Epoch 23/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1852 - acc: 0.4638 - val_loss: 1.4200 - val_acc: 0.3028\n",
            "Epoch 24/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1843 - acc: 0.4655 - val_loss: 1.4360 - val_acc: 0.2958\n",
            "Epoch 25/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1896 - acc: 0.4523 - val_loss: 1.4435 - val_acc: 0.2923\n",
            "Epoch 26/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1647 - acc: 0.4965 - val_loss: 1.4453 - val_acc: 0.3028\n",
            "Epoch 27/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1595 - acc: 0.4965 - val_loss: 1.4386 - val_acc: 0.3028\n",
            "Epoch 28/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1631 - acc: 0.4876 - val_loss: 1.4542 - val_acc: 0.2923\n",
            "Epoch 29/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1503 - acc: 0.4920 - val_loss: 1.4614 - val_acc: 0.2993\n",
            "Epoch 30/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1518 - acc: 0.5044 - val_loss: 1.4436 - val_acc: 0.2958\n",
            "Epoch 31/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1444 - acc: 0.5150 - val_loss: 1.4441 - val_acc: 0.3204\n",
            "Epoch 32/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1336 - acc: 0.5106 - val_loss: 1.4610 - val_acc: 0.3028\n",
            "Epoch 33/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1524 - acc: 0.5009 - val_loss: 1.4798 - val_acc: 0.3063\n",
            "Epoch 34/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1348 - acc: 0.4956 - val_loss: 1.4428 - val_acc: 0.2852\n",
            "Epoch 35/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1459 - acc: 0.5000 - val_loss: 1.4325 - val_acc: 0.3345\n",
            "Epoch 36/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1153 - acc: 0.5230 - val_loss: 1.4724 - val_acc: 0.3063\n",
            "Epoch 37/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.1130 - acc: 0.5194 - val_loss: 1.4767 - val_acc: 0.3169\n",
            "Epoch 38/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1120 - acc: 0.4982 - val_loss: 1.4552 - val_acc: 0.2817\n",
            "Epoch 39/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1478 - acc: 0.4973 - val_loss: 1.4418 - val_acc: 0.2993\n",
            "Epoch 40/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1160 - acc: 0.5309 - val_loss: 1.4545 - val_acc: 0.3134\n",
            "Epoch 41/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1006 - acc: 0.5133 - val_loss: 1.4540 - val_acc: 0.3099\n",
            "Epoch 42/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0891 - acc: 0.5336 - val_loss: 1.4545 - val_acc: 0.3028\n",
            "Epoch 43/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1225 - acc: 0.5203 - val_loss: 1.4476 - val_acc: 0.3169\n",
            "Epoch 44/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1007 - acc: 0.5309 - val_loss: 1.4423 - val_acc: 0.3204\n",
            "Epoch 45/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.1034 - acc: 0.5186 - val_loss: 1.4602 - val_acc: 0.2958\n",
            "Epoch 46/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0716 - acc: 0.5548 - val_loss: 1.4778 - val_acc: 0.2993\n",
            "Epoch 47/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0909 - acc: 0.5345 - val_loss: 1.4603 - val_acc: 0.2923\n",
            "Epoch 48/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0936 - acc: 0.5133 - val_loss: 1.4587 - val_acc: 0.3099\n",
            "Epoch 49/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0578 - acc: 0.5539 - val_loss: 1.5002 - val_acc: 0.2993\n",
            "Epoch 50/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0845 - acc: 0.5424 - val_loss: 1.4839 - val_acc: 0.3204\n",
            "Epoch 51/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0733 - acc: 0.5274 - val_loss: 1.4598 - val_acc: 0.3380\n",
            "Epoch 52/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0783 - acc: 0.5292 - val_loss: 1.4533 - val_acc: 0.3345\n",
            "Epoch 53/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.0711 - acc: 0.5415 - val_loss: 1.4710 - val_acc: 0.3345\n",
            "Epoch 54/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0640 - acc: 0.5389 - val_loss: 1.4682 - val_acc: 0.3275\n",
            "Epoch 55/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0483 - acc: 0.5557 - val_loss: 1.4673 - val_acc: 0.3415\n",
            "Epoch 56/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 1.0463 - acc: 0.5733 - val_loss: 1.4712 - val_acc: 0.3451\n",
            "Epoch 57/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0479 - acc: 0.5548 - val_loss: 1.4635 - val_acc: 0.3134\n",
            "Epoch 58/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0493 - acc: 0.5486 - val_loss: 1.4706 - val_acc: 0.3099\n",
            "Epoch 59/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0374 - acc: 0.5627 - val_loss: 1.4742 - val_acc: 0.3451\n",
            "Epoch 60/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0505 - acc: 0.5442 - val_loss: 1.4660 - val_acc: 0.3310\n",
            "Epoch 61/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0170 - acc: 0.5645 - val_loss: 1.4661 - val_acc: 0.3239\n",
            "Epoch 62/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0378 - acc: 0.5512 - val_loss: 1.4422 - val_acc: 0.3239\n",
            "Epoch 63/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0305 - acc: 0.5786 - val_loss: 1.4554 - val_acc: 0.2887\n",
            "Epoch 64/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0213 - acc: 0.5724 - val_loss: 1.4818 - val_acc: 0.3063\n",
            "Epoch 65/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0303 - acc: 0.5574 - val_loss: 1.4819 - val_acc: 0.2887\n",
            "Epoch 66/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0085 - acc: 0.5698 - val_loss: 1.4943 - val_acc: 0.3134\n",
            "Epoch 67/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0093 - acc: 0.5645 - val_loss: 1.4950 - val_acc: 0.3415\n",
            "Epoch 68/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0350 - acc: 0.5610 - val_loss: 1.5002 - val_acc: 0.3486\n",
            "Epoch 69/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0166 - acc: 0.5698 - val_loss: 1.4856 - val_acc: 0.3451\n",
            "Epoch 70/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0058 - acc: 0.5751 - val_loss: 1.4830 - val_acc: 0.3204\n",
            "Epoch 71/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0146 - acc: 0.5698 - val_loss: 1.5074 - val_acc: 0.3275\n",
            "Epoch 72/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0008 - acc: 0.5698 - val_loss: 1.4966 - val_acc: 0.3204\n",
            "Epoch 73/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0056 - acc: 0.5707 - val_loss: 1.4897 - val_acc: 0.3275\n",
            "Epoch 74/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9698 - acc: 0.5813 - val_loss: 1.5299 - val_acc: 0.3310\n",
            "Epoch 75/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0224 - acc: 0.5689 - val_loss: 1.5203 - val_acc: 0.3169\n",
            "Epoch 76/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 0.9968 - acc: 0.5883 - val_loss: 1.5126 - val_acc: 0.3275\n",
            "Epoch 77/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9871 - acc: 0.5804 - val_loss: 1.5146 - val_acc: 0.3415\n",
            "Epoch 78/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 1.0044 - acc: 0.5751 - val_loss: 1.5039 - val_acc: 0.3345\n",
            "Epoch 79/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9769 - acc: 0.5830 - val_loss: 1.5225 - val_acc: 0.3275\n",
            "Epoch 80/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9987 - acc: 0.5892 - val_loss: 1.5190 - val_acc: 0.2993\n",
            "Epoch 81/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9909 - acc: 0.5751 - val_loss: 1.5296 - val_acc: 0.3204\n",
            "Epoch 82/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 0.9634 - acc: 0.5998 - val_loss: 1.5314 - val_acc: 0.3239\n",
            "Epoch 83/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9504 - acc: 0.5989 - val_loss: 1.5336 - val_acc: 0.3310\n",
            "Epoch 84/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9752 - acc: 0.5954 - val_loss: 1.5307 - val_acc: 0.3204\n",
            "Epoch 85/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9630 - acc: 0.5989 - val_loss: 1.5426 - val_acc: 0.3310\n",
            "Epoch 86/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9734 - acc: 0.6069 - val_loss: 1.5288 - val_acc: 0.3204\n",
            "Epoch 87/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9699 - acc: 0.5777 - val_loss: 1.5395 - val_acc: 0.3028\n",
            "Epoch 88/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9970 - acc: 0.5866 - val_loss: 1.5269 - val_acc: 0.2923\n",
            "Epoch 89/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9478 - acc: 0.6007 - val_loss: 1.5389 - val_acc: 0.3380\n",
            "Epoch 90/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9818 - acc: 0.5936 - val_loss: 1.5351 - val_acc: 0.3239\n",
            "Epoch 91/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9815 - acc: 0.5875 - val_loss: 1.5289 - val_acc: 0.3063\n",
            "Epoch 92/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9448 - acc: 0.6122 - val_loss: 1.5218 - val_acc: 0.3028\n",
            "Epoch 93/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9457 - acc: 0.6104 - val_loss: 1.5237 - val_acc: 0.3275\n",
            "Epoch 94/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9808 - acc: 0.5804 - val_loss: 1.5288 - val_acc: 0.2993\n",
            "Epoch 95/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9477 - acc: 0.6069 - val_loss: 1.5391 - val_acc: 0.3134\n",
            "Epoch 96/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9704 - acc: 0.6087 - val_loss: 1.5191 - val_acc: 0.3275\n",
            "Epoch 97/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9571 - acc: 0.5998 - val_loss: 1.5163 - val_acc: 0.3310\n",
            "Epoch 98/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9593 - acc: 0.5945 - val_loss: 1.5244 - val_acc: 0.3345\n",
            "Epoch 99/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9163 - acc: 0.6210 - val_loss: 1.5307 - val_acc: 0.3239\n",
            "Epoch 100/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9414 - acc: 0.6025 - val_loss: 1.5409 - val_acc: 0.3204\n",
            "Epoch 101/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9546 - acc: 0.6034 - val_loss: 1.5574 - val_acc: 0.3204\n",
            "Epoch 102/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9287 - acc: 0.6122 - val_loss: 1.5524 - val_acc: 0.3028\n",
            "Epoch 103/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9396 - acc: 0.6042 - val_loss: 1.5278 - val_acc: 0.3486\n",
            "Epoch 104/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9516 - acc: 0.6184 - val_loss: 1.5527 - val_acc: 0.3345\n",
            "Epoch 105/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9167 - acc: 0.6263 - val_loss: 1.5782 - val_acc: 0.3204\n",
            "Epoch 106/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9231 - acc: 0.6237 - val_loss: 1.5662 - val_acc: 0.3204\n",
            "Epoch 107/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9529 - acc: 0.6069 - val_loss: 1.5510 - val_acc: 0.3169\n",
            "Epoch 108/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9239 - acc: 0.6237 - val_loss: 1.5624 - val_acc: 0.3310\n",
            "Epoch 109/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9264 - acc: 0.6210 - val_loss: 1.5510 - val_acc: 0.3275\n",
            "Epoch 110/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9079 - acc: 0.6131 - val_loss: 1.5339 - val_acc: 0.3169\n",
            "Epoch 111/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9300 - acc: 0.6228 - val_loss: 1.5441 - val_acc: 0.3451\n",
            "Epoch 112/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.8929 - acc: 0.6369 - val_loss: 1.5742 - val_acc: 0.3521\n",
            "Epoch 113/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9252 - acc: 0.6166 - val_loss: 1.6336 - val_acc: 0.3239\n",
            "Epoch 114/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9097 - acc: 0.6334 - val_loss: 1.6110 - val_acc: 0.3345\n",
            "Epoch 115/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 0.8960 - acc: 0.6307 - val_loss: 1.5976 - val_acc: 0.3345\n",
            "Epoch 116/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9079 - acc: 0.6316 - val_loss: 1.6040 - val_acc: 0.3310\n",
            "Epoch 117/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9113 - acc: 0.6316 - val_loss: 1.6189 - val_acc: 0.3239\n",
            "Epoch 118/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 0.9080 - acc: 0.6299 - val_loss: 1.5703 - val_acc: 0.3415\n",
            "Epoch 119/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.8917 - acc: 0.6131 - val_loss: 1.5818 - val_acc: 0.3310\n",
            "Epoch 120/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.8994 - acc: 0.6228 - val_loss: 1.5937 - val_acc: 0.3275\n",
            "Epoch 121/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9133 - acc: 0.6290 - val_loss: 1.5983 - val_acc: 0.3239\n",
            "Epoch 122/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9006 - acc: 0.6299 - val_loss: 1.5937 - val_acc: 0.3486\n",
            "Epoch 123/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9172 - acc: 0.6325 - val_loss: 1.5722 - val_acc: 0.3521\n",
            "Epoch 124/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 0.9121 - acc: 0.6069 - val_loss: 1.6314 - val_acc: 0.3063\n",
            "Epoch 125/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9084 - acc: 0.6228 - val_loss: 1.6340 - val_acc: 0.3134\n",
            "Epoch 126/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.8878 - acc: 0.6387 - val_loss: 1.6457 - val_acc: 0.3380\n",
            "Epoch 127/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9073 - acc: 0.6352 - val_loss: 1.6261 - val_acc: 0.3310\n",
            "Epoch 128/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 0.8927 - acc: 0.6272 - val_loss: 1.6056 - val_acc: 0.3415\n",
            "Epoch 129/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9126 - acc: 0.6087 - val_loss: 1.5905 - val_acc: 0.3169\n",
            "Epoch 130/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.8706 - acc: 0.6528 - val_loss: 1.5905 - val_acc: 0.3415\n",
            "Epoch 131/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 0.8810 - acc: 0.6546 - val_loss: 1.6086 - val_acc: 0.3169\n",
            "Epoch 132/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.8673 - acc: 0.6422 - val_loss: 1.6310 - val_acc: 0.3169\n",
            "Epoch 133/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.9022 - acc: 0.6210 - val_loss: 1.6288 - val_acc: 0.3310\n",
            "Epoch 134/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.8723 - acc: 0.6511 - val_loss: 1.6322 - val_acc: 0.3521\n",
            "Epoch 135/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.8835 - acc: 0.6378 - val_loss: 1.6209 - val_acc: 0.3239\n",
            "Epoch 136/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.8573 - acc: 0.6475 - val_loss: 1.6202 - val_acc: 0.3239\n",
            "Epoch 137/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.8974 - acc: 0.6263 - val_loss: 1.6257 - val_acc: 0.3239\n",
            "Epoch 138/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 0.8745 - acc: 0.6316 - val_loss: 1.6402 - val_acc: 0.3204\n",
            "Epoch 139/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 0.8707 - acc: 0.6422 - val_loss: 1.6465 - val_acc: 0.3345\n",
            "Epoch 140/200\n",
            "1132/1132 [==============================] - 2s 2ms/step - loss: 0.8600 - acc: 0.6466 - val_loss: 1.6249 - val_acc: 0.3380\n",
            "Epoch 141/200\n",
            "1132/1132 [==============================] - 2s 1ms/step - loss: 0.8480 - acc: 0.6564 - val_loss: 1.6263 - val_acc: 0.3415\n",
            "Epoch 142/200\n",
            " 256/1132 [=====>........................] - ETA: 1s - loss: 0.8566 - acc: 0.6602"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vaMLCmTcPeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(cross_subject_accu)\n",
        "plt.grid(False)\n",
        "subject_id = ['subject1','subject2','subject3','subject4','subject5',\n",
        "              'subject6','subject7','subject8','subject9']\n",
        "plt.xticks(range(9), subject_id, rotation='45')\n",
        "plt.yticks(range(9), subject_id, rotation='0')\n",
        "plt.colorbar()\n",
        "\n",
        "# plt.xlabel(subject_id)\n",
        "# plt.ylabel(subject_id)\n",
        "# plt.yticks(rotation='vertical')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK33kcPiGXHs",
        "colab_type": "text"
      },
      "source": [
        "## Leave one subject out during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37QQlNDRFCWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_subject = np.unique(person_train_valid).shape[0]\n",
        "cross_subject_accu = np.zeros((num_subject,num_subject))\n",
        "for num_s in  np.arange(num_subject):\n",
        "  idx_s_train = np.where(person_train_idx_argment != num_s)[0]\n",
        "  model_s = deepcnn(0.6)\n",
        "  early_stop = EarlyStopping(monitor='loss', patience=15, verbose=1)\n",
        "  num_train = X_train_argment[idx_s_train].shape[0]\n",
        "\n",
        "  history_s = model_s.fit(X_train_argment[idx_s_train].reshape(num_train,22,window,1),\n",
        "                          Y_train_argment[idx_s_train], \n",
        "                          batch_size=256, epochs = 200, validation_split = 0.2,\n",
        "                          callbacks = [early_stop], verbose = 1)\n",
        "  print(\"Done with one subject\")\n",
        "#   test the accuracy on other subjects\n",
        "  for num_s_j in np.arange(num_subject):\n",
        "    idx_s_test = np.where(person_test_idx_argment == num_s_j)[0]\n",
        "    cross_subject_accu[num_s, num_s_j] = model_s.evaluate(X_test_argment[idx_s_test].reshape(-1,22,window,1), \n",
        "                                                          Y_test_argment[idx_s_test])[1]\n",
        "plt.imshow(cross_subject_accu)\n",
        "plt.grid(False)\n",
        "subject_id = ['subject1','subject2','subject3','subject4','subject5',\n",
        "              'subject6','subject7','subject8','subject9']\n",
        "plt.title('Leave one subject out')\n",
        "plt.xticks(range(9), subject_id, rotation='45')\n",
        "plt.yticks(range(9), subject_id, rotation='0')\n",
        "plt.colorbar()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E24sSUxahK_M",
        "colab_type": "text"
      },
      "source": [
        "## Fine tune with individaul subject before making prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hrkh_RVGsiR",
        "colab_type": "code",
        "outputId": "fe797729-1b6b-4145-ff42-54c26c6cbc14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# load json and create model\n",
        "# json_file = open('model.json', 'r')\n",
        "# loaded_model_json = json_file.read()\n",
        "# json_file.close()\n",
        "# pretrain_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "\n",
        "num_subject = np.unique(person_train_valid).shape[0]\n",
        "cross_subject_accu = np.zeros((2,num_subject))\n",
        "\n",
        "for num_s in  np.arange(num_subject):\n",
        "  idx_s_train = np.where(person_train_idx_argment == num_s)[0]\n",
        "  num_train = X_train_argment[idx_s_train].shape[0]\n",
        "  \n",
        "  \n",
        "  model_s = keras.models.load_model('weights.best_w_l2.hdf5')\n",
        "  # load weights into new model\n",
        "#   pretrain_model = model_from_json(loaded_model_json)\n",
        "#   pretrain_model.load_weights(\"model_deep_arg.h5\")\n",
        "  \n",
        "  early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
        "  \n",
        "  model_s.compile(loss='categorical_crossentropy',\n",
        "                optimizer = 'adam',\n",
        "                metrics = ['accuracy'])\n",
        "  \n",
        "  idx_s_test = np.where(person_test_idx_argment == num_s)[0]\n",
        "  \n",
        "  y_pred = model_s.predict_classes(X_test_argment[idx_s_test].reshape(-1,22,window,1))\n",
        "\n",
        "#   cross_subject_accu[0,num_s] = model_s.evaluate(X_test_argment[idx_s_test].reshape(-1,22,window,1), \n",
        "#                                                         Y_test_argment[idx_s_test])[1]\n",
        "  acc_test = findaccu(y_pred, Y_test_argment[idx_s_test], window, stride)\n",
        "  cross_subject_accu[0,num_s] = acc_test\n",
        "\n",
        "  # fine tune the model\n",
        "  model_s.fit(X_train_argment[idx_s_train].reshape(num_train,22,window,1),\n",
        "                          Y_train_argment[idx_s_train], \n",
        "                          batch_size=256, epochs = 30, validation_split = 0.2,\n",
        "                          verbose = 1)\n",
        "  y_pred = model_s.predict_classes(X_test_argment[idx_s_test].reshape(-1,22,window,1))\n",
        "  acc_test = findaccu(y_pred, Y_test_argment[idx_s_test], window, stride)\n",
        "  cross_subject_accu[1,num_s] = acc_test\n",
        "  print('complete tuning on {} subject'.format(num_s))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2085 samples, validate on 522 samples\n",
            "Epoch 1/30\n",
            "2085/2085 [==============================] - 10s 5ms/step - loss: 0.7112 - acc: 0.7966 - val_loss: 0.6793 - val_acc: 0.8046\n",
            "Epoch 2/30\n",
            "2085/2085 [==============================] - 2s 992us/step - loss: 0.6457 - acc: 0.8177 - val_loss: 0.6319 - val_acc: 0.8276\n",
            "Epoch 3/30\n",
            "2085/2085 [==============================] - 2s 992us/step - loss: 0.6145 - acc: 0.8245 - val_loss: 0.6380 - val_acc: 0.8257\n",
            "Epoch 4/30\n",
            "2085/2085 [==============================] - 2s 990us/step - loss: 0.5871 - acc: 0.8513 - val_loss: 0.6135 - val_acc: 0.8295\n",
            "Epoch 5/30\n",
            "2085/2085 [==============================] - 2s 985us/step - loss: 0.5577 - acc: 0.8614 - val_loss: 0.5992 - val_acc: 0.8352\n",
            "Epoch 6/30\n",
            "2085/2085 [==============================] - 2s 988us/step - loss: 0.5256 - acc: 0.8676 - val_loss: 0.5651 - val_acc: 0.8563\n",
            "Epoch 7/30\n",
            "2085/2085 [==============================] - 2s 982us/step - loss: 0.4988 - acc: 0.8796 - val_loss: 0.6132 - val_acc: 0.8199\n",
            "Epoch 8/30\n",
            "2085/2085 [==============================] - 2s 987us/step - loss: 0.4831 - acc: 0.8945 - val_loss: 0.5443 - val_acc: 0.8793\n",
            "Epoch 9/30\n",
            "2085/2085 [==============================] - 2s 981us/step - loss: 0.4610 - acc: 0.9041 - val_loss: 0.5437 - val_acc: 0.8736\n",
            "Epoch 10/30\n",
            "2085/2085 [==============================] - 2s 986us/step - loss: 0.4478 - acc: 0.9094 - val_loss: 0.5283 - val_acc: 0.8908\n",
            "Epoch 11/30\n",
            "2085/2085 [==============================] - 2s 980us/step - loss: 0.4446 - acc: 0.8969 - val_loss: 0.5311 - val_acc: 0.8716\n",
            "Epoch 12/30\n",
            "2085/2085 [==============================] - 2s 989us/step - loss: 0.4377 - acc: 0.9094 - val_loss: 0.5438 - val_acc: 0.8774\n",
            "Epoch 13/30\n",
            "2085/2085 [==============================] - 2s 979us/step - loss: 0.4480 - acc: 0.9041 - val_loss: 0.5797 - val_acc: 0.8506\n",
            "Epoch 14/30\n",
            "2085/2085 [==============================] - 2s 976us/step - loss: 0.4331 - acc: 0.9065 - val_loss: 0.5712 - val_acc: 0.8506\n",
            "Epoch 15/30\n",
            "2085/2085 [==============================] - 2s 976us/step - loss: 0.4403 - acc: 0.9060 - val_loss: 0.5847 - val_acc: 0.8448\n",
            "Epoch 16/30\n",
            "2085/2085 [==============================] - 2s 984us/step - loss: 0.4404 - acc: 0.9108 - val_loss: 0.5208 - val_acc: 0.8582\n",
            "Epoch 17/30\n",
            "2085/2085 [==============================] - 2s 986us/step - loss: 0.4076 - acc: 0.9165 - val_loss: 0.5403 - val_acc: 0.8697\n",
            "Epoch 18/30\n",
            "2085/2085 [==============================] - 2s 990us/step - loss: 0.4149 - acc: 0.9127 - val_loss: 0.5199 - val_acc: 0.8716\n",
            "Epoch 19/30\n",
            "2085/2085 [==============================] - 2s 982us/step - loss: 0.3930 - acc: 0.9165 - val_loss: 0.5520 - val_acc: 0.8352\n",
            "Epoch 20/30\n",
            "2085/2085 [==============================] - 2s 981us/step - loss: 0.3809 - acc: 0.9228 - val_loss: 0.6296 - val_acc: 0.7969\n",
            "Epoch 21/30\n",
            "2085/2085 [==============================] - 2s 980us/step - loss: 0.3977 - acc: 0.9175 - val_loss: 0.5898 - val_acc: 0.8352\n",
            "Epoch 22/30\n",
            "2085/2085 [==============================] - 2s 990us/step - loss: 0.3756 - acc: 0.9295 - val_loss: 0.5668 - val_acc: 0.8314\n",
            "Epoch 23/30\n",
            "2085/2085 [==============================] - 2s 976us/step - loss: 0.3796 - acc: 0.9300 - val_loss: 0.5791 - val_acc: 0.8333\n",
            "Epoch 24/30\n",
            "2085/2085 [==============================] - 2s 986us/step - loss: 0.3853 - acc: 0.9223 - val_loss: 0.5445 - val_acc: 0.8563\n",
            "Epoch 25/30\n",
            "2085/2085 [==============================] - 2s 986us/step - loss: 0.3813 - acc: 0.9271 - val_loss: 0.5266 - val_acc: 0.8621\n",
            "Epoch 26/30\n",
            "2085/2085 [==============================] - 2s 986us/step - loss: 0.3715 - acc: 0.9324 - val_loss: 0.5673 - val_acc: 0.8582\n",
            "Epoch 27/30\n",
            "2085/2085 [==============================] - 2s 985us/step - loss: 0.4068 - acc: 0.9103 - val_loss: 0.5145 - val_acc: 0.8831\n",
            "Epoch 28/30\n",
            "2085/2085 [==============================] - 2s 998us/step - loss: 0.3556 - acc: 0.9353 - val_loss: 0.5926 - val_acc: 0.8410\n",
            "Epoch 29/30\n",
            "2085/2085 [==============================] - 2s 988us/step - loss: 0.3716 - acc: 0.9290 - val_loss: 0.5579 - val_acc: 0.8506\n",
            "Epoch 30/30\n",
            "2085/2085 [==============================] - 2s 986us/step - loss: 0.3571 - acc: 0.9381 - val_loss: 0.5442 - val_acc: 0.8640\n",
            "complete tuning on 0 subject\n",
            "Train on 2076 samples, validate on 520 samples\n",
            "Epoch 1/30\n",
            "2076/2076 [==============================] - 10s 5ms/step - loss: 1.0095 - acc: 0.6744 - val_loss: 1.2212 - val_acc: 0.5538\n",
            "Epoch 2/30\n",
            "2076/2076 [==============================] - 2s 987us/step - loss: 0.9252 - acc: 0.7086 - val_loss: 1.1562 - val_acc: 0.6173\n",
            "Epoch 3/30\n",
            "2076/2076 [==============================] - 2s 987us/step - loss: 0.8489 - acc: 0.7394 - val_loss: 1.0918 - val_acc: 0.6019\n",
            "Epoch 4/30\n",
            "2076/2076 [==============================] - 2s 986us/step - loss: 0.8162 - acc: 0.7495 - val_loss: 1.1517 - val_acc: 0.5904\n",
            "Epoch 5/30\n",
            "2076/2076 [==============================] - 2s 993us/step - loss: 0.7171 - acc: 0.8025 - val_loss: 1.1446 - val_acc: 0.5712\n",
            "Epoch 6/30\n",
            "2076/2076 [==============================] - 2s 983us/step - loss: 0.7466 - acc: 0.7943 - val_loss: 1.2346 - val_acc: 0.5558\n",
            "Epoch 7/30\n",
            "2076/2076 [==============================] - 2s 983us/step - loss: 0.7204 - acc: 0.7967 - val_loss: 1.3132 - val_acc: 0.5558\n",
            "Epoch 8/30\n",
            "2076/2076 [==============================] - 2s 988us/step - loss: 0.7229 - acc: 0.7972 - val_loss: 1.2041 - val_acc: 0.5635\n",
            "Epoch 9/30\n",
            "2076/2076 [==============================] - 2s 987us/step - loss: 0.6758 - acc: 0.8155 - val_loss: 1.1846 - val_acc: 0.5846\n",
            "Epoch 10/30\n",
            "2076/2076 [==============================] - 2s 987us/step - loss: 0.6487 - acc: 0.8266 - val_loss: 1.1463 - val_acc: 0.6135\n",
            "Epoch 11/30\n",
            "2076/2076 [==============================] - 2s 996us/step - loss: 0.6641 - acc: 0.8223 - val_loss: 1.1834 - val_acc: 0.6038\n",
            "Epoch 12/30\n",
            "2076/2076 [==============================] - 2s 986us/step - loss: 0.6021 - acc: 0.8512 - val_loss: 1.2488 - val_acc: 0.5962\n",
            "Epoch 13/30\n",
            "2076/2076 [==============================] - 2s 991us/step - loss: 0.6229 - acc: 0.8516 - val_loss: 1.1688 - val_acc: 0.6250\n",
            "Epoch 14/30\n",
            "2076/2076 [==============================] - 2s 994us/step - loss: 0.5740 - acc: 0.8579 - val_loss: 1.0518 - val_acc: 0.6654\n",
            "Epoch 15/30\n",
            "2076/2076 [==============================] - 2s 989us/step - loss: 0.5658 - acc: 0.8637 - val_loss: 1.1188 - val_acc: 0.6404\n",
            "Epoch 16/30\n",
            "2076/2076 [==============================] - 2s 981us/step - loss: 0.5729 - acc: 0.8593 - val_loss: 1.2190 - val_acc: 0.5808\n",
            "Epoch 17/30\n",
            "2076/2076 [==============================] - 2s 983us/step - loss: 0.5872 - acc: 0.8560 - val_loss: 1.1619 - val_acc: 0.6019\n",
            "Epoch 18/30\n",
            "2076/2076 [==============================] - 2s 983us/step - loss: 0.5512 - acc: 0.8719 - val_loss: 1.0751 - val_acc: 0.6269\n",
            "Epoch 19/30\n",
            "2076/2076 [==============================] - 2s 988us/step - loss: 0.5539 - acc: 0.8637 - val_loss: 1.0854 - val_acc: 0.6308\n",
            "Epoch 20/30\n",
            "2076/2076 [==============================] - 2s 980us/step - loss: 0.5194 - acc: 0.8815 - val_loss: 1.0943 - val_acc: 0.6288\n",
            "Epoch 21/30\n",
            "2076/2076 [==============================] - 2s 985us/step - loss: 0.5289 - acc: 0.8801 - val_loss: 1.0132 - val_acc: 0.6519\n",
            "Epoch 22/30\n",
            "2076/2076 [==============================] - 2s 988us/step - loss: 0.5265 - acc: 0.8849 - val_loss: 1.0029 - val_acc: 0.6558\n",
            "Epoch 23/30\n",
            "2076/2076 [==============================] - 2s 989us/step - loss: 0.5079 - acc: 0.8887 - val_loss: 1.0793 - val_acc: 0.6519\n",
            "Epoch 24/30\n",
            "2076/2076 [==============================] - 2s 992us/step - loss: 0.5162 - acc: 0.8791 - val_loss: 1.0870 - val_acc: 0.6288\n",
            "Epoch 25/30\n",
            "2076/2076 [==============================] - 2s 987us/step - loss: 0.4906 - acc: 0.8935 - val_loss: 1.1974 - val_acc: 0.6154\n",
            "Epoch 26/30\n",
            "2076/2076 [==============================] - 2s 989us/step - loss: 0.4981 - acc: 0.8849 - val_loss: 1.0684 - val_acc: 0.6500\n",
            "Epoch 27/30\n",
            "2076/2076 [==============================] - 2s 994us/step - loss: 0.4699 - acc: 0.9017 - val_loss: 1.0895 - val_acc: 0.6462\n",
            "Epoch 28/30\n",
            "2076/2076 [==============================] - 2s 988us/step - loss: 0.4770 - acc: 0.8950 - val_loss: 1.0263 - val_acc: 0.6577\n",
            "Epoch 29/30\n",
            "2076/2076 [==============================] - 2s 985us/step - loss: 0.4753 - acc: 0.8902 - val_loss: 1.0802 - val_acc: 0.6038\n",
            "Epoch 30/30\n",
            "2076/2076 [==============================] - 2s 981us/step - loss: 0.4585 - acc: 0.8984 - val_loss: 1.1362 - val_acc: 0.6327\n",
            "complete tuning on 1 subject\n",
            "Train on 2076 samples, validate on 520 samples\n",
            "Epoch 1/30\n",
            "2076/2076 [==============================] - 11s 5ms/step - loss: 0.7179 - acc: 0.7866 - val_loss: 0.6020 - val_acc: 0.8423\n",
            "Epoch 2/30\n",
            "2076/2076 [==============================] - 2s 993us/step - loss: 0.6414 - acc: 0.8227 - val_loss: 0.5885 - val_acc: 0.8192\n",
            "Epoch 3/30\n",
            "2076/2076 [==============================] - 2s 984us/step - loss: 0.5988 - acc: 0.8507 - val_loss: 0.5485 - val_acc: 0.8577\n",
            "Epoch 4/30\n",
            "2076/2076 [==============================] - 2s 988us/step - loss: 0.5982 - acc: 0.8396 - val_loss: 0.5993 - val_acc: 0.8173\n",
            "Epoch 5/30\n",
            "2076/2076 [==============================] - 2s 985us/step - loss: 0.5756 - acc: 0.8613 - val_loss: 0.5942 - val_acc: 0.8269\n",
            "Epoch 6/30\n",
            "2076/2076 [==============================] - 2s 986us/step - loss: 0.5349 - acc: 0.8743 - val_loss: 0.5673 - val_acc: 0.8154\n",
            "Epoch 7/30\n",
            "2076/2076 [==============================] - 2s 984us/step - loss: 0.5030 - acc: 0.8834 - val_loss: 0.6504 - val_acc: 0.7731\n",
            "Epoch 8/30\n",
            "2076/2076 [==============================] - 2s 991us/step - loss: 0.5031 - acc: 0.8873 - val_loss: 0.6196 - val_acc: 0.7846\n",
            "Epoch 9/30\n",
            "2076/2076 [==============================] - 2s 995us/step - loss: 0.5102 - acc: 0.8839 - val_loss: 0.5486 - val_acc: 0.8288\n",
            "Epoch 10/30\n",
            "2076/2076 [==============================] - 2s 983us/step - loss: 0.4646 - acc: 0.8998 - val_loss: 0.5939 - val_acc: 0.8173\n",
            "Epoch 11/30\n",
            "2076/2076 [==============================] - 2s 982us/step - loss: 0.4601 - acc: 0.9037 - val_loss: 0.5804 - val_acc: 0.8327\n",
            "Epoch 12/30\n",
            "2076/2076 [==============================] - 2s 991us/step - loss: 0.4574 - acc: 0.9003 - val_loss: 0.6030 - val_acc: 0.8077\n",
            "Epoch 13/30\n",
            "2076/2076 [==============================] - 2s 985us/step - loss: 0.4560 - acc: 0.8960 - val_loss: 0.5930 - val_acc: 0.8096\n",
            "Epoch 14/30\n",
            "2076/2076 [==============================] - 2s 979us/step - loss: 0.4492 - acc: 0.9066 - val_loss: 0.6035 - val_acc: 0.8173\n",
            "Epoch 15/30\n",
            "2076/2076 [==============================] - 2s 992us/step - loss: 0.4425 - acc: 0.9094 - val_loss: 0.6529 - val_acc: 0.8000\n",
            "Epoch 16/30\n",
            "2076/2076 [==============================] - 2s 983us/step - loss: 0.4146 - acc: 0.9200 - val_loss: 0.6883 - val_acc: 0.7827\n",
            "Epoch 17/30\n",
            "2076/2076 [==============================] - 2s 987us/step - loss: 0.4196 - acc: 0.9181 - val_loss: 0.5860 - val_acc: 0.8192\n",
            "Epoch 18/30\n",
            "2076/2076 [==============================] - 2s 992us/step - loss: 0.4180 - acc: 0.9147 - val_loss: 0.5891 - val_acc: 0.8192\n",
            "Epoch 19/30\n",
            "2076/2076 [==============================] - 2s 985us/step - loss: 0.4148 - acc: 0.9085 - val_loss: 0.6029 - val_acc: 0.8231\n",
            "Epoch 20/30\n",
            "2076/2076 [==============================] - 2s 993us/step - loss: 0.3989 - acc: 0.9253 - val_loss: 0.5606 - val_acc: 0.8365\n",
            "Epoch 21/30\n",
            "2076/2076 [==============================] - 2s 991us/step - loss: 0.4194 - acc: 0.9162 - val_loss: 0.6749 - val_acc: 0.7942\n",
            "Epoch 22/30\n",
            "2076/2076 [==============================] - 2s 985us/step - loss: 0.4064 - acc: 0.9215 - val_loss: 0.8181 - val_acc: 0.7731\n",
            "Epoch 23/30\n",
            "2076/2076 [==============================] - 2s 979us/step - loss: 0.4022 - acc: 0.9220 - val_loss: 0.7202 - val_acc: 0.7885\n",
            "Epoch 24/30\n",
            "2076/2076 [==============================] - 2s 983us/step - loss: 0.3901 - acc: 0.9302 - val_loss: 0.7014 - val_acc: 0.7923\n",
            "Epoch 25/30\n",
            "2076/2076 [==============================] - 2s 979us/step - loss: 0.3879 - acc: 0.9302 - val_loss: 0.6776 - val_acc: 0.7942\n",
            "Epoch 26/30\n",
            "2076/2076 [==============================] - 2s 982us/step - loss: 0.3816 - acc: 0.9302 - val_loss: 0.6513 - val_acc: 0.7981\n",
            "Epoch 27/30\n",
            "2076/2076 [==============================] - 2s 982us/step - loss: 0.3774 - acc: 0.9297 - val_loss: 0.6477 - val_acc: 0.8000\n",
            "Epoch 28/30\n",
            "2076/2076 [==============================] - 2s 987us/step - loss: 0.3544 - acc: 0.9398 - val_loss: 0.6658 - val_acc: 0.7827\n",
            "Epoch 29/30\n",
            "2076/2076 [==============================] - 2s 981us/step - loss: 0.3885 - acc: 0.9277 - val_loss: 0.7105 - val_acc: 0.7808\n",
            "Epoch 30/30\n",
            "2076/2076 [==============================] - 2s 982us/step - loss: 0.3716 - acc: 0.9277 - val_loss: 0.8464 - val_acc: 0.7635\n",
            "complete tuning on 2 subject\n",
            "Train on 2059 samples, validate on 515 samples\n",
            "Epoch 1/30\n",
            "2059/2059 [==============================] - 12s 6ms/step - loss: 0.9542 - acc: 0.7018 - val_loss: 1.0620 - val_acc: 0.6660\n",
            "Epoch 2/30\n",
            "2059/2059 [==============================] - 2s 991us/step - loss: 0.8357 - acc: 0.7591 - val_loss: 1.1342 - val_acc: 0.6680\n",
            "Epoch 3/30\n",
            "2059/2059 [==============================] - 2s 993us/step - loss: 0.8051 - acc: 0.7562 - val_loss: 1.1959 - val_acc: 0.6311\n",
            "Epoch 4/30\n",
            "2059/2059 [==============================] - 2s 988us/step - loss: 0.8134 - acc: 0.7523 - val_loss: 1.2752 - val_acc: 0.6039\n",
            "Epoch 5/30\n",
            "2059/2059 [==============================] - 2s 1ms/step - loss: 0.7785 - acc: 0.7620 - val_loss: 1.2191 - val_acc: 0.6660\n",
            "Epoch 6/30\n",
            "2059/2059 [==============================] - 2s 1ms/step - loss: 0.7199 - acc: 0.7970 - val_loss: 1.1864 - val_acc: 0.6544\n",
            "Epoch 7/30\n",
            "2059/2059 [==============================] - 2s 993us/step - loss: 0.7176 - acc: 0.7970 - val_loss: 1.1188 - val_acc: 0.6505\n",
            "Epoch 8/30\n",
            "2059/2059 [==============================] - 2s 993us/step - loss: 0.7328 - acc: 0.7902 - val_loss: 1.1349 - val_acc: 0.6583\n",
            "Epoch 9/30\n",
            "2059/2059 [==============================] - 2s 993us/step - loss: 0.6955 - acc: 0.8077 - val_loss: 1.1954 - val_acc: 0.6602\n",
            "Epoch 10/30\n",
            "2059/2059 [==============================] - 2s 984us/step - loss: 0.6865 - acc: 0.8043 - val_loss: 1.3312 - val_acc: 0.6155\n",
            "Epoch 11/30\n",
            "2059/2059 [==============================] - 2s 990us/step - loss: 0.6621 - acc: 0.8208 - val_loss: 1.2818 - val_acc: 0.6233\n",
            "Epoch 12/30\n",
            "2059/2059 [==============================] - 2s 983us/step - loss: 0.6560 - acc: 0.8266 - val_loss: 1.4588 - val_acc: 0.6194\n",
            "Epoch 13/30\n",
            "2059/2059 [==============================] - 2s 989us/step - loss: 0.6667 - acc: 0.8218 - val_loss: 1.3171 - val_acc: 0.6738\n",
            "Epoch 14/30\n",
            "2059/2059 [==============================] - 2s 993us/step - loss: 0.6511 - acc: 0.8256 - val_loss: 1.2344 - val_acc: 0.7184\n",
            "Epoch 15/30\n",
            "2059/2059 [==============================] - 2s 984us/step - loss: 0.6444 - acc: 0.8266 - val_loss: 1.2427 - val_acc: 0.6913\n",
            "Epoch 16/30\n",
            "2059/2059 [==============================] - 2s 985us/step - loss: 0.6507 - acc: 0.8334 - val_loss: 1.3089 - val_acc: 0.6466\n",
            "Epoch 17/30\n",
            "2059/2059 [==============================] - 2s 985us/step - loss: 0.6587 - acc: 0.8276 - val_loss: 1.2886 - val_acc: 0.6544\n",
            "Epoch 18/30\n",
            "2059/2059 [==============================] - 2s 991us/step - loss: 0.6340 - acc: 0.8324 - val_loss: 1.1742 - val_acc: 0.6718\n",
            "Epoch 19/30\n",
            "2059/2059 [==============================] - 2s 991us/step - loss: 0.5817 - acc: 0.8606 - val_loss: 1.0961 - val_acc: 0.6990\n",
            "Epoch 20/30\n",
            "2059/2059 [==============================] - 2s 995us/step - loss: 0.5836 - acc: 0.8533 - val_loss: 1.1161 - val_acc: 0.6874\n",
            "Epoch 21/30\n",
            "2059/2059 [==============================] - 2s 997us/step - loss: 0.6054 - acc: 0.8470 - val_loss: 1.1960 - val_acc: 0.6971\n",
            "Epoch 22/30\n",
            "2059/2059 [==============================] - 2s 997us/step - loss: 0.6476 - acc: 0.8320 - val_loss: 1.1954 - val_acc: 0.6680\n",
            "Epoch 23/30\n",
            "2059/2059 [==============================] - 2s 990us/step - loss: 0.6210 - acc: 0.8354 - val_loss: 1.2186 - val_acc: 0.6252\n",
            "Epoch 24/30\n",
            "2059/2059 [==============================] - 2s 991us/step - loss: 0.6173 - acc: 0.8431 - val_loss: 1.1864 - val_acc: 0.6893\n",
            "Epoch 25/30\n",
            "2059/2059 [==============================] - 2s 997us/step - loss: 0.6378 - acc: 0.8417 - val_loss: 1.2278 - val_acc: 0.6699\n",
            "Epoch 26/30\n",
            "2059/2059 [==============================] - 2s 990us/step - loss: 0.6125 - acc: 0.8465 - val_loss: 1.1963 - val_acc: 0.6816\n",
            "Epoch 27/30\n",
            "2059/2059 [==============================] - 2s 990us/step - loss: 0.6108 - acc: 0.8451 - val_loss: 1.1984 - val_acc: 0.6660\n",
            "Epoch 28/30\n",
            "2059/2059 [==============================] - 2s 985us/step - loss: 0.5730 - acc: 0.8635 - val_loss: 1.2132 - val_acc: 0.6641\n",
            "Epoch 29/30\n",
            "2059/2059 [==============================] - 2s 987us/step - loss: 0.5585 - acc: 0.8713 - val_loss: 1.1518 - val_acc: 0.6757\n",
            "Epoch 30/30\n",
            "2059/2059 [==============================] - 2s 987us/step - loss: 0.5336 - acc: 0.8786 - val_loss: 1.1801 - val_acc: 0.6621\n",
            "complete tuning on 3 subject\n",
            "Train on 2068 samples, validate on 517 samples\n",
            "Epoch 1/30\n",
            "2068/2068 [==============================] - 13s 6ms/step - loss: 0.8630 - acc: 0.7307 - val_loss: 0.8650 - val_acc: 0.7369\n",
            "Epoch 2/30\n",
            "2068/2068 [==============================] - 2s 984us/step - loss: 0.8020 - acc: 0.7722 - val_loss: 0.8850 - val_acc: 0.7369\n",
            "Epoch 3/30\n",
            "2068/2068 [==============================] - 2s 994us/step - loss: 0.7161 - acc: 0.7950 - val_loss: 0.8552 - val_acc: 0.7331\n",
            "Epoch 4/30\n",
            "2068/2068 [==============================] - 2s 989us/step - loss: 0.6851 - acc: 0.8158 - val_loss: 0.9097 - val_acc: 0.7369\n",
            "Epoch 5/30\n",
            "2068/2068 [==============================] - 2s 990us/step - loss: 0.6543 - acc: 0.8279 - val_loss: 1.0198 - val_acc: 0.6634\n",
            "Epoch 6/30\n",
            "2068/2068 [==============================] - 2s 991us/step - loss: 0.6250 - acc: 0.8424 - val_loss: 1.0489 - val_acc: 0.6634\n",
            "Epoch 7/30\n",
            "2068/2068 [==============================] - 2s 992us/step - loss: 0.6216 - acc: 0.8467 - val_loss: 0.9599 - val_acc: 0.6963\n",
            "Epoch 8/30\n",
            "2068/2068 [==============================] - 2s 995us/step - loss: 0.5816 - acc: 0.8496 - val_loss: 0.9327 - val_acc: 0.7060\n",
            "Epoch 9/30\n",
            "2068/2068 [==============================] - 2s 999us/step - loss: 0.5656 - acc: 0.8607 - val_loss: 0.9557 - val_acc: 0.7041\n",
            "Epoch 10/30\n",
            "2068/2068 [==============================] - 2s 987us/step - loss: 0.5651 - acc: 0.8651 - val_loss: 0.8080 - val_acc: 0.7408\n",
            "Epoch 11/30\n",
            "2068/2068 [==============================] - 2s 992us/step - loss: 0.5663 - acc: 0.8583 - val_loss: 0.8975 - val_acc: 0.7195\n",
            "Epoch 12/30\n",
            "2068/2068 [==============================] - 2s 989us/step - loss: 0.5689 - acc: 0.8622 - val_loss: 0.9938 - val_acc: 0.6770\n",
            "Epoch 13/30\n",
            "2068/2068 [==============================] - 2s 993us/step - loss: 0.5864 - acc: 0.8511 - val_loss: 0.9147 - val_acc: 0.7118\n",
            "Epoch 14/30\n",
            "2068/2068 [==============================] - 2s 995us/step - loss: 0.5464 - acc: 0.8646 - val_loss: 0.7802 - val_acc: 0.7718\n",
            "Epoch 15/30\n",
            "2068/2068 [==============================] - 2s 987us/step - loss: 0.5264 - acc: 0.8791 - val_loss: 0.8558 - val_acc: 0.7505\n",
            "Epoch 16/30\n",
            "2068/2068 [==============================] - 2s 995us/step - loss: 0.5388 - acc: 0.8704 - val_loss: 0.8448 - val_acc: 0.7524\n",
            "Epoch 17/30\n",
            "2068/2068 [==============================] - 2s 990us/step - loss: 0.5061 - acc: 0.8873 - val_loss: 1.0221 - val_acc: 0.7176\n",
            "Epoch 18/30\n",
            "2068/2068 [==============================] - 2s 991us/step - loss: 0.5351 - acc: 0.8699 - val_loss: 1.2573 - val_acc: 0.6383\n",
            "Epoch 19/30\n",
            "2068/2068 [==============================] - 2s 992us/step - loss: 0.5030 - acc: 0.8873 - val_loss: 1.2253 - val_acc: 0.6344\n",
            "Epoch 20/30\n",
            "2068/2068 [==============================] - 2s 987us/step - loss: 0.4630 - acc: 0.9072 - val_loss: 1.0264 - val_acc: 0.6963\n",
            "Epoch 21/30\n",
            "2068/2068 [==============================] - 2s 992us/step - loss: 0.4707 - acc: 0.9047 - val_loss: 0.8913 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "2068/2068 [==============================] - 2s 991us/step - loss: 0.4789 - acc: 0.9018 - val_loss: 0.9107 - val_acc: 0.7447\n",
            "Epoch 23/30\n",
            "2068/2068 [==============================] - 2s 988us/step - loss: 0.4404 - acc: 0.9067 - val_loss: 0.8340 - val_acc: 0.7582\n",
            "Epoch 24/30\n",
            "2068/2068 [==============================] - 2s 988us/step - loss: 0.4324 - acc: 0.9125 - val_loss: 0.8249 - val_acc: 0.7582\n",
            "Epoch 25/30\n",
            "2068/2068 [==============================] - 2s 981us/step - loss: 0.4536 - acc: 0.8965 - val_loss: 0.7502 - val_acc: 0.8027\n",
            "Epoch 26/30\n",
            "2068/2068 [==============================] - 2s 988us/step - loss: 0.4672 - acc: 0.8999 - val_loss: 0.8561 - val_acc: 0.7582\n",
            "Epoch 27/30\n",
            "2068/2068 [==============================] - 2s 987us/step - loss: 0.4512 - acc: 0.9057 - val_loss: 0.9972 - val_acc: 0.7273\n",
            "Epoch 28/30\n",
            "2068/2068 [==============================] - 2s 985us/step - loss: 0.4287 - acc: 0.9163 - val_loss: 0.9323 - val_acc: 0.7369\n",
            "Epoch 29/30\n",
            "2068/2068 [==============================] - 2s 987us/step - loss: 0.4471 - acc: 0.9062 - val_loss: 0.8320 - val_acc: 0.7698\n",
            "Epoch 30/30\n",
            "2068/2068 [==============================] - 2s 985us/step - loss: 0.4150 - acc: 0.9207 - val_loss: 0.8978 - val_acc: 0.7602\n",
            "complete tuning on 4 subject\n",
            "Train on 2076 samples, validate on 520 samples\n",
            "Epoch 1/30\n",
            "2076/2076 [==============================] - 13s 6ms/step - loss: 0.9135 - acc: 0.7076 - val_loss: 1.2015 - val_acc: 0.7115\n",
            "Epoch 2/30\n",
            "2076/2076 [==============================] - 2s 988us/step - loss: 0.8092 - acc: 0.7649 - val_loss: 1.1144 - val_acc: 0.7327\n",
            "Epoch 3/30\n",
            "2076/2076 [==============================] - 2s 983us/step - loss: 0.7820 - acc: 0.7760 - val_loss: 1.2306 - val_acc: 0.6558\n",
            "Epoch 4/30\n",
            "2076/2076 [==============================] - 2s 985us/step - loss: 0.7400 - acc: 0.7972 - val_loss: 1.3334 - val_acc: 0.6500\n",
            "Epoch 5/30\n",
            "2076/2076 [==============================] - 2s 990us/step - loss: 0.6933 - acc: 0.8097 - val_loss: 1.2504 - val_acc: 0.7173\n",
            "Epoch 6/30\n",
            "2076/2076 [==============================] - 2s 984us/step - loss: 0.6874 - acc: 0.8145 - val_loss: 1.2574 - val_acc: 0.6788\n",
            "Epoch 7/30\n",
            "2076/2076 [==============================] - 2s 982us/step - loss: 0.6378 - acc: 0.8300 - val_loss: 1.2053 - val_acc: 0.6808\n",
            "Epoch 8/30\n",
            "2076/2076 [==============================] - 2s 989us/step - loss: 0.6377 - acc: 0.8406 - val_loss: 1.1235 - val_acc: 0.7365\n",
            "Epoch 9/30\n",
            "2076/2076 [==============================] - 2s 984us/step - loss: 0.6304 - acc: 0.8444 - val_loss: 1.0785 - val_acc: 0.7615\n",
            "Epoch 10/30\n",
            "2076/2076 [==============================] - 2s 986us/step - loss: 0.5931 - acc: 0.8497 - val_loss: 1.0784 - val_acc: 0.7500\n",
            "Epoch 11/30\n",
            "2076/2076 [==============================] - 2s 985us/step - loss: 0.5642 - acc: 0.8656 - val_loss: 1.0825 - val_acc: 0.7462\n",
            "Epoch 12/30\n",
            "2076/2076 [==============================] - 2s 996us/step - loss: 0.5747 - acc: 0.8560 - val_loss: 1.1466 - val_acc: 0.7327\n",
            "Epoch 13/30\n",
            "2076/2076 [==============================] - 2s 993us/step - loss: 0.5751 - acc: 0.8627 - val_loss: 1.2595 - val_acc: 0.6788\n",
            "Epoch 14/30\n",
            "2076/2076 [==============================] - 2s 992us/step - loss: 0.5654 - acc: 0.8593 - val_loss: 1.2031 - val_acc: 0.6865\n",
            "Epoch 15/30\n",
            "2076/2076 [==============================] - 2s 991us/step - loss: 0.5517 - acc: 0.8651 - val_loss: 1.2126 - val_acc: 0.7115\n",
            "Epoch 16/30\n",
            "2076/2076 [==============================] - 2s 998us/step - loss: 0.5202 - acc: 0.8858 - val_loss: 1.2639 - val_acc: 0.6981\n",
            "Epoch 17/30\n",
            "2076/2076 [==============================] - 2s 998us/step - loss: 0.5198 - acc: 0.8733 - val_loss: 1.2743 - val_acc: 0.6981\n",
            "Epoch 18/30\n",
            "2076/2076 [==============================] - 2s 984us/step - loss: 0.5477 - acc: 0.8743 - val_loss: 1.2119 - val_acc: 0.6673\n",
            "Epoch 19/30\n",
            "2076/2076 [==============================] - 2s 990us/step - loss: 0.5242 - acc: 0.8834 - val_loss: 1.2315 - val_acc: 0.6692\n",
            "Epoch 20/30\n",
            "2076/2076 [==============================] - 2s 988us/step - loss: 0.5069 - acc: 0.8858 - val_loss: 1.2611 - val_acc: 0.7038\n",
            "Epoch 21/30\n",
            "2076/2076 [==============================] - 2s 994us/step - loss: 0.4833 - acc: 0.8921 - val_loss: 1.2370 - val_acc: 0.7250\n",
            "Epoch 22/30\n",
            "2076/2076 [==============================] - 2s 994us/step - loss: 0.5045 - acc: 0.8834 - val_loss: 1.2214 - val_acc: 0.7269\n",
            "Epoch 23/30\n",
            "2076/2076 [==============================] - 2s 988us/step - loss: 0.5044 - acc: 0.8863 - val_loss: 1.4161 - val_acc: 0.6615\n",
            "Epoch 24/30\n",
            "2076/2076 [==============================] - 2s 993us/step - loss: 0.4985 - acc: 0.8844 - val_loss: 1.4762 - val_acc: 0.6769\n",
            "Epoch 25/30\n",
            "2076/2076 [==============================] - 2s 998us/step - loss: 0.4693 - acc: 0.9041 - val_loss: 1.4684 - val_acc: 0.6865\n",
            "Epoch 26/30\n",
            "2076/2076 [==============================] - 2s 992us/step - loss: 0.4656 - acc: 0.9032 - val_loss: 1.2710 - val_acc: 0.7154\n",
            "Epoch 27/30\n",
            "2076/2076 [==============================] - 2s 988us/step - loss: 0.4798 - acc: 0.8935 - val_loss: 1.3277 - val_acc: 0.7250\n",
            "Epoch 28/30\n",
            "2076/2076 [==============================] - 2s 987us/step - loss: 0.4881 - acc: 0.8931 - val_loss: 1.4422 - val_acc: 0.7308\n",
            "Epoch 29/30\n",
            "2076/2076 [==============================] - 2s 988us/step - loss: 0.4965 - acc: 0.8907 - val_loss: 1.4794 - val_acc: 0.7250\n",
            "Epoch 30/30\n",
            "2076/2076 [==============================] - 2s 994us/step - loss: 0.4703 - acc: 0.9027 - val_loss: 1.3902 - val_acc: 0.7365\n",
            "complete tuning on 5 subject\n",
            "Train on 2094 samples, validate on 524 samples\n",
            "Epoch 1/30\n",
            "2094/2094 [==============================] - 14s 7ms/step - loss: 0.6583 - acc: 0.8204 - val_loss: 0.6073 - val_acc: 0.8454\n",
            "Epoch 2/30\n",
            "2094/2094 [==============================] - 2s 983us/step - loss: 0.5752 - acc: 0.8596 - val_loss: 0.5321 - val_acc: 0.8721\n",
            "Epoch 3/30\n",
            "2094/2094 [==============================] - 2s 985us/step - loss: 0.5309 - acc: 0.8682 - val_loss: 0.5663 - val_acc: 0.8511\n",
            "Epoch 4/30\n",
            "2094/2094 [==============================] - 2s 993us/step - loss: 0.4939 - acc: 0.8868 - val_loss: 0.6067 - val_acc: 0.8092\n",
            "Epoch 5/30\n",
            "2094/2094 [==============================] - 2s 994us/step - loss: 0.4461 - acc: 0.9107 - val_loss: 0.5407 - val_acc: 0.8397\n",
            "Epoch 6/30\n",
            "2094/2094 [==============================] - 2s 989us/step - loss: 0.4450 - acc: 0.9016 - val_loss: 0.5599 - val_acc: 0.8607\n",
            "Epoch 7/30\n",
            "2094/2094 [==============================] - 2s 996us/step - loss: 0.4215 - acc: 0.9160 - val_loss: 0.5753 - val_acc: 0.8416\n",
            "Epoch 8/30\n",
            "2094/2094 [==============================] - 2s 1ms/step - loss: 0.3798 - acc: 0.9327 - val_loss: 0.5430 - val_acc: 0.8473\n",
            "Epoch 9/30\n",
            "2094/2094 [==============================] - 2s 986us/step - loss: 0.3826 - acc: 0.9269 - val_loss: 0.5883 - val_acc: 0.8282\n",
            "Epoch 10/30\n",
            "2094/2094 [==============================] - 2s 994us/step - loss: 0.3940 - acc: 0.9274 - val_loss: 0.5788 - val_acc: 0.8321\n",
            "Epoch 11/30\n",
            "2094/2094 [==============================] - 2s 988us/step - loss: 0.3742 - acc: 0.9331 - val_loss: 0.5572 - val_acc: 0.8244\n",
            "Epoch 12/30\n",
            "2094/2094 [==============================] - 2s 989us/step - loss: 0.3630 - acc: 0.9398 - val_loss: 0.4790 - val_acc: 0.8702\n",
            "Epoch 13/30\n",
            "2094/2094 [==============================] - 2s 994us/step - loss: 0.3459 - acc: 0.9446 - val_loss: 0.5993 - val_acc: 0.8531\n",
            "Epoch 14/30\n",
            "2094/2094 [==============================] - 2s 988us/step - loss: 0.3535 - acc: 0.9427 - val_loss: 0.6376 - val_acc: 0.8168\n",
            "Epoch 15/30\n",
            "2094/2094 [==============================] - 2s 992us/step - loss: 0.3376 - acc: 0.9446 - val_loss: 0.5852 - val_acc: 0.8073\n",
            "Epoch 16/30\n",
            "2094/2094 [==============================] - 2s 990us/step - loss: 0.3301 - acc: 0.9460 - val_loss: 0.6233 - val_acc: 0.8282\n",
            "Epoch 17/30\n",
            "2094/2094 [==============================] - 2s 990us/step - loss: 0.3357 - acc: 0.9446 - val_loss: 0.7317 - val_acc: 0.7939\n",
            "Epoch 18/30\n",
            "2094/2094 [==============================] - 2s 986us/step - loss: 0.3288 - acc: 0.9484 - val_loss: 0.6131 - val_acc: 0.8015\n",
            "Epoch 19/30\n",
            "2094/2094 [==============================] - 2s 984us/step - loss: 0.3305 - acc: 0.9489 - val_loss: 0.5937 - val_acc: 0.8454\n",
            "Epoch 20/30\n",
            "2094/2094 [==============================] - 2s 990us/step - loss: 0.3374 - acc: 0.9475 - val_loss: 0.6057 - val_acc: 0.8244\n",
            "Epoch 21/30\n",
            "2094/2094 [==============================] - 2s 990us/step - loss: 0.3054 - acc: 0.9575 - val_loss: 0.6808 - val_acc: 0.8053\n",
            "Epoch 22/30\n",
            "2094/2094 [==============================] - 2s 987us/step - loss: 0.3086 - acc: 0.9551 - val_loss: 0.5923 - val_acc: 0.8378\n",
            "Epoch 23/30\n",
            "2094/2094 [==============================] - 2s 988us/step - loss: 0.2975 - acc: 0.9561 - val_loss: 0.5500 - val_acc: 0.8607\n",
            "Epoch 24/30\n",
            "2094/2094 [==============================] - 2s 986us/step - loss: 0.2969 - acc: 0.9599 - val_loss: 0.5377 - val_acc: 0.8836\n",
            "Epoch 25/30\n",
            "2094/2094 [==============================] - 2s 989us/step - loss: 0.2843 - acc: 0.9651 - val_loss: 0.5598 - val_acc: 0.8626\n",
            "Epoch 26/30\n",
            "2094/2094 [==============================] - 2s 988us/step - loss: 0.2989 - acc: 0.9589 - val_loss: 0.5107 - val_acc: 0.8798\n",
            "Epoch 27/30\n",
            "2094/2094 [==============================] - 2s 982us/step - loss: 0.3035 - acc: 0.9556 - val_loss: 0.5192 - val_acc: 0.8969\n",
            "Epoch 28/30\n",
            "2094/2094 [==============================] - 2s 988us/step - loss: 0.2962 - acc: 0.9551 - val_loss: 0.5054 - val_acc: 0.8931\n",
            "Epoch 29/30\n",
            "2094/2094 [==============================] - 2s 987us/step - loss: 0.3052 - acc: 0.9532 - val_loss: 0.5037 - val_acc: 0.8645\n",
            "Epoch 30/30\n",
            "2094/2094 [==============================] - 2s 986us/step - loss: 0.3059 - acc: 0.9499 - val_loss: 0.5923 - val_acc: 0.8244\n",
            "complete tuning on 6 subject\n",
            "Train on 2041 samples, validate on 511 samples\n",
            "Epoch 1/30\n",
            "2041/2041 [==============================] - 16s 8ms/step - loss: 0.6887 - acc: 0.8119 - val_loss: 0.5396 - val_acc: 0.8395\n",
            "Epoch 2/30\n",
            "2041/2041 [==============================] - 2s 979us/step - loss: 0.6167 - acc: 0.8447 - val_loss: 0.5234 - val_acc: 0.8845\n",
            "Epoch 3/30\n",
            "2041/2041 [==============================] - 2s 984us/step - loss: 0.5450 - acc: 0.8711 - val_loss: 0.4801 - val_acc: 0.9002\n",
            "Epoch 4/30\n",
            "2041/2041 [==============================] - 2s 983us/step - loss: 0.5237 - acc: 0.8687 - val_loss: 0.4957 - val_acc: 0.8885\n",
            "Epoch 5/30\n",
            "2041/2041 [==============================] - 2s 977us/step - loss: 0.5014 - acc: 0.8849 - val_loss: 0.4679 - val_acc: 0.9119\n",
            "Epoch 6/30\n",
            "2041/2041 [==============================] - 2s 981us/step - loss: 0.4700 - acc: 0.8966 - val_loss: 0.4605 - val_acc: 0.9100\n",
            "Epoch 7/30\n",
            "2041/2041 [==============================] - 2s 983us/step - loss: 0.4508 - acc: 0.9069 - val_loss: 0.4336 - val_acc: 0.9178\n",
            "Epoch 8/30\n",
            "2041/2041 [==============================] - 2s 976us/step - loss: 0.4171 - acc: 0.9192 - val_loss: 0.4269 - val_acc: 0.9178\n",
            "Epoch 9/30\n",
            "2041/2041 [==============================] - 2s 988us/step - loss: 0.4115 - acc: 0.9172 - val_loss: 0.4466 - val_acc: 0.8963\n",
            "Epoch 10/30\n",
            "2041/2041 [==============================] - 2s 982us/step - loss: 0.3981 - acc: 0.9216 - val_loss: 0.4918 - val_acc: 0.8924\n",
            "Epoch 11/30\n",
            "2041/2041 [==============================] - 2s 982us/step - loss: 0.3888 - acc: 0.9304 - val_loss: 0.4361 - val_acc: 0.9002\n",
            "Epoch 12/30\n",
            "2041/2041 [==============================] - 2s 974us/step - loss: 0.3868 - acc: 0.9245 - val_loss: 0.4609 - val_acc: 0.8943\n",
            "Epoch 13/30\n",
            "2041/2041 [==============================] - 2s 978us/step - loss: 0.3687 - acc: 0.9339 - val_loss: 0.4893 - val_acc: 0.8865\n",
            "Epoch 14/30\n",
            "2041/2041 [==============================] - 2s 985us/step - loss: 0.3695 - acc: 0.9339 - val_loss: 0.4719 - val_acc: 0.8845\n",
            "Epoch 15/30\n",
            "2041/2041 [==============================] - 2s 979us/step - loss: 0.3621 - acc: 0.9334 - val_loss: 0.4885 - val_acc: 0.8767\n",
            "Epoch 16/30\n",
            "2041/2041 [==============================] - 2s 980us/step - loss: 0.3573 - acc: 0.9388 - val_loss: 0.4763 - val_acc: 0.8904\n",
            "Epoch 17/30\n",
            "2041/2041 [==============================] - 2s 984us/step - loss: 0.3334 - acc: 0.9466 - val_loss: 0.4544 - val_acc: 0.8826\n",
            "Epoch 18/30\n",
            "2041/2041 [==============================] - 2s 977us/step - loss: 0.3280 - acc: 0.9451 - val_loss: 0.4561 - val_acc: 0.8845\n",
            "Epoch 19/30\n",
            "2041/2041 [==============================] - 2s 983us/step - loss: 0.3371 - acc: 0.9417 - val_loss: 0.4164 - val_acc: 0.9041\n",
            "Epoch 20/30\n",
            "2041/2041 [==============================] - 2s 985us/step - loss: 0.3410 - acc: 0.9383 - val_loss: 0.4434 - val_acc: 0.8885\n",
            "Epoch 21/30\n",
            "2041/2041 [==============================] - 2s 977us/step - loss: 0.3395 - acc: 0.9451 - val_loss: 0.4495 - val_acc: 0.8943\n",
            "Epoch 22/30\n",
            "2041/2041 [==============================] - 2s 982us/step - loss: 0.3245 - acc: 0.9500 - val_loss: 0.5007 - val_acc: 0.8904\n",
            "Epoch 23/30\n",
            "2041/2041 [==============================] - 2s 981us/step - loss: 0.3376 - acc: 0.9388 - val_loss: 0.4794 - val_acc: 0.8982\n",
            "Epoch 24/30\n",
            "2041/2041 [==============================] - 2s 976us/step - loss: 0.3467 - acc: 0.9388 - val_loss: 0.4905 - val_acc: 0.8982\n",
            "Epoch 25/30\n",
            "2041/2041 [==============================] - 2s 972us/step - loss: 0.3097 - acc: 0.9486 - val_loss: 0.4647 - val_acc: 0.8963\n",
            "Epoch 26/30\n",
            "2041/2041 [==============================] - 2s 980us/step - loss: 0.3027 - acc: 0.9520 - val_loss: 0.4439 - val_acc: 0.8924\n",
            "Epoch 27/30\n",
            "2041/2041 [==============================] - 2s 980us/step - loss: 0.3002 - acc: 0.9520 - val_loss: 0.4627 - val_acc: 0.8943\n",
            "Epoch 28/30\n",
            "2041/2041 [==============================] - 2s 983us/step - loss: 0.3092 - acc: 0.9466 - val_loss: 0.5863 - val_acc: 0.8728\n",
            "Epoch 29/30\n",
            "2041/2041 [==============================] - 2s 978us/step - loss: 0.3032 - acc: 0.9574 - val_loss: 0.4773 - val_acc: 0.8885\n",
            "Epoch 30/30\n",
            "2041/2041 [==============================] - 2s 984us/step - loss: 0.3015 - acc: 0.9505 - val_loss: 0.4475 - val_acc: 0.8943\n",
            "complete tuning on 7 subject\n",
            "Train on 2032 samples, validate on 509 samples\n",
            "Epoch 1/30\n",
            "2032/2032 [==============================] - 16s 8ms/step - loss: 0.9083 - acc: 0.7249 - val_loss: 0.9713 - val_acc: 0.7014\n",
            "Epoch 2/30\n",
            "2032/2032 [==============================] - 2s 985us/step - loss: 0.7319 - acc: 0.7859 - val_loss: 0.8439 - val_acc: 0.7505\n",
            "Epoch 3/30\n",
            "2032/2032 [==============================] - 2s 977us/step - loss: 0.6751 - acc: 0.8204 - val_loss: 0.7764 - val_acc: 0.7780\n",
            "Epoch 4/30\n",
            "2032/2032 [==============================] - 2s 987us/step - loss: 0.6206 - acc: 0.8351 - val_loss: 0.7362 - val_acc: 0.7819\n",
            "Epoch 5/30\n",
            "2032/2032 [==============================] - 2s 979us/step - loss: 0.5774 - acc: 0.8538 - val_loss: 0.7398 - val_acc: 0.7721\n",
            "Epoch 6/30\n",
            "2032/2032 [==============================] - 2s 980us/step - loss: 0.5474 - acc: 0.8583 - val_loss: 0.7505 - val_acc: 0.7760\n",
            "Epoch 7/30\n",
            "2032/2032 [==============================] - 2s 981us/step - loss: 0.5137 - acc: 0.8848 - val_loss: 0.6872 - val_acc: 0.7859\n",
            "Epoch 8/30\n",
            "2032/2032 [==============================] - 2s 980us/step - loss: 0.4917 - acc: 0.8912 - val_loss: 0.7141 - val_acc: 0.7721\n",
            "Epoch 9/30\n",
            "2032/2032 [==============================] - 2s 977us/step - loss: 0.4815 - acc: 0.8922 - val_loss: 0.6726 - val_acc: 0.7839\n",
            "Epoch 10/30\n",
            "2032/2032 [==============================] - 2s 986us/step - loss: 0.4541 - acc: 0.9109 - val_loss: 0.7226 - val_acc: 0.7780\n",
            "Epoch 11/30\n",
            "2032/2032 [==============================] - 2s 982us/step - loss: 0.4417 - acc: 0.9104 - val_loss: 0.7614 - val_acc: 0.7682\n",
            "Epoch 12/30\n",
            "2032/2032 [==============================] - 2s 983us/step - loss: 0.4252 - acc: 0.9154 - val_loss: 0.6867 - val_acc: 0.7741\n",
            "Epoch 13/30\n",
            "2032/2032 [==============================] - 2s 984us/step - loss: 0.4349 - acc: 0.9055 - val_loss: 0.6621 - val_acc: 0.7819\n",
            "Epoch 14/30\n",
            "2032/2032 [==============================] - 2s 983us/step - loss: 0.4172 - acc: 0.9144 - val_loss: 0.6544 - val_acc: 0.8016\n",
            "Epoch 15/30\n",
            "2032/2032 [==============================] - 2s 987us/step - loss: 0.3999 - acc: 0.9188 - val_loss: 0.6884 - val_acc: 0.7878\n",
            "Epoch 16/30\n",
            "2032/2032 [==============================] - 2s 979us/step - loss: 0.3802 - acc: 0.9291 - val_loss: 0.6491 - val_acc: 0.8035\n",
            "Epoch 17/30\n",
            "2032/2032 [==============================] - 2s 992us/step - loss: 0.3794 - acc: 0.9158 - val_loss: 0.6591 - val_acc: 0.8035\n",
            "Epoch 18/30\n",
            "2032/2032 [==============================] - 2s 983us/step - loss: 0.3591 - acc: 0.9355 - val_loss: 0.7119 - val_acc: 0.7760\n",
            "Epoch 19/30\n",
            "2032/2032 [==============================] - 2s 984us/step - loss: 0.3758 - acc: 0.9277 - val_loss: 0.6920 - val_acc: 0.8055\n",
            "Epoch 20/30\n",
            "2032/2032 [==============================] - 2s 982us/step - loss: 0.3780 - acc: 0.9286 - val_loss: 0.7433 - val_acc: 0.7839\n",
            "Epoch 21/30\n",
            "2032/2032 [==============================] - 2s 983us/step - loss: 0.3744 - acc: 0.9247 - val_loss: 0.8112 - val_acc: 0.7623\n",
            "Epoch 22/30\n",
            "2032/2032 [==============================] - 2s 978us/step - loss: 0.3620 - acc: 0.9350 - val_loss: 0.7027 - val_acc: 0.7819\n",
            "Epoch 23/30\n",
            "2032/2032 [==============================] - 2s 975us/step - loss: 0.3543 - acc: 0.9345 - val_loss: 0.7020 - val_acc: 0.7800\n",
            "Epoch 24/30\n",
            "2032/2032 [==============================] - 2s 984us/step - loss: 0.3447 - acc: 0.9395 - val_loss: 0.7431 - val_acc: 0.7819\n",
            "Epoch 25/30\n",
            "2032/2032 [==============================] - 2s 980us/step - loss: 0.3244 - acc: 0.9439 - val_loss: 0.7253 - val_acc: 0.7682\n",
            "Epoch 26/30\n",
            "2032/2032 [==============================] - 2s 979us/step - loss: 0.3467 - acc: 0.9365 - val_loss: 0.6792 - val_acc: 0.7701\n",
            "Epoch 27/30\n",
            "2032/2032 [==============================] - 2s 977us/step - loss: 0.3290 - acc: 0.9444 - val_loss: 0.7182 - val_acc: 0.7701\n",
            "Epoch 28/30\n",
            "2032/2032 [==============================] - 2s 984us/step - loss: 0.3357 - acc: 0.9414 - val_loss: 0.6560 - val_acc: 0.7878\n",
            "Epoch 29/30\n",
            "2032/2032 [==============================] - 2s 978us/step - loss: 0.3161 - acc: 0.9419 - val_loss: 0.6690 - val_acc: 0.7859\n",
            "Epoch 30/30\n",
            "2032/2032 [==============================] - 2s 978us/step - loss: 0.3155 - acc: 0.9464 - val_loss: 0.6573 - val_acc: 0.7878\n",
            "complete tuning on 8 subject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOPOx3SlREIb",
        "colab_type": "code",
        "outputId": "4ff3db0c-c2db-46c1-fdde-24a913cf5ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "source": [
        "print(cross_subject_accu)\n",
        "subject_id = ['baseline','subject1', 'subject2', 'subject3', 'subject4', \n",
        "              'subject5', 'subject6', 'subject7', 'subject8', 'subject9']\n",
        "plt.style.use('ggplot')\n",
        "plt.plot([0,1],[0,1])\n",
        "marktypes = ['>','x','.','^','1']\n",
        "colors = ['r','g','b']\n",
        "for i in np.arange(num_subject):\n",
        "  plt.scatter(cross_subject_accu[0,i],cross_subject_accu[1,i], \n",
        "              marker=marktypes[i%5], color=colors[i%3])\n",
        "  plt.legend(subject_id)\n",
        "\n",
        "plt.xlabel('Accuracy of DeepCNN among subject')\n",
        "plt.ylabel('Accuracy after fine tuning')\n",
        "ax = plt.axes()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.76       0.52       0.68       0.62       0.70212766 0.63265306\n",
            "  0.88       0.66       0.80851064]\n",
            " [0.8        0.52       0.8        0.58       0.70212766 0.67346939\n",
            "  0.84       0.78       0.82978723]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py:98: MatplotlibDeprecationWarning: \n",
            "Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  \"Adding an axes using the same arguments as a previous axes \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAENCAYAAAAVPvJNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJBoIQRAEQaf/B/fe/fb7/YjiAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MnBTKSX3kOc",
        "colab_type": "code",
        "outputId": "30543ae5-8f67-4413-88e9-fab2a0c806bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_s.layers[1].get_weights()[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoKI9A4J3ay6",
        "colab_type": "code",
        "outputId": "e06b22a5-44b5-4b20-8d66-768b1cb7b5df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 22, 741, 25)       275       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 22, 741, 25)       100       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_1 (GaussianNo (None, 22, 741, 25)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 741, 25)        13775     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1, 741, 25)        100       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_2 (GaussianNo (None, 1, 741, 25)        0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_1 (Spatial (None, 1, 741, 25)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1, 247, 25)        0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 247, 25, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 238, 1, 50)        12550     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 238, 1, 50)        200       \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_2 (Spatial (None, 238, 1, 50)        0         \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 238, 50, 1)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 79, 50, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 70, 1, 100)        50100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 70, 1, 100)        400       \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_3 (Spatial (None, 70, 1, 100)        0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 70, 100, 1)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 23, 100, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 21, 1, 200)        60200     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 21, 1, 200)        800       \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_4 (Spatial (None, 21, 1, 200)        0         \n",
            "_________________________________________________________________\n",
            "reshape_4 (Reshape)          (None, 21, 200, 1)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 7, 200, 1)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1400)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 5604      \n",
            "=================================================================\n",
            "Total params: 144,104\n",
            "Trainable params: 143,304\n",
            "Non-trainable params: 800\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86xz62sfa2Nr",
        "colab_type": "code",
        "outputId": "acaa0aa6-1ef1-4519-b02c-d72335ffeeb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16085
        }
      },
      "source": [
        "# load json and create model\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "pretrain_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "\n",
        "\n",
        "num_subject = np.unique(person_train_valid).shape[0]\n",
        "cross_subject_accu_50 = np.zeros((2,num_subject))\n",
        "\n",
        "for num_s in  np.arange(num_subject):\n",
        "  idx_s_train = np.where(person_train_idx_argment == num_s)[0]\n",
        "  num_train = X_train_argment[idx_s_train].shape[0]\n",
        "\n",
        "  pretrain_model = model_from_json(loaded_model_json)\n",
        "  # load weights into new model\n",
        "  pretrain_model.load_weights(\"model_deep_arg.h5\")\n",
        "  model_s = pretrain_model\n",
        "  \n",
        "  early_stop = EarlyStopping(monitor='loss', patience=15, verbose=1)\n",
        "  \n",
        "  model_s.compile(loss='categorical_crossentropy',\n",
        "                optimizer = 'adam',\n",
        "                metrics = ['accuracy'])\n",
        "  \n",
        "  idx_s_test = np.where(person_test_idx_argment == num_s)[0]\n",
        "  cross_subject_accu_50[0,num_s] = model_s.evaluate(X_test_argment[idx_s_test].reshape(-1,22,window,1), \n",
        "                                                        Y_test_argment[idx_s_test])[1]\n",
        "\n",
        "  # fine tune the model\n",
        "  model_s.fit(X_train_argment[idx_s_train].reshape(num_train,22,window,1),\n",
        "                          Y_train_argment[idx_s_train], \n",
        "                          batch_size=256, epochs = 50, validation_split = 0.2,\n",
        "                          verbose = 1)\n",
        "\n",
        "  cross_subject_accu_50[1,num_s] = model_s.evaluate(X_test_argment[idx_s_test].reshape(-1,22,window,1), \n",
        "                                                        Y_test_argment[idx_s_test])[1]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300/300 [==============================] - 6s 19ms/step\n",
            "Train on 1137 samples, validate on 285 samples\n",
            "Epoch 1/50\n",
            "1137/1137 [==============================] - 7s 6ms/step - loss: 0.6718 - acc: 0.7361 - val_loss: 0.4333 - val_acc: 0.8702\n",
            "Epoch 2/50\n",
            "1137/1137 [==============================] - 1s 793us/step - loss: 0.6022 - acc: 0.7740 - val_loss: 0.4219 - val_acc: 0.8807\n",
            "Epoch 3/50\n",
            "1137/1137 [==============================] - 1s 788us/step - loss: 0.5811 - acc: 0.7643 - val_loss: 0.4072 - val_acc: 0.8702\n",
            "Epoch 4/50\n",
            "1137/1137 [==============================] - 1s 806us/step - loss: 0.5615 - acc: 0.7748 - val_loss: 0.3928 - val_acc: 0.8772\n",
            "Epoch 5/50\n",
            "1137/1137 [==============================] - 1s 793us/step - loss: 0.5431 - acc: 0.7880 - val_loss: 0.3857 - val_acc: 0.8842\n",
            "Epoch 6/50\n",
            "1137/1137 [==============================] - 1s 792us/step - loss: 0.5538 - acc: 0.7757 - val_loss: 0.3864 - val_acc: 0.8877\n",
            "Epoch 7/50\n",
            "1137/1137 [==============================] - 1s 794us/step - loss: 0.5039 - acc: 0.8004 - val_loss: 0.3831 - val_acc: 0.8877\n",
            "Epoch 8/50\n",
            "1137/1137 [==============================] - 1s 797us/step - loss: 0.4524 - acc: 0.8329 - val_loss: 0.3946 - val_acc: 0.8702\n",
            "Epoch 9/50\n",
            "1137/1137 [==============================] - 1s 798us/step - loss: 0.4635 - acc: 0.8153 - val_loss: 0.4000 - val_acc: 0.8632\n",
            "Epoch 10/50\n",
            "1137/1137 [==============================] - 1s 792us/step - loss: 0.4573 - acc: 0.8250 - val_loss: 0.3735 - val_acc: 0.8982\n",
            "Epoch 11/50\n",
            "1137/1137 [==============================] - 1s 797us/step - loss: 0.4816 - acc: 0.8012 - val_loss: 0.3572 - val_acc: 0.9018\n",
            "Epoch 12/50\n",
            "1137/1137 [==============================] - 1s 794us/step - loss: 0.4394 - acc: 0.8338 - val_loss: 0.3786 - val_acc: 0.8877\n",
            "Epoch 13/50\n",
            "1137/1137 [==============================] - 1s 795us/step - loss: 0.4769 - acc: 0.8083 - val_loss: 0.4215 - val_acc: 0.8351\n",
            "Epoch 14/50\n",
            "1137/1137 [==============================] - 1s 798us/step - loss: 0.4685 - acc: 0.8162 - val_loss: 0.4545 - val_acc: 0.8140\n",
            "Epoch 15/50\n",
            "1137/1137 [==============================] - 1s 793us/step - loss: 0.4503 - acc: 0.8215 - val_loss: 0.4565 - val_acc: 0.8211\n",
            "Epoch 16/50\n",
            "1137/1137 [==============================] - 1s 803us/step - loss: 0.4083 - acc: 0.8320 - val_loss: 0.4112 - val_acc: 0.8561\n",
            "Epoch 17/50\n",
            "1137/1137 [==============================] - 1s 802us/step - loss: 0.4340 - acc: 0.8391 - val_loss: 0.4070 - val_acc: 0.8702\n",
            "Epoch 18/50\n",
            "1137/1137 [==============================] - 1s 799us/step - loss: 0.4238 - acc: 0.8294 - val_loss: 0.4010 - val_acc: 0.8737\n",
            "Epoch 19/50\n",
            "1137/1137 [==============================] - 1s 798us/step - loss: 0.4310 - acc: 0.8179 - val_loss: 0.3675 - val_acc: 0.8807\n",
            "Epoch 20/50\n",
            "1137/1137 [==============================] - 1s 802us/step - loss: 0.3996 - acc: 0.8382 - val_loss: 0.3393 - val_acc: 0.9158\n",
            "Epoch 21/50\n",
            "1137/1137 [==============================] - 1s 795us/step - loss: 0.4094 - acc: 0.8452 - val_loss: 0.3305 - val_acc: 0.9053\n",
            "Epoch 22/50\n",
            "1137/1137 [==============================] - 1s 800us/step - loss: 0.3925 - acc: 0.8443 - val_loss: 0.3328 - val_acc: 0.9053\n",
            "Epoch 23/50\n",
            "1137/1137 [==============================] - 1s 814us/step - loss: 0.3862 - acc: 0.8443 - val_loss: 0.3530 - val_acc: 0.8982\n",
            "Epoch 24/50\n",
            "1137/1137 [==============================] - 1s 797us/step - loss: 0.4030 - acc: 0.8294 - val_loss: 0.3916 - val_acc: 0.8526\n",
            "Epoch 25/50\n",
            "1137/1137 [==============================] - 1s 805us/step - loss: 0.3979 - acc: 0.8452 - val_loss: 0.4361 - val_acc: 0.8316\n",
            "Epoch 26/50\n",
            "1137/1137 [==============================] - 1s 801us/step - loss: 0.3904 - acc: 0.8434 - val_loss: 0.4005 - val_acc: 0.8702\n",
            "Epoch 27/50\n",
            "1137/1137 [==============================] - 1s 801us/step - loss: 0.3968 - acc: 0.8514 - val_loss: 0.3559 - val_acc: 0.8982\n",
            "Epoch 28/50\n",
            "1137/1137 [==============================] - 1s 798us/step - loss: 0.3891 - acc: 0.8540 - val_loss: 0.3713 - val_acc: 0.8842\n",
            "Epoch 29/50\n",
            "1137/1137 [==============================] - 1s 794us/step - loss: 0.3633 - acc: 0.8593 - val_loss: 0.3766 - val_acc: 0.8807\n",
            "Epoch 30/50\n",
            "1137/1137 [==============================] - 1s 796us/step - loss: 0.3718 - acc: 0.8505 - val_loss: 0.3492 - val_acc: 0.8982\n",
            "Epoch 31/50\n",
            "1137/1137 [==============================] - 1s 799us/step - loss: 0.3755 - acc: 0.8619 - val_loss: 0.3500 - val_acc: 0.8982\n",
            "Epoch 32/50\n",
            "1137/1137 [==============================] - 1s 804us/step - loss: 0.3551 - acc: 0.8637 - val_loss: 0.4071 - val_acc: 0.8772\n",
            "Epoch 33/50\n",
            "1137/1137 [==============================] - 1s 794us/step - loss: 0.3440 - acc: 0.8734 - val_loss: 0.4928 - val_acc: 0.7965\n",
            "Epoch 34/50\n",
            "1137/1137 [==============================] - 1s 797us/step - loss: 0.3275 - acc: 0.8707 - val_loss: 0.5161 - val_acc: 0.7860\n",
            "Epoch 35/50\n",
            "1137/1137 [==============================] - 1s 800us/step - loss: 0.3401 - acc: 0.8804 - val_loss: 0.4709 - val_acc: 0.8246\n",
            "Epoch 36/50\n",
            "1137/1137 [==============================] - 1s 794us/step - loss: 0.3542 - acc: 0.8540 - val_loss: 0.4184 - val_acc: 0.8421\n",
            "Epoch 37/50\n",
            "1137/1137 [==============================] - 1s 800us/step - loss: 0.3342 - acc: 0.8698 - val_loss: 0.3843 - val_acc: 0.8737\n",
            "Epoch 38/50\n",
            "1137/1137 [==============================] - 1s 789us/step - loss: 0.3388 - acc: 0.8707 - val_loss: 0.3729 - val_acc: 0.8842\n",
            "Epoch 39/50\n",
            "1137/1137 [==============================] - 1s 803us/step - loss: 0.3185 - acc: 0.8742 - val_loss: 0.3644 - val_acc: 0.8842\n",
            "Epoch 40/50\n",
            "1137/1137 [==============================] - 1s 795us/step - loss: 0.3096 - acc: 0.8839 - val_loss: 0.3388 - val_acc: 0.8877\n",
            "Epoch 41/50\n",
            "1137/1137 [==============================] - 1s 803us/step - loss: 0.3311 - acc: 0.8628 - val_loss: 0.3305 - val_acc: 0.8982\n",
            "Epoch 42/50\n",
            "1137/1137 [==============================] - 1s 801us/step - loss: 0.3286 - acc: 0.8804 - val_loss: 0.3281 - val_acc: 0.9053\n",
            "Epoch 43/50\n",
            "1137/1137 [==============================] - 1s 801us/step - loss: 0.3287 - acc: 0.8734 - val_loss: 0.3242 - val_acc: 0.9018\n",
            "Epoch 44/50\n",
            "1137/1137 [==============================] - 1s 792us/step - loss: 0.3120 - acc: 0.8830 - val_loss: 0.3229 - val_acc: 0.9088\n",
            "Epoch 45/50\n",
            "1137/1137 [==============================] - 1s 793us/step - loss: 0.2906 - acc: 0.8830 - val_loss: 0.3287 - val_acc: 0.9088\n",
            "Epoch 46/50\n",
            "1137/1137 [==============================] - 1s 794us/step - loss: 0.3278 - acc: 0.8698 - val_loss: 0.3288 - val_acc: 0.9018\n",
            "Epoch 47/50\n",
            "1137/1137 [==============================] - 1s 793us/step - loss: 0.3345 - acc: 0.8725 - val_loss: 0.3306 - val_acc: 0.8947\n",
            "Epoch 48/50\n",
            "1137/1137 [==============================] - 1s 791us/step - loss: 0.2919 - acc: 0.8918 - val_loss: 0.3154 - val_acc: 0.8982\n",
            "Epoch 49/50\n",
            "1137/1137 [==============================] - 1s 795us/step - loss: 0.3059 - acc: 0.8769 - val_loss: 0.3270 - val_acc: 0.8947\n",
            "Epoch 50/50\n",
            "1137/1137 [==============================] - 1s 799us/step - loss: 0.2953 - acc: 0.8848 - val_loss: 0.3411 - val_acc: 0.8842\n",
            "300/300 [==============================] - 0s 375us/step\n",
            "300/300 [==============================] - 6s 20ms/step\n",
            "Train on 1132 samples, validate on 284 samples\n",
            "Epoch 1/50\n",
            "1132/1132 [==============================] - 8s 7ms/step - loss: 1.1403 - acc: 0.5300 - val_loss: 1.2433 - val_acc: 0.4930\n",
            "Epoch 2/50\n",
            "1132/1132 [==============================] - 1s 794us/step - loss: 1.0375 - acc: 0.5618 - val_loss: 1.1820 - val_acc: 0.5387\n",
            "Epoch 3/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.9723 - acc: 0.6060 - val_loss: 1.1233 - val_acc: 0.5599\n",
            "Epoch 4/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.9783 - acc: 0.5989 - val_loss: 1.0812 - val_acc: 0.5634\n",
            "Epoch 5/50\n",
            "1132/1132 [==============================] - 1s 809us/step - loss: 0.8989 - acc: 0.6431 - val_loss: 1.0753 - val_acc: 0.5599\n",
            "Epoch 6/50\n",
            "1132/1132 [==============================] - 1s 793us/step - loss: 0.9405 - acc: 0.6069 - val_loss: 1.0679 - val_acc: 0.5599\n",
            "Epoch 7/50\n",
            "1132/1132 [==============================] - 1s 798us/step - loss: 0.8950 - acc: 0.6422 - val_loss: 1.0688 - val_acc: 0.5669\n",
            "Epoch 8/50\n",
            "1132/1132 [==============================] - 1s 795us/step - loss: 0.9053 - acc: 0.6095 - val_loss: 1.0757 - val_acc: 0.5599\n",
            "Epoch 9/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.8942 - acc: 0.6352 - val_loss: 1.0900 - val_acc: 0.5423\n",
            "Epoch 10/50\n",
            "1132/1132 [==============================] - 1s 794us/step - loss: 0.8442 - acc: 0.6625 - val_loss: 1.1201 - val_acc: 0.5000\n",
            "Epoch 11/50\n",
            "1132/1132 [==============================] - 1s 801us/step - loss: 0.8360 - acc: 0.6617 - val_loss: 1.1429 - val_acc: 0.4965\n",
            "Epoch 12/50\n",
            "1132/1132 [==============================] - 1s 799us/step - loss: 0.8378 - acc: 0.6811 - val_loss: 1.1455 - val_acc: 0.5000\n",
            "Epoch 13/50\n",
            "1132/1132 [==============================] - 1s 807us/step - loss: 0.7970 - acc: 0.6793 - val_loss: 1.1308 - val_acc: 0.5176\n",
            "Epoch 14/50\n",
            "1132/1132 [==============================] - 1s 793us/step - loss: 0.8514 - acc: 0.6528 - val_loss: 1.1367 - val_acc: 0.5176\n",
            "Epoch 15/50\n",
            "1132/1132 [==============================] - 1s 810us/step - loss: 0.7803 - acc: 0.6952 - val_loss: 1.1362 - val_acc: 0.5352\n",
            "Epoch 16/50\n",
            "1132/1132 [==============================] - 1s 795us/step - loss: 0.8185 - acc: 0.6935 - val_loss: 1.1417 - val_acc: 0.5246\n",
            "Epoch 17/50\n",
            "1132/1132 [==============================] - 1s 789us/step - loss: 0.7876 - acc: 0.6749 - val_loss: 1.1830 - val_acc: 0.4930\n",
            "Epoch 18/50\n",
            "1132/1132 [==============================] - 1s 800us/step - loss: 0.7794 - acc: 0.6943 - val_loss: 1.2141 - val_acc: 0.4789\n",
            "Epoch 19/50\n",
            "1132/1132 [==============================] - 1s 803us/step - loss: 0.7457 - acc: 0.6952 - val_loss: 1.1672 - val_acc: 0.5000\n",
            "Epoch 20/50\n",
            "1132/1132 [==============================] - 1s 800us/step - loss: 0.7452 - acc: 0.6943 - val_loss: 1.1146 - val_acc: 0.5246\n",
            "Epoch 21/50\n",
            "1132/1132 [==============================] - 1s 794us/step - loss: 0.7712 - acc: 0.6935 - val_loss: 1.1276 - val_acc: 0.5246\n",
            "Epoch 22/50\n",
            "1132/1132 [==============================] - 1s 803us/step - loss: 0.7936 - acc: 0.6846 - val_loss: 1.1388 - val_acc: 0.5176\n",
            "Epoch 23/50\n",
            "1132/1132 [==============================] - 1s 789us/step - loss: 0.7259 - acc: 0.7191 - val_loss: 1.1699 - val_acc: 0.5035\n",
            "Epoch 24/50\n",
            "1132/1132 [==============================] - 1s 795us/step - loss: 0.7597 - acc: 0.6899 - val_loss: 1.1785 - val_acc: 0.5035\n",
            "Epoch 25/50\n",
            "1132/1132 [==============================] - 1s 790us/step - loss: 0.7258 - acc: 0.7191 - val_loss: 1.1260 - val_acc: 0.5317\n",
            "Epoch 26/50\n",
            "1132/1132 [==============================] - 1s 791us/step - loss: 0.7234 - acc: 0.7094 - val_loss: 1.0961 - val_acc: 0.5387\n",
            "Epoch 27/50\n",
            "1132/1132 [==============================] - 1s 805us/step - loss: 0.6969 - acc: 0.7253 - val_loss: 1.1628 - val_acc: 0.5176\n",
            "Epoch 28/50\n",
            "1132/1132 [==============================] - 1s 800us/step - loss: 0.6884 - acc: 0.7332 - val_loss: 1.1656 - val_acc: 0.5246\n",
            "Epoch 29/50\n",
            "1132/1132 [==============================] - 1s 801us/step - loss: 0.7348 - acc: 0.7102 - val_loss: 1.0581 - val_acc: 0.5599\n",
            "Epoch 30/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.6617 - acc: 0.7544 - val_loss: 1.0444 - val_acc: 0.5634\n",
            "Epoch 31/50\n",
            "1132/1132 [==============================] - 1s 792us/step - loss: 0.6917 - acc: 0.7376 - val_loss: 1.0946 - val_acc: 0.5423\n",
            "Epoch 32/50\n",
            "1132/1132 [==============================] - 1s 798us/step - loss: 0.6715 - acc: 0.7138 - val_loss: 1.1915 - val_acc: 0.5176\n",
            "Epoch 33/50\n",
            "1132/1132 [==============================] - 1s 803us/step - loss: 0.7074 - acc: 0.7323 - val_loss: 1.1580 - val_acc: 0.5423\n",
            "Epoch 34/50\n",
            "1132/1132 [==============================] - 1s 802us/step - loss: 0.6593 - acc: 0.7491 - val_loss: 1.0541 - val_acc: 0.5634\n",
            "Epoch 35/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.6723 - acc: 0.7350 - val_loss: 0.9892 - val_acc: 0.6021\n",
            "Epoch 36/50\n",
            "1132/1132 [==============================] - 1s 788us/step - loss: 0.6808 - acc: 0.7261 - val_loss: 0.9675 - val_acc: 0.6092\n",
            "Epoch 37/50\n",
            "1132/1132 [==============================] - 1s 790us/step - loss: 0.6912 - acc: 0.7306 - val_loss: 1.0000 - val_acc: 0.5810\n",
            "Epoch 38/50\n",
            "1132/1132 [==============================] - 1s 793us/step - loss: 0.6370 - acc: 0.7544 - val_loss: 1.0484 - val_acc: 0.5669\n",
            "Epoch 39/50\n",
            "1132/1132 [==============================] - 1s 800us/step - loss: 0.6455 - acc: 0.7509 - val_loss: 1.0357 - val_acc: 0.5775\n",
            "Epoch 40/50\n",
            "1132/1132 [==============================] - 1s 798us/step - loss: 0.6434 - acc: 0.7527 - val_loss: 1.0370 - val_acc: 0.5599\n",
            "Epoch 41/50\n",
            "1132/1132 [==============================] - 1s 806us/step - loss: 0.5880 - acc: 0.7659 - val_loss: 1.0319 - val_acc: 0.5810\n",
            "Epoch 42/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.6414 - acc: 0.7429 - val_loss: 1.0523 - val_acc: 0.5704\n",
            "Epoch 43/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.6083 - acc: 0.7712 - val_loss: 1.0424 - val_acc: 0.5775\n",
            "Epoch 44/50\n",
            "1132/1132 [==============================] - 1s 798us/step - loss: 0.6592 - acc: 0.7420 - val_loss: 1.0541 - val_acc: 0.5810\n",
            "Epoch 45/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.6231 - acc: 0.7562 - val_loss: 1.0371 - val_acc: 0.5951\n",
            "Epoch 46/50\n",
            "1132/1132 [==============================] - 1s 794us/step - loss: 0.6236 - acc: 0.7544 - val_loss: 1.0211 - val_acc: 0.5986\n",
            "Epoch 47/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.6198 - acc: 0.7730 - val_loss: 1.0043 - val_acc: 0.6056\n",
            "Epoch 48/50\n",
            "1132/1132 [==============================] - 1s 799us/step - loss: 0.6056 - acc: 0.7739 - val_loss: 0.9876 - val_acc: 0.6021\n",
            "Epoch 49/50\n",
            "1132/1132 [==============================] - 1s 800us/step - loss: 0.5904 - acc: 0.7597 - val_loss: 0.9959 - val_acc: 0.5704\n",
            "Epoch 50/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.5948 - acc: 0.7580 - val_loss: 1.0122 - val_acc: 0.5845\n",
            "300/300 [==============================] - 0s 410us/step\n",
            "300/300 [==============================] - 6s 22ms/step\n",
            "Train on 1132 samples, validate on 284 samples\n",
            "Epoch 1/50\n",
            "1132/1132 [==============================] - 8s 7ms/step - loss: 0.6919 - acc: 0.7270 - val_loss: 0.5522 - val_acc: 0.7535\n",
            "Epoch 2/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.6265 - acc: 0.7677 - val_loss: 0.5408 - val_acc: 0.7394\n",
            "Epoch 3/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.5873 - acc: 0.7712 - val_loss: 0.5130 - val_acc: 0.7430\n",
            "Epoch 4/50\n",
            "1132/1132 [==============================] - 1s 792us/step - loss: 0.5999 - acc: 0.7739 - val_loss: 0.5016 - val_acc: 0.7500\n",
            "Epoch 5/50\n",
            "1132/1132 [==============================] - 1s 791us/step - loss: 0.5368 - acc: 0.7862 - val_loss: 0.5097 - val_acc: 0.7289\n",
            "Epoch 6/50\n",
            "1132/1132 [==============================] - 1s 791us/step - loss: 0.5370 - acc: 0.8004 - val_loss: 0.5019 - val_acc: 0.7430\n",
            "Epoch 7/50\n",
            "1132/1132 [==============================] - 1s 790us/step - loss: 0.5618 - acc: 0.7694 - val_loss: 0.4939 - val_acc: 0.7641\n",
            "Epoch 8/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.5543 - acc: 0.7862 - val_loss: 0.4883 - val_acc: 0.7641\n",
            "Epoch 9/50\n",
            "1132/1132 [==============================] - 1s 789us/step - loss: 0.5098 - acc: 0.7986 - val_loss: 0.4732 - val_acc: 0.7746\n",
            "Epoch 10/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.4906 - acc: 0.8092 - val_loss: 0.4778 - val_acc: 0.7500\n",
            "Epoch 11/50\n",
            "1132/1132 [==============================] - 1s 787us/step - loss: 0.4864 - acc: 0.8118 - val_loss: 0.4854 - val_acc: 0.7465\n",
            "Epoch 12/50\n",
            "1132/1132 [==============================] - 1s 792us/step - loss: 0.4602 - acc: 0.8313 - val_loss: 0.4851 - val_acc: 0.7746\n",
            "Epoch 13/50\n",
            "1132/1132 [==============================] - 1s 790us/step - loss: 0.5089 - acc: 0.8101 - val_loss: 0.4757 - val_acc: 0.7641\n",
            "Epoch 14/50\n",
            "1132/1132 [==============================] - 1s 787us/step - loss: 0.4548 - acc: 0.8383 - val_loss: 0.4728 - val_acc: 0.7676\n",
            "Epoch 15/50\n",
            "1132/1132 [==============================] - 1s 793us/step - loss: 0.4710 - acc: 0.8207 - val_loss: 0.4910 - val_acc: 0.7570\n",
            "Epoch 16/50\n",
            "1132/1132 [==============================] - 1s 788us/step - loss: 0.4809 - acc: 0.8180 - val_loss: 0.4896 - val_acc: 0.7430\n",
            "Epoch 17/50\n",
            "1132/1132 [==============================] - 1s 794us/step - loss: 0.4835 - acc: 0.8163 - val_loss: 0.4732 - val_acc: 0.7746\n",
            "Epoch 18/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.4753 - acc: 0.8145 - val_loss: 0.4663 - val_acc: 0.7887\n",
            "Epoch 19/50\n",
            "1132/1132 [==============================] - 1s 789us/step - loss: 0.4449 - acc: 0.8286 - val_loss: 0.4633 - val_acc: 0.7923\n",
            "Epoch 20/50\n",
            "1132/1132 [==============================] - 1s 793us/step - loss: 0.4441 - acc: 0.8242 - val_loss: 0.4595 - val_acc: 0.7817\n",
            "Epoch 21/50\n",
            "1132/1132 [==============================] - 1s 793us/step - loss: 0.4576 - acc: 0.8286 - val_loss: 0.4658 - val_acc: 0.7641\n",
            "Epoch 22/50\n",
            "1132/1132 [==============================] - 1s 792us/step - loss: 0.4496 - acc: 0.8224 - val_loss: 0.4767 - val_acc: 0.7711\n",
            "Epoch 23/50\n",
            "1132/1132 [==============================] - 1s 792us/step - loss: 0.4421 - acc: 0.8339 - val_loss: 0.4880 - val_acc: 0.7641\n",
            "Epoch 24/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.4358 - acc: 0.8401 - val_loss: 0.4660 - val_acc: 0.7782\n",
            "Epoch 25/50\n",
            "1132/1132 [==============================] - 1s 792us/step - loss: 0.4360 - acc: 0.8207 - val_loss: 0.4449 - val_acc: 0.7852\n",
            "Epoch 26/50\n",
            "1132/1132 [==============================] - 1s 787us/step - loss: 0.4297 - acc: 0.8348 - val_loss: 0.4483 - val_acc: 0.7852\n",
            "Epoch 27/50\n",
            "1132/1132 [==============================] - 1s 788us/step - loss: 0.4481 - acc: 0.8286 - val_loss: 0.4798 - val_acc: 0.7711\n",
            "Epoch 28/50\n",
            "1132/1132 [==============================] - 1s 789us/step - loss: 0.4029 - acc: 0.8542 - val_loss: 0.5088 - val_acc: 0.7535\n",
            "Epoch 29/50\n",
            "1132/1132 [==============================] - 1s 790us/step - loss: 0.4290 - acc: 0.8392 - val_loss: 0.5250 - val_acc: 0.7535\n",
            "Epoch 30/50\n",
            "1132/1132 [==============================] - 1s 792us/step - loss: 0.4680 - acc: 0.8286 - val_loss: 0.5074 - val_acc: 0.7817\n",
            "Epoch 31/50\n",
            "1132/1132 [==============================] - 1s 791us/step - loss: 0.3954 - acc: 0.8595 - val_loss: 0.4813 - val_acc: 0.7817\n",
            "Epoch 32/50\n",
            "1132/1132 [==============================] - 1s 800us/step - loss: 0.4079 - acc: 0.8560 - val_loss: 0.4735 - val_acc: 0.7852\n",
            "Epoch 33/50\n",
            "1132/1132 [==============================] - 1s 788us/step - loss: 0.3833 - acc: 0.8595 - val_loss: 0.4906 - val_acc: 0.7746\n",
            "Epoch 34/50\n",
            "1132/1132 [==============================] - 1s 791us/step - loss: 0.3968 - acc: 0.8578 - val_loss: 0.4959 - val_acc: 0.7782\n",
            "Epoch 35/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.3736 - acc: 0.8542 - val_loss: 0.4785 - val_acc: 0.7993\n",
            "Epoch 36/50\n",
            "1132/1132 [==============================] - 1s 799us/step - loss: 0.4060 - acc: 0.8481 - val_loss: 0.4580 - val_acc: 0.7887\n",
            "Epoch 37/50\n",
            "1132/1132 [==============================] - 1s 798us/step - loss: 0.3944 - acc: 0.8595 - val_loss: 0.4454 - val_acc: 0.7923\n",
            "Epoch 38/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.3704 - acc: 0.8569 - val_loss: 0.4787 - val_acc: 0.7958\n",
            "Epoch 39/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.3530 - acc: 0.8684 - val_loss: 0.5155 - val_acc: 0.7852\n",
            "Epoch 40/50\n",
            "1132/1132 [==============================] - 1s 795us/step - loss: 0.3725 - acc: 0.8693 - val_loss: 0.5221 - val_acc: 0.7852\n",
            "Epoch 41/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.4211 - acc: 0.8516 - val_loss: 0.5069 - val_acc: 0.7782\n",
            "Epoch 42/50\n",
            "1132/1132 [==============================] - 1s 804us/step - loss: 0.3947 - acc: 0.8534 - val_loss: 0.5013 - val_acc: 0.7746\n",
            "Epoch 43/50\n",
            "1132/1132 [==============================] - 1s 795us/step - loss: 0.3723 - acc: 0.8631 - val_loss: 0.5149 - val_acc: 0.7676\n",
            "Epoch 44/50\n",
            "1132/1132 [==============================] - 1s 789us/step - loss: 0.3697 - acc: 0.8498 - val_loss: 0.5148 - val_acc: 0.7676\n",
            "Epoch 45/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.3914 - acc: 0.8445 - val_loss: 0.5291 - val_acc: 0.7606\n",
            "Epoch 46/50\n",
            "1132/1132 [==============================] - 1s 795us/step - loss: 0.3514 - acc: 0.8648 - val_loss: 0.5761 - val_acc: 0.7535\n",
            "Epoch 47/50\n",
            "1132/1132 [==============================] - 1s 798us/step - loss: 0.3812 - acc: 0.8542 - val_loss: 0.5731 - val_acc: 0.7641\n",
            "Epoch 48/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.3670 - acc: 0.8657 - val_loss: 0.5238 - val_acc: 0.7711\n",
            "Epoch 49/50\n",
            "1132/1132 [==============================] - 1s 793us/step - loss: 0.3290 - acc: 0.8799 - val_loss: 0.4959 - val_acc: 0.7782\n",
            "Epoch 50/50\n",
            "1132/1132 [==============================] - 1s 794us/step - loss: 0.3562 - acc: 0.8648 - val_loss: 0.4862 - val_acc: 0.7746\n",
            "300/300 [==============================] - 0s 378us/step\n",
            "300/300 [==============================] - 7s 23ms/step\n",
            "Train on 1123 samples, validate on 281 samples\n",
            "Epoch 1/50\n",
            "1123/1123 [==============================] - 8s 8ms/step - loss: 1.0440 - acc: 0.5654 - val_loss: 1.2072 - val_acc: 0.5231\n",
            "Epoch 2/50\n",
            "1123/1123 [==============================] - 1s 799us/step - loss: 1.0185 - acc: 0.5859 - val_loss: 1.1626 - val_acc: 0.5231\n",
            "Epoch 3/50\n",
            "1123/1123 [==============================] - 1s 790us/step - loss: 0.9242 - acc: 0.6394 - val_loss: 1.1605 - val_acc: 0.5302\n",
            "Epoch 4/50\n",
            "1123/1123 [==============================] - 1s 785us/step - loss: 0.9239 - acc: 0.6322 - val_loss: 1.1712 - val_acc: 0.5302\n",
            "Epoch 5/50\n",
            "1123/1123 [==============================] - 1s 793us/step - loss: 0.8729 - acc: 0.6456 - val_loss: 1.1861 - val_acc: 0.5267\n",
            "Epoch 6/50\n",
            "1123/1123 [==============================] - 1s 800us/step - loss: 0.8262 - acc: 0.6616 - val_loss: 1.2133 - val_acc: 0.5160\n",
            "Epoch 7/50\n",
            "1123/1123 [==============================] - 1s 800us/step - loss: 0.8595 - acc: 0.6492 - val_loss: 1.2272 - val_acc: 0.5053\n",
            "Epoch 8/50\n",
            "1123/1123 [==============================] - 1s 787us/step - loss: 0.8398 - acc: 0.6652 - val_loss: 1.2042 - val_acc: 0.5231\n",
            "Epoch 9/50\n",
            "1123/1123 [==============================] - 1s 799us/step - loss: 0.8380 - acc: 0.6589 - val_loss: 1.2053 - val_acc: 0.5374\n",
            "Epoch 10/50\n",
            "1123/1123 [==============================] - 1s 798us/step - loss: 0.8625 - acc: 0.6545 - val_loss: 1.2298 - val_acc: 0.5338\n",
            "Epoch 11/50\n",
            "1123/1123 [==============================] - 1s 801us/step - loss: 0.8195 - acc: 0.6741 - val_loss: 1.2557 - val_acc: 0.5338\n",
            "Epoch 12/50\n",
            "1123/1123 [==============================] - 1s 793us/step - loss: 0.7854 - acc: 0.6874 - val_loss: 1.3339 - val_acc: 0.5053\n",
            "Epoch 13/50\n",
            "1123/1123 [==============================] - 1s 798us/step - loss: 0.7827 - acc: 0.6839 - val_loss: 1.3965 - val_acc: 0.4698\n",
            "Epoch 14/50\n",
            "1123/1123 [==============================] - 1s 787us/step - loss: 0.7753 - acc: 0.6990 - val_loss: 1.3648 - val_acc: 0.4804\n",
            "Epoch 15/50\n",
            "1123/1123 [==============================] - 1s 798us/step - loss: 0.7552 - acc: 0.6901 - val_loss: 1.3310 - val_acc: 0.4947\n",
            "Epoch 16/50\n",
            "1123/1123 [==============================] - 1s 803us/step - loss: 0.7737 - acc: 0.6955 - val_loss: 1.3015 - val_acc: 0.4982\n",
            "Epoch 17/50\n",
            "1123/1123 [==============================] - 1s 796us/step - loss: 0.7697 - acc: 0.6919 - val_loss: 1.2755 - val_acc: 0.5053\n",
            "Epoch 18/50\n",
            "1123/1123 [==============================] - 1s 792us/step - loss: 0.7678 - acc: 0.7035 - val_loss: 1.2757 - val_acc: 0.5053\n",
            "Epoch 19/50\n",
            "1123/1123 [==============================] - 1s 801us/step - loss: 0.7516 - acc: 0.7008 - val_loss: 1.2397 - val_acc: 0.5125\n",
            "Epoch 20/50\n",
            "1123/1123 [==============================] - 1s 787us/step - loss: 0.7352 - acc: 0.7231 - val_loss: 1.2513 - val_acc: 0.5125\n",
            "Epoch 21/50\n",
            "1123/1123 [==============================] - 1s 790us/step - loss: 0.7449 - acc: 0.7124 - val_loss: 1.2725 - val_acc: 0.5196\n",
            "Epoch 22/50\n",
            "1123/1123 [==============================] - 1s 787us/step - loss: 0.7257 - acc: 0.7266 - val_loss: 1.2634 - val_acc: 0.5409\n",
            "Epoch 23/50\n",
            "1123/1123 [==============================] - 1s 793us/step - loss: 0.7354 - acc: 0.7088 - val_loss: 1.2810 - val_acc: 0.5302\n",
            "Epoch 24/50\n",
            "1123/1123 [==============================] - 1s 781us/step - loss: 0.7290 - acc: 0.7106 - val_loss: 1.3145 - val_acc: 0.5196\n",
            "Epoch 25/50\n",
            "1123/1123 [==============================] - 1s 787us/step - loss: 0.7062 - acc: 0.7159 - val_loss: 1.3246 - val_acc: 0.5267\n",
            "Epoch 26/50\n",
            "1123/1123 [==============================] - 1s 787us/step - loss: 0.6782 - acc: 0.7382 - val_loss: 1.3156 - val_acc: 0.5409\n",
            "Epoch 27/50\n",
            "1123/1123 [==============================] - 1s 805us/step - loss: 0.6700 - acc: 0.7409 - val_loss: 1.3625 - val_acc: 0.5338\n",
            "Epoch 28/50\n",
            "1123/1123 [==============================] - 1s 798us/step - loss: 0.6707 - acc: 0.7346 - val_loss: 1.3802 - val_acc: 0.5338\n",
            "Epoch 29/50\n",
            "1123/1123 [==============================] - 1s 795us/step - loss: 0.6969 - acc: 0.7329 - val_loss: 1.4024 - val_acc: 0.5231\n",
            "Epoch 30/50\n",
            "1123/1123 [==============================] - 1s 788us/step - loss: 0.6712 - acc: 0.7373 - val_loss: 1.3901 - val_acc: 0.5267\n",
            "Epoch 31/50\n",
            "1123/1123 [==============================] - 1s 791us/step - loss: 0.6926 - acc: 0.7257 - val_loss: 1.3662 - val_acc: 0.5374\n",
            "Epoch 32/50\n",
            "1123/1123 [==============================] - 1s 795us/step - loss: 0.6868 - acc: 0.7382 - val_loss: 1.3871 - val_acc: 0.5196\n",
            "Epoch 33/50\n",
            "1123/1123 [==============================] - 1s 792us/step - loss: 0.6496 - acc: 0.7400 - val_loss: 1.4352 - val_acc: 0.4982\n",
            "Epoch 34/50\n",
            "1123/1123 [==============================] - 1s 787us/step - loss: 0.6561 - acc: 0.7391 - val_loss: 1.3811 - val_acc: 0.5338\n",
            "Epoch 35/50\n",
            "1123/1123 [==============================] - 1s 785us/step - loss: 0.6606 - acc: 0.7542 - val_loss: 1.3846 - val_acc: 0.5267\n",
            "Epoch 36/50\n",
            "1123/1123 [==============================] - 1s 791us/step - loss: 0.6844 - acc: 0.7373 - val_loss: 1.4088 - val_acc: 0.5267\n",
            "Epoch 37/50\n",
            "1123/1123 [==============================] - 1s 795us/step - loss: 0.6789 - acc: 0.7560 - val_loss: 1.4548 - val_acc: 0.5196\n",
            "Epoch 38/50\n",
            "1123/1123 [==============================] - 1s 790us/step - loss: 0.6317 - acc: 0.7542 - val_loss: 1.4772 - val_acc: 0.4911\n",
            "Epoch 39/50\n",
            "1123/1123 [==============================] - 1s 791us/step - loss: 0.6353 - acc: 0.7524 - val_loss: 1.4823 - val_acc: 0.4911\n",
            "Epoch 40/50\n",
            "1123/1123 [==============================] - 1s 793us/step - loss: 0.5945 - acc: 0.7622 - val_loss: 1.4753 - val_acc: 0.5160\n",
            "Epoch 41/50\n",
            "1123/1123 [==============================] - 1s 797us/step - loss: 0.5898 - acc: 0.7774 - val_loss: 1.4694 - val_acc: 0.5552\n",
            "Epoch 42/50\n",
            "1123/1123 [==============================] - 1s 791us/step - loss: 0.5957 - acc: 0.7676 - val_loss: 1.5041 - val_acc: 0.5445\n",
            "Epoch 43/50\n",
            "1123/1123 [==============================] - 1s 792us/step - loss: 0.6457 - acc: 0.7560 - val_loss: 1.6385 - val_acc: 0.5053\n",
            "Epoch 44/50\n",
            "1123/1123 [==============================] - 1s 795us/step - loss: 0.6021 - acc: 0.7747 - val_loss: 1.6852 - val_acc: 0.4982\n",
            "Epoch 45/50\n",
            "1123/1123 [==============================] - 1s 789us/step - loss: 0.6535 - acc: 0.7427 - val_loss: 1.5881 - val_acc: 0.5018\n",
            "Epoch 46/50\n",
            "1123/1123 [==============================] - 1s 798us/step - loss: 0.6217 - acc: 0.7516 - val_loss: 1.5335 - val_acc: 0.5231\n",
            "Epoch 47/50\n",
            "1123/1123 [==============================] - 1s 789us/step - loss: 0.5800 - acc: 0.7720 - val_loss: 1.4441 - val_acc: 0.5374\n",
            "Epoch 48/50\n",
            "1123/1123 [==============================] - 1s 795us/step - loss: 0.6003 - acc: 0.7738 - val_loss: 1.3861 - val_acc: 0.5516\n",
            "Epoch 49/50\n",
            "1123/1123 [==============================] - 1s 787us/step - loss: 0.5844 - acc: 0.7649 - val_loss: 1.4008 - val_acc: 0.5836\n",
            "Epoch 50/50\n",
            "1123/1123 [==============================] - 1s 799us/step - loss: 0.5715 - acc: 0.7881 - val_loss: 1.4206 - val_acc: 0.6050\n",
            "300/300 [==============================] - 0s 373us/step\n",
            "282/282 [==============================] - 7s 26ms/step\n",
            "Train on 1128 samples, validate on 282 samples\n",
            "Epoch 1/50\n",
            "1128/1128 [==============================] - 9s 8ms/step - loss: 1.0526 - acc: 0.5780 - val_loss: 1.0479 - val_acc: 0.6312\n",
            "Epoch 2/50\n",
            "1128/1128 [==============================] - 1s 808us/step - loss: 0.8881 - acc: 0.6543 - val_loss: 0.9891 - val_acc: 0.6702\n",
            "Epoch 3/50\n",
            "1128/1128 [==============================] - 1s 788us/step - loss: 0.8157 - acc: 0.6800 - val_loss: 0.8915 - val_acc: 0.6631\n",
            "Epoch 4/50\n",
            "1128/1128 [==============================] - 1s 791us/step - loss: 0.7729 - acc: 0.7048 - val_loss: 0.8581 - val_acc: 0.6702\n",
            "Epoch 5/50\n",
            "1128/1128 [==============================] - 1s 797us/step - loss: 0.7752 - acc: 0.6924 - val_loss: 0.8601 - val_acc: 0.6986\n",
            "Epoch 6/50\n",
            "1128/1128 [==============================] - 1s 795us/step - loss: 0.7388 - acc: 0.7110 - val_loss: 0.8769 - val_acc: 0.6773\n",
            "Epoch 7/50\n",
            "1128/1128 [==============================] - 1s 793us/step - loss: 0.6844 - acc: 0.7340 - val_loss: 0.8767 - val_acc: 0.6667\n",
            "Epoch 8/50\n",
            "1128/1128 [==============================] - 1s 801us/step - loss: 0.7078 - acc: 0.7287 - val_loss: 0.8623 - val_acc: 0.6702\n",
            "Epoch 9/50\n",
            "1128/1128 [==============================] - 1s 788us/step - loss: 0.6787 - acc: 0.7349 - val_loss: 0.8546 - val_acc: 0.6844\n",
            "Epoch 10/50\n",
            "1128/1128 [==============================] - 1s 795us/step - loss: 0.6923 - acc: 0.7216 - val_loss: 0.8536 - val_acc: 0.6950\n",
            "Epoch 11/50\n",
            "1128/1128 [==============================] - 1s 795us/step - loss: 0.6419 - acc: 0.7527 - val_loss: 0.8328 - val_acc: 0.6915\n",
            "Epoch 12/50\n",
            "1128/1128 [==============================] - 1s 797us/step - loss: 0.6474 - acc: 0.7677 - val_loss: 0.8223 - val_acc: 0.6915\n",
            "Epoch 13/50\n",
            "1128/1128 [==============================] - 1s 797us/step - loss: 0.6580 - acc: 0.7385 - val_loss: 0.8259 - val_acc: 0.6915\n",
            "Epoch 14/50\n",
            "1128/1128 [==============================] - 1s 786us/step - loss: 0.6293 - acc: 0.7562 - val_loss: 0.8184 - val_acc: 0.6915\n",
            "Epoch 15/50\n",
            "1128/1128 [==============================] - 1s 793us/step - loss: 0.6165 - acc: 0.7571 - val_loss: 0.8177 - val_acc: 0.6986\n",
            "Epoch 16/50\n",
            "1128/1128 [==============================] - 1s 796us/step - loss: 0.6278 - acc: 0.7544 - val_loss: 0.8674 - val_acc: 0.6915\n",
            "Epoch 17/50\n",
            "1128/1128 [==============================] - 1s 796us/step - loss: 0.6149 - acc: 0.7739 - val_loss: 0.8839 - val_acc: 0.6667\n",
            "Epoch 18/50\n",
            "1128/1128 [==============================] - 1s 806us/step - loss: 0.6227 - acc: 0.7527 - val_loss: 0.9431 - val_acc: 0.6348\n",
            "Epoch 19/50\n",
            "1128/1128 [==============================] - 1s 803us/step - loss: 0.5612 - acc: 0.7828 - val_loss: 1.0301 - val_acc: 0.6170\n",
            "Epoch 20/50\n",
            "1128/1128 [==============================] - 1s 798us/step - loss: 0.6015 - acc: 0.7793 - val_loss: 1.1307 - val_acc: 0.5957\n",
            "Epoch 21/50\n",
            "1128/1128 [==============================] - 1s 799us/step - loss: 0.5914 - acc: 0.7881 - val_loss: 1.1784 - val_acc: 0.5957\n",
            "Epoch 22/50\n",
            "1128/1128 [==============================] - 1s 802us/step - loss: 0.5731 - acc: 0.7819 - val_loss: 1.0932 - val_acc: 0.6135\n",
            "Epoch 23/50\n",
            "1128/1128 [==============================] - 1s 801us/step - loss: 0.5308 - acc: 0.8023 - val_loss: 1.0312 - val_acc: 0.6241\n",
            "Epoch 24/50\n",
            "1128/1128 [==============================] - 1s 801us/step - loss: 0.5608 - acc: 0.7757 - val_loss: 1.0126 - val_acc: 0.6348\n",
            "Epoch 25/50\n",
            "1128/1128 [==============================] - 1s 795us/step - loss: 0.5282 - acc: 0.7996 - val_loss: 1.0215 - val_acc: 0.6454\n",
            "Epoch 26/50\n",
            "1128/1128 [==============================] - 1s 793us/step - loss: 0.5148 - acc: 0.8023 - val_loss: 1.0568 - val_acc: 0.6241\n",
            "Epoch 27/50\n",
            "1128/1128 [==============================] - 1s 790us/step - loss: 0.5299 - acc: 0.7926 - val_loss: 1.0479 - val_acc: 0.6028\n",
            "Epoch 28/50\n",
            "1128/1128 [==============================] - 1s 793us/step - loss: 0.5284 - acc: 0.7996 - val_loss: 1.0585 - val_acc: 0.6241\n",
            "Epoch 29/50\n",
            "1128/1128 [==============================] - 1s 793us/step - loss: 0.5196 - acc: 0.8023 - val_loss: 1.0210 - val_acc: 0.6454\n",
            "Epoch 30/50\n",
            "1128/1128 [==============================] - 1s 794us/step - loss: 0.4954 - acc: 0.8094 - val_loss: 1.0289 - val_acc: 0.6348\n",
            "Epoch 31/50\n",
            "1128/1128 [==============================] - 1s 799us/step - loss: 0.5042 - acc: 0.8059 - val_loss: 1.0474 - val_acc: 0.6241\n",
            "Epoch 32/50\n",
            "1128/1128 [==============================] - 1s 792us/step - loss: 0.5017 - acc: 0.8023 - val_loss: 1.0646 - val_acc: 0.6241\n",
            "Epoch 33/50\n",
            "1128/1128 [==============================] - 1s 797us/step - loss: 0.4858 - acc: 0.8147 - val_loss: 1.0612 - val_acc: 0.6170\n",
            "Epoch 34/50\n",
            "1128/1128 [==============================] - 1s 791us/step - loss: 0.4869 - acc: 0.8129 - val_loss: 1.0419 - val_acc: 0.6206\n",
            "Epoch 35/50\n",
            "1128/1128 [==============================] - 1s 793us/step - loss: 0.4934 - acc: 0.8165 - val_loss: 1.0941 - val_acc: 0.6206\n",
            "Epoch 36/50\n",
            "1128/1128 [==============================] - 1s 795us/step - loss: 0.4934 - acc: 0.8076 - val_loss: 1.0631 - val_acc: 0.6383\n",
            "Epoch 37/50\n",
            "1128/1128 [==============================] - 1s 793us/step - loss: 0.4688 - acc: 0.8289 - val_loss: 1.1137 - val_acc: 0.6348\n",
            "Epoch 38/50\n",
            "1128/1128 [==============================] - 1s 791us/step - loss: 0.4973 - acc: 0.8094 - val_loss: 1.1254 - val_acc: 0.6383\n",
            "Epoch 39/50\n",
            "1128/1128 [==============================] - 1s 795us/step - loss: 0.4829 - acc: 0.8289 - val_loss: 1.0770 - val_acc: 0.6383\n",
            "Epoch 40/50\n",
            "1128/1128 [==============================] - 1s 805us/step - loss: 0.4800 - acc: 0.8165 - val_loss: 1.0367 - val_acc: 0.6809\n",
            "Epoch 41/50\n",
            "1128/1128 [==============================] - 1s 792us/step - loss: 0.4882 - acc: 0.8200 - val_loss: 1.0260 - val_acc: 0.6915\n",
            "Epoch 42/50\n",
            "1128/1128 [==============================] - 1s 797us/step - loss: 0.4737 - acc: 0.8174 - val_loss: 1.0218 - val_acc: 0.6596\n",
            "Epoch 43/50\n",
            "1128/1128 [==============================] - 1s 799us/step - loss: 0.4554 - acc: 0.8387 - val_loss: 1.0311 - val_acc: 0.6525\n",
            "Epoch 44/50\n",
            "1128/1128 [==============================] - 1s 802us/step - loss: 0.4402 - acc: 0.8395 - val_loss: 1.0236 - val_acc: 0.6667\n",
            "Epoch 45/50\n",
            "1128/1128 [==============================] - 1s 799us/step - loss: 0.4596 - acc: 0.8262 - val_loss: 1.0816 - val_acc: 0.6525\n",
            "Epoch 46/50\n",
            "1128/1128 [==============================] - 1s 799us/step - loss: 0.4564 - acc: 0.8307 - val_loss: 1.1056 - val_acc: 0.6454\n",
            "Epoch 47/50\n",
            "1128/1128 [==============================] - 1s 809us/step - loss: 0.4146 - acc: 0.8395 - val_loss: 1.0726 - val_acc: 0.6489\n",
            "Epoch 48/50\n",
            "1128/1128 [==============================] - 1s 797us/step - loss: 0.4424 - acc: 0.8449 - val_loss: 1.0542 - val_acc: 0.6596\n",
            "Epoch 49/50\n",
            "1128/1128 [==============================] - 1s 797us/step - loss: 0.4407 - acc: 0.8387 - val_loss: 1.1080 - val_acc: 0.6454\n",
            "Epoch 50/50\n",
            "1128/1128 [==============================] - 1s 791us/step - loss: 0.4135 - acc: 0.8413 - val_loss: 1.1130 - val_acc: 0.6631\n",
            "282/282 [==============================] - 0s 394us/step\n",
            "294/294 [==============================] - 8s 27ms/step\n",
            "Train on 1132 samples, validate on 284 samples\n",
            "Epoch 1/50\n",
            "1132/1132 [==============================] - 9s 8ms/step - loss: 1.0002 - acc: 0.6069 - val_loss: 0.9218 - val_acc: 0.7289\n",
            "Epoch 2/50\n",
            "1132/1132 [==============================] - 1s 793us/step - loss: 0.9041 - acc: 0.6528 - val_loss: 0.9600 - val_acc: 0.7254\n",
            "Epoch 3/50\n",
            "1132/1132 [==============================] - 1s 801us/step - loss: 0.8782 - acc: 0.6661 - val_loss: 0.9849 - val_acc: 0.7289\n",
            "Epoch 4/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.8296 - acc: 0.6723 - val_loss: 0.9610 - val_acc: 0.7324\n",
            "Epoch 5/50\n",
            "1132/1132 [==============================] - 1s 790us/step - loss: 0.7939 - acc: 0.6837 - val_loss: 0.9371 - val_acc: 0.7324\n",
            "Epoch 6/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.8141 - acc: 0.6979 - val_loss: 0.9189 - val_acc: 0.7359\n",
            "Epoch 7/50\n",
            "1132/1132 [==============================] - 1s 793us/step - loss: 0.7782 - acc: 0.7005 - val_loss: 0.9238 - val_acc: 0.7254\n",
            "Epoch 8/50\n",
            "1132/1132 [==============================] - 1s 793us/step - loss: 0.7432 - acc: 0.7067 - val_loss: 0.9469 - val_acc: 0.7183\n",
            "Epoch 9/50\n",
            "1132/1132 [==============================] - 1s 800us/step - loss: 0.7515 - acc: 0.7032 - val_loss: 0.9706 - val_acc: 0.7148\n",
            "Epoch 10/50\n",
            "1132/1132 [==============================] - 1s 795us/step - loss: 0.7505 - acc: 0.7076 - val_loss: 0.9490 - val_acc: 0.7113\n",
            "Epoch 11/50\n",
            "1132/1132 [==============================] - 1s 788us/step - loss: 0.7396 - acc: 0.7014 - val_loss: 0.9158 - val_acc: 0.7500\n",
            "Epoch 12/50\n",
            "1132/1132 [==============================] - 1s 794us/step - loss: 0.7030 - acc: 0.7314 - val_loss: 0.9257 - val_acc: 0.7606\n",
            "Epoch 13/50\n",
            "1132/1132 [==============================] - 1s 789us/step - loss: 0.6979 - acc: 0.7447 - val_loss: 0.9415 - val_acc: 0.7570\n",
            "Epoch 14/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.7275 - acc: 0.7111 - val_loss: 0.9326 - val_acc: 0.7113\n",
            "Epoch 15/50\n",
            "1132/1132 [==============================] - 1s 793us/step - loss: 0.7087 - acc: 0.7208 - val_loss: 0.8920 - val_acc: 0.7394\n",
            "Epoch 16/50\n",
            "1132/1132 [==============================] - 1s 791us/step - loss: 0.6452 - acc: 0.7482 - val_loss: 0.8772 - val_acc: 0.7500\n",
            "Epoch 17/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.7073 - acc: 0.7270 - val_loss: 0.8875 - val_acc: 0.7430\n",
            "Epoch 18/50\n",
            "1132/1132 [==============================] - 1s 793us/step - loss: 0.6661 - acc: 0.7447 - val_loss: 0.9053 - val_acc: 0.7711\n",
            "Epoch 19/50\n",
            "1132/1132 [==============================] - 1s 794us/step - loss: 0.6854 - acc: 0.7085 - val_loss: 0.9138 - val_acc: 0.7746\n",
            "Epoch 20/50\n",
            "1132/1132 [==============================] - 1s 795us/step - loss: 0.6648 - acc: 0.7420 - val_loss: 0.9140 - val_acc: 0.7887\n",
            "Epoch 21/50\n",
            "1132/1132 [==============================] - 1s 800us/step - loss: 0.6676 - acc: 0.7341 - val_loss: 0.9170 - val_acc: 0.7782\n",
            "Epoch 22/50\n",
            "1132/1132 [==============================] - 1s 792us/step - loss: 0.6254 - acc: 0.7615 - val_loss: 0.9244 - val_acc: 0.7817\n",
            "Epoch 23/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.6343 - acc: 0.7544 - val_loss: 0.9236 - val_acc: 0.7746\n",
            "Epoch 24/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.6297 - acc: 0.7482 - val_loss: 0.9297 - val_acc: 0.7782\n",
            "Epoch 25/50\n",
            "1132/1132 [==============================] - 1s 797us/step - loss: 0.6474 - acc: 0.7412 - val_loss: 0.8995 - val_acc: 0.7606\n",
            "Epoch 26/50\n",
            "1132/1132 [==============================] - 1s 798us/step - loss: 0.6197 - acc: 0.7739 - val_loss: 0.8918 - val_acc: 0.7676\n",
            "Epoch 27/50\n",
            "1132/1132 [==============================] - 1s 795us/step - loss: 0.6247 - acc: 0.7500 - val_loss: 0.8927 - val_acc: 0.7676\n",
            "Epoch 28/50\n",
            "1132/1132 [==============================] - 1s 795us/step - loss: 0.5982 - acc: 0.7747 - val_loss: 0.9160 - val_acc: 0.7711\n",
            "Epoch 29/50\n",
            "1132/1132 [==============================] - 1s 800us/step - loss: 0.6046 - acc: 0.7686 - val_loss: 0.9346 - val_acc: 0.7676\n",
            "Epoch 30/50\n",
            "1132/1132 [==============================] - 1s 793us/step - loss: 0.6030 - acc: 0.7686 - val_loss: 0.9559 - val_acc: 0.7676\n",
            "Epoch 31/50\n",
            "1132/1132 [==============================] - 1s 794us/step - loss: 0.5700 - acc: 0.7915 - val_loss: 0.9635 - val_acc: 0.7606\n",
            "Epoch 32/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.5683 - acc: 0.7756 - val_loss: 0.9501 - val_acc: 0.7570\n",
            "Epoch 33/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.5718 - acc: 0.7889 - val_loss: 0.9255 - val_acc: 0.7430\n",
            "Epoch 34/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.5947 - acc: 0.7712 - val_loss: 0.9171 - val_acc: 0.7535\n",
            "Epoch 35/50\n",
            "1132/1132 [==============================] - 1s 791us/step - loss: 0.5418 - acc: 0.8030 - val_loss: 0.9386 - val_acc: 0.7570\n",
            "Epoch 36/50\n",
            "1132/1132 [==============================] - 1s 792us/step - loss: 0.5963 - acc: 0.7703 - val_loss: 0.9683 - val_acc: 0.7641\n",
            "Epoch 37/50\n",
            "1132/1132 [==============================] - 1s 794us/step - loss: 0.5688 - acc: 0.7792 - val_loss: 0.9879 - val_acc: 0.7641\n",
            "Epoch 38/50\n",
            "1132/1132 [==============================] - 1s 796us/step - loss: 0.5964 - acc: 0.7659 - val_loss: 0.9840 - val_acc: 0.7570\n",
            "Epoch 39/50\n",
            "1132/1132 [==============================] - 1s 801us/step - loss: 0.5827 - acc: 0.7871 - val_loss: 0.9948 - val_acc: 0.7606\n",
            "Epoch 40/50\n",
            "1132/1132 [==============================] - 1s 791us/step - loss: 0.5585 - acc: 0.7871 - val_loss: 1.0010 - val_acc: 0.7535\n",
            "Epoch 41/50\n",
            "1132/1132 [==============================] - 1s 798us/step - loss: 0.5456 - acc: 0.7959 - val_loss: 0.9947 - val_acc: 0.7430\n",
            "Epoch 42/50\n",
            "1132/1132 [==============================] - 1s 789us/step - loss: 0.5337 - acc: 0.8030 - val_loss: 0.9734 - val_acc: 0.7430\n",
            "Epoch 43/50\n",
            "1132/1132 [==============================] - 1s 801us/step - loss: 0.5342 - acc: 0.7986 - val_loss: 0.9794 - val_acc: 0.7606\n",
            "Epoch 44/50\n",
            "1132/1132 [==============================] - 1s 801us/step - loss: 0.5192 - acc: 0.8065 - val_loss: 0.9604 - val_acc: 0.7641\n",
            "Epoch 45/50\n",
            "1132/1132 [==============================] - 1s 789us/step - loss: 0.5345 - acc: 0.7986 - val_loss: 0.9493 - val_acc: 0.7606\n",
            "Epoch 46/50\n",
            "1132/1132 [==============================] - 1s 794us/step - loss: 0.5202 - acc: 0.8065 - val_loss: 0.9511 - val_acc: 0.7641\n",
            "Epoch 47/50\n",
            "1132/1132 [==============================] - 1s 789us/step - loss: 0.5268 - acc: 0.8083 - val_loss: 0.9573 - val_acc: 0.7606\n",
            "Epoch 48/50\n",
            "1132/1132 [==============================] - 1s 794us/step - loss: 0.5316 - acc: 0.7924 - val_loss: 0.9301 - val_acc: 0.7641\n",
            "Epoch 49/50\n",
            "1132/1132 [==============================] - 1s 798us/step - loss: 0.5604 - acc: 0.7871 - val_loss: 0.9219 - val_acc: 0.7500\n",
            "Epoch 50/50\n",
            "1132/1132 [==============================] - 1s 794us/step - loss: 0.5359 - acc: 0.8057 - val_loss: 0.9253 - val_acc: 0.7570\n",
            "294/294 [==============================] - 0s 370us/step\n",
            "300/300 [==============================] - 8s 27ms/step\n",
            "Train on 1142 samples, validate on 286 samples\n",
            "Epoch 1/50\n",
            "1142/1142 [==============================] - 10s 9ms/step - loss: 0.7280 - acc: 0.7285 - val_loss: 0.5860 - val_acc: 0.7657\n",
            "Epoch 2/50\n",
            "1142/1142 [==============================] - 1s 797us/step - loss: 0.6662 - acc: 0.7618 - val_loss: 0.5452 - val_acc: 0.7797\n",
            "Epoch 3/50\n",
            "1142/1142 [==============================] - 1s 803us/step - loss: 0.6265 - acc: 0.7618 - val_loss: 0.5294 - val_acc: 0.7692\n",
            "Epoch 4/50\n",
            "1142/1142 [==============================] - 1s 801us/step - loss: 0.5438 - acc: 0.8021 - val_loss: 0.5281 - val_acc: 0.7902\n",
            "Epoch 5/50\n",
            "1142/1142 [==============================] - 1s 794us/step - loss: 0.5471 - acc: 0.7925 - val_loss: 0.5457 - val_acc: 0.7902\n",
            "Epoch 6/50\n",
            "1142/1142 [==============================] - 1s 795us/step - loss: 0.5257 - acc: 0.8144 - val_loss: 0.5689 - val_acc: 0.7657\n",
            "Epoch 7/50\n",
            "1142/1142 [==============================] - 1s 792us/step - loss: 0.5291 - acc: 0.8047 - val_loss: 0.6006 - val_acc: 0.7692\n",
            "Epoch 8/50\n",
            "1142/1142 [==============================] - 1s 791us/step - loss: 0.4989 - acc: 0.8135 - val_loss: 0.5809 - val_acc: 0.7622\n",
            "Epoch 9/50\n",
            "1142/1142 [==============================] - 1s 792us/step - loss: 0.4905 - acc: 0.8205 - val_loss: 0.5280 - val_acc: 0.7832\n",
            "Epoch 10/50\n",
            "1142/1142 [==============================] - 1s 793us/step - loss: 0.4461 - acc: 0.8424 - val_loss: 0.4978 - val_acc: 0.7867\n",
            "Epoch 11/50\n",
            "1142/1142 [==============================] - 1s 793us/step - loss: 0.4488 - acc: 0.8371 - val_loss: 0.4905 - val_acc: 0.7797\n",
            "Epoch 12/50\n",
            "1142/1142 [==============================] - 1s 795us/step - loss: 0.4305 - acc: 0.8406 - val_loss: 0.4920 - val_acc: 0.7867\n",
            "Epoch 13/50\n",
            "1142/1142 [==============================] - 1s 799us/step - loss: 0.4472 - acc: 0.8319 - val_loss: 0.5131 - val_acc: 0.7797\n",
            "Epoch 14/50\n",
            "1142/1142 [==============================] - 1s 797us/step - loss: 0.4028 - acc: 0.8485 - val_loss: 0.5179 - val_acc: 0.7797\n",
            "Epoch 15/50\n",
            "1142/1142 [==============================] - 1s 794us/step - loss: 0.4292 - acc: 0.8354 - val_loss: 0.5097 - val_acc: 0.7902\n",
            "Epoch 16/50\n",
            "1142/1142 [==============================] - 1s 794us/step - loss: 0.4046 - acc: 0.8441 - val_loss: 0.5257 - val_acc: 0.7972\n",
            "Epoch 17/50\n",
            "1142/1142 [==============================] - 1s 798us/step - loss: 0.3844 - acc: 0.8625 - val_loss: 0.5300 - val_acc: 0.8042\n",
            "Epoch 18/50\n",
            "1142/1142 [==============================] - 1s 796us/step - loss: 0.3910 - acc: 0.8581 - val_loss: 0.5059 - val_acc: 0.8042\n",
            "Epoch 19/50\n",
            "1142/1142 [==============================] - 1s 798us/step - loss: 0.4085 - acc: 0.8468 - val_loss: 0.5092 - val_acc: 0.8042\n",
            "Epoch 20/50\n",
            "1142/1142 [==============================] - 1s 795us/step - loss: 0.3818 - acc: 0.8643 - val_loss: 0.5145 - val_acc: 0.7902\n",
            "Epoch 21/50\n",
            "1142/1142 [==============================] - 1s 803us/step - loss: 0.3560 - acc: 0.8678 - val_loss: 0.5537 - val_acc: 0.7832\n",
            "Epoch 22/50\n",
            "1142/1142 [==============================] - 1s 795us/step - loss: 0.3342 - acc: 0.8757 - val_loss: 0.5580 - val_acc: 0.7832\n",
            "Epoch 23/50\n",
            "1142/1142 [==============================] - 1s 799us/step - loss: 0.3477 - acc: 0.8748 - val_loss: 0.5563 - val_acc: 0.7867\n",
            "Epoch 24/50\n",
            "1142/1142 [==============================] - 1s 795us/step - loss: 0.3588 - acc: 0.8687 - val_loss: 0.5641 - val_acc: 0.7797\n",
            "Epoch 25/50\n",
            "1142/1142 [==============================] - 1s 795us/step - loss: 0.3529 - acc: 0.8713 - val_loss: 0.5835 - val_acc: 0.7657\n",
            "Epoch 26/50\n",
            "1142/1142 [==============================] - 1s 789us/step - loss: 0.3214 - acc: 0.8940 - val_loss: 0.5941 - val_acc: 0.7622\n",
            "Epoch 27/50\n",
            "1142/1142 [==============================] - 1s 789us/step - loss: 0.3431 - acc: 0.8809 - val_loss: 0.5916 - val_acc: 0.7622\n",
            "Epoch 28/50\n",
            "1142/1142 [==============================] - 1s 807us/step - loss: 0.3095 - acc: 0.8923 - val_loss: 0.5783 - val_acc: 0.7867\n",
            "Epoch 29/50\n",
            "1142/1142 [==============================] - 1s 791us/step - loss: 0.3645 - acc: 0.8757 - val_loss: 0.6074 - val_acc: 0.7797\n",
            "Epoch 30/50\n",
            "1142/1142 [==============================] - 1s 793us/step - loss: 0.3127 - acc: 0.8853 - val_loss: 0.6019 - val_acc: 0.7587\n",
            "Epoch 31/50\n",
            "1142/1142 [==============================] - 1s 794us/step - loss: 0.3028 - acc: 0.8967 - val_loss: 0.5518 - val_acc: 0.7867\n",
            "Epoch 32/50\n",
            "1142/1142 [==============================] - 1s 789us/step - loss: 0.3156 - acc: 0.8897 - val_loss: 0.5452 - val_acc: 0.7832\n",
            "Epoch 33/50\n",
            "1142/1142 [==============================] - 1s 789us/step - loss: 0.3113 - acc: 0.8923 - val_loss: 0.6054 - val_acc: 0.7587\n",
            "Epoch 34/50\n",
            "1142/1142 [==============================] - 1s 791us/step - loss: 0.2958 - acc: 0.8774 - val_loss: 0.6277 - val_acc: 0.7657\n",
            "Epoch 35/50\n",
            "1142/1142 [==============================] - 1s 807us/step - loss: 0.3083 - acc: 0.8958 - val_loss: 0.5842 - val_acc: 0.7762\n",
            "Epoch 36/50\n",
            "1142/1142 [==============================] - 1s 800us/step - loss: 0.2870 - acc: 0.8940 - val_loss: 0.5332 - val_acc: 0.7972\n",
            "Epoch 37/50\n",
            "1142/1142 [==============================] - 1s 794us/step - loss: 0.3033 - acc: 0.8835 - val_loss: 0.5225 - val_acc: 0.7937\n",
            "Epoch 38/50\n",
            "1142/1142 [==============================] - 1s 788us/step - loss: 0.2826 - acc: 0.9028 - val_loss: 0.5205 - val_acc: 0.7867\n",
            "Epoch 39/50\n",
            "1142/1142 [==============================] - 1s 798us/step - loss: 0.2920 - acc: 0.9011 - val_loss: 0.5064 - val_acc: 0.8007\n",
            "Epoch 40/50\n",
            "1142/1142 [==============================] - 1s 796us/step - loss: 0.2790 - acc: 0.9054 - val_loss: 0.5129 - val_acc: 0.8007\n",
            "Epoch 41/50\n",
            "1142/1142 [==============================] - 1s 796us/step - loss: 0.2871 - acc: 0.9046 - val_loss: 0.5415 - val_acc: 0.7937\n",
            "Epoch 42/50\n",
            "1142/1142 [==============================] - 1s 797us/step - loss: 0.2831 - acc: 0.8984 - val_loss: 0.5355 - val_acc: 0.7867\n",
            "Epoch 43/50\n",
            "1142/1142 [==============================] - 1s 790us/step - loss: 0.2653 - acc: 0.9089 - val_loss: 0.5644 - val_acc: 0.7832\n",
            "Epoch 44/50\n",
            "1142/1142 [==============================] - 1s 800us/step - loss: 0.2577 - acc: 0.9133 - val_loss: 0.5649 - val_acc: 0.7832\n",
            "Epoch 45/50\n",
            "1142/1142 [==============================] - 1s 797us/step - loss: 0.2549 - acc: 0.9107 - val_loss: 0.5736 - val_acc: 0.7902\n",
            "Epoch 46/50\n",
            "1142/1142 [==============================] - 1s 795us/step - loss: 0.2534 - acc: 0.9142 - val_loss: 0.5176 - val_acc: 0.8042\n",
            "Epoch 47/50\n",
            "1142/1142 [==============================] - 1s 797us/step - loss: 0.2557 - acc: 0.9054 - val_loss: 0.4880 - val_acc: 0.8007\n",
            "Epoch 48/50\n",
            "1142/1142 [==============================] - 1s 794us/step - loss: 0.2636 - acc: 0.9063 - val_loss: 0.4823 - val_acc: 0.8042\n",
            "Epoch 49/50\n",
            "1142/1142 [==============================] - 1s 794us/step - loss: 0.2437 - acc: 0.9098 - val_loss: 0.4819 - val_acc: 0.7972\n",
            "Epoch 50/50\n",
            "1142/1142 [==============================] - 1s 790us/step - loss: 0.2374 - acc: 0.9116 - val_loss: 0.4680 - val_acc: 0.8077\n",
            "300/300 [==============================] - 0s 374us/step\n",
            "300/300 [==============================] - 9s 30ms/step\n",
            "Train on 1113 samples, validate on 279 samples\n",
            "Epoch 1/50\n",
            "1113/1113 [==============================] - 10s 9ms/step - loss: 0.7139 - acc: 0.7035 - val_loss: 0.5127 - val_acc: 0.8029\n",
            "Epoch 2/50\n",
            "1113/1113 [==============================] - 1s 794us/step - loss: 0.6112 - acc: 0.7700 - val_loss: 0.4732 - val_acc: 0.8065\n",
            "Epoch 3/50\n",
            "1113/1113 [==============================] - 1s 794us/step - loss: 0.5823 - acc: 0.7817 - val_loss: 0.4459 - val_acc: 0.8029\n",
            "Epoch 4/50\n",
            "1113/1113 [==============================] - 1s 795us/step - loss: 0.5773 - acc: 0.7781 - val_loss: 0.4363 - val_acc: 0.7814\n",
            "Epoch 5/50\n",
            "1113/1113 [==============================] - 1s 784us/step - loss: 0.5496 - acc: 0.7960 - val_loss: 0.4289 - val_acc: 0.7849\n",
            "Epoch 6/50\n",
            "1113/1113 [==============================] - 1s 800us/step - loss: 0.5407 - acc: 0.7960 - val_loss: 0.4284 - val_acc: 0.7957\n",
            "Epoch 7/50\n",
            "1113/1113 [==============================] - 1s 803us/step - loss: 0.5175 - acc: 0.8068 - val_loss: 0.4432 - val_acc: 0.8065\n",
            "Epoch 8/50\n",
            "1113/1113 [==============================] - 1s 798us/step - loss: 0.5197 - acc: 0.8050 - val_loss: 0.4792 - val_acc: 0.8172\n",
            "Epoch 9/50\n",
            "1113/1113 [==============================] - 1s 803us/step - loss: 0.5026 - acc: 0.8203 - val_loss: 0.4913 - val_acc: 0.8136\n",
            "Epoch 10/50\n",
            "1113/1113 [==============================] - 1s 796us/step - loss: 0.5050 - acc: 0.8158 - val_loss: 0.4467 - val_acc: 0.8315\n",
            "Epoch 11/50\n",
            "1113/1113 [==============================] - 1s 805us/step - loss: 0.4751 - acc: 0.8230 - val_loss: 0.4175 - val_acc: 0.8315\n",
            "Epoch 12/50\n",
            "1113/1113 [==============================] - 1s 798us/step - loss: 0.4694 - acc: 0.8284 - val_loss: 0.3874 - val_acc: 0.8315\n",
            "Epoch 13/50\n",
            "1113/1113 [==============================] - 1s 795us/step - loss: 0.4758 - acc: 0.8176 - val_loss: 0.3787 - val_acc: 0.8280\n",
            "Epoch 14/50\n",
            "1113/1113 [==============================] - 1s 788us/step - loss: 0.4944 - acc: 0.8140 - val_loss: 0.3740 - val_acc: 0.8208\n",
            "Epoch 15/50\n",
            "1113/1113 [==============================] - 1s 802us/step - loss: 0.4986 - acc: 0.8167 - val_loss: 0.3670 - val_acc: 0.8315\n",
            "Epoch 16/50\n",
            "1113/1113 [==============================] - 1s 800us/step - loss: 0.4233 - acc: 0.8266 - val_loss: 0.3742 - val_acc: 0.8315\n",
            "Epoch 17/50\n",
            "1113/1113 [==============================] - 1s 800us/step - loss: 0.4735 - acc: 0.8248 - val_loss: 0.3844 - val_acc: 0.8244\n",
            "Epoch 18/50\n",
            "1113/1113 [==============================] - 1s 798us/step - loss: 0.4228 - acc: 0.8455 - val_loss: 0.3699 - val_acc: 0.8280\n",
            "Epoch 19/50\n",
            "1113/1113 [==============================] - 1s 795us/step - loss: 0.4242 - acc: 0.8383 - val_loss: 0.3759 - val_acc: 0.8315\n",
            "Epoch 20/50\n",
            "1113/1113 [==============================] - 1s 807us/step - loss: 0.4596 - acc: 0.8167 - val_loss: 0.3820 - val_acc: 0.8315\n",
            "Epoch 21/50\n",
            "1113/1113 [==============================] - 1s 796us/step - loss: 0.4590 - acc: 0.8239 - val_loss: 0.3862 - val_acc: 0.8100\n",
            "Epoch 22/50\n",
            "1113/1113 [==============================] - 1s 802us/step - loss: 0.4178 - acc: 0.8473 - val_loss: 0.4005 - val_acc: 0.8244\n",
            "Epoch 23/50\n",
            "1113/1113 [==============================] - 1s 798us/step - loss: 0.4203 - acc: 0.8419 - val_loss: 0.4176 - val_acc: 0.8172\n",
            "Epoch 24/50\n",
            "1113/1113 [==============================] - 1s 797us/step - loss: 0.4135 - acc: 0.8338 - val_loss: 0.4183 - val_acc: 0.8244\n",
            "Epoch 25/50\n",
            "1113/1113 [==============================] - 1s 797us/step - loss: 0.3926 - acc: 0.8401 - val_loss: 0.4098 - val_acc: 0.8351\n",
            "Epoch 26/50\n",
            "1113/1113 [==============================] - 1s 792us/step - loss: 0.3843 - acc: 0.8535 - val_loss: 0.3836 - val_acc: 0.8423\n",
            "Epoch 27/50\n",
            "1113/1113 [==============================] - 1s 792us/step - loss: 0.3678 - acc: 0.8634 - val_loss: 0.3704 - val_acc: 0.8315\n",
            "Epoch 28/50\n",
            "1113/1113 [==============================] - 1s 798us/step - loss: 0.3863 - acc: 0.8589 - val_loss: 0.3545 - val_acc: 0.8423\n",
            "Epoch 29/50\n",
            "1113/1113 [==============================] - 1s 805us/step - loss: 0.3486 - acc: 0.8751 - val_loss: 0.3353 - val_acc: 0.8459\n",
            "Epoch 30/50\n",
            "1113/1113 [==============================] - 1s 802us/step - loss: 0.4074 - acc: 0.8428 - val_loss: 0.3216 - val_acc: 0.8602\n",
            "Epoch 31/50\n",
            "1113/1113 [==============================] - 1s 798us/step - loss: 0.3864 - acc: 0.8598 - val_loss: 0.3247 - val_acc: 0.8566\n",
            "Epoch 32/50\n",
            "1113/1113 [==============================] - 1s 798us/step - loss: 0.3869 - acc: 0.8607 - val_loss: 0.3322 - val_acc: 0.8459\n",
            "Epoch 33/50\n",
            "1113/1113 [==============================] - 1s 800us/step - loss: 0.3654 - acc: 0.8634 - val_loss: 0.3179 - val_acc: 0.8602\n",
            "Epoch 34/50\n",
            "1113/1113 [==============================] - 1s 803us/step - loss: 0.3860 - acc: 0.8473 - val_loss: 0.3187 - val_acc: 0.8638\n",
            "Epoch 35/50\n",
            "1113/1113 [==============================] - 1s 803us/step - loss: 0.3460 - acc: 0.8688 - val_loss: 0.3352 - val_acc: 0.8387\n",
            "Epoch 36/50\n",
            "1113/1113 [==============================] - 1s 793us/step - loss: 0.3591 - acc: 0.8688 - val_loss: 0.3387 - val_acc: 0.8351\n",
            "Epoch 37/50\n",
            "1113/1113 [==============================] - 1s 797us/step - loss: 0.3793 - acc: 0.8544 - val_loss: 0.3487 - val_acc: 0.8351\n",
            "Epoch 38/50\n",
            "1113/1113 [==============================] - 1s 806us/step - loss: 0.3751 - acc: 0.8589 - val_loss: 0.3476 - val_acc: 0.8423\n",
            "Epoch 39/50\n",
            "1113/1113 [==============================] - 1s 803us/step - loss: 0.3425 - acc: 0.8706 - val_loss: 0.3476 - val_acc: 0.8459\n",
            "Epoch 40/50\n",
            "1113/1113 [==============================] - 1s 811us/step - loss: 0.3669 - acc: 0.8562 - val_loss: 0.3386 - val_acc: 0.8459\n",
            "Epoch 41/50\n",
            "1113/1113 [==============================] - 1s 797us/step - loss: 0.3291 - acc: 0.8733 - val_loss: 0.3376 - val_acc: 0.8459\n",
            "Epoch 42/50\n",
            "1113/1113 [==============================] - 1s 797us/step - loss: 0.3257 - acc: 0.8859 - val_loss: 0.3561 - val_acc: 0.8351\n",
            "Epoch 43/50\n",
            "1113/1113 [==============================] - 1s 802us/step - loss: 0.3262 - acc: 0.8742 - val_loss: 0.3855 - val_acc: 0.8172\n",
            "Epoch 44/50\n",
            "1113/1113 [==============================] - 1s 795us/step - loss: 0.4107 - acc: 0.8553 - val_loss: 0.3548 - val_acc: 0.8280\n",
            "Epoch 45/50\n",
            "1113/1113 [==============================] - 1s 805us/step - loss: 0.3539 - acc: 0.8751 - val_loss: 0.3261 - val_acc: 0.8781\n",
            "Epoch 46/50\n",
            "1113/1113 [==============================] - 1s 793us/step - loss: 0.3339 - acc: 0.8841 - val_loss: 0.3133 - val_acc: 0.8746\n",
            "Epoch 47/50\n",
            "1113/1113 [==============================] - 1s 797us/step - loss: 0.3224 - acc: 0.8724 - val_loss: 0.3273 - val_acc: 0.8530\n",
            "Epoch 48/50\n",
            "1113/1113 [==============================] - 1s 799us/step - loss: 0.3223 - acc: 0.8787 - val_loss: 0.3424 - val_acc: 0.8423\n",
            "Epoch 49/50\n",
            "1113/1113 [==============================] - 1s 793us/step - loss: 0.3808 - acc: 0.8553 - val_loss: 0.3379 - val_acc: 0.8530\n",
            "Epoch 50/50\n",
            "1113/1113 [==============================] - 1s 796us/step - loss: 0.3436 - acc: 0.8742 - val_loss: 0.3392 - val_acc: 0.8459\n",
            "300/300 [==============================] - 0s 370us/step\n",
            "282/282 [==============================] - 9s 32ms/step\n",
            "Train on 1108 samples, validate on 278 samples\n",
            "Epoch 1/50\n",
            "1108/1108 [==============================] - 11s 10ms/step - loss: 0.8625 - acc: 0.6561 - val_loss: 0.8973 - val_acc: 0.6151\n",
            "Epoch 2/50\n",
            "1108/1108 [==============================] - 1s 792us/step - loss: 0.7704 - acc: 0.6913 - val_loss: 0.8718 - val_acc: 0.6259\n",
            "Epoch 3/50\n",
            "1108/1108 [==============================] - 1s 798us/step - loss: 0.7313 - acc: 0.7013 - val_loss: 0.8216 - val_acc: 0.6475\n",
            "Epoch 4/50\n",
            "1108/1108 [==============================] - 1s 798us/step - loss: 0.6691 - acc: 0.7365 - val_loss: 0.7857 - val_acc: 0.6403\n",
            "Epoch 5/50\n",
            "1108/1108 [==============================] - 1s 800us/step - loss: 0.6155 - acc: 0.7653 - val_loss: 0.7657 - val_acc: 0.6583\n",
            "Epoch 6/50\n",
            "1108/1108 [==============================] - 1s 797us/step - loss: 0.6277 - acc: 0.7491 - val_loss: 0.7696 - val_acc: 0.6547\n",
            "Epoch 7/50\n",
            "1108/1108 [==============================] - 1s 794us/step - loss: 0.6359 - acc: 0.7383 - val_loss: 0.7515 - val_acc: 0.6655\n",
            "Epoch 8/50\n",
            "1108/1108 [==============================] - 1s 805us/step - loss: 0.6108 - acc: 0.7635 - val_loss: 0.7195 - val_acc: 0.6655\n",
            "Epoch 9/50\n",
            "1108/1108 [==============================] - 1s 794us/step - loss: 0.6064 - acc: 0.7653 - val_loss: 0.6963 - val_acc: 0.6871\n",
            "Epoch 10/50\n",
            "1108/1108 [==============================] - 1s 799us/step - loss: 0.5536 - acc: 0.7897 - val_loss: 0.6771 - val_acc: 0.6906\n",
            "Epoch 11/50\n",
            "1108/1108 [==============================] - 1s 792us/step - loss: 0.5249 - acc: 0.8060 - val_loss: 0.6774 - val_acc: 0.6799\n",
            "Epoch 12/50\n",
            "1108/1108 [==============================] - 1s 804us/step - loss: 0.5411 - acc: 0.7807 - val_loss: 0.6796 - val_acc: 0.6906\n",
            "Epoch 13/50\n",
            "1108/1108 [==============================] - 1s 805us/step - loss: 0.5034 - acc: 0.8051 - val_loss: 0.6915 - val_acc: 0.6906\n",
            "Epoch 14/50\n",
            "1108/1108 [==============================] - 1s 798us/step - loss: 0.5380 - acc: 0.7870 - val_loss: 0.6937 - val_acc: 0.6942\n",
            "Epoch 15/50\n",
            "1108/1108 [==============================] - 1s 798us/step - loss: 0.5134 - acc: 0.7915 - val_loss: 0.6810 - val_acc: 0.7086\n",
            "Epoch 16/50\n",
            "1108/1108 [==============================] - 1s 793us/step - loss: 0.5112 - acc: 0.8060 - val_loss: 0.6668 - val_acc: 0.7014\n",
            "Epoch 17/50\n",
            "1108/1108 [==============================] - 1s 803us/step - loss: 0.5119 - acc: 0.8032 - val_loss: 0.6476 - val_acc: 0.7050\n",
            "Epoch 18/50\n",
            "1108/1108 [==============================] - 1s 799us/step - loss: 0.4948 - acc: 0.8168 - val_loss: 0.6360 - val_acc: 0.7014\n",
            "Epoch 19/50\n",
            "1108/1108 [==============================] - 1s 793us/step - loss: 0.4660 - acc: 0.8222 - val_loss: 0.6416 - val_acc: 0.6978\n",
            "Epoch 20/50\n",
            "1108/1108 [==============================] - 1s 797us/step - loss: 0.4799 - acc: 0.8023 - val_loss: 0.6252 - val_acc: 0.7158\n",
            "Epoch 21/50\n",
            "1108/1108 [==============================] - 1s 798us/step - loss: 0.4809 - acc: 0.8177 - val_loss: 0.6052 - val_acc: 0.7446\n",
            "Epoch 22/50\n",
            "1108/1108 [==============================] - 1s 798us/step - loss: 0.4884 - acc: 0.8078 - val_loss: 0.6150 - val_acc: 0.7194\n",
            "Epoch 23/50\n",
            "1108/1108 [==============================] - 1s 804us/step - loss: 0.4672 - acc: 0.8159 - val_loss: 0.6497 - val_acc: 0.6978\n",
            "Epoch 24/50\n",
            "1108/1108 [==============================] - 1s 796us/step - loss: 0.4387 - acc: 0.8312 - val_loss: 0.6672 - val_acc: 0.6835\n",
            "Epoch 25/50\n",
            "1108/1108 [==============================] - 1s 801us/step - loss: 0.4191 - acc: 0.8412 - val_loss: 0.6459 - val_acc: 0.7014\n",
            "Epoch 26/50\n",
            "1108/1108 [==============================] - 1s 797us/step - loss: 0.4074 - acc: 0.8484 - val_loss: 0.6014 - val_acc: 0.7122\n",
            "Epoch 27/50\n",
            "1108/1108 [==============================] - 1s 798us/step - loss: 0.4619 - acc: 0.8123 - val_loss: 0.6038 - val_acc: 0.6978\n",
            "Epoch 28/50\n",
            "1108/1108 [==============================] - 1s 798us/step - loss: 0.4561 - acc: 0.8348 - val_loss: 0.6368 - val_acc: 0.7086\n",
            "Epoch 29/50\n",
            "1108/1108 [==============================] - 1s 796us/step - loss: 0.4149 - acc: 0.8475 - val_loss: 0.6637 - val_acc: 0.7122\n",
            "Epoch 30/50\n",
            "1108/1108 [==============================] - 1s 797us/step - loss: 0.4016 - acc: 0.8394 - val_loss: 0.6624 - val_acc: 0.7086\n",
            "Epoch 31/50\n",
            "1108/1108 [==============================] - 1s 804us/step - loss: 0.4161 - acc: 0.8466 - val_loss: 0.6294 - val_acc: 0.7194\n",
            "Epoch 32/50\n",
            "1108/1108 [==============================] - 1s 795us/step - loss: 0.4275 - acc: 0.8394 - val_loss: 0.6087 - val_acc: 0.7194\n",
            "Epoch 33/50\n",
            "1108/1108 [==============================] - 1s 792us/step - loss: 0.4207 - acc: 0.8348 - val_loss: 0.6138 - val_acc: 0.7230\n",
            "Epoch 34/50\n",
            "1108/1108 [==============================] - 1s 794us/step - loss: 0.3864 - acc: 0.8448 - val_loss: 0.6359 - val_acc: 0.7194\n",
            "Epoch 35/50\n",
            "1108/1108 [==============================] - 1s 798us/step - loss: 0.3915 - acc: 0.8592 - val_loss: 0.6172 - val_acc: 0.7230\n",
            "Epoch 36/50\n",
            "1108/1108 [==============================] - 1s 795us/step - loss: 0.4018 - acc: 0.8457 - val_loss: 0.6118 - val_acc: 0.7302\n",
            "Epoch 37/50\n",
            "1108/1108 [==============================] - 1s 802us/step - loss: 0.3876 - acc: 0.8601 - val_loss: 0.6335 - val_acc: 0.7374\n",
            "Epoch 38/50\n",
            "1108/1108 [==============================] - 1s 799us/step - loss: 0.3806 - acc: 0.8592 - val_loss: 0.6531 - val_acc: 0.7302\n",
            "Epoch 39/50\n",
            "1108/1108 [==============================] - 1s 800us/step - loss: 0.3805 - acc: 0.8628 - val_loss: 0.6823 - val_acc: 0.7230\n",
            "Epoch 40/50\n",
            "1108/1108 [==============================] - 1s 796us/step - loss: 0.3732 - acc: 0.8556 - val_loss: 0.6600 - val_acc: 0.7194\n",
            "Epoch 41/50\n",
            "1108/1108 [==============================] - 1s 803us/step - loss: 0.3722 - acc: 0.8655 - val_loss: 0.6255 - val_acc: 0.7266\n",
            "Epoch 42/50\n",
            "1108/1108 [==============================] - 1s 802us/step - loss: 0.3664 - acc: 0.8691 - val_loss: 0.6439 - val_acc: 0.7158\n",
            "Epoch 43/50\n",
            "1108/1108 [==============================] - 1s 799us/step - loss: 0.3729 - acc: 0.8520 - val_loss: 0.6401 - val_acc: 0.7158\n",
            "Epoch 44/50\n",
            "1108/1108 [==============================] - 1s 804us/step - loss: 0.4102 - acc: 0.8475 - val_loss: 0.6187 - val_acc: 0.7266\n",
            "Epoch 45/50\n",
            "1108/1108 [==============================] - 1s 793us/step - loss: 0.3735 - acc: 0.8601 - val_loss: 0.6278 - val_acc: 0.7230\n",
            "Epoch 46/50\n",
            "1108/1108 [==============================] - 1s 799us/step - loss: 0.3448 - acc: 0.8773 - val_loss: 0.6522 - val_acc: 0.7158\n",
            "Epoch 47/50\n",
            "1108/1108 [==============================] - 1s 820us/step - loss: 0.3718 - acc: 0.8619 - val_loss: 0.6922 - val_acc: 0.7014\n",
            "Epoch 48/50\n",
            "1108/1108 [==============================] - 1s 797us/step - loss: 0.3549 - acc: 0.8682 - val_loss: 0.6686 - val_acc: 0.7050\n",
            "Epoch 49/50\n",
            "1108/1108 [==============================] - 1s 802us/step - loss: 0.3660 - acc: 0.8610 - val_loss: 0.6479 - val_acc: 0.7122\n",
            "Epoch 50/50\n",
            "1108/1108 [==============================] - 1s 791us/step - loss: 0.3227 - acc: 0.8755 - val_loss: 0.6504 - val_acc: 0.7122\n",
            "282/282 [==============================] - 0s 387us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rgTzYUsds_q",
        "colab_type": "code",
        "outputId": "61ebed73-311d-4133-c6b8-81e8ab7d2797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "print(cross_subject_accu_50)\n",
        "subject_id = ['baseline','subject1', 'subject2', 'subject3', 'subject4', \n",
        "              'subject5', 'subject6', 'subject7', 'subject8', 'subject9']\n",
        "plt.plot([0,1],[0,1])\n",
        "marktypes = ['>','x','.','^','1']\n",
        "colors = ['r','g','b']\n",
        "for i in np.arange(num_subject):\n",
        "  plt.scatter(cross_subject_accu_50[0,i],cross_subject_accu_50[1,i], \n",
        "              marker=marktypes[i%5], color=colors[i%3])\n",
        "  plt.legend(subject_id)\n",
        "\n",
        "plt.xlabel('Accuracy of DeepCNN among subject')\n",
        "plt.ylabel('Accuracy after fine tuning')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Accuracy after fine tuning')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFYCAYAAAB6RnQAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gH94SebtCeh",
        "colab_type": "code",
        "outputId": "3bfa754c-7a9c-497e-cc31-d598bf134346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebMYmP9SJ9fa",
        "colab_type": "text"
      },
      "source": [
        "##RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcWE6RNhEShw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0mNrmm7jUh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vediosegment(X_train, Y_train, window, stride):\n",
        "  num_trail, num_eletrode, num_bin = X_train.shape\n",
        "  num_class = Y_train.shape[1]\n",
        "  N = (num_bin - window)//stride+1\n",
        "  X_train_argment = np.empty((num_trail, num_eletrode*N, window))\n",
        "  Y_train_argment = np.empty((num_trail, num_class))\n",
        "  for idx_trail in range(num_trail):\n",
        "    for n in range(N):\n",
        "      X_train_argment[idx_trail*N+n,:,:] = X_train[idx_trail,:,n*stride:n*stride+window]\n",
        "      Y_train_argment[idx_trail*N+n] = Y_train[idx_trail]\n",
        "      person_idx_argment[idx_trail*N+n] = person_idx[idx_trail]\n",
        "  return X_train_argment, Y_train_argment, person_idx_argment\n",
        "\n",
        "window = 750\n",
        "stride = 50\n",
        "X_train_argment, Y_train_argment, person_train_idx_argment = dataargment(X_train, Y_train, window, stride,person_train_valid)\n",
        "X_test_argment, Y_test_argment, person_test_idx_argment = dataargment(X_test, Y_test, window, stride,person_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8FWWxZ5fAc3",
        "colab_type": "code",
        "outputId": "52ced47b-0523-4cba-c87f-348af381c237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "dropout_rate = 0.6\n",
        "window = 750\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=25, kernel_size=(25,1), \n",
        "                strides=(2, 1), input_shape = (750,22,1),\n",
        "                padding ='valid'))\n",
        "model.add(MaxPool2D(pool_size = (5,1)))\n",
        "model.add(Conv2D(filters=25, kernel_size=(2,1), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid'))\n",
        "model.add(Reshape((-1,25)))\n",
        "# model.add(LSTM(64,return_sequences = True, \n",
        "#                recurrent_dropout = dropout_rate))\n",
        "model.add(LSTM(16,return_sequences = True, \n",
        "               recurrent_dropout = dropout_rate))\n",
        "model.add(LSTM(8,return_sequences = False, \n",
        "               recurrent_dropout = dropout_rate))\n",
        "# model.add(LSTM(50, return_sequences = True, \n",
        "#                recurrent_dropout = dropout_rate))\n",
        "\n",
        "# model.add(Reshape((-1,50,1)))\n",
        "# model.add(Conv2D(filters=50, kernel_size=(25,1), \n",
        "#                 strides=(25, 1),\n",
        "#                 padding ='valid'))\n",
        "\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(SpatialDropout2D(dropout_rate))\n",
        "# model.add(GaussianNoise(0.5))\n",
        "# model.add(Conv2D(filters=50, kernel_size=(5,2), \n",
        "#                 strides=(1, 1),\n",
        "#                 padding ='valid'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(SpatialDropout2D(dropout_rate))\n",
        "# model.add(Reshape((-1,25)))\n",
        "\n",
        "model.add(Dense(4, activation = 'softmax'))\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', metrics = ['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 363, 22, 25)       650       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 72, 22, 25)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 71, 22, 25)        1275      \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 1562, 25)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 1562, 16)          2688      \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 8)                 800       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 36        \n",
            "=================================================================\n",
            "Total params: 5,449\n",
            "Trainable params: 5,449\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWewL-5qRLKb",
        "colab_type": "code",
        "outputId": "3d4d9dd3-713d-4d35-cb43-bc0dc9b0ee8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1139
        }
      },
      "source": [
        "history = model.fit(X_train_argment.reshape(-1,750,22,1), Y_train_argment, \n",
        "                    batch_size=256, epochs = 50, validation_split = 0.2,\n",
        "                    callbacks = [early_stop], verbose = 1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/50\n",
            "10152/10152 [==============================] - 273s 27ms/step - loss: 1.4004 - acc: 0.2443 - val_loss: 1.3897 - val_acc: 0.2565\n",
            "Epoch 2/50\n",
            "10152/10152 [==============================] - 270s 27ms/step - loss: 1.3863 - acc: 0.2626 - val_loss: 1.3868 - val_acc: 0.2553\n",
            "Epoch 3/50\n",
            "10152/10152 [==============================] - 271s 27ms/step - loss: 1.3845 - acc: 0.2642 - val_loss: 1.3869 - val_acc: 0.2620\n",
            "Epoch 4/50\n",
            "10152/10152 [==============================] - 270s 27ms/step - loss: 1.3816 - acc: 0.2757 - val_loss: 1.3874 - val_acc: 0.2510\n",
            "Epoch 5/50\n",
            " 9984/10152 [============================>.] - ETA: 4s - loss: 1.3787 - acc: 0.2869 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-156-4a01b0babf12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(X_train_argment.reshape(-1,750,22,1), Y_train_argment, \n\u001b[1;32m      2\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     callbacks = [early_stop], verbose = 1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDxvDEaqRa2P",
        "colab_type": "code",
        "outputId": "99f8f3bb-4f99-422e-9460-dd584bdb5a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "print(\"best val_accu is : {:2.2%}\".format(np.max(history.history['val_acc'])))\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model acc')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best val_accu is : 63.99%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0QAAAFMCAYAAADr1XxqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiIpKwtCESEREREZGE9QfvgWDHsiVz8QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1008x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li0VnHA7scq7",
        "colab_type": "code",
        "outputId": "2a815b54-29ea-46ee-ebe0-639449e514a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "dropout_rate = 0.8\n",
        "window = 750\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(LSTM(16, input_shape=(750,22),return_sequences = True, \n",
        "               recurrent_dropout = dropout_rate,\n",
        "               dropout = dropout_rate)\n",
        "         )\n",
        "\n",
        "model.add(LSTM(16, return_sequences = True, \n",
        "              recurrent_dropout = dropout_rate,\n",
        "               dropout = dropout_rate)\n",
        "         )\n",
        "\n",
        "model.add(Reshape((-1,16,1)))\n",
        "model.add(Conv2D(filters=25, kernel_size=(10,16), \n",
        "                strides=(5, 1),\n",
        "                padding ='valid'))\n",
        "model.add(Dense(4, activation = 'relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4, activation = 'softmax'))\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', metrics = ['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 750, 16)           2496      \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 750, 16)           2112      \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 750, 16, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 149, 1, 25)        4025      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 149, 1, 4)         104       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 596)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 2388      \n",
            "=================================================================\n",
            "Total params: 11,125\n",
            "Trainable params: 11,125\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxkiIxKoK1Qd",
        "colab_type": "code",
        "outputId": "872b808b-9a8b-4458-b81e-bd3e349815fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "history = model.fit(X_train_argment.reshape(-1,750,22), Y_train_argment, \n",
        "                    batch_size=256, epochs = 50, validation_split = 0.2,\n",
        "                    callbacks = [early_stop], verbose = 1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/50\n",
            "10152/10152 [==============================] - 142s 14ms/step - loss: 1.3873 - acc: 0.2482 - val_loss: 1.3855 - val_acc: 0.2723\n",
            "Epoch 2/50\n",
            "10152/10152 [==============================] - 139s 14ms/step - loss: 1.3877 - acc: 0.2529 - val_loss: 1.3858 - val_acc: 0.2723\n",
            "Epoch 3/50\n",
            "10152/10152 [==============================] - 139s 14ms/step - loss: 1.3866 - acc: 0.2529 - val_loss: 1.3859 - val_acc: 0.2565\n",
            "Epoch 4/50\n",
            "10152/10152 [==============================] - 139s 14ms/step - loss: 1.3870 - acc: 0.2514 - val_loss: 1.3860 - val_acc: 0.2593\n",
            "Epoch 5/50\n",
            "10152/10152 [==============================] - 138s 14ms/step - loss: 1.3862 - acc: 0.2522 - val_loss: 1.3860 - val_acc: 0.2585\n",
            "Epoch 6/50\n",
            "10152/10152 [==============================] - 138s 14ms/step - loss: 1.3864 - acc: 0.2505 - val_loss: 1.3859 - val_acc: 0.2600\n",
            "Epoch 7/50\n",
            "10152/10152 [==============================] - 137s 14ms/step - loss: 1.3858 - acc: 0.2572 - val_loss: 1.3858 - val_acc: 0.2593\n",
            "Epoch 8/50\n",
            "10152/10152 [==============================] - 137s 14ms/step - loss: 1.3863 - acc: 0.2611 - val_loss: 1.3859 - val_acc: 0.2589\n",
            "Epoch 9/50\n",
            "10152/10152 [==============================] - 138s 14ms/step - loss: 1.3866 - acc: 0.2514 - val_loss: 1.3859 - val_acc: 0.2620\n",
            "Epoch 10/50\n",
            "10152/10152 [==============================] - 137s 14ms/step - loss: 1.3860 - acc: 0.2560 - val_loss: 1.3856 - val_acc: 0.2652\n",
            "Epoch 11/50\n",
            "10152/10152 [==============================] - 137s 13ms/step - loss: 1.3867 - acc: 0.2533 - val_loss: 1.3859 - val_acc: 0.2600\n",
            "Epoch 12/50\n",
            "10152/10152 [==============================] - 138s 14ms/step - loss: 1.3862 - acc: 0.2477 - val_loss: 1.3857 - val_acc: 0.2667\n",
            "Epoch 00012: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEV_aZdbSDbR",
        "colab_type": "text"
      },
      "source": [
        "## Using frequency doamin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO0ei5LsPt-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataargment(X_train, Y_train, window, stride, person_idx):\n",
        "  num_trail, num_eletrode, num_bin = X_train.shape\n",
        "  num_class = Y_train.shape[1]\n",
        "  N = (num_bin - window)//stride+1\n",
        "  X_train_argment = np.empty((num_trail*N, num_eletrode, window))\n",
        "  Y_train_argment = np.empty((num_trail*N, num_class))\n",
        "  person_idx_argment = np.empty(num_trail*N)\n",
        "  for idx_trail in range(num_trail):\n",
        "    for n in range(N):\n",
        "      X_train_argment[idx_trail*N+n,:,:] = X_train[idx_trail,:,n*stride:n*stride+window]\n",
        "      Y_train_argment[idx_trail*N+n] = Y_train[idx_trail]\n",
        "      person_idx_argment[idx_trail*N+n] = person_idx[idx_trail]\n",
        "  return X_train_argment, Y_train_argment, person_idx_argment\n",
        "\n",
        "window = 200\n",
        "stride = 200\n",
        "X_train_argment, Y_train_argment, person_train_idx_argment = dataargment(X_train, Y_train, window, stride,person_train_valid)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22hf2jGPSJCv",
        "colab_type": "code",
        "outputId": "4c5bf170-bdf3-4612-a1e6-97df72779fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "plt.figure(figsize = (9,8))\n",
        "plt.imshow(X_train[18,:], cmap = 'gray',aspect = 20)\n",
        "plt.grid(False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAD8CAYAAABZyI+oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLf8PIXVgLOl0z0QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 648x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkpJkDrNVIF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def findpowerspectrum(X):\n",
        "  N = X[0,0:200].shape[1]\n",
        "  X_spectrum = 2.0/N * np.abs(np.fft.fft(X))\n",
        "  return X_spectrum[:,:,0:N//2]\n",
        "X_spectrum = findpowerspectrum(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-fa7ylHjw37",
        "colab_type": "code",
        "outputId": "55ceae1f-b668-4e6f-f8a4-a72f94cc43a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "plt.imshow(X_spectrum[55,:],cmap='jet',aspect = 20)\n",
        "plt.grid(False)\n",
        "X_spectrum[18,:].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAFKCAYAAADynUMpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOPHd-QXsJgn",
        "colab_type": "code",
        "outputId": "6fc978d3-3061-48e3-8090-1ab36d8f78fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_spectrum.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2115, 25, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPbNY8IpsMqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "windows = [150,300,450,600,750]\n",
        "row_idx = ['train','val','test']\n",
        "stride = 50\n",
        "accu = np.zeros(3,5)\n",
        "df = pd.DataFrame(accu, columes = windows, index = row_id)\n",
        "for window in windows:\n",
        "  X_train_argment, Y_train_argment, person_train_idx_argment = dataargment(X_train, Y_train, window, stride,person_train_valid)\n",
        "  X_test_augment, Y_test_augment, person_test_idx_argment = dataargment(X_test, Y_test, window, stride,person_test)\n",
        "  model = deepCNN()\n",
        "  history = model.fit(X_train_argment.reshape(-1,22,window,1),\n",
        "                          Y_train_argment[], \n",
        "                          batch_size=256, epochs = 200, validation_split = 0.2,\n",
        "                          callbacks = [early_stop], verbose = 1)\n",
        "  _, accu = model.evaluate(X_test_augment.reshape(-1,22,window,1), Y_test_augment)\n",
        "  accu[window,:] = history.history['accu'],  history.history['val_accu'], "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJVnIw1YCwOk",
        "colab_type": "code",
        "outputId": "f91d818f-ebf6-476f-f671-b42f4093a3b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dropout_rate = 0.3\n",
        "l2_reg = 0.1\n",
        "\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=25, kernel_size=(3,3), \n",
        "                 strides=(1, 1), input_shape=(22,501,1),\n",
        "                 kernel_regularizer = regularizers.l2(l2_reg),\n",
        "                 padding ='valid'\n",
        "                )\n",
        "         )\n",
        "model.add(BatchNormalization())\n",
        "# model.add(GaussianNoise(0.1))\n",
        "model.add(Conv2D(filters=25, kernel_size=(3,3), \n",
        "                 strides=(1, 1),\n",
        "                 kernel_regularizer = regularizers.l2(l2_reg),\n",
        "                 padding ='valid', activation = 'elu'))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(GaussianNoise(0.1))\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(MaxPool2D(pool_size=(1, 3), strides=(1, 3)))\n",
        "model.add(Reshape((-1,25,1)))\n",
        "model.add(Conv2D(filters=50, kernel_size=(10,25), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(Reshape((-1,50,1)))\n",
        "model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "model.add(Conv2D(filters=100, kernel_size=(10,50), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "# model.add(Dropout(dropout_rate))\n",
        "# model.add(SpatialDropout2D(dropout_rate))\n",
        "\n",
        "model.add(Reshape((-1,100,1)))\n",
        "model.add(MaxPool2D(pool_size=(2, 1), strides=(2, 1)))\n",
        "model.add(Conv2D(filters=200, kernel_size=(3,100), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', activation = 'elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(SpatialDropout2D(dropout_rate))\n",
        "\n",
        "model.add(Reshape((-1,200,1)))\n",
        "model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "model.add(Flatten())\n",
        "# model.add(Dense(units = 20))\n",
        "# model.add(Activation('elu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(Dense(units = 4, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])\n",
        "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
        "\n",
        "history = model.fit(X_train_freq.reshape(-1,22,501,1), Y_train, \n",
        "                    batch_size=256, epochs = 200, validation_split = 0.2,\n",
        "                    callbacks = [early_stop])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1692 samples, validate on 423 samples\n",
            "Epoch 1/200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py:538: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1692/1692 [==============================] - 5s 3ms/step - loss: 10.4818 - acc: 0.2441 - val_loss: 13.8645 - val_acc: 0.2506\n",
            "Epoch 2/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 13.0152 - acc: 0.2435 - val_loss: 13.7200 - val_acc: 0.2600\n",
            "Epoch 3/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 13.4105 - acc: 0.2565 - val_loss: 13.8109 - val_acc: 0.2577\n",
            "Epoch 4/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 12.0598 - acc: 0.2553 - val_loss: 12.4934 - val_acc: 0.2459\n",
            "Epoch 5/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 10.9994 - acc: 0.2470 - val_loss: 13.4682 - val_acc: 0.2506\n",
            "Epoch 6/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 10.3540 - acc: 0.2660 - val_loss: 11.0715 - val_acc: 0.2577\n",
            "Epoch 7/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 10.1960 - acc: 0.2530 - val_loss: 10.4152 - val_acc: 0.2506\n",
            "Epoch 8/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 10.1063 - acc: 0.2518 - val_loss: 10.1148 - val_acc: 0.2624\n",
            "Epoch 9/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 8.7238 - acc: 0.2530 - val_loss: 10.7475 - val_acc: 0.2600\n",
            "Epoch 10/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 8.5538 - acc: 0.2459 - val_loss: 7.6039 - val_acc: 0.2719\n",
            "Epoch 11/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 6.8000 - acc: 0.2600 - val_loss: 9.2861 - val_acc: 0.2577\n",
            "Epoch 12/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 6.3744 - acc: 0.2671 - val_loss: 11.8216 - val_acc: 0.2317\n",
            "Epoch 13/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 5.7481 - acc: 0.2701 - val_loss: 7.3813 - val_acc: 0.2411\n",
            "Epoch 14/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 4.7610 - acc: 0.2790 - val_loss: 4.4374 - val_acc: 0.2175\n",
            "Epoch 15/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 4.4704 - acc: 0.2695 - val_loss: 6.8645 - val_acc: 0.2293\n",
            "Epoch 16/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 3.7895 - acc: 0.2801 - val_loss: 4.5370 - val_acc: 0.2553\n",
            "Epoch 17/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 5.0580 - acc: 0.2453 - val_loss: 6.4645 - val_acc: 0.2648\n",
            "Epoch 18/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 4.9548 - acc: 0.2748 - val_loss: 7.7823 - val_acc: 0.2813\n",
            "Epoch 19/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 6.5069 - acc: 0.2660 - val_loss: 8.1042 - val_acc: 0.2600\n",
            "Epoch 20/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 6.0782 - acc: 0.2642 - val_loss: 3.2646 - val_acc: 0.2388\n",
            "Epoch 21/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 3.8288 - acc: 0.2796 - val_loss: 5.3299 - val_acc: 0.2482\n",
            "Epoch 22/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 3.0943 - acc: 0.2884 - val_loss: 3.0460 - val_acc: 0.2506\n",
            "Epoch 23/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 2.7289 - acc: 0.3097 - val_loss: 2.6320 - val_acc: 0.2530\n",
            "Epoch 24/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 2.4250 - acc: 0.3327 - val_loss: 2.5883 - val_acc: 0.2648\n",
            "Epoch 25/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 2.3339 - acc: 0.3197 - val_loss: 3.5670 - val_acc: 0.2577\n",
            "Epoch 26/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 2.0262 - acc: 0.3197 - val_loss: 3.3445 - val_acc: 0.2648\n",
            "Epoch 27/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.9121 - acc: 0.3617 - val_loss: 2.2296 - val_acc: 0.2459\n",
            "Epoch 28/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.7784 - acc: 0.3641 - val_loss: 2.0610 - val_acc: 0.2364\n",
            "Epoch 29/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.7131 - acc: 0.3989 - val_loss: 1.5584 - val_acc: 0.2908\n",
            "Epoch 30/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.5634 - acc: 0.4202 - val_loss: 1.4972 - val_acc: 0.2955\n",
            "Epoch 31/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.5080 - acc: 0.4102 - val_loss: 1.6645 - val_acc: 0.2695\n",
            "Epoch 32/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.5805 - acc: 0.4155 - val_loss: 2.0580 - val_acc: 0.2506\n",
            "Epoch 33/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.5684 - acc: 0.4108 - val_loss: 1.5418 - val_acc: 0.2837\n",
            "Epoch 34/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.5437 - acc: 0.4338 - val_loss: 1.8063 - val_acc: 0.2908\n",
            "Epoch 35/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.5440 - acc: 0.4492 - val_loss: 1.6967 - val_acc: 0.2790\n",
            "Epoch 36/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.3804 - acc: 0.4728 - val_loss: 1.6375 - val_acc: 0.2317\n",
            "Epoch 37/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.3650 - acc: 0.4941 - val_loss: 1.6957 - val_acc: 0.2813\n",
            "Epoch 38/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.2711 - acc: 0.5165 - val_loss: 1.5637 - val_acc: 0.2884\n",
            "Epoch 39/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.2356 - acc: 0.5284 - val_loss: 1.5557 - val_acc: 0.2766\n",
            "Epoch 40/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.1441 - acc: 0.5650 - val_loss: 1.5526 - val_acc: 0.2884\n",
            "Epoch 41/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.0737 - acc: 0.5892 - val_loss: 1.4623 - val_acc: 0.3191\n",
            "Epoch 42/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.0343 - acc: 0.6046 - val_loss: 1.4850 - val_acc: 0.2955\n",
            "Epoch 43/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.1344 - acc: 0.5762 - val_loss: 1.5440 - val_acc: 0.3002\n",
            "Epoch 44/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 1.0558 - acc: 0.5957 - val_loss: 1.5445 - val_acc: 0.3050\n",
            "Epoch 45/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.9473 - acc: 0.6478 - val_loss: 1.5813 - val_acc: 0.3168\n",
            "Epoch 46/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.9025 - acc: 0.6761 - val_loss: 1.5125 - val_acc: 0.2979\n",
            "Epoch 47/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.7945 - acc: 0.7086 - val_loss: 1.5144 - val_acc: 0.3262\n",
            "Epoch 48/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.7482 - acc: 0.7110 - val_loss: 1.4829 - val_acc: 0.3333\n",
            "Epoch 49/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.7380 - acc: 0.7400 - val_loss: 1.6535 - val_acc: 0.2908\n",
            "Epoch 50/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.7142 - acc: 0.7352 - val_loss: 1.6066 - val_acc: 0.3404\n",
            "Epoch 51/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.7019 - acc: 0.7323 - val_loss: 1.4957 - val_acc: 0.3262\n",
            "Epoch 52/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.6358 - acc: 0.7660 - val_loss: 1.5857 - val_acc: 0.3262\n",
            "Epoch 53/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.5545 - acc: 0.8085 - val_loss: 1.5419 - val_acc: 0.3191\n",
            "Epoch 54/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.5951 - acc: 0.7866 - val_loss: 1.5077 - val_acc: 0.3215\n",
            "Epoch 55/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.4608 - acc: 0.8446 - val_loss: 1.6434 - val_acc: 0.3499\n",
            "Epoch 56/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.4389 - acc: 0.8647 - val_loss: 1.5167 - val_acc: 0.3144\n",
            "Epoch 57/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.4586 - acc: 0.8392 - val_loss: 1.6021 - val_acc: 0.3452\n",
            "Epoch 58/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.4181 - acc: 0.8759 - val_loss: 1.5622 - val_acc: 0.3428\n",
            "Epoch 59/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.3588 - acc: 0.9013 - val_loss: 1.6832 - val_acc: 0.3215\n",
            "Epoch 60/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.3229 - acc: 0.9102 - val_loss: 1.5767 - val_acc: 0.3475\n",
            "Epoch 61/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.2977 - acc: 0.9173 - val_loss: 1.6901 - val_acc: 0.3404\n",
            "Epoch 62/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.2840 - acc: 0.9220 - val_loss: 1.6175 - val_acc: 0.3404\n",
            "Epoch 63/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.2491 - acc: 0.9320 - val_loss: 1.6210 - val_acc: 0.3735\n",
            "Epoch 64/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.2110 - acc: 0.9563 - val_loss: 1.6872 - val_acc: 0.3381\n",
            "Epoch 65/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.2110 - acc: 0.9515 - val_loss: 1.7018 - val_acc: 0.3475\n",
            "Epoch 66/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.1692 - acc: 0.9752 - val_loss: 1.6356 - val_acc: 0.3688\n",
            "Epoch 67/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.1514 - acc: 0.9752 - val_loss: 1.6363 - val_acc: 0.3617\n",
            "Epoch 68/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.1578 - acc: 0.9704 - val_loss: 1.7072 - val_acc: 0.3475\n",
            "Epoch 69/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.1394 - acc: 0.9758 - val_loss: 1.7512 - val_acc: 0.3593\n",
            "Epoch 70/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.1391 - acc: 0.9835 - val_loss: 1.7808 - val_acc: 0.3546\n",
            "Epoch 71/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.1234 - acc: 0.9882 - val_loss: 1.7242 - val_acc: 0.3357\n",
            "Epoch 72/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.1279 - acc: 0.9846 - val_loss: 1.7722 - val_acc: 0.3570\n",
            "Epoch 73/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.1194 - acc: 0.9858 - val_loss: 1.8227 - val_acc: 0.3570\n",
            "Epoch 74/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.1121 - acc: 0.9852 - val_loss: 1.8029 - val_acc: 0.3617\n",
            "Epoch 75/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.1048 - acc: 0.9900 - val_loss: 1.7870 - val_acc: 0.3664\n",
            "Epoch 76/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0971 - acc: 0.9905 - val_loss: 1.8039 - val_acc: 0.3641\n",
            "Epoch 77/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0897 - acc: 0.9929 - val_loss: 1.8794 - val_acc: 0.3475\n",
            "Epoch 78/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0891 - acc: 0.9917 - val_loss: 1.8523 - val_acc: 0.3641\n",
            "Epoch 79/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0897 - acc: 0.9888 - val_loss: 1.9389 - val_acc: 0.3617\n",
            "Epoch 80/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0925 - acc: 0.9876 - val_loss: 1.8351 - val_acc: 0.3570\n",
            "Epoch 81/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0923 - acc: 0.9882 - val_loss: 1.9581 - val_acc: 0.3475\n",
            "Epoch 82/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0796 - acc: 0.9917 - val_loss: 1.9063 - val_acc: 0.3593\n",
            "Epoch 83/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0842 - acc: 0.9882 - val_loss: 2.0067 - val_acc: 0.3593\n",
            "Epoch 84/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0889 - acc: 0.9882 - val_loss: 2.1626 - val_acc: 0.3759\n",
            "Epoch 85/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0860 - acc: 0.9894 - val_loss: 2.0608 - val_acc: 0.3830\n",
            "Epoch 86/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0720 - acc: 0.9935 - val_loss: 2.0226 - val_acc: 0.3522\n",
            "Epoch 87/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0705 - acc: 0.9953 - val_loss: 2.0506 - val_acc: 0.3641\n",
            "Epoch 88/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0715 - acc: 0.9935 - val_loss: 2.1244 - val_acc: 0.3641\n",
            "Epoch 89/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0666 - acc: 0.9923 - val_loss: 2.0658 - val_acc: 0.3759\n",
            "Epoch 90/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0707 - acc: 0.9917 - val_loss: 2.0885 - val_acc: 0.3593\n",
            "Epoch 91/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0647 - acc: 0.9941 - val_loss: 2.2323 - val_acc: 0.3735\n",
            "Epoch 92/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0560 - acc: 0.9970 - val_loss: 2.1345 - val_acc: 0.3664\n",
            "Epoch 93/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0549 - acc: 0.9959 - val_loss: 2.1862 - val_acc: 0.3712\n",
            "Epoch 94/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0493 - acc: 0.9965 - val_loss: 2.1226 - val_acc: 0.3712\n",
            "Epoch 95/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0465 - acc: 0.9982 - val_loss: 2.2219 - val_acc: 0.3759\n",
            "Epoch 96/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0455 - acc: 0.9976 - val_loss: 2.1353 - val_acc: 0.3735\n",
            "Epoch 97/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0422 - acc: 0.9982 - val_loss: 2.2658 - val_acc: 0.3735\n",
            "Epoch 98/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0401 - acc: 0.9988 - val_loss: 2.0516 - val_acc: 0.3593\n",
            "Epoch 99/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0370 - acc: 0.9994 - val_loss: 2.2235 - val_acc: 0.3664\n",
            "Epoch 100/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0378 - acc: 0.9976 - val_loss: 2.2784 - val_acc: 0.3522\n",
            "Epoch 101/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0360 - acc: 0.9994 - val_loss: 2.2002 - val_acc: 0.3735\n",
            "Epoch 102/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0364 - acc: 1.0000 - val_loss: 2.2524 - val_acc: 0.3617\n",
            "Epoch 103/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0332 - acc: 0.9982 - val_loss: 2.1821 - val_acc: 0.3783\n",
            "Epoch 104/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0317 - acc: 0.9988 - val_loss: 2.1914 - val_acc: 0.3830\n",
            "Epoch 105/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0300 - acc: 1.0000 - val_loss: 2.2009 - val_acc: 0.3641\n",
            "Epoch 106/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0351 - acc: 0.9982 - val_loss: 2.1627 - val_acc: 0.3830\n",
            "Epoch 107/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0328 - acc: 0.9994 - val_loss: 2.2250 - val_acc: 0.3806\n",
            "Epoch 108/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0316 - acc: 0.9982 - val_loss: 2.4244 - val_acc: 0.3546\n",
            "Epoch 109/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0331 - acc: 0.9994 - val_loss: 2.3866 - val_acc: 0.3759\n",
            "Epoch 110/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0323 - acc: 0.9982 - val_loss: 2.3351 - val_acc: 0.3688\n",
            "Epoch 111/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0339 - acc: 0.9976 - val_loss: 2.3799 - val_acc: 0.3641\n",
            "Epoch 112/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0310 - acc: 0.9976 - val_loss: 2.2324 - val_acc: 0.3522\n",
            "Epoch 113/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0303 - acc: 1.0000 - val_loss: 2.4218 - val_acc: 0.3901\n",
            "Epoch 114/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0299 - acc: 0.9988 - val_loss: 2.3557 - val_acc: 0.3522\n",
            "Epoch 115/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0262 - acc: 1.0000 - val_loss: 2.4347 - val_acc: 0.3452\n",
            "Epoch 116/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0269 - acc: 0.9994 - val_loss: 2.3290 - val_acc: 0.3664\n",
            "Epoch 117/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0261 - acc: 0.9988 - val_loss: 2.4306 - val_acc: 0.3806\n",
            "Epoch 118/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0258 - acc: 0.9994 - val_loss: 2.2573 - val_acc: 0.3806\n",
            "Epoch 119/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0245 - acc: 0.9994 - val_loss: 2.4116 - val_acc: 0.3830\n",
            "Epoch 120/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0245 - acc: 0.9994 - val_loss: 2.4870 - val_acc: 0.3570\n",
            "Epoch 121/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0259 - acc: 0.9988 - val_loss: 2.3860 - val_acc: 0.3664\n",
            "Epoch 122/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0236 - acc: 0.9988 - val_loss: 2.4840 - val_acc: 0.3641\n",
            "Epoch 123/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0235 - acc: 0.9988 - val_loss: 2.3695 - val_acc: 0.3806\n",
            "Epoch 124/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0253 - acc: 0.9994 - val_loss: 2.4661 - val_acc: 0.3783\n",
            "Epoch 125/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0238 - acc: 0.9988 - val_loss: 2.5573 - val_acc: 0.3735\n",
            "Epoch 126/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0223 - acc: 1.0000 - val_loss: 2.4959 - val_acc: 0.3783\n",
            "Epoch 127/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0212 - acc: 0.9994 - val_loss: 2.6397 - val_acc: 0.3853\n",
            "Epoch 128/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0197 - acc: 1.0000 - val_loss: 2.4862 - val_acc: 0.3617\n",
            "Epoch 129/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0201 - acc: 1.0000 - val_loss: 2.5098 - val_acc: 0.3783\n",
            "Epoch 130/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0203 - acc: 0.9994 - val_loss: 2.4866 - val_acc: 0.3664\n",
            "Epoch 131/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0191 - acc: 0.9994 - val_loss: 2.5618 - val_acc: 0.3593\n",
            "Epoch 132/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0200 - acc: 0.9994 - val_loss: 2.5125 - val_acc: 0.3759\n",
            "Epoch 133/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0189 - acc: 1.0000 - val_loss: 2.6726 - val_acc: 0.3499\n",
            "Epoch 134/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0179 - acc: 1.0000 - val_loss: 2.4475 - val_acc: 0.3664\n",
            "Epoch 135/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0168 - acc: 1.0000 - val_loss: 2.5661 - val_acc: 0.3783\n",
            "Epoch 136/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0162 - acc: 1.0000 - val_loss: 2.5028 - val_acc: 0.3546\n",
            "Epoch 137/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0192 - acc: 0.9982 - val_loss: 2.7050 - val_acc: 0.3381\n",
            "Epoch 138/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0184 - acc: 0.9994 - val_loss: 2.4713 - val_acc: 0.3570\n",
            "Epoch 139/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0178 - acc: 0.9994 - val_loss: 2.7331 - val_acc: 0.3310\n",
            "Epoch 140/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0174 - acc: 1.0000 - val_loss: 2.5961 - val_acc: 0.3522\n",
            "Epoch 141/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0176 - acc: 1.0000 - val_loss: 2.6369 - val_acc: 0.3546\n",
            "Epoch 142/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0153 - acc: 1.0000 - val_loss: 2.7106 - val_acc: 0.3641\n",
            "Epoch 143/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0179 - acc: 0.9988 - val_loss: 2.5833 - val_acc: 0.3688\n",
            "Epoch 144/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0152 - acc: 1.0000 - val_loss: 2.7441 - val_acc: 0.3593\n",
            "Epoch 145/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0165 - acc: 0.9994 - val_loss: 2.5490 - val_acc: 0.3783\n",
            "Epoch 146/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0151 - acc: 1.0000 - val_loss: 2.5491 - val_acc: 0.3546\n",
            "Epoch 147/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0152 - acc: 1.0000 - val_loss: 2.5454 - val_acc: 0.3712\n",
            "Epoch 148/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0139 - acc: 1.0000 - val_loss: 2.7175 - val_acc: 0.3593\n",
            "Epoch 149/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0139 - acc: 1.0000 - val_loss: 2.6722 - val_acc: 0.3735\n",
            "Epoch 150/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0138 - acc: 0.9994 - val_loss: 2.7233 - val_acc: 0.3664\n",
            "Epoch 151/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0149 - acc: 0.9994 - val_loss: 2.7758 - val_acc: 0.3499\n",
            "Epoch 152/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0158 - acc: 1.0000 - val_loss: 2.6498 - val_acc: 0.3735\n",
            "Epoch 153/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0169 - acc: 0.9994 - val_loss: 2.6183 - val_acc: 0.3688\n",
            "Epoch 154/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0165 - acc: 0.9994 - val_loss: 2.9098 - val_acc: 0.3475\n",
            "Epoch 155/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0152 - acc: 1.0000 - val_loss: 2.7299 - val_acc: 0.3830\n",
            "Epoch 156/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0153 - acc: 1.0000 - val_loss: 2.7270 - val_acc: 0.3759\n",
            "Epoch 157/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0140 - acc: 1.0000 - val_loss: 2.7918 - val_acc: 0.3570\n",
            "Epoch 158/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0143 - acc: 1.0000 - val_loss: 2.8324 - val_acc: 0.3641\n",
            "Epoch 159/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0117 - acc: 1.0000 - val_loss: 2.7727 - val_acc: 0.3783\n",
            "Epoch 160/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0120 - acc: 1.0000 - val_loss: 2.7022 - val_acc: 0.3759\n",
            "Epoch 161/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0133 - acc: 0.9994 - val_loss: 2.9660 - val_acc: 0.3546\n",
            "Epoch 162/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0144 - acc: 0.9988 - val_loss: 2.6862 - val_acc: 0.3641\n",
            "Epoch 163/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0134 - acc: 1.0000 - val_loss: 2.8975 - val_acc: 0.3570\n",
            "Epoch 164/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0139 - acc: 1.0000 - val_loss: 2.8425 - val_acc: 0.3593\n",
            "Epoch 165/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0155 - acc: 0.9994 - val_loss: 2.8725 - val_acc: 0.3546\n",
            "Epoch 166/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0144 - acc: 0.9994 - val_loss: 2.7594 - val_acc: 0.3830\n",
            "Epoch 167/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0151 - acc: 1.0000 - val_loss: 2.8127 - val_acc: 0.3877\n",
            "Epoch 168/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0135 - acc: 1.0000 - val_loss: 2.8580 - val_acc: 0.3735\n",
            "Epoch 169/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0157 - acc: 0.9988 - val_loss: 2.8153 - val_acc: 0.3617\n",
            "Epoch 170/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0142 - acc: 1.0000 - val_loss: 2.8040 - val_acc: 0.3641\n",
            "Epoch 171/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0142 - acc: 1.0000 - val_loss: 2.8129 - val_acc: 0.3783\n",
            "Epoch 172/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0124 - acc: 1.0000 - val_loss: 2.8934 - val_acc: 0.3735\n",
            "Epoch 173/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0138 - acc: 0.9988 - val_loss: 2.9299 - val_acc: 0.3712\n",
            "Epoch 174/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0118 - acc: 1.0000 - val_loss: 2.9768 - val_acc: 0.3593\n",
            "Epoch 175/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0115 - acc: 1.0000 - val_loss: 2.8736 - val_acc: 0.3522\n",
            "Epoch 176/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 2.8367 - val_acc: 0.3404\n",
            "Epoch 177/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0098 - acc: 1.0000 - val_loss: 2.8534 - val_acc: 0.3522\n",
            "Epoch 178/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0097 - acc: 0.9994 - val_loss: 2.6490 - val_acc: 0.3688\n",
            "Epoch 179/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0090 - acc: 1.0000 - val_loss: 2.6076 - val_acc: 0.3853\n",
            "Epoch 180/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0092 - acc: 1.0000 - val_loss: 2.6458 - val_acc: 0.3617\n",
            "Epoch 181/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 2.6273 - val_acc: 0.3783\n",
            "Epoch 182/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0097 - acc: 1.0000 - val_loss: 2.5988 - val_acc: 0.3688\n",
            "Epoch 183/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0114 - acc: 0.9994 - val_loss: 2.8283 - val_acc: 0.3546\n",
            "Epoch 184/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0159 - acc: 0.9982 - val_loss: 2.9603 - val_acc: 0.3641\n",
            "Epoch 185/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0160 - acc: 0.9994 - val_loss: 3.3781 - val_acc: 0.3570\n",
            "Epoch 186/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0160 - acc: 0.9994 - val_loss: 3.3424 - val_acc: 0.3735\n",
            "Epoch 187/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0179 - acc: 0.9994 - val_loss: 3.5639 - val_acc: 0.3641\n",
            "Epoch 188/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0160 - acc: 0.9994 - val_loss: 3.3967 - val_acc: 0.3806\n",
            "Epoch 189/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0154 - acc: 0.9994 - val_loss: 3.4637 - val_acc: 0.3712\n",
            "Epoch 190/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0145 - acc: 0.9994 - val_loss: 3.3966 - val_acc: 0.3452\n",
            "Epoch 191/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0146 - acc: 0.9994 - val_loss: 3.3508 - val_acc: 0.3570\n",
            "Epoch 192/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0135 - acc: 0.9994 - val_loss: 3.3215 - val_acc: 0.3404\n",
            "Epoch 193/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0137 - acc: 0.9994 - val_loss: 3.0390 - val_acc: 0.3522\n",
            "Epoch 194/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0162 - acc: 0.9982 - val_loss: 3.4133 - val_acc: 0.3499\n",
            "Epoch 195/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0158 - acc: 1.0000 - val_loss: 3.1750 - val_acc: 0.3381\n",
            "Epoch 196/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0154 - acc: 0.9994 - val_loss: 3.2951 - val_acc: 0.3546\n",
            "Epoch 197/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0145 - acc: 1.0000 - val_loss: 3.1854 - val_acc: 0.3759\n",
            "Epoch 198/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0142 - acc: 0.9994 - val_loss: 3.0516 - val_acc: 0.3688\n",
            "Epoch 199/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0127 - acc: 1.0000 - val_loss: 3.1662 - val_acc: 0.3641\n",
            "Epoch 200/200\n",
            "1692/1692 [==============================] - 4s 2ms/step - loss: 0.0112 - acc: 1.0000 - val_loss: 2.9744 - val_acc: 0.3664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSURaASEgbDb",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61L_D_Evq0ax",
        "colab_type": "text"
      },
      "source": [
        "## Treat each recording as a chnnel\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjpIMrLTq6J0",
        "colab_type": "code",
        "outputId": "833d5c32-b173-42d0-d4c5-27cbe538d1d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "dropout_rate = 0.3\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=25, kernel_size=(1,10), \n",
        "                strides=(1, 1), input_shape=(1,1000,22),\n",
        "#                 activation = 'relu',\n",
        "                padding ='valid'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(GaussianNoise(0.1))\n",
        "model.add(Conv2D(filters=25, kernel_size=(1,10), \n",
        "                strides=(1, 1),\n",
        "                padding ='valid', \n",
        "                activation = 'relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(GaussianNoise(0.1))\n",
        "# model.add(SpatialDropout2D(dropout_rate))\n",
        "model.add(MaxPool2D(pool_size=(1, 2), strides=(1, 2)))\n",
        "\n",
        "model.add(Conv2D(filters=50, kernel_size=(1,10), \n",
        "                strides=(1, 3),\n",
        "                padding ='valid', \n",
        "                activation = 'relu'))\n",
        "\n",
        "model.add(Conv2D(filters=50, kernel_size=(1,10), \n",
        "                strides=(1, 3),\n",
        "                padding ='valid', \n",
        "                activation = 'relu'))\n",
        "\n",
        "# model.add(Conv2D(filters=50, kernel_size=(1,10), \n",
        "#                 strides=(1, 2),\n",
        "#                 padding ='valid', activation = 'elu'))\n",
        "# model.add(Conv2D(filters=50, kernel_size=(1,10), \n",
        "#                 strides=(1, 2),\n",
        "#                 padding ='valid', activation = 'elu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(SpatialDropout2D(dropout_rate))\n",
        "\n",
        "# model.add(Conv2D(filters=100, kernel_size=(1,10), \n",
        "#                 strides=(1, 2),\n",
        "#                 padding ='valid', activation = 'elu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(SpatialDropout2D(dropout_rate))\n",
        "\n",
        "# model.add(Reshape((-1,200,1)))\n",
        "# model.add(MaxPool2D(pool_size=(3, 1), strides=(3, 1)))\n",
        "model.add(Flatten())\n",
        "# model.add(Dense(units = 20))\n",
        "# model.add(Activation('elu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dense(units = 50, activation='relu', kernel_regularizer =  'l2'))\n",
        "model.add(Dense(units = 4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 1, 991, 25)        5525      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 982, 25)        6275      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1, 491, 25)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 1, 161, 50)        12550     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 1, 51, 50)         25050     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2550)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 10204     \n",
            "=================================================================\n",
            "Total params: 59,604\n",
            "Trainable params: 59,604\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFurO42VtqSd",
        "colab_type": "code",
        "outputId": "e03fad52-3e82-4bc2-cbcf-8dd3e69aebf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "early_stop = EarlyStopping(monitor = 'loss',\n",
        "                           patience = 20)\n",
        "history = model.fit(X_train.reshape(-1,1,1000,22), Y_train, \n",
        "                    batch_size=64, epochs = 200, validation_split = 0.2,\n",
        "                    callbacks = [early_stop])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1692 samples, validate on 423 samples\n",
            "Epoch 1/200\n",
            "1692/1692 [==============================] - 1s 468us/step - loss: 2.5607 - acc: 0.2595 - val_loss: 1.3852 - val_acc: 0.2837\n",
            "Epoch 2/200\n",
            "1692/1692 [==============================] - 0s 235us/step - loss: 1.3660 - acc: 0.3239 - val_loss: 1.3902 - val_acc: 0.2979\n",
            "Epoch 3/200\n",
            "1692/1692 [==============================] - 0s 237us/step - loss: 1.2976 - acc: 0.4066 - val_loss: 1.3603 - val_acc: 0.3475\n",
            "Epoch 4/200\n",
            "1692/1692 [==============================] - 0s 233us/step - loss: 1.1755 - acc: 0.4835 - val_loss: 1.4222 - val_acc: 0.3144\n",
            "Epoch 5/200\n",
            "1692/1692 [==============================] - 0s 233us/step - loss: 0.9839 - acc: 0.5993 - val_loss: 1.4935 - val_acc: 0.2955\n",
            "Epoch 6/200\n",
            "1692/1692 [==============================] - 0s 236us/step - loss: 0.7675 - acc: 0.7163 - val_loss: 1.6528 - val_acc: 0.3310\n",
            "Epoch 7/200\n",
            "1692/1692 [==============================] - 0s 242us/step - loss: 0.5565 - acc: 0.8233 - val_loss: 1.7211 - val_acc: 0.3168\n",
            "Epoch 8/200\n",
            "1692/1692 [==============================] - 0s 223us/step - loss: 0.3622 - acc: 0.9102 - val_loss: 2.0409 - val_acc: 0.3381\n",
            "Epoch 9/200\n",
            "1692/1692 [==============================] - 0s 222us/step - loss: 0.2084 - acc: 0.9557 - val_loss: 2.3430 - val_acc: 0.3191\n",
            "Epoch 10/200\n",
            "1692/1692 [==============================] - 0s 228us/step - loss: 0.1042 - acc: 0.9905 - val_loss: 2.6455 - val_acc: 0.3239\n",
            "Epoch 11/200\n",
            "1692/1692 [==============================] - 0s 222us/step - loss: 0.0475 - acc: 0.9988 - val_loss: 2.8422 - val_acc: 0.3239\n",
            "Epoch 12/200\n",
            "1692/1692 [==============================] - 0s 223us/step - loss: 0.0217 - acc: 1.0000 - val_loss: 3.0393 - val_acc: 0.3168\n",
            "Epoch 13/200\n",
            "1692/1692 [==============================] - 0s 231us/step - loss: 0.0115 - acc: 1.0000 - val_loss: 3.2725 - val_acc: 0.3191\n",
            "Epoch 14/200\n",
            "1692/1692 [==============================] - 0s 235us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 3.3412 - val_acc: 0.3262\n",
            "Epoch 15/200\n",
            "1692/1692 [==============================] - 0s 228us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.4424 - val_acc: 0.3239\n",
            "Epoch 16/200\n",
            "1692/1692 [==============================] - 0s 225us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5035 - val_acc: 0.3191\n",
            "Epoch 17/200\n",
            "1692/1692 [==============================] - 0s 235us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5572 - val_acc: 0.3121\n",
            "Epoch 18/200\n",
            "1692/1692 [==============================] - 0s 231us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.6129 - val_acc: 0.3144\n",
            "Epoch 19/200\n",
            "1692/1692 [==============================] - 0s 228us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 3.6615 - val_acc: 0.3191\n",
            "Epoch 20/200\n",
            "1692/1692 [==============================] - 0s 236us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 3.6978 - val_acc: 0.3191\n",
            "Epoch 21/200\n",
            "1692/1692 [==============================] - 0s 227us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 3.7365 - val_acc: 0.3144\n",
            "Epoch 22/200\n",
            "1692/1692 [==============================] - 0s 226us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 3.7713 - val_acc: 0.3215\n",
            "Epoch 23/200\n",
            "1692/1692 [==============================] - 0s 234us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 3.8066 - val_acc: 0.3168\n",
            "Epoch 24/200\n",
            "1692/1692 [==============================] - 0s 233us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 3.8384 - val_acc: 0.3191\n",
            "Epoch 25/200\n",
            "1692/1692 [==============================] - 0s 234us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 3.8673 - val_acc: 0.3168\n",
            "Epoch 26/200\n",
            "1692/1692 [==============================] - 0s 227us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 3.8997 - val_acc: 0.3215\n",
            "Epoch 27/200\n",
            "1692/1692 [==============================] - 0s 226us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 3.9247 - val_acc: 0.3262\n",
            "Epoch 28/200\n",
            "1692/1692 [==============================] - 0s 241us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 3.9499 - val_acc: 0.3239\n",
            "Epoch 29/200\n",
            "1692/1692 [==============================] - 0s 231us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 3.9766 - val_acc: 0.3239\n",
            "Epoch 30/200\n",
            "1692/1692 [==============================] - 0s 227us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 3.9996 - val_acc: 0.3310\n",
            "Epoch 31/200\n",
            "1692/1692 [==============================] - 0s 232us/step - loss: 9.9264e-04 - acc: 1.0000 - val_loss: 4.0215 - val_acc: 0.3262\n",
            "Epoch 32/200\n",
            "1692/1692 [==============================] - 0s 243us/step - loss: 9.2645e-04 - acc: 1.0000 - val_loss: 4.0460 - val_acc: 0.3310\n",
            "Epoch 33/200\n",
            "1692/1692 [==============================] - 0s 237us/step - loss: 8.6817e-04 - acc: 1.0000 - val_loss: 4.0717 - val_acc: 0.3333\n",
            "Epoch 34/200\n",
            "1692/1692 [==============================] - 0s 233us/step - loss: 8.1084e-04 - acc: 1.0000 - val_loss: 4.0901 - val_acc: 0.3310\n",
            "Epoch 35/200\n",
            "1692/1692 [==============================] - 0s 243us/step - loss: 7.6123e-04 - acc: 1.0000 - val_loss: 4.1130 - val_acc: 0.3333\n",
            "Epoch 36/200\n",
            "1692/1692 [==============================] - 0s 234us/step - loss: 7.2083e-04 - acc: 1.0000 - val_loss: 4.1360 - val_acc: 0.3333\n",
            "Epoch 37/200\n",
            "1692/1692 [==============================] - 0s 237us/step - loss: 6.7579e-04 - acc: 1.0000 - val_loss: 4.1505 - val_acc: 0.3333\n",
            "Epoch 38/200\n",
            "1692/1692 [==============================] - 0s 227us/step - loss: 6.3812e-04 - acc: 1.0000 - val_loss: 4.1712 - val_acc: 0.3357\n",
            "Epoch 39/200\n",
            "1692/1692 [==============================] - 0s 232us/step - loss: 6.0387e-04 - acc: 1.0000 - val_loss: 4.1916 - val_acc: 0.3357\n",
            "Epoch 40/200\n",
            "1692/1692 [==============================] - 0s 237us/step - loss: 5.7004e-04 - acc: 1.0000 - val_loss: 4.2073 - val_acc: 0.3333\n",
            "Epoch 41/200\n",
            "1692/1692 [==============================] - 0s 230us/step - loss: 5.4083e-04 - acc: 1.0000 - val_loss: 4.2279 - val_acc: 0.3357\n",
            "Epoch 42/200\n",
            "1692/1692 [==============================] - 0s 227us/step - loss: 5.1291e-04 - acc: 1.0000 - val_loss: 4.2431 - val_acc: 0.3357\n",
            "Epoch 43/200\n",
            "1692/1692 [==============================] - 0s 235us/step - loss: 4.8750e-04 - acc: 1.0000 - val_loss: 4.2625 - val_acc: 0.3357\n",
            "Epoch 44/200\n",
            "1692/1692 [==============================] - 0s 232us/step - loss: 4.6408e-04 - acc: 1.0000 - val_loss: 4.2774 - val_acc: 0.3357\n",
            "Epoch 45/200\n",
            "1692/1692 [==============================] - 0s 232us/step - loss: 4.4202e-04 - acc: 1.0000 - val_loss: 4.2926 - val_acc: 0.3381\n",
            "Epoch 46/200\n",
            "1692/1692 [==============================] - 0s 235us/step - loss: 4.2193e-04 - acc: 1.0000 - val_loss: 4.3091 - val_acc: 0.3357\n",
            "Epoch 47/200\n",
            "1692/1692 [==============================] - 0s 225us/step - loss: 4.0181e-04 - acc: 1.0000 - val_loss: 4.3239 - val_acc: 0.3357\n",
            "Epoch 48/200\n",
            "1692/1692 [==============================] - 0s 238us/step - loss: 3.8408e-04 - acc: 1.0000 - val_loss: 4.3427 - val_acc: 0.3357\n",
            "Epoch 49/200\n",
            "1692/1692 [==============================] - 0s 234us/step - loss: 3.6686e-04 - acc: 1.0000 - val_loss: 4.3547 - val_acc: 0.3357\n",
            "Epoch 50/200\n",
            "1692/1692 [==============================] - 0s 236us/step - loss: 3.5176e-04 - acc: 1.0000 - val_loss: 4.3718 - val_acc: 0.3357\n",
            "Epoch 51/200\n",
            "1692/1692 [==============================] - 0s 223us/step - loss: 3.3624e-04 - acc: 1.0000 - val_loss: 4.3846 - val_acc: 0.3333\n",
            "Epoch 52/200\n",
            "1692/1692 [==============================] - 0s 227us/step - loss: 3.2224e-04 - acc: 1.0000 - val_loss: 4.3960 - val_acc: 0.3357\n",
            "Epoch 53/200\n",
            "1692/1692 [==============================] - 0s 234us/step - loss: 3.0912e-04 - acc: 1.0000 - val_loss: 4.4117 - val_acc: 0.3357\n",
            "Epoch 54/200\n",
            "1692/1692 [==============================] - 0s 226us/step - loss: 2.9619e-04 - acc: 1.0000 - val_loss: 4.4264 - val_acc: 0.3357\n",
            "Epoch 55/200\n",
            "1692/1692 [==============================] - 0s 226us/step - loss: 2.8483e-04 - acc: 1.0000 - val_loss: 4.4351 - val_acc: 0.3381\n",
            "Epoch 56/200\n",
            "1692/1692 [==============================] - 0s 231us/step - loss: 2.7352e-04 - acc: 1.0000 - val_loss: 4.4502 - val_acc: 0.3357\n",
            "Epoch 57/200\n",
            "1692/1692 [==============================] - 0s 227us/step - loss: 2.6293e-04 - acc: 1.0000 - val_loss: 4.4624 - val_acc: 0.3381\n",
            "Epoch 58/200\n",
            "1692/1692 [==============================] - 0s 229us/step - loss: 2.5305e-04 - acc: 1.0000 - val_loss: 4.4707 - val_acc: 0.3381\n",
            "Epoch 59/200\n",
            "1692/1692 [==============================] - 0s 229us/step - loss: 2.4352e-04 - acc: 1.0000 - val_loss: 4.4853 - val_acc: 0.3333\n",
            "Epoch 60/200\n",
            "1692/1692 [==============================] - 0s 231us/step - loss: 2.3464e-04 - acc: 1.0000 - val_loss: 4.4952 - val_acc: 0.3404\n",
            "Epoch 61/200\n",
            "1692/1692 [==============================] - 0s 230us/step - loss: 2.2632e-04 - acc: 1.0000 - val_loss: 4.5052 - val_acc: 0.3357\n",
            "Epoch 62/200\n",
            "1692/1692 [==============================] - 0s 229us/step - loss: 2.1789e-04 - acc: 1.0000 - val_loss: 4.5175 - val_acc: 0.3381\n",
            "Epoch 63/200\n",
            "1692/1692 [==============================] - 0s 232us/step - loss: 2.1059e-04 - acc: 1.0000 - val_loss: 4.5318 - val_acc: 0.3381\n",
            "Epoch 64/200\n",
            "1692/1692 [==============================] - 0s 229us/step - loss: 2.0315e-04 - acc: 1.0000 - val_loss: 4.5414 - val_acc: 0.3381\n",
            "Epoch 65/200\n",
            "1692/1692 [==============================] - 0s 229us/step - loss: 1.9598e-04 - acc: 1.0000 - val_loss: 4.5518 - val_acc: 0.3381\n",
            "Epoch 66/200\n",
            "1692/1692 [==============================] - 0s 223us/step - loss: 1.8914e-04 - acc: 1.0000 - val_loss: 4.5640 - val_acc: 0.3381\n",
            "Epoch 67/200\n",
            "1692/1692 [==============================] - 0s 231us/step - loss: 1.8271e-04 - acc: 1.0000 - val_loss: 4.5745 - val_acc: 0.3381\n",
            "Epoch 68/200\n",
            "1692/1692 [==============================] - 0s 234us/step - loss: 1.7701e-04 - acc: 1.0000 - val_loss: 4.5845 - val_acc: 0.3381\n",
            "Epoch 69/200\n",
            "1692/1692 [==============================] - 0s 232us/step - loss: 1.7095e-04 - acc: 1.0000 - val_loss: 4.5928 - val_acc: 0.3381\n",
            "Epoch 70/200\n",
            "1692/1692 [==============================] - 0s 234us/step - loss: 1.6547e-04 - acc: 1.0000 - val_loss: 4.6047 - val_acc: 0.3404\n",
            "Epoch 71/200\n",
            "1692/1692 [==============================] - 0s 232us/step - loss: 1.6045e-04 - acc: 1.0000 - val_loss: 4.6124 - val_acc: 0.3404\n",
            "Epoch 72/200\n",
            "1692/1692 [==============================] - 0s 239us/step - loss: 1.5484e-04 - acc: 1.0000 - val_loss: 4.6231 - val_acc: 0.3404\n",
            "Epoch 73/200\n",
            "1692/1692 [==============================] - 0s 237us/step - loss: 1.4972e-04 - acc: 1.0000 - val_loss: 4.6320 - val_acc: 0.3404\n",
            "Epoch 74/200\n",
            "1692/1692 [==============================] - 0s 236us/step - loss: 1.4532e-04 - acc: 1.0000 - val_loss: 4.6449 - val_acc: 0.3357\n",
            "Epoch 75/200\n",
            "1692/1692 [==============================] - 0s 242us/step - loss: 1.4076e-04 - acc: 1.0000 - val_loss: 4.6504 - val_acc: 0.3333\n",
            "Epoch 76/200\n",
            "1692/1692 [==============================] - 0s 237us/step - loss: 1.3655e-04 - acc: 1.0000 - val_loss: 4.6593 - val_acc: 0.3404\n",
            "Epoch 77/200\n",
            "1692/1692 [==============================] - 0s 234us/step - loss: 1.3234e-04 - acc: 1.0000 - val_loss: 4.6701 - val_acc: 0.3404\n",
            "Epoch 78/200\n",
            "1692/1692 [==============================] - 0s 230us/step - loss: 1.2883e-04 - acc: 1.0000 - val_loss: 4.6809 - val_acc: 0.3404\n",
            "Epoch 79/200\n",
            "1692/1692 [==============================] - 0s 239us/step - loss: 1.2478e-04 - acc: 1.0000 - val_loss: 4.6869 - val_acc: 0.3381\n",
            "Epoch 80/200\n",
            "1692/1692 [==============================] - 0s 239us/step - loss: 1.2112e-04 - acc: 1.0000 - val_loss: 4.6999 - val_acc: 0.3381\n",
            "Epoch 81/200\n",
            "1692/1692 [==============================] - 0s 215us/step - loss: 1.1752e-04 - acc: 1.0000 - val_loss: 4.7088 - val_acc: 0.3381\n",
            "Epoch 82/200\n",
            "1692/1692 [==============================] - 0s 213us/step - loss: 1.1405e-04 - acc: 1.0000 - val_loss: 4.7145 - val_acc: 0.3381\n",
            "Epoch 83/200\n",
            "1692/1692 [==============================] - 0s 217us/step - loss: 1.1071e-04 - acc: 1.0000 - val_loss: 4.7227 - val_acc: 0.3381\n",
            "Epoch 84/200\n",
            "1692/1692 [==============================] - 0s 221us/step - loss: 1.0764e-04 - acc: 1.0000 - val_loss: 4.7318 - val_acc: 0.3381\n",
            "Epoch 85/200\n",
            "1692/1692 [==============================] - 0s 225us/step - loss: 1.0464e-04 - acc: 1.0000 - val_loss: 4.7419 - val_acc: 0.3381\n",
            "Epoch 86/200\n",
            "1692/1692 [==============================] - 0s 225us/step - loss: 1.0179e-04 - acc: 1.0000 - val_loss: 4.7503 - val_acc: 0.3381\n",
            "Epoch 87/200\n",
            "1692/1692 [==============================] - 0s 216us/step - loss: 9.8873e-05 - acc: 1.0000 - val_loss: 4.7570 - val_acc: 0.3357\n",
            "Epoch 88/200\n",
            "1692/1692 [==============================] - 0s 226us/step - loss: 9.6293e-05 - acc: 1.0000 - val_loss: 4.7649 - val_acc: 0.3357\n",
            "Epoch 89/200\n",
            "1692/1692 [==============================] - 0s 225us/step - loss: 9.3619e-05 - acc: 1.0000 - val_loss: 4.7738 - val_acc: 0.3381\n",
            "Epoch 90/200\n",
            "1692/1692 [==============================] - 0s 216us/step - loss: 9.1192e-05 - acc: 1.0000 - val_loss: 4.7820 - val_acc: 0.3381\n",
            "Epoch 91/200\n",
            "1692/1692 [==============================] - 0s 221us/step - loss: 8.8686e-05 - acc: 1.0000 - val_loss: 4.7887 - val_acc: 0.3381\n",
            "Epoch 92/200\n",
            "1692/1692 [==============================] - 0s 219us/step - loss: 8.6298e-05 - acc: 1.0000 - val_loss: 4.7957 - val_acc: 0.3381\n",
            "Epoch 93/200\n",
            "1692/1692 [==============================] - 0s 217us/step - loss: 8.4138e-05 - acc: 1.0000 - val_loss: 4.8041 - val_acc: 0.3357\n",
            "Epoch 94/200\n",
            "1692/1692 [==============================] - 0s 228us/step - loss: 8.1893e-05 - acc: 1.0000 - val_loss: 4.8145 - val_acc: 0.3381\n",
            "Epoch 95/200\n",
            "1692/1692 [==============================] - 0s 214us/step - loss: 7.9917e-05 - acc: 1.0000 - val_loss: 4.8199 - val_acc: 0.3381\n",
            "Epoch 96/200\n",
            "1692/1692 [==============================] - 0s 225us/step - loss: 7.7682e-05 - acc: 1.0000 - val_loss: 4.8271 - val_acc: 0.3381\n",
            "Epoch 97/200\n",
            "1692/1692 [==============================] - 0s 222us/step - loss: 7.5794e-05 - acc: 1.0000 - val_loss: 4.8349 - val_acc: 0.3381\n",
            "Epoch 98/200\n",
            "1692/1692 [==============================] - 0s 218us/step - loss: 7.3796e-05 - acc: 1.0000 - val_loss: 4.8421 - val_acc: 0.3381\n",
            "Epoch 99/200\n",
            "1692/1692 [==============================] - 0s 219us/step - loss: 7.1976e-05 - acc: 1.0000 - val_loss: 4.8515 - val_acc: 0.3381\n",
            "Epoch 100/200\n",
            "1692/1692 [==============================] - 0s 235us/step - loss: 7.0271e-05 - acc: 1.0000 - val_loss: 4.8547 - val_acc: 0.3381\n",
            "Epoch 101/200\n",
            "1692/1692 [==============================] - 0s 219us/step - loss: 6.8424e-05 - acc: 1.0000 - val_loss: 4.8640 - val_acc: 0.3381\n",
            "Epoch 102/200\n",
            "1692/1692 [==============================] - 0s 221us/step - loss: 6.6702e-05 - acc: 1.0000 - val_loss: 4.8704 - val_acc: 0.3404\n",
            "Epoch 103/200\n",
            "1692/1692 [==============================] - 0s 222us/step - loss: 6.5149e-05 - acc: 1.0000 - val_loss: 4.8764 - val_acc: 0.3381\n",
            "Epoch 104/200\n",
            "1692/1692 [==============================] - 0s 224us/step - loss: 6.3542e-05 - acc: 1.0000 - val_loss: 4.8840 - val_acc: 0.3404\n",
            "Epoch 105/200\n",
            "1692/1692 [==============================] - 0s 231us/step - loss: 6.1946e-05 - acc: 1.0000 - val_loss: 4.8926 - val_acc: 0.3381\n",
            "Epoch 106/200\n",
            "1692/1692 [==============================] - 0s 222us/step - loss: 6.0556e-05 - acc: 1.0000 - val_loss: 4.8977 - val_acc: 0.3404\n",
            "Epoch 107/200\n",
            "1692/1692 [==============================] - 0s 222us/step - loss: 5.9084e-05 - acc: 1.0000 - val_loss: 4.9056 - val_acc: 0.3404\n",
            "Epoch 108/200\n",
            "1692/1692 [==============================] - 0s 221us/step - loss: 5.7689e-05 - acc: 1.0000 - val_loss: 4.9100 - val_acc: 0.3404\n",
            "Epoch 109/200\n",
            "1692/1692 [==============================] - 0s 220us/step - loss: 5.6337e-05 - acc: 1.0000 - val_loss: 4.9171 - val_acc: 0.3428\n",
            "Epoch 110/200\n",
            "1692/1692 [==============================] - 0s 217us/step - loss: 5.4970e-05 - acc: 1.0000 - val_loss: 4.9234 - val_acc: 0.3404\n",
            "Epoch 111/200\n",
            "1692/1692 [==============================] - 0s 211us/step - loss: 5.3716e-05 - acc: 1.0000 - val_loss: 4.9299 - val_acc: 0.3404\n",
            "Epoch 112/200\n",
            "1692/1692 [==============================] - 0s 222us/step - loss: 5.2463e-05 - acc: 1.0000 - val_loss: 4.9373 - val_acc: 0.3404\n",
            "Epoch 113/200\n",
            "1692/1692 [==============================] - 0s 228us/step - loss: 5.1254e-05 - acc: 1.0000 - val_loss: 4.9435 - val_acc: 0.3404\n",
            "Epoch 114/200\n",
            "1692/1692 [==============================] - 0s 214us/step - loss: 5.0148e-05 - acc: 1.0000 - val_loss: 4.9498 - val_acc: 0.3404\n",
            "Epoch 115/200\n",
            "1692/1692 [==============================] - 0s 213us/step - loss: 4.8981e-05 - acc: 1.0000 - val_loss: 4.9565 - val_acc: 0.3404\n",
            "Epoch 116/200\n",
            "1692/1692 [==============================] - 0s 215us/step - loss: 4.7843e-05 - acc: 1.0000 - val_loss: 4.9621 - val_acc: 0.3428\n",
            "Epoch 117/200\n",
            "1692/1692 [==============================] - 0s 208us/step - loss: 4.6736e-05 - acc: 1.0000 - val_loss: 4.9676 - val_acc: 0.3428\n",
            "Epoch 118/200\n",
            "1692/1692 [==============================] - 0s 214us/step - loss: 4.5772e-05 - acc: 1.0000 - val_loss: 4.9755 - val_acc: 0.3404\n",
            "Epoch 119/200\n",
            "1692/1692 [==============================] - 0s 215us/step - loss: 4.4741e-05 - acc: 1.0000 - val_loss: 4.9834 - val_acc: 0.3404\n",
            "Epoch 120/200\n",
            "1692/1692 [==============================] - 0s 217us/step - loss: 4.3746e-05 - acc: 1.0000 - val_loss: 4.9868 - val_acc: 0.3428\n",
            "Epoch 121/200\n",
            "1692/1692 [==============================] - 0s 216us/step - loss: 4.2742e-05 - acc: 1.0000 - val_loss: 4.9937 - val_acc: 0.3428\n",
            "Epoch 122/200\n",
            "1692/1692 [==============================] - 0s 217us/step - loss: 4.1794e-05 - acc: 1.0000 - val_loss: 5.0002 - val_acc: 0.3404\n",
            "Epoch 123/200\n",
            "1692/1692 [==============================] - 0s 220us/step - loss: 4.0897e-05 - acc: 1.0000 - val_loss: 5.0063 - val_acc: 0.3428\n",
            "Epoch 124/200\n",
            "1692/1692 [==============================] - 0s 227us/step - loss: 4.0026e-05 - acc: 1.0000 - val_loss: 5.0138 - val_acc: 0.3452\n",
            "Epoch 125/200\n",
            "1692/1692 [==============================] - 0s 214us/step - loss: 3.9096e-05 - acc: 1.0000 - val_loss: 5.0164 - val_acc: 0.3475\n",
            "Epoch 126/200\n",
            "1692/1692 [==============================] - 0s 214us/step - loss: 3.8288e-05 - acc: 1.0000 - val_loss: 5.0223 - val_acc: 0.3452\n",
            "Epoch 127/200\n",
            "1692/1692 [==============================] - 0s 216us/step - loss: 3.7440e-05 - acc: 1.0000 - val_loss: 5.0307 - val_acc: 0.3475\n",
            "Epoch 128/200\n",
            "1692/1692 [==============================] - 0s 217us/step - loss: 3.6656e-05 - acc: 1.0000 - val_loss: 5.0346 - val_acc: 0.3475\n",
            "Epoch 129/200\n",
            "1692/1692 [==============================] - 0s 217us/step - loss: 3.5899e-05 - acc: 1.0000 - val_loss: 5.0416 - val_acc: 0.3499\n",
            "Epoch 130/200\n",
            "1692/1692 [==============================] - 0s 214us/step - loss: 3.5131e-05 - acc: 1.0000 - val_loss: 5.0481 - val_acc: 0.3452\n",
            "Epoch 131/200\n",
            "1692/1692 [==============================] - 0s 213us/step - loss: 3.4418e-05 - acc: 1.0000 - val_loss: 5.0542 - val_acc: 0.3499\n",
            "Epoch 132/200\n",
            "1692/1692 [==============================] - 0s 220us/step - loss: 3.3652e-05 - acc: 1.0000 - val_loss: 5.0583 - val_acc: 0.3475\n",
            "Epoch 133/200\n",
            "1692/1692 [==============================] - 0s 210us/step - loss: 3.2947e-05 - acc: 1.0000 - val_loss: 5.0649 - val_acc: 0.3499\n",
            "Epoch 134/200\n",
            "1692/1692 [==============================] - 0s 218us/step - loss: 3.2273e-05 - acc: 1.0000 - val_loss: 5.0719 - val_acc: 0.3452\n",
            "Epoch 135/200\n",
            "1692/1692 [==============================] - 0s 214us/step - loss: 3.1573e-05 - acc: 1.0000 - val_loss: 5.0767 - val_acc: 0.3499\n",
            "Epoch 136/200\n",
            "1692/1692 [==============================] - 0s 217us/step - loss: 3.0929e-05 - acc: 1.0000 - val_loss: 5.0828 - val_acc: 0.3499\n",
            "Epoch 137/200\n",
            "1692/1692 [==============================] - 0s 217us/step - loss: 3.0261e-05 - acc: 1.0000 - val_loss: 5.0883 - val_acc: 0.3499\n",
            "Epoch 138/200\n",
            "1692/1692 [==============================] - 0s 217us/step - loss: 2.9651e-05 - acc: 1.0000 - val_loss: 5.0946 - val_acc: 0.3475\n",
            "Epoch 139/200\n",
            "1692/1692 [==============================] - 0s 216us/step - loss: 2.9036e-05 - acc: 1.0000 - val_loss: 5.0994 - val_acc: 0.3475\n",
            "Epoch 140/200\n",
            "1692/1692 [==============================] - 0s 214us/step - loss: 2.8469e-05 - acc: 1.0000 - val_loss: 5.1058 - val_acc: 0.3475\n",
            "Epoch 141/200\n",
            "1692/1692 [==============================] - 0s 223us/step - loss: 2.7842e-05 - acc: 1.0000 - val_loss: 5.1129 - val_acc: 0.3475\n",
            "Epoch 142/200\n",
            "1692/1692 [==============================] - 0s 216us/step - loss: 2.7286e-05 - acc: 1.0000 - val_loss: 5.1198 - val_acc: 0.3475\n",
            "Epoch 143/200\n",
            "1692/1692 [==============================] - 0s 218us/step - loss: 2.6757e-05 - acc: 1.0000 - val_loss: 5.1229 - val_acc: 0.3475\n",
            "Epoch 144/200\n",
            "1692/1692 [==============================] - 0s 213us/step - loss: 2.6198e-05 - acc: 1.0000 - val_loss: 5.1287 - val_acc: 0.3499\n",
            "Epoch 145/200\n",
            "1692/1692 [==============================] - 0s 216us/step - loss: 2.5658e-05 - acc: 1.0000 - val_loss: 5.1357 - val_acc: 0.3499\n",
            "Epoch 146/200\n",
            "1692/1692 [==============================] - 0s 220us/step - loss: 2.5179e-05 - acc: 1.0000 - val_loss: 5.1425 - val_acc: 0.3452\n",
            "Epoch 147/200\n",
            "1692/1692 [==============================] - 0s 210us/step - loss: 2.4696e-05 - acc: 1.0000 - val_loss: 5.1465 - val_acc: 0.3475\n",
            "Epoch 148/200\n",
            "1692/1692 [==============================] - 0s 218us/step - loss: 2.4159e-05 - acc: 1.0000 - val_loss: 5.1525 - val_acc: 0.3475\n",
            "Epoch 149/200\n",
            "1692/1692 [==============================] - 0s 222us/step - loss: 2.3691e-05 - acc: 1.0000 - val_loss: 5.1579 - val_acc: 0.3475\n",
            "Epoch 150/200\n",
            "1692/1692 [==============================] - 0s 217us/step - loss: 2.3218e-05 - acc: 1.0000 - val_loss: 5.1636 - val_acc: 0.3475\n",
            "Epoch 151/200\n",
            "1692/1692 [==============================] - 0s 215us/step - loss: 2.2754e-05 - acc: 1.0000 - val_loss: 5.1674 - val_acc: 0.3475\n",
            "Epoch 152/200\n",
            "1692/1692 [==============================] - 0s 211us/step - loss: 2.2301e-05 - acc: 1.0000 - val_loss: 5.1732 - val_acc: 0.3475\n",
            "Epoch 153/200\n",
            "1692/1692 [==============================] - 0s 214us/step - loss: 2.1880e-05 - acc: 1.0000 - val_loss: 5.1794 - val_acc: 0.3475\n",
            "Epoch 154/200\n",
            "1692/1692 [==============================] - 0s 225us/step - loss: 2.1440e-05 - acc: 1.0000 - val_loss: 5.1844 - val_acc: 0.3475\n",
            "Epoch 155/200\n",
            "1692/1692 [==============================] - 0s 213us/step - loss: 2.1032e-05 - acc: 1.0000 - val_loss: 5.1890 - val_acc: 0.3475\n",
            "Epoch 156/200\n",
            "1692/1692 [==============================] - 0s 214us/step - loss: 2.0636e-05 - acc: 1.0000 - val_loss: 5.1946 - val_acc: 0.3475\n",
            "Epoch 157/200\n",
            "1692/1692 [==============================] - 0s 218us/step - loss: 2.0213e-05 - acc: 1.0000 - val_loss: 5.2012 - val_acc: 0.3475\n",
            "Epoch 158/200\n",
            "1692/1692 [==============================] - 0s 221us/step - loss: 1.9867e-05 - acc: 1.0000 - val_loss: 5.2064 - val_acc: 0.3475\n",
            "Epoch 159/200\n",
            "1692/1692 [==============================] - 0s 219us/step - loss: 1.9447e-05 - acc: 1.0000 - val_loss: 5.2106 - val_acc: 0.3475\n",
            "Epoch 160/200\n",
            "1692/1692 [==============================] - 0s 222us/step - loss: 1.9078e-05 - acc: 1.0000 - val_loss: 5.2162 - val_acc: 0.3475\n",
            "Epoch 161/200\n",
            "1692/1692 [==============================] - 0s 218us/step - loss: 1.8733e-05 - acc: 1.0000 - val_loss: 5.2218 - val_acc: 0.3475\n",
            "Epoch 162/200\n",
            "1692/1692 [==============================] - 0s 224us/step - loss: 1.8378e-05 - acc: 1.0000 - val_loss: 5.2275 - val_acc: 0.3475\n",
            "Epoch 163/200\n",
            "1692/1692 [==============================] - 0s 215us/step - loss: 1.8009e-05 - acc: 1.0000 - val_loss: 5.2329 - val_acc: 0.3475\n",
            "Epoch 164/200\n",
            "1692/1692 [==============================] - 0s 214us/step - loss: 1.7675e-05 - acc: 1.0000 - val_loss: 5.2379 - val_acc: 0.3475\n",
            "Epoch 165/200\n",
            "1692/1692 [==============================] - 0s 216us/step - loss: 1.7330e-05 - acc: 1.0000 - val_loss: 5.2413 - val_acc: 0.3475\n",
            "Epoch 166/200\n",
            "1692/1692 [==============================] - 0s 219us/step - loss: 1.7010e-05 - acc: 1.0000 - val_loss: 5.2483 - val_acc: 0.3475\n",
            "Epoch 167/200\n",
            "1692/1692 [==============================] - 0s 207us/step - loss: 1.6700e-05 - acc: 1.0000 - val_loss: 5.2538 - val_acc: 0.3475\n",
            "Epoch 168/200\n",
            "1692/1692 [==============================] - 0s 217us/step - loss: 1.6381e-05 - acc: 1.0000 - val_loss: 5.2573 - val_acc: 0.3475\n",
            "Epoch 169/200\n",
            "1692/1692 [==============================] - 0s 216us/step - loss: 1.6076e-05 - acc: 1.0000 - val_loss: 5.2629 - val_acc: 0.3475\n",
            "Epoch 170/200\n",
            "1692/1692 [==============================] - 0s 212us/step - loss: 1.5794e-05 - acc: 1.0000 - val_loss: 5.2673 - val_acc: 0.3475\n",
            "Epoch 171/200\n",
            "1692/1692 [==============================] - 0s 207us/step - loss: 1.5480e-05 - acc: 1.0000 - val_loss: 5.2728 - val_acc: 0.3475\n",
            "Epoch 172/200\n",
            "1692/1692 [==============================] - 0s 208us/step - loss: 1.5197e-05 - acc: 1.0000 - val_loss: 5.2795 - val_acc: 0.3475\n",
            "Epoch 173/200\n",
            "1692/1692 [==============================] - 0s 217us/step - loss: 1.4903e-05 - acc: 1.0000 - val_loss: 5.2822 - val_acc: 0.3475\n",
            "Epoch 174/200\n",
            "1692/1692 [==============================] - 0s 213us/step - loss: 1.4642e-05 - acc: 1.0000 - val_loss: 5.2863 - val_acc: 0.3475\n",
            "Epoch 175/200\n",
            "1692/1692 [==============================] - 0s 207us/step - loss: 1.4372e-05 - acc: 1.0000 - val_loss: 5.2922 - val_acc: 0.3475\n",
            "Epoch 176/200\n",
            "1692/1692 [==============================] - 0s 204us/step - loss: 1.4103e-05 - acc: 1.0000 - val_loss: 5.2981 - val_acc: 0.3475\n",
            "Epoch 177/200\n",
            "1692/1692 [==============================] - 0s 212us/step - loss: 1.3846e-05 - acc: 1.0000 - val_loss: 5.3029 - val_acc: 0.3522\n",
            "Epoch 178/200\n",
            "1692/1692 [==============================] - 0s 210us/step - loss: 1.3598e-05 - acc: 1.0000 - val_loss: 5.3079 - val_acc: 0.3475\n",
            "Epoch 179/200\n",
            "1692/1692 [==============================] - 0s 208us/step - loss: 1.3338e-05 - acc: 1.0000 - val_loss: 5.3115 - val_acc: 0.3475\n",
            "Epoch 180/200\n",
            "1692/1692 [==============================] - 0s 206us/step - loss: 1.3103e-05 - acc: 1.0000 - val_loss: 5.3173 - val_acc: 0.3499\n",
            "Epoch 181/200\n",
            "1692/1692 [==============================] - 0s 215us/step - loss: 1.2864e-05 - acc: 1.0000 - val_loss: 5.3231 - val_acc: 0.3499\n",
            "Epoch 182/200\n",
            "1692/1692 [==============================] - 0s 202us/step - loss: 1.2647e-05 - acc: 1.0000 - val_loss: 5.3276 - val_acc: 0.3522\n",
            "Epoch 183/200\n",
            "1692/1692 [==============================] - 0s 206us/step - loss: 1.2416e-05 - acc: 1.0000 - val_loss: 5.3316 - val_acc: 0.3475\n",
            "Epoch 184/200\n",
            "1692/1692 [==============================] - 0s 203us/step - loss: 1.2184e-05 - acc: 1.0000 - val_loss: 5.3356 - val_acc: 0.3499\n",
            "Epoch 185/200\n",
            "1692/1692 [==============================] - 0s 207us/step - loss: 1.1974e-05 - acc: 1.0000 - val_loss: 5.3416 - val_acc: 0.3499\n",
            "Epoch 186/200\n",
            "1692/1692 [==============================] - 0s 219us/step - loss: 1.1751e-05 - acc: 1.0000 - val_loss: 5.3459 - val_acc: 0.3499\n",
            "Epoch 187/200\n",
            "1692/1692 [==============================] - 0s 209us/step - loss: 1.1534e-05 - acc: 1.0000 - val_loss: 5.3495 - val_acc: 0.3499\n",
            "Epoch 188/200\n",
            "1692/1692 [==============================] - 0s 213us/step - loss: 1.1322e-05 - acc: 1.0000 - val_loss: 5.3555 - val_acc: 0.3499\n",
            "Epoch 189/200\n",
            "1692/1692 [==============================] - 0s 211us/step - loss: 1.1122e-05 - acc: 1.0000 - val_loss: 5.3608 - val_acc: 0.3475\n",
            "Epoch 190/200\n",
            "1692/1692 [==============================] - 0s 212us/step - loss: 1.0931e-05 - acc: 1.0000 - val_loss: 5.3627 - val_acc: 0.3499\n",
            "Epoch 191/200\n",
            "1692/1692 [==============================] - 0s 207us/step - loss: 1.0740e-05 - acc: 1.0000 - val_loss: 5.3706 - val_acc: 0.3475\n",
            "Epoch 192/200\n",
            "1692/1692 [==============================] - 0s 206us/step - loss: 1.0556e-05 - acc: 1.0000 - val_loss: 5.3729 - val_acc: 0.3499\n",
            "Epoch 193/200\n",
            "1692/1692 [==============================] - 0s 209us/step - loss: 1.0356e-05 - acc: 1.0000 - val_loss: 5.3783 - val_acc: 0.3475\n",
            "Epoch 194/200\n",
            "1692/1692 [==============================] - 0s 211us/step - loss: 1.0174e-05 - acc: 1.0000 - val_loss: 5.3845 - val_acc: 0.3499\n",
            "Epoch 195/200\n",
            "1692/1692 [==============================] - 0s 209us/step - loss: 9.9904e-06 - acc: 1.0000 - val_loss: 5.3881 - val_acc: 0.3475\n",
            "Epoch 196/200\n",
            "1692/1692 [==============================] - 0s 211us/step - loss: 9.8136e-06 - acc: 1.0000 - val_loss: 5.3931 - val_acc: 0.3475\n",
            "Epoch 197/200\n",
            "1692/1692 [==============================] - 0s 218us/step - loss: 9.6448e-06 - acc: 1.0000 - val_loss: 5.3980 - val_acc: 0.3475\n",
            "Epoch 198/200\n",
            "1692/1692 [==============================] - 0s 219us/step - loss: 9.4767e-06 - acc: 1.0000 - val_loss: 5.4021 - val_acc: 0.3475\n",
            "Epoch 199/200\n",
            "1692/1692 [==============================] - 0s 208us/step - loss: 9.3101e-06 - acc: 1.0000 - val_loss: 5.4073 - val_acc: 0.3499\n",
            "Epoch 200/200\n",
            "1692/1692 [==============================] - 0s 212us/step - loss: 9.1439e-06 - acc: 1.0000 - val_loss: 5.4108 - val_acc: 0.3475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFO08-FIvMvI",
        "colab_type": "code",
        "outputId": "1420f402-5a6e-4e8a-bdd4-2492afd0d65d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(X_test.reshape(-1,1,1000,22), Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "443/443 [==============================] - 0s 149us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11.497332301688786, 0.2866817148356082]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsHHzQqWGz8J",
        "colab_type": "text"
      },
      "source": [
        "#### try data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkgcOV1MG4-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09MfhS3Ry8P-",
        "colab_type": "code",
        "outputId": "c4f17ca0-b458-4c1d-9e5e-eb7c8243e1dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras import regularizers\n",
        "\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=200, kernel_size=(1,5), \n",
        "                 strides=(1, 3), input_shape=(1,1000,22),\n",
        "                 activation = 'relu', \n",
        "                 kernel_regularizer = regularizers.l2(0.1),\n",
        "                 padding ='valid'\n",
        "                )\n",
        "         )\n",
        "# model.add(BatchNormalization(axis = -1))\n",
        "model.add(Conv2D(filters=200, kernel_size=(1,5), \n",
        "                 strides=(1, 3),\n",
        "                 activation = 'relu', \n",
        "                 kernel_regularizer = regularizers.l2(0.1),\n",
        "                 padding ='valid'\n",
        "                )\n",
        "         )\n",
        "\n",
        "model.add(MaxPool2D(strides = (1,3), pool_size = (1,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# model.add(Conv2D(filters=200, kernel_size=(1,2), \n",
        "#                  strides=(1, 2),\n",
        "#                  activation = 'relu', \n",
        "#                  kernel_regularizer = regularizers.l2(0.1),\n",
        "#                  padding ='valid'\n",
        "#                 )\n",
        "#          )\n",
        "# model.add(Dropout(0.3))\n",
        "\n",
        "# keras.regularizers.l2\n",
        "model.add(Flatten())\n",
        "# model.add(Dense(units = 20))\n",
        "# model.add(Activation('elu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(Dense(units = 64, activation='relu', \n",
        "                kernel_regularizer = regularizers.l2(0.1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(units = 4, activation='softmax',\n",
        "               kernel_regularizer = regularizers.l2(0.1)))\n",
        "\n",
        "model.compile(loss='kullback_leibler_divergence',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "early_stop = EarlyStopping(monitor = 'loss',\n",
        "                           patience = 20)\n",
        "\n",
        "X_train_ = np.transpose(X_train, axes = (0,2,1))\n",
        "X_train_ = X_train_.reshape(-1,1,1000,22)\n",
        "\n",
        "\n",
        "history = model.fit(X_train_, Y_train, \n",
        "                    batch_size=128, epochs = 300, validation_split = 0.2, \n",
        "                    verbose = 1, callbacks = [early_stop])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 1, 332, 200)       22200     \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 110, 200)       200200    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1, 36, 200)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1, 36, 200)        800       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 36, 200)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 7200)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                460864    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 684,580\n",
            "Trainable params: 684,052\n",
            "Non-trainable params: 528\n",
            "_________________________________________________________________\n",
            "Train on 1692 samples, validate on 423 samples\n",
            "Epoch 1/300\n",
            "1692/1692 [==============================] - 1s 868us/step - loss: 35.0306 - acc: 0.3180 - val_loss: 29.5271 - val_acc: 0.3593\n",
            "Epoch 2/300\n",
            "1692/1692 [==============================] - 1s 311us/step - loss: 24.5218 - acc: 0.5035 - val_loss: 19.9610 - val_acc: 0.3641\n",
            "Epoch 3/300\n",
            "1692/1692 [==============================] - 1s 304us/step - loss: 16.3951 - acc: 0.5922 - val_loss: 13.4099 - val_acc: 0.3499\n",
            "Epoch 4/300\n",
            "1692/1692 [==============================] - 1s 305us/step - loss: 10.8452 - acc: 0.6962 - val_loss: 9.6031 - val_acc: 0.3168\n",
            "Epoch 5/300\n",
            "1692/1692 [==============================] - 1s 300us/step - loss: 7.5436 - acc: 0.7394 - val_loss: 7.4970 - val_acc: 0.3286\n",
            "Epoch 6/300\n",
            "1692/1692 [==============================] - 0s 291us/step - loss: 5.6543 - acc: 0.7748 - val_loss: 6.1764 - val_acc: 0.3121\n",
            "Epoch 7/300\n",
            "1692/1692 [==============================] - 0s 294us/step - loss: 4.7358 - acc: 0.7577 - val_loss: 5.3420 - val_acc: 0.3002\n",
            "Epoch 8/300\n",
            "1692/1692 [==============================] - 0s 294us/step - loss: 4.1375 - acc: 0.7790 - val_loss: 4.5817 - val_acc: 0.4113\n",
            "Epoch 9/300\n",
            "1692/1692 [==============================] - 1s 297us/step - loss: 3.5418 - acc: 0.8251 - val_loss: 4.5224 - val_acc: 0.3286\n",
            "Epoch 10/300\n",
            "1692/1692 [==============================] - 0s 294us/step - loss: 3.1149 - acc: 0.8156 - val_loss: 3.9807 - val_acc: 0.3924\n",
            "Epoch 11/300\n",
            "1692/1692 [==============================] - 1s 304us/step - loss: 2.7939 - acc: 0.8375 - val_loss: 3.5103 - val_acc: 0.4515\n",
            "Epoch 12/300\n",
            "1692/1692 [==============================] - 1s 304us/step - loss: 2.5242 - acc: 0.8517 - val_loss: 3.5557 - val_acc: 0.3617\n",
            "Epoch 13/300\n",
            "1692/1692 [==============================] - 1s 296us/step - loss: 2.3437 - acc: 0.8475 - val_loss: 3.2869 - val_acc: 0.3546\n",
            "Epoch 14/300\n",
            "1692/1692 [==============================] - 0s 293us/step - loss: 2.2494 - acc: 0.8398 - val_loss: 3.0634 - val_acc: 0.3853\n",
            "Epoch 15/300\n",
            "1692/1692 [==============================] - 1s 296us/step - loss: 2.2583 - acc: 0.8174 - val_loss: 3.1465 - val_acc: 0.4208\n",
            "Epoch 16/300\n",
            "1692/1692 [==============================] - 0s 290us/step - loss: 2.2378 - acc: 0.8387 - val_loss: 3.2751 - val_acc: 0.4019\n",
            "Epoch 17/300\n",
            "1692/1692 [==============================] - 0s 292us/step - loss: 2.1502 - acc: 0.8404 - val_loss: 3.0724 - val_acc: 0.4090\n",
            "Epoch 18/300\n",
            "1692/1692 [==============================] - 0s 284us/step - loss: 1.9681 - acc: 0.8605 - val_loss: 2.7030 - val_acc: 0.4681\n",
            "Epoch 19/300\n",
            "1692/1692 [==============================] - 0s 295us/step - loss: 1.8657 - acc: 0.8434 - val_loss: 2.7477 - val_acc: 0.4303\n",
            "Epoch 20/300\n",
            "1692/1692 [==============================] - 0s 293us/step - loss: 1.8735 - acc: 0.8363 - val_loss: 2.7414 - val_acc: 0.4563\n",
            "Epoch 21/300\n",
            "1692/1692 [==============================] - 1s 298us/step - loss: 1.9765 - acc: 0.8257 - val_loss: 2.9204 - val_acc: 0.4255\n",
            "Epoch 22/300\n",
            "1692/1692 [==============================] - 1s 297us/step - loss: 2.0322 - acc: 0.8398 - val_loss: 2.8002 - val_acc: 0.4090\n",
            "Epoch 23/300\n",
            "1692/1692 [==============================] - 0s 292us/step - loss: 2.0028 - acc: 0.8351 - val_loss: 2.6068 - val_acc: 0.4752\n",
            "Epoch 24/300\n",
            "1692/1692 [==============================] - 0s 294us/step - loss: 1.8588 - acc: 0.8670 - val_loss: 2.5908 - val_acc: 0.5059\n",
            "Epoch 25/300\n",
            "1692/1692 [==============================] - 0s 292us/step - loss: 1.6894 - acc: 0.8978 - val_loss: 2.4242 - val_acc: 0.4728\n",
            "Epoch 26/300\n",
            "1692/1692 [==============================] - 0s 294us/step - loss: 1.6265 - acc: 0.8788 - val_loss: 2.4658 - val_acc: 0.4350\n",
            "Epoch 27/300\n",
            "1692/1692 [==============================] - 0s 285us/step - loss: 1.6135 - acc: 0.8611 - val_loss: 2.3833 - val_acc: 0.4704\n",
            "Epoch 28/300\n",
            "1692/1692 [==============================] - 0s 289us/step - loss: 1.6154 - acc: 0.8582 - val_loss: 2.3441 - val_acc: 0.5177\n",
            "Epoch 29/300\n",
            "1692/1692 [==============================] - 0s 291us/step - loss: 1.6303 - acc: 0.8688 - val_loss: 2.3967 - val_acc: 0.5154\n",
            "Epoch 30/300\n",
            "1692/1692 [==============================] - 0s 290us/step - loss: 1.7596 - acc: 0.8310 - val_loss: 2.6133 - val_acc: 0.4894\n",
            "Epoch 31/300\n",
            "1692/1692 [==============================] - 1s 300us/step - loss: 1.8291 - acc: 0.8286 - val_loss: 2.6449 - val_acc: 0.4799\n",
            "Epoch 32/300\n",
            "1692/1692 [==============================] - 0s 292us/step - loss: 1.7611 - acc: 0.8729 - val_loss: 2.3804 - val_acc: 0.5248\n",
            "Epoch 33/300\n",
            "1692/1692 [==============================] - 0s 294us/step - loss: 1.6333 - acc: 0.8901 - val_loss: 2.3867 - val_acc: 0.4894\n",
            "Epoch 34/300\n",
            "1692/1692 [==============================] - 0s 294us/step - loss: 1.5012 - acc: 0.8942 - val_loss: 2.2602 - val_acc: 0.5012\n",
            "Epoch 35/300\n",
            "1692/1692 [==============================] - 1s 298us/step - loss: 1.5668 - acc: 0.8499 - val_loss: 2.5568 - val_acc: 0.4728\n",
            "Epoch 36/300\n",
            "1692/1692 [==============================] - 1s 299us/step - loss: 1.7306 - acc: 0.8475 - val_loss: 2.7505 - val_acc: 0.4468\n",
            "Epoch 37/300\n",
            "1692/1692 [==============================] - 1s 296us/step - loss: 1.8778 - acc: 0.8150 - val_loss: 2.5157 - val_acc: 0.5461\n",
            "Epoch 38/300\n",
            "1692/1692 [==============================] - 1s 303us/step - loss: 1.8667 - acc: 0.8392 - val_loss: 2.6293 - val_acc: 0.5225\n",
            "Epoch 39/300\n",
            "1692/1692 [==============================] - 0s 293us/step - loss: 1.7504 - acc: 0.8818 - val_loss: 2.5802 - val_acc: 0.5225\n",
            "Epoch 40/300\n",
            "1692/1692 [==============================] - 0s 294us/step - loss: 1.6360 - acc: 0.8836 - val_loss: 2.5171 - val_acc: 0.4374\n",
            "Epoch 41/300\n",
            "1692/1692 [==============================] - 0s 282us/step - loss: 1.5720 - acc: 0.8735 - val_loss: 2.5303 - val_acc: 0.4752\n",
            "Epoch 42/300\n",
            "1692/1692 [==============================] - 0s 290us/step - loss: 1.5392 - acc: 0.8783 - val_loss: 2.4305 - val_acc: 0.5083\n",
            "Epoch 43/300\n",
            "1692/1692 [==============================] - 0s 279us/step - loss: 1.4869 - acc: 0.8747 - val_loss: 2.2611 - val_acc: 0.5248\n",
            "Epoch 44/300\n",
            "1692/1692 [==============================] - 0s 281us/step - loss: 1.4513 - acc: 0.8895 - val_loss: 2.4949 - val_acc: 0.4657\n",
            "Epoch 45/300\n",
            "1692/1692 [==============================] - 0s 269us/step - loss: 1.4290 - acc: 0.8759 - val_loss: 2.9252 - val_acc: 0.4113\n",
            "Epoch 46/300\n",
            "1692/1692 [==============================] - 0s 277us/step - loss: 1.5991 - acc: 0.8363 - val_loss: 2.5458 - val_acc: 0.5177\n",
            "Epoch 47/300\n",
            "1692/1692 [==============================] - 0s 271us/step - loss: 1.6990 - acc: 0.8522 - val_loss: 2.7448 - val_acc: 0.4421\n",
            "Epoch 48/300\n",
            "1692/1692 [==============================] - 0s 273us/step - loss: 1.6631 - acc: 0.8747 - val_loss: 2.6417 - val_acc: 0.4657\n",
            "Epoch 49/300\n",
            "1692/1692 [==============================] - 0s 274us/step - loss: 1.6275 - acc: 0.8777 - val_loss: 2.7166 - val_acc: 0.3995\n",
            "Epoch 50/300\n",
            "1692/1692 [==============================] - 0s 270us/step - loss: 1.6446 - acc: 0.8735 - val_loss: 2.8212 - val_acc: 0.4184\n",
            "Epoch 51/300\n",
            "1692/1692 [==============================] - 0s 288us/step - loss: 1.6188 - acc: 0.8877 - val_loss: 2.3943 - val_acc: 0.5556\n",
            "Epoch 52/300\n",
            "1692/1692 [==============================] - 0s 266us/step - loss: 1.5615 - acc: 0.8972 - val_loss: 2.3445 - val_acc: 0.5414\n",
            "Epoch 53/300\n",
            "1692/1692 [==============================] - 0s 290us/step - loss: 1.4797 - acc: 0.9155 - val_loss: 2.3961 - val_acc: 0.4870\n",
            "Epoch 54/300\n",
            "1692/1692 [==============================] - 0s 275us/step - loss: 1.3976 - acc: 0.9072 - val_loss: 2.4312 - val_acc: 0.4704\n",
            "Epoch 55/300\n",
            "1692/1692 [==============================] - 0s 277us/step - loss: 1.4458 - acc: 0.8765 - val_loss: 3.5456 - val_acc: 0.3239\n",
            "Epoch 56/300\n",
            "1692/1692 [==============================] - 0s 273us/step - loss: 1.6288 - acc: 0.8463 - val_loss: 2.8318 - val_acc: 0.4799\n",
            "Epoch 57/300\n",
            "1692/1692 [==============================] - 0s 279us/step - loss: 1.6932 - acc: 0.8735 - val_loss: 2.4762 - val_acc: 0.5106\n",
            "Epoch 58/300\n",
            "1692/1692 [==============================] - 0s 263us/step - loss: 1.6587 - acc: 0.8788 - val_loss: 2.5195 - val_acc: 0.5035\n",
            "Epoch 59/300\n",
            "1692/1692 [==============================] - 0s 270us/step - loss: 1.6369 - acc: 0.8765 - val_loss: 2.4010 - val_acc: 0.5012\n",
            "Epoch 60/300\n",
            "1692/1692 [==============================] - 0s 278us/step - loss: 1.5894 - acc: 0.8676 - val_loss: 2.4130 - val_acc: 0.5130\n",
            "Epoch 61/300\n",
            "1692/1692 [==============================] - 0s 276us/step - loss: 1.5644 - acc: 0.8788 - val_loss: 3.0690 - val_acc: 0.4397\n",
            "Epoch 62/300\n",
            "1692/1692 [==============================] - 0s 274us/step - loss: 1.4823 - acc: 0.9096 - val_loss: 2.5616 - val_acc: 0.4894\n",
            "Epoch 63/300\n",
            "1692/1692 [==============================] - 0s 272us/step - loss: 1.4138 - acc: 0.9125 - val_loss: 2.3244 - val_acc: 0.4941\n",
            "Epoch 64/300\n",
            "1692/1692 [==============================] - 0s 280us/step - loss: 1.3857 - acc: 0.9037 - val_loss: 2.4630 - val_acc: 0.5083\n",
            "Epoch 65/300\n",
            "1692/1692 [==============================] - 0s 276us/step - loss: 1.4227 - acc: 0.8859 - val_loss: 2.3787 - val_acc: 0.4917\n",
            "Epoch 66/300\n",
            "1692/1692 [==============================] - 0s 282us/step - loss: 1.4894 - acc: 0.8777 - val_loss: 2.4089 - val_acc: 0.4846\n",
            "Epoch 67/300\n",
            "1692/1692 [==============================] - 0s 270us/step - loss: 1.5126 - acc: 0.8865 - val_loss: 2.4694 - val_acc: 0.5106\n",
            "Epoch 68/300\n",
            "1692/1692 [==============================] - 0s 278us/step - loss: 1.5548 - acc: 0.8836 - val_loss: 3.1436 - val_acc: 0.4255\n",
            "Epoch 69/300\n",
            "1692/1692 [==============================] - 0s 274us/step - loss: 1.6467 - acc: 0.8576 - val_loss: 2.9648 - val_acc: 0.4279\n",
            "Epoch 70/300\n",
            "1692/1692 [==============================] - 0s 282us/step - loss: 1.7066 - acc: 0.8658 - val_loss: 2.5614 - val_acc: 0.5343\n",
            "Epoch 71/300\n",
            "1692/1692 [==============================] - 0s 272us/step - loss: 1.6943 - acc: 0.8836 - val_loss: 2.5767 - val_acc: 0.5508\n",
            "Epoch 72/300\n",
            "1692/1692 [==============================] - 0s 275us/step - loss: 1.6229 - acc: 0.8889 - val_loss: 2.4145 - val_acc: 0.5697\n",
            "Epoch 73/300\n",
            "1692/1692 [==============================] - 0s 269us/step - loss: 1.4753 - acc: 0.9161 - val_loss: 2.3122 - val_acc: 0.5556\n",
            "Epoch 74/300\n",
            "1692/1692 [==============================] - 0s 272us/step - loss: 1.4086 - acc: 0.9013 - val_loss: 2.2596 - val_acc: 0.5508\n",
            "Epoch 75/300\n",
            "1692/1692 [==============================] - 0s 274us/step - loss: 1.4589 - acc: 0.8836 - val_loss: 2.4561 - val_acc: 0.5012\n",
            "Epoch 76/300\n",
            "1692/1692 [==============================] - 0s 278us/step - loss: 1.5539 - acc: 0.8676 - val_loss: 2.4937 - val_acc: 0.5154\n",
            "Epoch 77/300\n",
            "1692/1692 [==============================] - 0s 272us/step - loss: 1.5735 - acc: 0.8865 - val_loss: 2.8796 - val_acc: 0.4350\n",
            "Epoch 78/300\n",
            "1692/1692 [==============================] - 0s 273us/step - loss: 1.6304 - acc: 0.8599 - val_loss: 2.6780 - val_acc: 0.4610\n",
            "Epoch 79/300\n",
            "1692/1692 [==============================] - 0s 279us/step - loss: 1.6902 - acc: 0.8564 - val_loss: 2.8081 - val_acc: 0.5106\n",
            "Epoch 80/300\n",
            "1692/1692 [==============================] - 0s 271us/step - loss: 1.6559 - acc: 0.8765 - val_loss: 2.6666 - val_acc: 0.5059\n",
            "Epoch 81/300\n",
            "1692/1692 [==============================] - 0s 276us/step - loss: 1.5571 - acc: 0.9037 - val_loss: 2.5061 - val_acc: 0.5035\n",
            "Epoch 82/300\n",
            "1692/1692 [==============================] - 0s 270us/step - loss: 1.5043 - acc: 0.8883 - val_loss: 2.8353 - val_acc: 0.4515\n",
            "Epoch 83/300\n",
            "1692/1692 [==============================] - 0s 274us/step - loss: 1.5228 - acc: 0.8635 - val_loss: 2.3948 - val_acc: 0.5083\n",
            "Epoch 84/300\n",
            "1692/1692 [==============================] - 0s 276us/step - loss: 1.5827 - acc: 0.8723 - val_loss: 3.0112 - val_acc: 0.4374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnw8Vp4WYRh9",
        "colab_type": "code",
        "outputId": "1ec82cb9-b6cc-4d93-ffc2-dc4d4532d04f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "X_test_ = np.transpose(X_test, axes = (0,2,1))\n",
        "X_test_ = X_test_.reshape(-1,1,1000,22)\n",
        "\n",
        "\n",
        "_, accu = model.evaluate(X_test_, Y_test)\n",
        "\n",
        "print(\"training accu is : {:2.2%}\".format(history.history['acc'][-1]))\n",
        "print(\"val accu is : {:2.2%}\".format(history.history['val_acc'][-1]))\n",
        "print(\"test accu is : {:2.2%}\".format(accu))\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model acc')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "443/443 [==============================] - 0s 202us/step\n",
            "training accu is : 87.23%\n",
            "val accu is : 43.74%\n",
            "test accu is : 40.63%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0YAAAFRCAYAAABQaL0SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1008x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O2PiHrHYWL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMEaOgaLY6AM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HHQqTQkHQLY",
        "colab_type": "text"
      },
      "source": [
        "### data augmentaion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xektrxacHSEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_augment_ = np.transpose(X_train_argment, axes = (0,2,1))\n",
        "X_train_augment_ = X_train_augment_.reshape(-1,1,750,22)\n",
        "\n",
        "X_test_augment_ = np.transpose(X_test_argment, axes = (0,2,1))\n",
        "X_test_augment_ = X_test_augment_.reshape(-1,1,750,22)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LocyPrh3HZl1",
        "colab_type": "code",
        "outputId": "db549e31-8a2f-43f8-b2d7-a1e809b217ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "from keras import regularizers\n",
        "l2_lambda = 0.1\n",
        "dropout_rate = 0.3\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=100, kernel_size=(1,5), \n",
        "                 strides=(1, 3), input_shape=(1,750,22),\n",
        "#                  activation = 'relu', \n",
        "                 kernel_regularizer = regularizers.l2(l2_lambda),\n",
        "                 padding ='valid'\n",
        "                )\n",
        "         )\n",
        "\n",
        "model.add(Conv2D(filters=100, kernel_size=(1,5), \n",
        "                 strides=(1, 3),\n",
        "                 activation = 'relu', \n",
        "                 kernel_regularizer = regularizers.l2(l2_lambda),\n",
        "                 padding ='valid'\n",
        "                )\n",
        "         )\n",
        "model.add(BatchNormalization())\n",
        "# model.add(MaxPool2D(strides = (1,3), pool_size = (1,3)))\n",
        "model.add(Dropout(dropout_rate))\n",
        "model.add(Conv2D(filters=200, kernel_size=(1,2), \n",
        "                 strides=(1, 2),\n",
        "                 activation = 'relu', \n",
        "                 kernel_regularizer = regularizers.l2(l2_lambda),\n",
        "                 padding ='valid'\n",
        "                )\n",
        "         )\n",
        "\n",
        "model.add(Conv2D(filters=200, kernel_size=(1,1), \n",
        "                 strides=(1, 1),\n",
        "                 activation = 'relu', \n",
        "                 kernel_regularizer = regularizers.l2(l2_lambda),\n",
        "                 padding ='valid'\n",
        "                )\n",
        "         )\n",
        "model.add(BatchNormalization())\n",
        "# model.add(MaxPool2D(strides = (1,3), pool_size = (1,3)))\n",
        "model.add(Dropout(dropout_rate))\n",
        "\n",
        "# keras.regularizers.l2\n",
        "model.add(Flatten())\n",
        "# model.add(Dense(units = 20))\n",
        "# model.add(Activation('elu'))\n",
        "# model.add(Dense(units = 50, activation='relu', kernel_regularizer =  'l2'))\n",
        "model.add(Dense(units = 4, activation='softmax',\n",
        "               kernel_regularizer = regularizers.l2(l2_lambda)))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = 'RMSprop',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "early_stop = EarlyStopping(monitor = 'loss',\n",
        "                           patience = 20)\n",
        "\n",
        "\n",
        "history = model.fit(X_train_augment_, Y_train_argment, \n",
        "                    batch_size=64*4, epochs = 300, \n",
        "                    validation_split = 0.2, \n",
        "                    callbacks = [early_stop],\n",
        "                    verbose = 1, \n",
        "                   )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-205402ebf64f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ml2_lambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdropout_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m model.add(Conv2D(filters=100, kernel_size=(1,5), \n",
            "\u001b[0;31mNameError\u001b[0m: name 'K' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FNdMgd9HkUf",
        "colab_type": "code",
        "outputId": "5616b218-0fac-4cbd-cf17-e0309516f1d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "X_test_augment_ = np.transpose(X_test_argment, axes = (0,2,1))\n",
        "X_test_augment_ = X_test_augment_.reshape(-1,1,750,22)\n",
        "\n",
        "\n",
        "_, accu = model.evaluate(X_test_augment_, Y_test_argment)\n",
        "\n",
        "print(\"training accu is : {:2.2%}\".format(history.history['acc'][-1]))\n",
        "print(\"val accu is : {:2.2%}\".format(history.history['val_acc'][-1]))\n",
        "print(\"test accu is : {:2.2%}\".format(accu))\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model acc')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2658/2658 [==============================] - 1s 231us/step\n",
            "training accu is : 60.70%\n",
            "val accu is : 43.18%\n",
            "test accu is : 39.35%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA00AAAFRCAYAAACoiEblAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nglEwCkbBKMADANdQZg8sY33kAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1008x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nexW9GBISfK",
        "colab_type": "code",
        "outputId": "dd58acae-1c4c-44d5-cac8-da80f5728126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train_augment_.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12690, 1, 750, 22)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FXEMROWewBD",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter search by talos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEeFUAWKevuh",
        "colab_type": "code",
        "outputId": "6f72805b-f83c-4f07-bfdc-605831e07e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install talos\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting talos\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/df/c352679af3259829dafa7d55f2d3e9fca201c848351cb3c841a062df001c/talos-0.6.3.tar.gz\n",
            "Collecting wrangle (from talos)\n",
            "  Downloading https://files.pythonhosted.org/packages/85/35/bc729e377417613f2d062a890faea5d649ef1a554df21499e9c3a4a5501a/wrangle-0.6.7.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from talos) (1.16.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from talos) (0.24.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from talos) (2.2.5)\n",
            "Collecting astetik (from talos)\n",
            "  Downloading https://files.pythonhosted.org/packages/3c/ba/f8622951da73d9b47b45bb847112c388651f9c6e413e712954f260301d9f/astetik-1.9.9.tar.gz\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from talos) (0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from talos) (4.28.1)\n",
            "Collecting chances (from talos)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/8a/e0ce40affac9c5292da615375cd2ce979728b8f5a5d3afd4a9e3acdf9166/chances-0.1.6.tar.gz (45kB)\n",
            "\u001b[K     || 51kB 23.3MB/s \n",
            "\u001b[?25hCollecting kerasplotlib (from talos)\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/2e/b8628bfef6a817da9be863f650cf67187676b10d27d94b23f248da35d2b4/kerasplotlib-0.1.4.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from talos) (2.21.0)\n",
            "Collecting scipy==1.2 (from wrangle->talos)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/e6/6d4edaceee6a110ecf6f318482f5229792f143e468b34a631f5a0899f56d/scipy-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (26.6MB)\n",
            "\u001b[K     || 26.6MB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from wrangle->talos) (0.10.1)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->talos) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->talos) (2.5.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->talos) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->talos) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->talos) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->talos) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->talos) (1.0.8)\n",
            "Collecting geonamescache (from astetik->talos)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/f7/2df10269978f37e378704d1b4bc7d62c6d0210f3c9e31c9dcf9b08a5af9c/geonamescache-1.0.3-py3-none-any.whl (786kB)\n",
            "\u001b[K     || 788kB 30.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->talos) (0.21.3)\n",
            "Collecting matplotlib==2.2.3 (from chances->talos)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/59/f235ab21bbe7b7c6570c4abf17ffb893071f4fa3b9cf557b09b60359ad9a/matplotlib-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (12.6MB)\n",
            "\u001b[K     || 12.6MB 16.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->talos) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->talos) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->talos) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->talos) (1.24.3)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->wrangle->talos) (0.5.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->talos) (0.13.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->chances->talos) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->chances->talos) (2.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->chances->talos) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib==2.2.3->chances->talos) (41.2.0)\n",
            "Building wheels for collected packages: talos, wrangle, astetik, chances, kerasplotlib\n",
            "  Building wheel for talos (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for talos: filename=talos-0.6.3-cp36-none-any.whl size=49626 sha256=e476c403883b2993717c4ab761d2a7cfac57ad5a6da6ad8f45054fb6eaefb3b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/d7/6b/86fd8b1fc7cfbd2c54796412f86efb5fb6a3a5c734014f6a66\n",
            "  Building wheel for wrangle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrangle: filename=wrangle-0.6.7-cp36-none-any.whl size=49894 sha256=eaab0082f6aedb5c565da94d6113df66c717cab95a4aab55502d37b41e1936c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/1b/50/d0403ce6ef269e364894da7b50db68db14c4ac62c577561e2d\n",
            "  Building wheel for astetik (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for astetik: filename=astetik-1.9.9-cp36-none-any.whl size=56960 sha256=86f81087d50d9f87f87465bd5b0e747d41a50c116c2dfd83bab41645ff02acd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/70/21/c475cd079ec401dd6e1b9b1d42b4c38554ce12679bfb214aad\n",
            "  Building wheel for chances (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chances: filename=chances-0.1.6-cp36-none-any.whl size=52548 sha256=a33b14b311289f6a270a7d2da53a0ef7912d3d33eb1d7429847a0a542b657101\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/12/37/957767d4ed95919b90081079c6eb74f83927930e652b30fa93\n",
            "  Building wheel for kerasplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kerasplotlib: filename=kerasplotlib-0.1.4-cp36-none-any.whl size=3579 sha256=03f303251f3f10486173c27a04dc1840bc90eda297fa3dd7c58688db5984d003\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/6b/4c/e1fc6d7d8811940fbea1147b1519c7baa6933e4baeff904433\n",
            "Successfully built talos wrangle astetik chances kerasplotlib\n",
            "\u001b[31mERROR: plotnine 0.5.1 has requirement matplotlib>=3.0.0, but you'll have matplotlib 2.2.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy, wrangle, geonamescache, astetik, matplotlib, chances, kerasplotlib, talos\n",
            "  Found existing installation: scipy 1.3.1\n",
            "    Uninstalling scipy-1.3.1:\n",
            "      Successfully uninstalled scipy-1.3.1\n",
            "  Found existing installation: matplotlib 3.0.3\n",
            "    Uninstalling matplotlib-3.0.3:\n",
            "      Successfully uninstalled matplotlib-3.0.3\n",
            "Successfully installed astetik-1.9.9 chances-0.1.6 geonamescache-1.0.3 kerasplotlib-0.1.4 matplotlib-2.2.3 scipy-1.2.0 talos-0.6.3 wrangle-0.6.7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "scipy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fCB6KzPfYUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import talos as ta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4Fq8Dege7Al",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def EEG_model(x_train, y_train, x_val, y_val, params):\n",
        "\n",
        "    \n",
        "    l2_lambda = params['l2_lambda']\n",
        "    dropout_rate = params['dropout']\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters=100, kernel_size=(1,5), \n",
        "                     strides=(1, 3), input_shape=(1,750,22),\n",
        "    #                  activation = 'relu', \n",
        "                     kernel_regularizer = regularizers.l2(l2_lambda),\n",
        "                     padding ='valid'\n",
        "                    )\n",
        "             )\n",
        "\n",
        "    model.add(Conv2D(filters=100, kernel_size=(1,5), \n",
        "                     strides=(1, 3),\n",
        "                     activation = params['activation'],\n",
        "                     kernel_regularizer = regularizers.l2(l2_lambda),\n",
        "                     padding ='valid'\n",
        "                    )\n",
        "             )\n",
        "    model.add(BatchNormalization())\n",
        "    # model.add(MaxPool2D(strides = (1,3), pool_size = (1,3)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Conv2D(filters=200, kernel_size=(1,2), \n",
        "                     strides=(1, 2),\n",
        "                     activation = params['activation'],\n",
        "                     kernel_regularizer = regularizers.l2(l2_lambda),\n",
        "                     padding ='valid'\n",
        "                    )\n",
        "             )\n",
        "\n",
        "    model.add(Conv2D(filters=200, kernel_size=(1,1), \n",
        "                     strides=(1, 1),\n",
        "                     activation = params['activation'], \n",
        "                     kernel_regularizer = regularizers.l2(l2_lambda),\n",
        "                     padding ='valid'\n",
        "                    )\n",
        "             )\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(strides = (1,3), pool_size = (1,3)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # keras.regularizers.l2\n",
        "    model.add(Flatten())\n",
        "    # model.add(Dense(units = 20))\n",
        "    # model.add(Activation('elu'))\n",
        "    # model.add(Dense(units = 50, activation='relu', kernel_regularizer =  'l2'))\n",
        "    model.add(Dense(units = 4, activation='softmax',\n",
        "                   kernel_regularizer = regularizers.l2(l2_lambda)))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer = params['optimizer'],\n",
        "                  metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "#     model.summary()\n",
        "\n",
        "    early_stop = EarlyStopping(monitor = 'loss',\n",
        "                               patience = 20)\n",
        "\n",
        "\n",
        "    history = model.fit(X_train_augment_, Y_train_argment, \n",
        "                        batch_size=params['batch_size'], epochs = 300, \n",
        "                        validation_split = 0.2, \n",
        "                        callbacks = [early_stop],\n",
        "                        verbose = 1, \n",
        "                       )\n",
        "\n",
        "\n",
        "    \n",
        "    return history, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LpStcprMkVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = {'l2_lambda': (0.01, 0.05, 0.25),\n",
        "     'batch_size': (64, 128, 256),\n",
        "     'dropout': (0.1, 0.2, 0.3),\n",
        "     'optimizer': ['Adam', 'RMSprop'],\n",
        "     'activation':['relu', 'elu']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HR7OI6QgVpk",
        "colab_type": "code",
        "outputId": "19bbd4c2-880c-46c8-c6d5-b8b6b0798bd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "t = ta.Scan(x=X_train_augment_,\n",
        "            y=Y_train_argment,\n",
        "            model=EEG_model,\n",
        "            params=p,\n",
        "            grid_downsample=0.1,\n",
        "            experiment_name = 'EEG_test'\n",
        "            )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/256 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 6s 590us/step - loss: 6.0508 - acc: 0.2901 - val_loss: 5.1990 - val_acc: 0.3077\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 4.3283 - acc: 0.3563 - val_loss: 3.6661 - val_acc: 0.3483\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 3.1694 - acc: 0.4329 - val_loss: 2.7996 - val_acc: 0.3905\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 2.4136 - acc: 0.5030 - val_loss: 2.2868 - val_acc: 0.4283\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 1.9808 - acc: 0.5429 - val_loss: 2.2718 - val_acc: 0.4362\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 1.6739 - acc: 0.5758 - val_loss: 2.0677 - val_acc: 0.4035\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 1.4899 - acc: 0.5937 - val_loss: 1.7073 - val_acc: 0.5181\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 1.3738 - acc: 0.6136 - val_loss: 1.5001 - val_acc: 0.5693\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 1.3112 - acc: 0.6294 - val_loss: 1.4505 - val_acc: 0.5729\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 5s 461us/step - loss: 1.2444 - acc: 0.6435 - val_loss: 1.5308 - val_acc: 0.5508\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 1.2218 - acc: 0.6504 - val_loss: 1.6465 - val_acc: 0.5299\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 5s 462us/step - loss: 1.1872 - acc: 0.6621 - val_loss: 1.4701 - val_acc: 0.5690\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 1.1260 - acc: 0.6887 - val_loss: 1.4990 - val_acc: 0.5792\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 1.1218 - acc: 0.6907 - val_loss: 2.3154 - val_acc: 0.4441\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 1.1177 - acc: 0.6964 - val_loss: 1.4617 - val_acc: 0.5926\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 1.0944 - acc: 0.7117 - val_loss: 1.6714 - val_acc: 0.5362\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 5s 455us/step - loss: 1.0609 - acc: 0.7267 - val_loss: 1.7540 - val_acc: 0.5221\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 1.0629 - acc: 0.7288 - val_loss: 1.5010 - val_acc: 0.5973\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 1.0199 - acc: 0.7530 - val_loss: 1.8281 - val_acc: 0.5130\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 1.0103 - acc: 0.7525 - val_loss: 1.6088 - val_acc: 0.5741\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 1.0097 - acc: 0.7624 - val_loss: 1.5900 - val_acc: 0.5658\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 0.9983 - acc: 0.7653 - val_loss: 1.6498 - val_acc: 0.5583\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 1.0029 - acc: 0.7704 - val_loss: 2.0311 - val_acc: 0.5276\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.9807 - acc: 0.7785 - val_loss: 2.1578 - val_acc: 0.5197\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 5s 469us/step - loss: 0.9885 - acc: 0.7834 - val_loss: 1.8124 - val_acc: 0.5634\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.9992 - acc: 0.7749 - val_loss: 1.7697 - val_acc: 0.5556\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.9594 - acc: 0.7989 - val_loss: 1.6603 - val_acc: 0.5926\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 0.9264 - acc: 0.8059 - val_loss: 1.6770 - val_acc: 0.5764\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.9118 - acc: 0.8130 - val_loss: 2.0320 - val_acc: 0.5284\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.9442 - acc: 0.8021 - val_loss: 1.9173 - val_acc: 0.5465\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 0.9331 - acc: 0.8082 - val_loss: 1.6823 - val_acc: 0.5867\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8964 - acc: 0.8267 - val_loss: 1.7439 - val_acc: 0.5784\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 5s 467us/step - loss: 0.9167 - acc: 0.8189 - val_loss: 1.7591 - val_acc: 0.5847\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 0.9100 - acc: 0.8231 - val_loss: 1.9225 - val_acc: 0.5493\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.9087 - acc: 0.8250 - val_loss: 1.7323 - val_acc: 0.5709\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 0.8985 - acc: 0.8279 - val_loss: 1.8240 - val_acc: 0.5804\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.8870 - acc: 0.8320 - val_loss: 2.2588 - val_acc: 0.5193\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 5s 455us/step - loss: 0.9125 - acc: 0.8310 - val_loss: 1.8392 - val_acc: 0.5784\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.8733 - acc: 0.8395 - val_loss: 2.2195 - val_acc: 0.5083\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8741 - acc: 0.8463 - val_loss: 1.7668 - val_acc: 0.5863\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 5s 462us/step - loss: 0.8751 - acc: 0.8424 - val_loss: 1.8307 - val_acc: 0.5662\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.8840 - acc: 0.8401 - val_loss: 2.0700 - val_acc: 0.5288\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 0.8487 - acc: 0.8512 - val_loss: 2.0972 - val_acc: 0.5296\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8820 - acc: 0.8364 - val_loss: 1.8777 - val_acc: 0.5796\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.8585 - acc: 0.8503 - val_loss: 2.1991 - val_acc: 0.5315\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8378 - acc: 0.8567 - val_loss: 2.0331 - val_acc: 0.5437\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 0.8416 - acc: 0.8547 - val_loss: 2.0558 - val_acc: 0.5603\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 0.8609 - acc: 0.8507 - val_loss: 2.0868 - val_acc: 0.5658\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 0.8499 - acc: 0.8530 - val_loss: 1.7871 - val_acc: 0.5898\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8210 - acc: 0.8682 - val_loss: 1.8883 - val_acc: 0.5784\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 5s 471us/step - loss: 0.8289 - acc: 0.8637 - val_loss: 2.0796 - val_acc: 0.5520\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 5s 461us/step - loss: 0.8469 - acc: 0.8558 - val_loss: 2.2317 - val_acc: 0.5465\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.8126 - acc: 0.8724 - val_loss: 1.7959 - val_acc: 0.5780\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8353 - acc: 0.8593 - val_loss: 1.9735 - val_acc: 0.5764\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 0.8261 - acc: 0.8649 - val_loss: 2.0586 - val_acc: 0.5733\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8227 - acc: 0.8662 - val_loss: 1.7897 - val_acc: 0.5831\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 5s 475us/step - loss: 0.8140 - acc: 0.8693 - val_loss: 1.9929 - val_acc: 0.5725\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 5s 471us/step - loss: 0.8195 - acc: 0.8641 - val_loss: 1.9409 - val_acc: 0.5410\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 5s 460us/step - loss: 0.8087 - acc: 0.8688 - val_loss: 1.8404 - val_acc: 0.5859\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.8233 - acc: 0.8681 - val_loss: 2.5459 - val_acc: 0.5087\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8278 - acc: 0.8683 - val_loss: 2.3953 - val_acc: 0.5213\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.8420 - acc: 0.8629 - val_loss: 2.1923 - val_acc: 0.5457\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8093 - acc: 0.8715 - val_loss: 2.3064 - val_acc: 0.5433\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 5s 467us/step - loss: 0.8041 - acc: 0.8738 - val_loss: 2.1490 - val_acc: 0.5567\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 5s 461us/step - loss: 0.8103 - acc: 0.8754 - val_loss: 2.1567 - val_acc: 0.5410\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.8455 - acc: 0.8602 - val_loss: 1.9749 - val_acc: 0.5733\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.7942 - acc: 0.8806 - val_loss: 2.3269 - val_acc: 0.5359\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 5s 459us/step - loss: 0.8023 - acc: 0.8727 - val_loss: 2.2754 - val_acc: 0.5213\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.8039 - acc: 0.8733 - val_loss: 1.9395 - val_acc: 0.5839\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7893 - acc: 0.8788 - val_loss: 2.0806 - val_acc: 0.5615\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.8246 - acc: 0.8725 - val_loss: 1.9344 - val_acc: 0.5804\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8000 - acc: 0.8781 - val_loss: 1.9558 - val_acc: 0.5749\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 5s 466us/step - loss: 0.7971 - acc: 0.8787 - val_loss: 2.4471 - val_acc: 0.5028\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.8096 - acc: 0.8730 - val_loss: 1.9598 - val_acc: 0.5792\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7786 - acc: 0.8881 - val_loss: 1.9685 - val_acc: 0.5642\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7749 - acc: 0.8825 - val_loss: 1.9166 - val_acc: 0.5737\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8073 - acc: 0.8732 - val_loss: 2.1643 - val_acc: 0.5603\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 5s 459us/step - loss: 0.8253 - acc: 0.8685 - val_loss: 2.2140 - val_acc: 0.5485\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7748 - acc: 0.8889 - val_loss: 1.9463 - val_acc: 0.5729\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7894 - acc: 0.8828 - val_loss: 1.8766 - val_acc: 0.5926\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8232 - acc: 0.8666 - val_loss: 1.9838 - val_acc: 0.5591\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7916 - acc: 0.8847 - val_loss: 2.0472 - val_acc: 0.5690\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7576 - acc: 0.8953 - val_loss: 2.2669 - val_acc: 0.5481\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7906 - acc: 0.8792 - val_loss: 2.1410 - val_acc: 0.5441\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 5s 468us/step - loss: 0.8131 - acc: 0.8727 - val_loss: 1.9524 - val_acc: 0.5729\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.7918 - acc: 0.8814 - val_loss: 1.9301 - val_acc: 0.5875\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.7807 - acc: 0.8869 - val_loss: 1.9319 - val_acc: 0.5623\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8211 - acc: 0.8683 - val_loss: 2.1693 - val_acc: 0.5473\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 0.7962 - acc: 0.8785 - val_loss: 2.2622 - val_acc: 0.5429\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 0.7917 - acc: 0.8805 - val_loss: 2.1844 - val_acc: 0.5426\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 5s 463us/step - loss: 0.7970 - acc: 0.8811 - val_loss: 2.1994 - val_acc: 0.5646\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 5s 463us/step - loss: 0.7889 - acc: 0.8821 - val_loss: 2.0677 - val_acc: 0.5603\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 5s 463us/step - loss: 0.8065 - acc: 0.8771 - val_loss: 2.1274 - val_acc: 0.5575\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 0.7686 - acc: 0.8911 - val_loss: 1.9814 - val_acc: 0.5686\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7581 - acc: 0.8917 - val_loss: 2.0417 - val_acc: 0.5682\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 0.8189 - acc: 0.8693 - val_loss: 2.9371 - val_acc: 0.4815\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.8318 - acc: 0.8675 - val_loss: 1.9031 - val_acc: 0.5875\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7774 - acc: 0.8901 - val_loss: 2.0098 - val_acc: 0.5717\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7618 - acc: 0.8930 - val_loss: 2.1660 - val_acc: 0.5599\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.7684 - acc: 0.8863 - val_loss: 2.0271 - val_acc: 0.5804\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7677 - acc: 0.8901 - val_loss: 2.2381 - val_acc: 0.5264\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7565 - acc: 0.8956 - val_loss: 2.2815 - val_acc: 0.5429\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7503 - acc: 0.8938 - val_loss: 2.0925 - val_acc: 0.5812\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 5s 469us/step - loss: 0.7924 - acc: 0.8811 - val_loss: 2.5805 - val_acc: 0.5020\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.7962 - acc: 0.8792 - val_loss: 2.0586 - val_acc: 0.5701\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.8225 - acc: 0.8767 - val_loss: 2.3817 - val_acc: 0.5268\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7942 - acc: 0.8800 - val_loss: 2.6197 - val_acc: 0.5217\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.7569 - acc: 0.8942 - val_loss: 2.2778 - val_acc: 0.5327\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 5s 461us/step - loss: 0.8100 - acc: 0.8745 - val_loss: 1.9494 - val_acc: 0.5678\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7785 - acc: 0.8904 - val_loss: 2.1106 - val_acc: 0.5626\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.7622 - acc: 0.8940 - val_loss: 2.3482 - val_acc: 0.5540\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7737 - acc: 0.8887 - val_loss: 2.5486 - val_acc: 0.5422\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 5s 469us/step - loss: 0.7706 - acc: 0.8915 - val_loss: 2.3195 - val_acc: 0.5548\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 5s 455us/step - loss: 0.7803 - acc: 0.8816 - val_loss: 2.7918 - val_acc: 0.4677\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 0.8218 - acc: 0.8749 - val_loss: 2.1588 - val_acc: 0.5516\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7787 - acc: 0.8908 - val_loss: 2.0094 - val_acc: 0.5745\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 5s 461us/step - loss: 0.7517 - acc: 0.8980 - val_loss: 2.0970 - val_acc: 0.5713\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 5s 460us/step - loss: 0.7607 - acc: 0.8901 - val_loss: 2.1468 - val_acc: 0.5607\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7770 - acc: 0.8840 - val_loss: 1.9421 - val_acc: 0.5863\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 0.7687 - acc: 0.8900 - val_loss: 2.8698 - val_acc: 0.5063\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7548 - acc: 0.8938 - val_loss: 2.4435 - val_acc: 0.5315\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 5s 455us/step - loss: 0.7619 - acc: 0.8879 - val_loss: 2.6017 - val_acc: 0.5520\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 0.8059 - acc: 0.8780 - val_loss: 2.3451 - val_acc: 0.5394\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 1/256 [09:28<40:16:52, 568.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 507us/step - loss: 5.9652 - acc: 0.2803 - val_loss: 4.6201 - val_acc: 0.2797\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 3.6810 - acc: 0.3432 - val_loss: 2.8698 - val_acc: 0.3396\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 2.6279 - acc: 0.4064 - val_loss: 2.1732 - val_acc: 0.4102\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 5s 455us/step - loss: 2.0447 - acc: 0.4684 - val_loss: 2.0427 - val_acc: 0.4236\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 1.7652 - acc: 0.5016 - val_loss: 1.8933 - val_acc: 0.4370\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 5s 460us/step - loss: 1.5765 - acc: 0.5371 - val_loss: 1.8001 - val_acc: 0.4673\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 1.4588 - acc: 0.5549 - val_loss: 1.5825 - val_acc: 0.5067\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 1.3590 - acc: 0.5771 - val_loss: 1.8563 - val_acc: 0.4708\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.3086 - acc: 0.5954 - val_loss: 2.3591 - val_acc: 0.3976\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 1.2455 - acc: 0.6191 - val_loss: 1.8613 - val_acc: 0.4760\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 1.1938 - acc: 0.6363 - val_loss: 1.4915 - val_acc: 0.5437\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 1.1489 - acc: 0.6537 - val_loss: 1.5177 - val_acc: 0.5359\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 1.1208 - acc: 0.6686 - val_loss: 1.7544 - val_acc: 0.5020\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 1.1028 - acc: 0.6719 - val_loss: 1.5451 - val_acc: 0.5469\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.0745 - acc: 0.6874 - val_loss: 1.6529 - val_acc: 0.5544\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 1.0520 - acc: 0.7051 - val_loss: 1.5352 - val_acc: 0.5355\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 1.0470 - acc: 0.7003 - val_loss: 1.8363 - val_acc: 0.4791\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 1.0212 - acc: 0.7190 - val_loss: 1.9925 - val_acc: 0.4945\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 1.0155 - acc: 0.7232 - val_loss: 1.6008 - val_acc: 0.5559\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 1.0072 - acc: 0.7319 - val_loss: 1.7400 - val_acc: 0.5288\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.9935 - acc: 0.7403 - val_loss: 1.5064 - val_acc: 0.5918\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.9793 - acc: 0.7425 - val_loss: 1.7282 - val_acc: 0.5532\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.9649 - acc: 0.7590 - val_loss: 2.2622 - val_acc: 0.4673\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 5s 455us/step - loss: 0.9783 - acc: 0.7537 - val_loss: 1.7457 - val_acc: 0.5536\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.9574 - acc: 0.7679 - val_loss: 1.7586 - val_acc: 0.5453\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.9516 - acc: 0.7731 - val_loss: 1.6848 - val_acc: 0.5453\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.9364 - acc: 0.7815 - val_loss: 1.6752 - val_acc: 0.5477\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 0.9331 - acc: 0.7826 - val_loss: 1.8964 - val_acc: 0.5284\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.9380 - acc: 0.7866 - val_loss: 1.6520 - val_acc: 0.5583\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.9268 - acc: 0.7896 - val_loss: 1.6809 - val_acc: 0.5686\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.9246 - acc: 0.7939 - val_loss: 1.7012 - val_acc: 0.5674\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.9248 - acc: 0.7943 - val_loss: 1.7552 - val_acc: 0.5725\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.9085 - acc: 0.8055 - val_loss: 1.7246 - val_acc: 0.5587\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.9100 - acc: 0.8024 - val_loss: 1.8619 - val_acc: 0.5429\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.9090 - acc: 0.8063 - val_loss: 1.6311 - val_acc: 0.5879\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.9020 - acc: 0.8087 - val_loss: 2.0539 - val_acc: 0.5264\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 5s 459us/step - loss: 0.9031 - acc: 0.8057 - val_loss: 1.8682 - val_acc: 0.5378\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.8830 - acc: 0.8158 - val_loss: 1.7832 - val_acc: 0.5504\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.8924 - acc: 0.8168 - val_loss: 1.8835 - val_acc: 0.5504\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8855 - acc: 0.8197 - val_loss: 1.7218 - val_acc: 0.5729\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8756 - acc: 0.8258 - val_loss: 2.1951 - val_acc: 0.5169\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.8699 - acc: 0.8271 - val_loss: 2.1084 - val_acc: 0.5197\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8743 - acc: 0.8307 - val_loss: 2.2121 - val_acc: 0.5008\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8878 - acc: 0.8242 - val_loss: 2.2319 - val_acc: 0.5177\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8751 - acc: 0.8252 - val_loss: 3.7649 - val_acc: 0.3830\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.8718 - acc: 0.8288 - val_loss: 1.9169 - val_acc: 0.5461\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8678 - acc: 0.8291 - val_loss: 2.3854 - val_acc: 0.4827\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8544 - acc: 0.8369 - val_loss: 2.0000 - val_acc: 0.5433\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 0.8551 - acc: 0.8323 - val_loss: 1.8728 - val_acc: 0.5729\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8656 - acc: 0.8339 - val_loss: 1.8218 - val_acc: 0.5753\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8522 - acc: 0.8390 - val_loss: 2.2416 - val_acc: 0.5189\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8474 - acc: 0.8375 - val_loss: 1.8404 - val_acc: 0.5504\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8458 - acc: 0.8400 - val_loss: 1.7049 - val_acc: 0.5843\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8434 - acc: 0.8460 - val_loss: 1.7255 - val_acc: 0.5859\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8487 - acc: 0.8395 - val_loss: 1.7828 - val_acc: 0.5808\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8496 - acc: 0.8429 - val_loss: 1.8991 - val_acc: 0.5749\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8426 - acc: 0.8444 - val_loss: 2.1511 - val_acc: 0.5280\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8478 - acc: 0.8454 - val_loss: 2.2290 - val_acc: 0.4968\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8319 - acc: 0.8516 - val_loss: 1.8859 - val_acc: 0.5701\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.8374 - acc: 0.8493 - val_loss: 2.3448 - val_acc: 0.5323\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8492 - acc: 0.8432 - val_loss: 1.8605 - val_acc: 0.5567\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.8345 - acc: 0.8477 - val_loss: 2.1995 - val_acc: 0.5410\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8222 - acc: 0.8543 - val_loss: 2.3593 - val_acc: 0.5327\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8412 - acc: 0.8490 - val_loss: 2.2533 - val_acc: 0.5292\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8290 - acc: 0.8496 - val_loss: 2.2514 - val_acc: 0.5193\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8289 - acc: 0.8503 - val_loss: 1.8750 - val_acc: 0.5890\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8375 - acc: 0.8485 - val_loss: 2.2807 - val_acc: 0.5229\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8280 - acc: 0.8586 - val_loss: 2.2301 - val_acc: 0.5410\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8263 - acc: 0.8583 - val_loss: 2.1218 - val_acc: 0.5426\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8237 - acc: 0.8576 - val_loss: 2.0331 - val_acc: 0.5398\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8237 - acc: 0.8539 - val_loss: 1.8521 - val_acc: 0.5753\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8259 - acc: 0.8537 - val_loss: 1.9813 - val_acc: 0.5567\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8158 - acc: 0.8630 - val_loss: 2.5710 - val_acc: 0.4862\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8082 - acc: 0.8640 - val_loss: 1.9195 - val_acc: 0.5717\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8153 - acc: 0.8629 - val_loss: 1.8127 - val_acc: 0.5969\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8203 - acc: 0.8554 - val_loss: 2.0514 - val_acc: 0.5634\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8064 - acc: 0.8617 - val_loss: 1.8841 - val_acc: 0.5646\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8187 - acc: 0.8599 - val_loss: 1.9376 - val_acc: 0.5599\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.8114 - acc: 0.8647 - val_loss: 2.9296 - val_acc: 0.4641\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8109 - acc: 0.8645 - val_loss: 2.6845 - val_acc: 0.4854\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8147 - acc: 0.8592 - val_loss: 2.0225 - val_acc: 0.5827\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8243 - acc: 0.8573 - val_loss: 1.7211 - val_acc: 0.5969\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8126 - acc: 0.8636 - val_loss: 2.2313 - val_acc: 0.5473\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8212 - acc: 0.8580 - val_loss: 1.7650 - val_acc: 0.5954\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8109 - acc: 0.8645 - val_loss: 1.8538 - val_acc: 0.5875\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8115 - acc: 0.8659 - val_loss: 2.5396 - val_acc: 0.4957\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8120 - acc: 0.8622 - val_loss: 1.8542 - val_acc: 0.5898\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8070 - acc: 0.8663 - val_loss: 1.8840 - val_acc: 0.5894\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8065 - acc: 0.8649 - val_loss: 2.2749 - val_acc: 0.5453\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8060 - acc: 0.8674 - val_loss: 1.9704 - val_acc: 0.5650\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8227 - acc: 0.8558 - val_loss: 2.1593 - val_acc: 0.5508\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8105 - acc: 0.8646 - val_loss: 2.7478 - val_acc: 0.4866\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8156 - acc: 0.8619 - val_loss: 2.0000 - val_acc: 0.5654\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8020 - acc: 0.8674 - val_loss: 2.0802 - val_acc: 0.5599\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8101 - acc: 0.8653 - val_loss: 1.8768 - val_acc: 0.5686\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8082 - acc: 0.8628 - val_loss: 1.9872 - val_acc: 0.5638\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7958 - acc: 0.8725 - val_loss: 2.3084 - val_acc: 0.5256\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8010 - acc: 0.8695 - val_loss: 2.5704 - val_acc: 0.4968\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8171 - acc: 0.8629 - val_loss: 2.4971 - val_acc: 0.5146\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7962 - acc: 0.8712 - val_loss: 2.3163 - val_acc: 0.5252\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8080 - acc: 0.8624 - val_loss: 2.5768 - val_acc: 0.4929\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 5s 455us/step - loss: 0.8186 - acc: 0.8661 - val_loss: 2.9259 - val_acc: 0.4807\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.8027 - acc: 0.8725 - val_loss: 2.1903 - val_acc: 0.5571\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8070 - acc: 0.8650 - val_loss: 1.9550 - val_acc: 0.5823\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7925 - acc: 0.8725 - val_loss: 2.7606 - val_acc: 0.5146\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.8074 - acc: 0.8662 - val_loss: 2.7666 - val_acc: 0.5095\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.7942 - acc: 0.8756 - val_loss: 2.0158 - val_acc: 0.5867\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8052 - acc: 0.8646 - val_loss: 1.9677 - val_acc: 0.5804\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7933 - acc: 0.8702 - val_loss: 2.2736 - val_acc: 0.5418\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8008 - acc: 0.8687 - val_loss: 2.2882 - val_acc: 0.5276\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8000 - acc: 0.8678 - val_loss: 2.2045 - val_acc: 0.5433\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7949 - acc: 0.8697 - val_loss: 2.8455 - val_acc: 0.4858\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7949 - acc: 0.8739 - val_loss: 2.5628 - val_acc: 0.5162\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7971 - acc: 0.8734 - val_loss: 1.9597 - val_acc: 0.5690\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.7902 - acc: 0.8731 - val_loss: 2.3924 - val_acc: 0.5138\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8068 - acc: 0.8665 - val_loss: 2.2128 - val_acc: 0.5378\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7949 - acc: 0.8759 - val_loss: 2.1474 - val_acc: 0.5433\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7975 - acc: 0.8684 - val_loss: 1.9802 - val_acc: 0.5792\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.7907 - acc: 0.8749 - val_loss: 2.1148 - val_acc: 0.5362\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7967 - acc: 0.8740 - val_loss: 1.9154 - val_acc: 0.5713\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7927 - acc: 0.8760 - val_loss: 1.8810 - val_acc: 0.5827\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.8029 - acc: 0.8706 - val_loss: 2.9387 - val_acc: 0.4634\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7855 - acc: 0.8754 - val_loss: 2.1148 - val_acc: 0.5556\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7918 - acc: 0.8719 - val_loss: 2.4233 - val_acc: 0.5339\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7914 - acc: 0.8723 - val_loss: 2.4313 - val_acc: 0.5268\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7863 - acc: 0.8769 - val_loss: 1.9871 - val_acc: 0.5780\n",
            "Epoch 127/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7918 - acc: 0.8730 - val_loss: 2.1447 - val_acc: 0.5544\n",
            "Epoch 128/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8000 - acc: 0.8676 - val_loss: 1.9655 - val_acc: 0.5733\n",
            "Epoch 129/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.8012 - acc: 0.8716 - val_loss: 2.7669 - val_acc: 0.4890\n",
            "Epoch 130/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7927 - acc: 0.8721 - val_loss: 1.9007 - val_acc: 0.5887\n",
            "Epoch 131/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7852 - acc: 0.8741 - val_loss: 2.8951 - val_acc: 0.5150\n",
            "Epoch 132/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7987 - acc: 0.8700 - val_loss: 2.1800 - val_acc: 0.5477\n",
            "Epoch 133/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7764 - acc: 0.8791 - val_loss: 2.2678 - val_acc: 0.5449\n",
            "Epoch 134/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7791 - acc: 0.8776 - val_loss: 2.3408 - val_acc: 0.5366\n",
            "Epoch 135/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7917 - acc: 0.8725 - val_loss: 1.9779 - val_acc: 0.5690\n",
            "Epoch 136/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.8032 - acc: 0.8666 - val_loss: 2.4093 - val_acc: 0.5272\n",
            "Epoch 137/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7904 - acc: 0.8729 - val_loss: 2.0051 - val_acc: 0.5780\n",
            "Epoch 138/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7885 - acc: 0.8765 - val_loss: 2.1696 - val_acc: 0.5615\n",
            "Epoch 139/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7881 - acc: 0.8717 - val_loss: 2.2564 - val_acc: 0.5481\n",
            "Epoch 140/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7903 - acc: 0.8733 - val_loss: 1.9892 - val_acc: 0.5544\n",
            "Epoch 141/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7898 - acc: 0.8738 - val_loss: 2.0568 - val_acc: 0.5835\n",
            "Epoch 142/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7955 - acc: 0.8719 - val_loss: 1.9912 - val_acc: 0.5808\n",
            "Epoch 143/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7915 - acc: 0.8751 - val_loss: 1.9807 - val_acc: 0.5666\n",
            "Epoch 144/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7884 - acc: 0.8783 - val_loss: 2.0575 - val_acc: 0.5626\n",
            "Epoch 145/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.7823 - acc: 0.8787 - val_loss: 2.2877 - val_acc: 0.5418\n",
            "Epoch 146/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7886 - acc: 0.8730 - val_loss: 2.0995 - val_acc: 0.5603\n",
            "Epoch 147/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7914 - acc: 0.8777 - val_loss: 2.5716 - val_acc: 0.5118\n",
            "Epoch 148/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7815 - acc: 0.8781 - val_loss: 2.3755 - val_acc: 0.5229\n",
            "Epoch 149/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7910 - acc: 0.8750 - val_loss: 2.3691 - val_acc: 0.5437\n",
            "Epoch 150/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.7727 - acc: 0.8821 - val_loss: 2.8862 - val_acc: 0.4677\n",
            "Epoch 151/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7900 - acc: 0.8743 - val_loss: 2.1642 - val_acc: 0.5445\n",
            "Epoch 152/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7732 - acc: 0.8805 - val_loss: 2.8782 - val_acc: 0.4933\n",
            "Epoch 153/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7776 - acc: 0.8770 - val_loss: 2.7878 - val_acc: 0.5043\n",
            "Epoch 154/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7847 - acc: 0.8767 - val_loss: 2.1357 - val_acc: 0.5682\n",
            "Epoch 155/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7832 - acc: 0.8766 - val_loss: 2.2731 - val_acc: 0.5630\n",
            "Epoch 156/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7729 - acc: 0.8818 - val_loss: 1.9111 - val_acc: 0.5859\n",
            "Epoch 157/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7786 - acc: 0.8782 - val_loss: 1.8538 - val_acc: 0.6099\n",
            "Epoch 158/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7847 - acc: 0.8761 - val_loss: 2.3744 - val_acc: 0.5189\n",
            "Epoch 159/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7709 - acc: 0.8816 - val_loss: 2.6392 - val_acc: 0.5075\n",
            "Epoch 160/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7806 - acc: 0.8773 - val_loss: 2.0779 - val_acc: 0.5662\n",
            "Epoch 161/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7834 - acc: 0.8767 - val_loss: 2.3805 - val_acc: 0.5516\n",
            "Epoch 162/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.7734 - acc: 0.8784 - val_loss: 1.9814 - val_acc: 0.5788\n",
            "Epoch 163/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7854 - acc: 0.8755 - val_loss: 2.1435 - val_acc: 0.5674\n",
            "Epoch 164/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7793 - acc: 0.8786 - val_loss: 2.6842 - val_acc: 0.5134\n",
            "Epoch 165/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7743 - acc: 0.8825 - val_loss: 2.4513 - val_acc: 0.5311\n",
            "Epoch 166/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7899 - acc: 0.8777 - val_loss: 1.9978 - val_acc: 0.5831\n",
            "Epoch 167/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7894 - acc: 0.8776 - val_loss: 2.5080 - val_acc: 0.5370\n",
            "Epoch 168/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7773 - acc: 0.8824 - val_loss: 2.6257 - val_acc: 0.5284\n",
            "Epoch 169/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7729 - acc: 0.8833 - val_loss: 2.6004 - val_acc: 0.5118\n",
            "Epoch 170/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7712 - acc: 0.8831 - val_loss: 2.0876 - val_acc: 0.5709\n",
            "Epoch 171/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7766 - acc: 0.8788 - val_loss: 2.3510 - val_acc: 0.5516\n",
            "Epoch 172/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7809 - acc: 0.8818 - val_loss: 2.1318 - val_acc: 0.5623\n",
            "Epoch 173/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7844 - acc: 0.8761 - val_loss: 2.6955 - val_acc: 0.5264\n",
            "Epoch 174/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7836 - acc: 0.8809 - val_loss: 2.0391 - val_acc: 0.5725\n",
            "Epoch 175/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7774 - acc: 0.8783 - val_loss: 2.8106 - val_acc: 0.4728\n",
            "Epoch 176/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7874 - acc: 0.8760 - val_loss: 2.0091 - val_acc: 0.5686\n",
            "Epoch 177/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7803 - acc: 0.8769 - val_loss: 2.1711 - val_acc: 0.5623\n",
            "Epoch 178/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7777 - acc: 0.8787 - val_loss: 2.2605 - val_acc: 0.5516\n",
            "Epoch 179/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7718 - acc: 0.8815 - val_loss: 2.8850 - val_acc: 0.5205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 2/256 [22:57<45:12:17, 640.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 528us/step - loss: 6.0586 - acc: 0.2901 - val_loss: 5.1627 - val_acc: 0.3156\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 4.3775 - acc: 0.3648 - val_loss: 3.7035 - val_acc: 0.3408\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 3.2191 - acc: 0.4481 - val_loss: 2.9338 - val_acc: 0.3983\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 2.4587 - acc: 0.5112 - val_loss: 2.4353 - val_acc: 0.4310\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 1.9810 - acc: 0.5543 - val_loss: 2.2106 - val_acc: 0.4173\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 1.7000 - acc: 0.5822 - val_loss: 1.7847 - val_acc: 0.5091\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 1.5142 - acc: 0.6027 - val_loss: 2.0565 - val_acc: 0.4377\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 1.3869 - acc: 0.6254 - val_loss: 2.2496 - val_acc: 0.4397\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 1.3459 - acc: 0.6273 - val_loss: 1.5599 - val_acc: 0.5406\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 1.2593 - acc: 0.6468 - val_loss: 1.8453 - val_acc: 0.5043\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 1.2360 - acc: 0.6535 - val_loss: 1.9697 - val_acc: 0.4504\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 1.1772 - acc: 0.6792 - val_loss: 1.5931 - val_acc: 0.5268\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 1.1546 - acc: 0.6928 - val_loss: 1.4883 - val_acc: 0.5737\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.1156 - acc: 0.6997 - val_loss: 2.0790 - val_acc: 0.4606\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 1.1160 - acc: 0.7001 - val_loss: 1.5739 - val_acc: 0.5686\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 1.1241 - acc: 0.6982 - val_loss: 1.4949 - val_acc: 0.5808\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.0729 - acc: 0.7262 - val_loss: 1.5343 - val_acc: 0.5579\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 1.0704 - acc: 0.7330 - val_loss: 1.7097 - val_acc: 0.5445\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 1.0192 - acc: 0.7547 - val_loss: 1.9372 - val_acc: 0.4850\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 1.0436 - acc: 0.7442 - val_loss: 1.5182 - val_acc: 0.5969\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 1.0357 - acc: 0.7495 - val_loss: 1.8044 - val_acc: 0.5536\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 1.0052 - acc: 0.7684 - val_loss: 1.7600 - val_acc: 0.5552\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.9722 - acc: 0.7808 - val_loss: 1.6390 - val_acc: 0.5737\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 1.0254 - acc: 0.7583 - val_loss: 1.6277 - val_acc: 0.5768\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.9633 - acc: 0.7880 - val_loss: 1.6783 - val_acc: 0.5674\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.9699 - acc: 0.7853 - val_loss: 2.2037 - val_acc: 0.4929\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.9700 - acc: 0.7859 - val_loss: 1.7458 - val_acc: 0.5650\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.9389 - acc: 0.8005 - val_loss: 1.6861 - val_acc: 0.5871\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.9372 - acc: 0.8100 - val_loss: 1.9058 - val_acc: 0.5402\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.9131 - acc: 0.8157 - val_loss: 1.6687 - val_acc: 0.5823\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.9258 - acc: 0.8119 - val_loss: 2.2071 - val_acc: 0.5398\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.9012 - acc: 0.8242 - val_loss: 1.7616 - val_acc: 0.5788\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.9249 - acc: 0.8179 - val_loss: 1.7839 - val_acc: 0.5816\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.9032 - acc: 0.8249 - val_loss: 2.0117 - val_acc: 0.5508\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.9092 - acc: 0.8239 - val_loss: 1.9105 - val_acc: 0.5493\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8858 - acc: 0.8354 - val_loss: 2.5385 - val_acc: 0.4835\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.8957 - acc: 0.8359 - val_loss: 1.9221 - val_acc: 0.5630\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.9300 - acc: 0.8180 - val_loss: 2.0024 - val_acc: 0.5595\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8748 - acc: 0.8457 - val_loss: 1.8794 - val_acc: 0.5591\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.9272 - acc: 0.8232 - val_loss: 2.0436 - val_acc: 0.5485\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8421 - acc: 0.8549 - val_loss: 2.0644 - val_acc: 0.5650\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8589 - acc: 0.8470 - val_loss: 1.9736 - val_acc: 0.5693\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8562 - acc: 0.8476 - val_loss: 1.9811 - val_acc: 0.5607\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.8764 - acc: 0.8444 - val_loss: 1.8565 - val_acc: 0.5717\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8711 - acc: 0.8460 - val_loss: 1.7385 - val_acc: 0.6064\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.8609 - acc: 0.8528 - val_loss: 2.0939 - val_acc: 0.5603\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.8532 - acc: 0.8503 - val_loss: 2.3266 - val_acc: 0.5114\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8728 - acc: 0.8431 - val_loss: 1.8425 - val_acc: 0.5737\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8256 - acc: 0.8648 - val_loss: 1.8037 - val_acc: 0.5827\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8477 - acc: 0.8565 - val_loss: 2.1428 - val_acc: 0.5311\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.8943 - acc: 0.8431 - val_loss: 2.1455 - val_acc: 0.5351\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8164 - acc: 0.8710 - val_loss: 1.9612 - val_acc: 0.5666\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7714 - acc: 0.8876 - val_loss: 2.0327 - val_acc: 0.5686\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8395 - acc: 0.8560 - val_loss: 1.9384 - val_acc: 0.5563\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.8426 - acc: 0.8578 - val_loss: 1.8968 - val_acc: 0.5784\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8238 - acc: 0.8694 - val_loss: 2.2427 - val_acc: 0.5445\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.8169 - acc: 0.8715 - val_loss: 2.1621 - val_acc: 0.5453\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8297 - acc: 0.8710 - val_loss: 1.8775 - val_acc: 0.5745\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8238 - acc: 0.8656 - val_loss: 2.2616 - val_acc: 0.5524\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8254 - acc: 0.8683 - val_loss: 1.9612 - val_acc: 0.5831\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8294 - acc: 0.8664 - val_loss: 1.7195 - val_acc: 0.6135\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8135 - acc: 0.8727 - val_loss: 1.9849 - val_acc: 0.5654\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8268 - acc: 0.8662 - val_loss: 1.8412 - val_acc: 0.5950\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8064 - acc: 0.8763 - val_loss: 2.1615 - val_acc: 0.5741\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.8097 - acc: 0.8681 - val_loss: 2.0560 - val_acc: 0.5662\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.7913 - acc: 0.8825 - val_loss: 2.1467 - val_acc: 0.5473\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.7855 - acc: 0.8824 - val_loss: 2.1555 - val_acc: 0.5512\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8094 - acc: 0.8765 - val_loss: 1.8624 - val_acc: 0.5682\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.8155 - acc: 0.8723 - val_loss: 2.0673 - val_acc: 0.5749\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.8573 - acc: 0.8631 - val_loss: 2.1778 - val_acc: 0.5571\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8167 - acc: 0.8755 - val_loss: 2.0689 - val_acc: 0.5607\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.8283 - acc: 0.8707 - val_loss: 1.9239 - val_acc: 0.5890\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.7750 - acc: 0.8844 - val_loss: 1.9332 - val_acc: 0.5906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 3/256 [28:04<37:59:42, 540.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 467us/step - loss: 6.0978 - acc: 0.2901 - val_loss: 5.1235 - val_acc: 0.2656\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 3.9091 - acc: 0.3391 - val_loss: 3.2412 - val_acc: 0.3318\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 2.7262 - acc: 0.4014 - val_loss: 2.6714 - val_acc: 0.3554\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 2.1288 - acc: 0.4717 - val_loss: 1.9117 - val_acc: 0.4618\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 1.7962 - acc: 0.5150 - val_loss: 2.3308 - val_acc: 0.3779\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 1.6085 - acc: 0.5379 - val_loss: 2.2124 - val_acc: 0.4003\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 1.4907 - acc: 0.5565 - val_loss: 1.5924 - val_acc: 0.5114\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 1.3928 - acc: 0.5745 - val_loss: 2.0167 - val_acc: 0.3987\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 1.3178 - acc: 0.5979 - val_loss: 2.0588 - val_acc: 0.4267\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 1.2595 - acc: 0.6140 - val_loss: 1.7043 - val_acc: 0.5032\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 1.2092 - acc: 0.6315 - val_loss: 1.4847 - val_acc: 0.5327\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 1.1779 - acc: 0.6490 - val_loss: 1.8222 - val_acc: 0.4894\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 1.1493 - acc: 0.6543 - val_loss: 2.1508 - val_acc: 0.4598\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 1.1188 - acc: 0.6683 - val_loss: 2.0563 - val_acc: 0.5382\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 1.0932 - acc: 0.6834 - val_loss: 1.4395 - val_acc: 0.5768\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 1.0667 - acc: 0.7001 - val_loss: 1.6892 - val_acc: 0.5366\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 1.0444 - acc: 0.7117 - val_loss: 2.1498 - val_acc: 0.4496\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 1.0342 - acc: 0.7211 - val_loss: 1.7352 - val_acc: 0.5524\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 1.0213 - acc: 0.7277 - val_loss: 2.7749 - val_acc: 0.4188\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 1.0031 - acc: 0.7399 - val_loss: 1.8611 - val_acc: 0.5232\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 1.0014 - acc: 0.7356 - val_loss: 1.7060 - val_acc: 0.5398\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 0.9862 - acc: 0.7494 - val_loss: 1.7969 - val_acc: 0.5402\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.9754 - acc: 0.7526 - val_loss: 2.0685 - val_acc: 0.4771\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.9714 - acc: 0.7629 - val_loss: 2.3499 - val_acc: 0.4866\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.9554 - acc: 0.7651 - val_loss: 1.9575 - val_acc: 0.5028\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.9491 - acc: 0.7783 - val_loss: 1.8791 - val_acc: 0.5185\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.9480 - acc: 0.7710 - val_loss: 2.1038 - val_acc: 0.4913\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.9367 - acc: 0.7797 - val_loss: 1.9089 - val_acc: 0.5146\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.9296 - acc: 0.7856 - val_loss: 2.0129 - val_acc: 0.5126\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.9250 - acc: 0.7861 - val_loss: 2.4490 - val_acc: 0.4965\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.9178 - acc: 0.7929 - val_loss: 3.3646 - val_acc: 0.4192\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.9216 - acc: 0.7970 - val_loss: 1.8422 - val_acc: 0.5422\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.9054 - acc: 0.8056 - val_loss: 1.5989 - val_acc: 0.5768\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.9137 - acc: 0.7993 - val_loss: 2.0502 - val_acc: 0.5189\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 0.9021 - acc: 0.8143 - val_loss: 3.1368 - val_acc: 0.3881\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.9154 - acc: 0.7992 - val_loss: 1.6658 - val_acc: 0.5725\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.8959 - acc: 0.8136 - val_loss: 1.9368 - val_acc: 0.5205\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8986 - acc: 0.8131 - val_loss: 2.8521 - val_acc: 0.4945\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8945 - acc: 0.8121 - val_loss: 1.8330 - val_acc: 0.5603\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8833 - acc: 0.8202 - val_loss: 1.8583 - val_acc: 0.5441\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8838 - acc: 0.8167 - val_loss: 2.2844 - val_acc: 0.5402\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8830 - acc: 0.8217 - val_loss: 2.3627 - val_acc: 0.4811\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8686 - acc: 0.8307 - val_loss: 2.0939 - val_acc: 0.5422\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8893 - acc: 0.8170 - val_loss: 2.0870 - val_acc: 0.5051\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8756 - acc: 0.8226 - val_loss: 1.6912 - val_acc: 0.5812\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8697 - acc: 0.8320 - val_loss: 2.2148 - val_acc: 0.5142\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8787 - acc: 0.8300 - val_loss: 1.8040 - val_acc: 0.5595\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8655 - acc: 0.8304 - val_loss: 2.3733 - val_acc: 0.4827\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8626 - acc: 0.8333 - val_loss: 2.9389 - val_acc: 0.4602\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8603 - acc: 0.8370 - val_loss: 2.3235 - val_acc: 0.5327\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8563 - acc: 0.8317 - val_loss: 1.9635 - val_acc: 0.5307\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8540 - acc: 0.8394 - val_loss: 1.8027 - val_acc: 0.5686\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8523 - acc: 0.8413 - val_loss: 2.0139 - val_acc: 0.5213\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8385 - acc: 0.8434 - val_loss: 2.0235 - val_acc: 0.5493\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8457 - acc: 0.8431 - val_loss: 2.3143 - val_acc: 0.4945\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8488 - acc: 0.8414 - val_loss: 2.1782 - val_acc: 0.5315\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8484 - acc: 0.8431 - val_loss: 2.6784 - val_acc: 0.4953\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8405 - acc: 0.8419 - val_loss: 2.3443 - val_acc: 0.5232\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8436 - acc: 0.8417 - val_loss: 1.8829 - val_acc: 0.5465\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8498 - acc: 0.8401 - val_loss: 1.8623 - val_acc: 0.5670\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8359 - acc: 0.8504 - val_loss: 3.1751 - val_acc: 0.4263\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8349 - acc: 0.8497 - val_loss: 2.6554 - val_acc: 0.4945\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8394 - acc: 0.8477 - val_loss: 2.0775 - val_acc: 0.5272\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8332 - acc: 0.8476 - val_loss: 1.9350 - val_acc: 0.5516\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 0.8274 - acc: 0.8523 - val_loss: 3.3228 - val_acc: 0.4539\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 0.8304 - acc: 0.8520 - val_loss: 3.6903 - val_acc: 0.4279\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.8455 - acc: 0.8492 - val_loss: 2.5189 - val_acc: 0.4819\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8313 - acc: 0.8515 - val_loss: 2.2778 - val_acc: 0.5181\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8204 - acc: 0.8555 - val_loss: 2.5567 - val_acc: 0.4894\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8244 - acc: 0.8555 - val_loss: 2.2707 - val_acc: 0.4968\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8276 - acc: 0.8527 - val_loss: 2.6139 - val_acc: 0.4842\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.8234 - acc: 0.8579 - val_loss: 2.0421 - val_acc: 0.5599\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.8116 - acc: 0.8612 - val_loss: 2.8206 - val_acc: 0.4783\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8272 - acc: 0.8573 - val_loss: 3.0812 - val_acc: 0.5079\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8182 - acc: 0.8589 - val_loss: 2.2519 - val_acc: 0.5433\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8271 - acc: 0.8573 - val_loss: 2.1663 - val_acc: 0.5359\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8270 - acc: 0.8548 - val_loss: 3.7846 - val_acc: 0.3980\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8204 - acc: 0.8599 - val_loss: 2.5076 - val_acc: 0.4838\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8344 - acc: 0.8576 - val_loss: 2.6687 - val_acc: 0.5079\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8227 - acc: 0.8607 - val_loss: 2.4066 - val_acc: 0.4988\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8132 - acc: 0.8632 - val_loss: 2.2666 - val_acc: 0.5197\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8194 - acc: 0.8576 - val_loss: 2.3278 - val_acc: 0.4965\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8132 - acc: 0.8641 - val_loss: 1.9201 - val_acc: 0.5599\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8237 - acc: 0.8542 - val_loss: 3.7978 - val_acc: 0.3680\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8135 - acc: 0.8636 - val_loss: 2.4021 - val_acc: 0.5185\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8228 - acc: 0.8565 - val_loss: 3.5076 - val_acc: 0.4409\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8205 - acc: 0.8623 - val_loss: 1.9323 - val_acc: 0.5626\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8098 - acc: 0.8652 - val_loss: 3.6016 - val_acc: 0.4243\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8146 - acc: 0.8625 - val_loss: 2.5372 - val_acc: 0.4913\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8095 - acc: 0.8667 - val_loss: 2.6716 - val_acc: 0.5079\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8222 - acc: 0.8597 - val_loss: 2.3844 - val_acc: 0.5091\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8153 - acc: 0.8619 - val_loss: 2.0578 - val_acc: 0.5532\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8013 - acc: 0.8682 - val_loss: 2.5099 - val_acc: 0.5201\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8124 - acc: 0.8619 - val_loss: 2.4716 - val_acc: 0.5276\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8060 - acc: 0.8669 - val_loss: 2.3301 - val_acc: 0.5366\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8150 - acc: 0.8617 - val_loss: 3.1101 - val_acc: 0.4433\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8086 - acc: 0.8630 - val_loss: 2.3608 - val_acc: 0.5110\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.8061 - acc: 0.8676 - val_loss: 2.1323 - val_acc: 0.5481\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8147 - acc: 0.8629 - val_loss: 3.1443 - val_acc: 0.4472\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8006 - acc: 0.8718 - val_loss: 2.2420 - val_acc: 0.5536\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8112 - acc: 0.8646 - val_loss: 1.9286 - val_acc: 0.5623\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 5s 455us/step - loss: 0.8064 - acc: 0.8682 - val_loss: 2.1142 - val_acc: 0.5705\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 0.8080 - acc: 0.8655 - val_loss: 2.1228 - val_acc: 0.5422\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7934 - acc: 0.8718 - val_loss: 3.3939 - val_acc: 0.4547\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7973 - acc: 0.8737 - val_loss: 2.2557 - val_acc: 0.5595\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8027 - acc: 0.8651 - val_loss: 3.7208 - val_acc: 0.3743\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8056 - acc: 0.8652 - val_loss: 2.6166 - val_acc: 0.5051\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8037 - acc: 0.8641 - val_loss: 3.5361 - val_acc: 0.4397\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8044 - acc: 0.8697 - val_loss: 1.9251 - val_acc: 0.5784\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7938 - acc: 0.8699 - val_loss: 2.0061 - val_acc: 0.5796\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8112 - acc: 0.8668 - val_loss: 3.3804 - val_acc: 0.4681\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8028 - acc: 0.8705 - val_loss: 3.1650 - val_acc: 0.5110\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8145 - acc: 0.8638 - val_loss: 1.9998 - val_acc: 0.5615\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7977 - acc: 0.8711 - val_loss: 2.4518 - val_acc: 0.4921\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7994 - acc: 0.8684 - val_loss: 2.0580 - val_acc: 0.5382\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8004 - acc: 0.8690 - val_loss: 2.0770 - val_acc: 0.5351\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7947 - acc: 0.8725 - val_loss: 2.8074 - val_acc: 0.4752\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7878 - acc: 0.8738 - val_loss: 2.6357 - val_acc: 0.5252\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7765 - acc: 0.8757 - val_loss: 3.7261 - val_acc: 0.4563\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8020 - acc: 0.8666 - val_loss: 2.1928 - val_acc: 0.5110\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7882 - acc: 0.8729 - val_loss: 3.2141 - val_acc: 0.4622\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7904 - acc: 0.8734 - val_loss: 2.1690 - val_acc: 0.5599\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7875 - acc: 0.8700 - val_loss: 3.6712 - val_acc: 0.4082\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7945 - acc: 0.8735 - val_loss: 1.9073 - val_acc: 0.5760\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7913 - acc: 0.8751 - val_loss: 2.7644 - val_acc: 0.4807\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7892 - acc: 0.8760 - val_loss: 2.0805 - val_acc: 0.5575\n",
            "Epoch 127/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7836 - acc: 0.8767 - val_loss: 2.1168 - val_acc: 0.5697\n",
            "Epoch 128/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7842 - acc: 0.8762 - val_loss: 2.6604 - val_acc: 0.4708\n",
            "Epoch 129/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7967 - acc: 0.8700 - val_loss: 2.3962 - val_acc: 0.5343\n",
            "Epoch 130/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7957 - acc: 0.8744 - val_loss: 2.3239 - val_acc: 0.5158\n",
            "Epoch 131/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7886 - acc: 0.8781 - val_loss: 2.3618 - val_acc: 0.5099\n",
            "Epoch 132/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7844 - acc: 0.8740 - val_loss: 2.8866 - val_acc: 0.5079\n",
            "Epoch 133/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7852 - acc: 0.8749 - val_loss: 3.2958 - val_acc: 0.4567\n",
            "Epoch 134/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7990 - acc: 0.8685 - val_loss: 2.7598 - val_acc: 0.4937\n",
            "Epoch 135/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.7951 - acc: 0.8741 - val_loss: 2.9832 - val_acc: 0.4760\n",
            "Epoch 136/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7910 - acc: 0.8758 - val_loss: 2.2439 - val_acc: 0.5390\n",
            "Epoch 137/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7769 - acc: 0.8776 - val_loss: 2.2295 - val_acc: 0.5327\n",
            "Epoch 138/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7821 - acc: 0.8753 - val_loss: 2.9279 - val_acc: 0.5067\n",
            "Epoch 139/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7785 - acc: 0.8767 - val_loss: 2.1046 - val_acc: 0.5583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|         | 4/256 [38:32<39:40:31, 566.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 6s 543us/step - loss: 6.0411 - acc: 0.2968 - val_loss: 5.1673 - val_acc: 0.3018\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 4.3859 - acc: 0.3810 - val_loss: 3.8615 - val_acc: 0.3416\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 3.2191 - acc: 0.4673 - val_loss: 2.9093 - val_acc: 0.4271\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 2.4870 - acc: 0.5150 - val_loss: 2.3051 - val_acc: 0.5035\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 2.0153 - acc: 0.5557 - val_loss: 1.8882 - val_acc: 0.5378\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 1.6961 - acc: 0.5967 - val_loss: 1.8340 - val_acc: 0.4941\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 1.5010 - acc: 0.6175 - val_loss: 1.6802 - val_acc: 0.5162\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 1.3851 - acc: 0.6349 - val_loss: 1.5076 - val_acc: 0.5768\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 1.3031 - acc: 0.6418 - val_loss: 1.5202 - val_acc: 0.5595\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 1.2302 - acc: 0.6597 - val_loss: 1.5372 - val_acc: 0.5429\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 1.1932 - acc: 0.6702 - val_loss: 1.4234 - val_acc: 0.5867\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 1.1535 - acc: 0.6911 - val_loss: 1.5637 - val_acc: 0.5591\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 1.1552 - acc: 0.6865 - val_loss: 1.6840 - val_acc: 0.5370\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 1.1129 - acc: 0.7012 - val_loss: 1.5654 - val_acc: 0.5457\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 1.0880 - acc: 0.7145 - val_loss: 1.8418 - val_acc: 0.5114\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 1.0734 - acc: 0.7223 - val_loss: 1.7597 - val_acc: 0.5327\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 1.0673 - acc: 0.7289 - val_loss: 1.5333 - val_acc: 0.5812\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 1.0702 - acc: 0.7344 - val_loss: 1.5105 - val_acc: 0.5784\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 1.0110 - acc: 0.7561 - val_loss: 1.5853 - val_acc: 0.5796\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 1.0180 - acc: 0.7557 - val_loss: 1.6866 - val_acc: 0.5595\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 1.0218 - acc: 0.7548 - val_loss: 1.8432 - val_acc: 0.5327\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 1.0028 - acc: 0.7686 - val_loss: 1.6647 - val_acc: 0.5607\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 1.0019 - acc: 0.7721 - val_loss: 1.6630 - val_acc: 0.5556\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.9605 - acc: 0.7945 - val_loss: 1.6889 - val_acc: 0.5619\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.9456 - acc: 0.7953 - val_loss: 1.7605 - val_acc: 0.5721\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 0.9551 - acc: 0.8019 - val_loss: 1.6712 - val_acc: 0.5784\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.9388 - acc: 0.8016 - val_loss: 2.1000 - val_acc: 0.5138\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.9398 - acc: 0.8099 - val_loss: 1.7433 - val_acc: 0.5792\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.9286 - acc: 0.8135 - val_loss: 2.0187 - val_acc: 0.5355\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.9261 - acc: 0.8135 - val_loss: 2.0092 - val_acc: 0.5390\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.9226 - acc: 0.8183 - val_loss: 1.6936 - val_acc: 0.6001\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 5s 467us/step - loss: 0.9090 - acc: 0.8251 - val_loss: 2.0529 - val_acc: 0.5465\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 0.9009 - acc: 0.8272 - val_loss: 2.1036 - val_acc: 0.5359\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.9157 - acc: 0.8219 - val_loss: 1.6379 - val_acc: 0.6060\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.8852 - acc: 0.8371 - val_loss: 1.7903 - val_acc: 0.5611\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.8838 - acc: 0.8399 - val_loss: 1.9184 - val_acc: 0.5729\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8881 - acc: 0.8404 - val_loss: 1.7564 - val_acc: 0.6040\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.8876 - acc: 0.8376 - val_loss: 1.8554 - val_acc: 0.5977\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 0.8811 - acc: 0.8388 - val_loss: 1.7097 - val_acc: 0.5961\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8254 - acc: 0.8639 - val_loss: 1.8726 - val_acc: 0.5741\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8234 - acc: 0.8621 - val_loss: 2.0908 - val_acc: 0.5323\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8921 - acc: 0.8348 - val_loss: 2.2800 - val_acc: 0.5276\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.8540 - acc: 0.8570 - val_loss: 2.0421 - val_acc: 0.5473\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8396 - acc: 0.8603 - val_loss: 1.9258 - val_acc: 0.5697\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.8657 - acc: 0.8493 - val_loss: 1.9139 - val_acc: 0.5686\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 5s 457us/step - loss: 0.8466 - acc: 0.8571 - val_loss: 2.0998 - val_acc: 0.5158\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.8228 - acc: 0.8618 - val_loss: 2.0551 - val_acc: 0.5784\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 0.8496 - acc: 0.8541 - val_loss: 2.0995 - val_acc: 0.5626\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8444 - acc: 0.8602 - val_loss: 2.0709 - val_acc: 0.5276\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8406 - acc: 0.8598 - val_loss: 2.1129 - val_acc: 0.5626\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8281 - acc: 0.8651 - val_loss: 1.9478 - val_acc: 0.5654\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.8210 - acc: 0.8664 - val_loss: 2.2245 - val_acc: 0.5142\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.8422 - acc: 0.8636 - val_loss: 2.1155 - val_acc: 0.5583\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8331 - acc: 0.8615 - val_loss: 1.8937 - val_acc: 0.5776\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.8360 - acc: 0.8594 - val_loss: 2.1299 - val_acc: 0.5626\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.8275 - acc: 0.8646 - val_loss: 2.0154 - val_acc: 0.5658\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8390 - acc: 0.8638 - val_loss: 1.9695 - val_acc: 0.5626\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8414 - acc: 0.8627 - val_loss: 2.0350 - val_acc: 0.5674\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8222 - acc: 0.8679 - val_loss: 1.8303 - val_acc: 0.5906\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7878 - acc: 0.8818 - val_loss: 1.9472 - val_acc: 0.5701\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7770 - acc: 0.8813 - val_loss: 2.2893 - val_acc: 0.5292\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.8503 - acc: 0.8577 - val_loss: 2.0280 - val_acc: 0.5871\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8382 - acc: 0.8654 - val_loss: 2.1663 - val_acc: 0.5276\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 5s 467us/step - loss: 0.8168 - acc: 0.8726 - val_loss: 1.9180 - val_acc: 0.5898\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 5s 464us/step - loss: 0.8007 - acc: 0.8785 - val_loss: 2.0900 - val_acc: 0.5575\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7958 - acc: 0.8783 - val_loss: 2.3346 - val_acc: 0.5008\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8266 - acc: 0.8691 - val_loss: 2.0287 - val_acc: 0.5496\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.8515 - acc: 0.8616 - val_loss: 2.1547 - val_acc: 0.5524\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7900 - acc: 0.8848 - val_loss: 2.1488 - val_acc: 0.5595\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7650 - acc: 0.8892 - val_loss: 1.9588 - val_acc: 0.5764\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7727 - acc: 0.8830 - val_loss: 1.9026 - val_acc: 0.6060\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 0.7558 - acc: 0.8933 - val_loss: 2.1895 - val_acc: 0.5516\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8155 - acc: 0.8681 - val_loss: 2.6452 - val_acc: 0.5028\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8072 - acc: 0.8769 - val_loss: 2.1863 - val_acc: 0.5583\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7736 - acc: 0.8890 - val_loss: 2.2277 - val_acc: 0.5642\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7663 - acc: 0.8921 - val_loss: 2.0488 - val_acc: 0.5678\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 0.7839 - acc: 0.8806 - val_loss: 2.4928 - val_acc: 0.5221\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 5s 455us/step - loss: 0.8063 - acc: 0.8834 - val_loss: 2.3261 - val_acc: 0.5355\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 5s 455us/step - loss: 0.7987 - acc: 0.8852 - val_loss: 2.1685 - val_acc: 0.5634\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7687 - acc: 0.8921 - val_loss: 2.3002 - val_acc: 0.5477\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.7574 - acc: 0.8948 - val_loss: 2.0573 - val_acc: 0.5800\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7754 - acc: 0.8852 - val_loss: 2.3678 - val_acc: 0.5508\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 5s 456us/step - loss: 0.8338 - acc: 0.8695 - val_loss: 2.0461 - val_acc: 0.5792\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.8161 - acc: 0.8749 - val_loss: 2.2252 - val_acc: 0.5429\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.8092 - acc: 0.8789 - val_loss: 2.7121 - val_acc: 0.4968\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7785 - acc: 0.8879 - val_loss: 2.1360 - val_acc: 0.5607\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7699 - acc: 0.8902 - val_loss: 2.2511 - val_acc: 0.5339\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.8135 - acc: 0.8770 - val_loss: 2.3493 - val_acc: 0.5240\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7987 - acc: 0.8824 - val_loss: 2.4485 - val_acc: 0.5264\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7777 - acc: 0.8901 - val_loss: 2.1004 - val_acc: 0.5575\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7328 - acc: 0.9004 - val_loss: 2.1937 - val_acc: 0.5477\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.7445 - acc: 0.8947 - val_loss: 2.0701 - val_acc: 0.5575\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 5s 459us/step - loss: 0.7820 - acc: 0.8822 - val_loss: 2.2593 - val_acc: 0.5477\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7859 - acc: 0.8819 - val_loss: 2.0484 - val_acc: 0.5591\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.8187 - acc: 0.8721 - val_loss: 1.9846 - val_acc: 0.5662\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7717 - acc: 0.8899 - val_loss: 2.1109 - val_acc: 0.5705\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8058 - acc: 0.8780 - val_loss: 2.1385 - val_acc: 0.5827\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 5s 459us/step - loss: 0.7759 - acc: 0.8882 - val_loss: 2.0989 - val_acc: 0.5615\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 5s 460us/step - loss: 0.7812 - acc: 0.8858 - val_loss: 2.0116 - val_acc: 0.5839\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7576 - acc: 0.8940 - val_loss: 1.9760 - val_acc: 0.5587\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.7751 - acc: 0.8875 - val_loss: 2.1484 - val_acc: 0.5583\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7924 - acc: 0.8816 - val_loss: 2.1024 - val_acc: 0.5532\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7847 - acc: 0.8839 - val_loss: 1.9103 - val_acc: 0.6001\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7605 - acc: 0.8930 - val_loss: 2.0432 - val_acc: 0.5733\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7744 - acc: 0.8890 - val_loss: 2.0899 - val_acc: 0.5599\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7427 - acc: 0.8994 - val_loss: 2.0940 - val_acc: 0.5477\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7585 - acc: 0.8890 - val_loss: 1.9815 - val_acc: 0.5839\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7773 - acc: 0.8887 - val_loss: 2.2677 - val_acc: 0.5496\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7675 - acc: 0.8919 - val_loss: 2.0089 - val_acc: 0.5764\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7695 - acc: 0.8866 - val_loss: 2.0678 - val_acc: 0.5733\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.8123 - acc: 0.8732 - val_loss: 2.2258 - val_acc: 0.5402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|         | 5/256 [46:57<38:13:06, 548.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 510us/step - loss: 5.8095 - acc: 0.2845 - val_loss: 4.9434 - val_acc: 0.2522\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 3.4379 - acc: 0.3280 - val_loss: 2.5170 - val_acc: 0.3763\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 2.4200 - acc: 0.3956 - val_loss: 2.1899 - val_acc: 0.3574\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 1.9746 - acc: 0.4464 - val_loss: 1.9052 - val_acc: 0.3987\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.6944 - acc: 0.4919 - val_loss: 1.6967 - val_acc: 0.4377\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 1.5426 - acc: 0.5268 - val_loss: 1.6171 - val_acc: 0.4823\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 1.4388 - acc: 0.5500 - val_loss: 1.4141 - val_acc: 0.5382\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.3496 - acc: 0.5816 - val_loss: 1.4683 - val_acc: 0.5181\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 1.2822 - acc: 0.6035 - val_loss: 1.5071 - val_acc: 0.5496\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.2263 - acc: 0.6177 - val_loss: 1.6299 - val_acc: 0.5154\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.1914 - acc: 0.6265 - val_loss: 1.5216 - val_acc: 0.5331\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.1595 - acc: 0.6433 - val_loss: 1.5171 - val_acc: 0.5516\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 1.1229 - acc: 0.6571 - val_loss: 1.4428 - val_acc: 0.5701\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 1.0995 - acc: 0.6735 - val_loss: 1.4265 - val_acc: 0.5820\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.0780 - acc: 0.6882 - val_loss: 1.3657 - val_acc: 0.5863\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 1.0555 - acc: 0.6976 - val_loss: 1.6748 - val_acc: 0.5071\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 1.0411 - acc: 0.7069 - val_loss: 1.6547 - val_acc: 0.5323\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 1.0343 - acc: 0.7121 - val_loss: 1.4625 - val_acc: 0.5954\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 1.0185 - acc: 0.7221 - val_loss: 1.5751 - val_acc: 0.5607\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.9958 - acc: 0.7361 - val_loss: 1.6405 - val_acc: 0.5323\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.9871 - acc: 0.7459 - val_loss: 1.9104 - val_acc: 0.5095\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.9796 - acc: 0.7468 - val_loss: 1.6808 - val_acc: 0.5544\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.9705 - acc: 0.7515 - val_loss: 1.4797 - val_acc: 0.5894\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.9632 - acc: 0.7564 - val_loss: 1.5679 - val_acc: 0.5603\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.9424 - acc: 0.7712 - val_loss: 1.6316 - val_acc: 0.5737\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.9413 - acc: 0.7715 - val_loss: 1.6561 - val_acc: 0.5690\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.9381 - acc: 0.7763 - val_loss: 1.6004 - val_acc: 0.5788\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.9309 - acc: 0.7819 - val_loss: 1.6133 - val_acc: 0.5847\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.9303 - acc: 0.7839 - val_loss: 1.7458 - val_acc: 0.5544\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.9095 - acc: 0.7927 - val_loss: 1.7051 - val_acc: 0.5571\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.9152 - acc: 0.7946 - val_loss: 1.6597 - val_acc: 0.5638\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.9105 - acc: 0.7969 - val_loss: 1.7587 - val_acc: 0.5611\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8881 - acc: 0.8062 - val_loss: 1.7590 - val_acc: 0.5646\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8931 - acc: 0.8119 - val_loss: 1.7280 - val_acc: 0.5493\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8895 - acc: 0.8043 - val_loss: 1.8496 - val_acc: 0.5524\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8778 - acc: 0.8136 - val_loss: 1.7495 - val_acc: 0.5607\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8733 - acc: 0.8169 - val_loss: 1.8574 - val_acc: 0.5469\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8825 - acc: 0.8182 - val_loss: 1.7020 - val_acc: 0.5686\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8820 - acc: 0.8113 - val_loss: 1.6084 - val_acc: 0.6009\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8816 - acc: 0.8197 - val_loss: 1.7934 - val_acc: 0.5595\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8731 - acc: 0.8205 - val_loss: 1.6932 - val_acc: 0.5705\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8712 - acc: 0.8227 - val_loss: 2.0808 - val_acc: 0.5311\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8618 - acc: 0.8293 - val_loss: 1.8568 - val_acc: 0.5717\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8746 - acc: 0.8183 - val_loss: 1.7677 - val_acc: 0.5690\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8775 - acc: 0.8238 - val_loss: 2.2419 - val_acc: 0.5079\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8567 - acc: 0.8320 - val_loss: 1.6814 - val_acc: 0.5800\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8494 - acc: 0.8377 - val_loss: 1.9685 - val_acc: 0.5516\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8524 - acc: 0.8350 - val_loss: 2.0308 - val_acc: 0.5378\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8556 - acc: 0.8350 - val_loss: 1.8523 - val_acc: 0.5516\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8434 - acc: 0.8407 - val_loss: 2.0360 - val_acc: 0.5508\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8527 - acc: 0.8392 - val_loss: 1.7773 - val_acc: 0.5733\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8412 - acc: 0.8417 - val_loss: 1.8074 - val_acc: 0.5875\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8380 - acc: 0.8399 - val_loss: 1.8841 - val_acc: 0.5426\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8435 - acc: 0.8373 - val_loss: 2.1239 - val_acc: 0.5374\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8367 - acc: 0.8437 - val_loss: 2.2376 - val_acc: 0.5327\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8354 - acc: 0.8420 - val_loss: 1.8444 - val_acc: 0.5772\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8330 - acc: 0.8438 - val_loss: 2.0172 - val_acc: 0.5481\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8354 - acc: 0.8458 - val_loss: 2.0365 - val_acc: 0.5236\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8486 - acc: 0.8445 - val_loss: 1.8026 - val_acc: 0.5654\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8394 - acc: 0.8455 - val_loss: 1.8210 - val_acc: 0.5634\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8269 - acc: 0.8505 - val_loss: 1.9271 - val_acc: 0.5701\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8251 - acc: 0.8492 - val_loss: 1.8031 - val_acc: 0.5839\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8351 - acc: 0.8454 - val_loss: 1.7427 - val_acc: 0.6064\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8281 - acc: 0.8474 - val_loss: 1.8243 - val_acc: 0.5816\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8323 - acc: 0.8488 - val_loss: 1.7565 - val_acc: 0.5741\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8278 - acc: 0.8501 - val_loss: 1.8522 - val_acc: 0.5678\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8226 - acc: 0.8539 - val_loss: 1.7611 - val_acc: 0.5835\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8320 - acc: 0.8483 - val_loss: 1.8458 - val_acc: 0.5693\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8174 - acc: 0.8566 - val_loss: 1.8636 - val_acc: 0.5693\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8175 - acc: 0.8558 - val_loss: 2.3679 - val_acc: 0.5280\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8169 - acc: 0.8548 - val_loss: 1.8255 - val_acc: 0.5831\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8195 - acc: 0.8552 - val_loss: 1.8125 - val_acc: 0.5839\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8188 - acc: 0.8538 - val_loss: 2.3183 - val_acc: 0.5303\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8200 - acc: 0.8540 - val_loss: 2.5221 - val_acc: 0.4972\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7938 - acc: 0.8643 - val_loss: 1.8439 - val_acc: 0.5879\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8103 - acc: 0.8571 - val_loss: 1.8476 - val_acc: 0.5835\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.8078 - acc: 0.8570 - val_loss: 1.8263 - val_acc: 0.5855\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8111 - acc: 0.8614 - val_loss: 2.0460 - val_acc: 0.5820\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8140 - acc: 0.8618 - val_loss: 2.5677 - val_acc: 0.5067\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8077 - acc: 0.8632 - val_loss: 2.7916 - val_acc: 0.4590\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8135 - acc: 0.8612 - val_loss: 2.1988 - val_acc: 0.5284\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7993 - acc: 0.8682 - val_loss: 1.8366 - val_acc: 0.5961\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7995 - acc: 0.8665 - val_loss: 2.0530 - val_acc: 0.5835\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.8082 - acc: 0.8634 - val_loss: 2.2821 - val_acc: 0.5173\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.8075 - acc: 0.8637 - val_loss: 1.9254 - val_acc: 0.5851\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7939 - acc: 0.8680 - val_loss: 1.8403 - val_acc: 0.5721\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8094 - acc: 0.8589 - val_loss: 2.5115 - val_acc: 0.4949\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7993 - acc: 0.8681 - val_loss: 2.2055 - val_acc: 0.5327\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7840 - acc: 0.8758 - val_loss: 2.0259 - val_acc: 0.5697\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.8100 - acc: 0.8625 - val_loss: 1.8718 - val_acc: 0.5753\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7986 - acc: 0.8654 - val_loss: 3.0300 - val_acc: 0.4519\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8022 - acc: 0.8611 - val_loss: 1.8287 - val_acc: 0.5934\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8036 - acc: 0.8656 - val_loss: 1.8686 - val_acc: 0.5678\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8047 - acc: 0.8644 - val_loss: 2.5951 - val_acc: 0.5067\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.7881 - acc: 0.8723 - val_loss: 2.6829 - val_acc: 0.5087\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8017 - acc: 0.8602 - val_loss: 2.9158 - val_acc: 0.5079\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7965 - acc: 0.8691 - val_loss: 1.9402 - val_acc: 0.5650\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7899 - acc: 0.8707 - val_loss: 2.0509 - val_acc: 0.5567\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7912 - acc: 0.8675 - val_loss: 2.0167 - val_acc: 0.5693\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7894 - acc: 0.8727 - val_loss: 1.9267 - val_acc: 0.5587\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7932 - acc: 0.8679 - val_loss: 2.0625 - val_acc: 0.5646\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7928 - acc: 0.8691 - val_loss: 2.5036 - val_acc: 0.4925\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7891 - acc: 0.8685 - val_loss: 2.0799 - val_acc: 0.5571\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8077 - acc: 0.8626 - val_loss: 2.4100 - val_acc: 0.5063\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7911 - acc: 0.8712 - val_loss: 2.4534 - val_acc: 0.5165\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7909 - acc: 0.8678 - val_loss: 1.8353 - val_acc: 0.5816\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7805 - acc: 0.8736 - val_loss: 2.5132 - val_acc: 0.5272\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7864 - acc: 0.8688 - val_loss: 1.7978 - val_acc: 0.5914\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7906 - acc: 0.8717 - val_loss: 2.2363 - val_acc: 0.5520\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7920 - acc: 0.8702 - val_loss: 2.4113 - val_acc: 0.5095\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7896 - acc: 0.8717 - val_loss: 2.0086 - val_acc: 0.5690\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7870 - acc: 0.8717 - val_loss: 2.1089 - val_acc: 0.5654\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7846 - acc: 0.8728 - val_loss: 2.3184 - val_acc: 0.5296\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7883 - acc: 0.8691 - val_loss: 1.9915 - val_acc: 0.5843\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7727 - acc: 0.8802 - val_loss: 2.2580 - val_acc: 0.5445\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7861 - acc: 0.8710 - val_loss: 2.3511 - val_acc: 0.5461\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7868 - acc: 0.8720 - val_loss: 1.8594 - val_acc: 0.5898\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7802 - acc: 0.8759 - val_loss: 1.9987 - val_acc: 0.5863\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7954 - acc: 0.8686 - val_loss: 2.0313 - val_acc: 0.5615\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7825 - acc: 0.8715 - val_loss: 1.8238 - val_acc: 0.5965\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7733 - acc: 0.8790 - val_loss: 1.9977 - val_acc: 0.5764\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7936 - acc: 0.8724 - val_loss: 2.0324 - val_acc: 0.5820\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7861 - acc: 0.8720 - val_loss: 1.9036 - val_acc: 0.5820\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7729 - acc: 0.8797 - val_loss: 1.9545 - val_acc: 0.5733\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7778 - acc: 0.8777 - val_loss: 2.1918 - val_acc: 0.5575\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7852 - acc: 0.8732 - val_loss: 2.3482 - val_acc: 0.5418\n",
            "Epoch 127/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7676 - acc: 0.8782 - val_loss: 2.4102 - val_acc: 0.5579\n",
            "Epoch 128/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7768 - acc: 0.8758 - val_loss: 3.6241 - val_acc: 0.4338\n",
            "Epoch 129/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7692 - acc: 0.8789 - val_loss: 2.6635 - val_acc: 0.5067\n",
            "Epoch 130/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7723 - acc: 0.8808 - val_loss: 2.1407 - val_acc: 0.5575\n",
            "Epoch 131/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7685 - acc: 0.8773 - val_loss: 2.0080 - val_acc: 0.5835\n",
            "Epoch 132/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7766 - acc: 0.8802 - val_loss: 2.4926 - val_acc: 0.5335\n",
            "Epoch 133/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7671 - acc: 0.8786 - val_loss: 2.0549 - val_acc: 0.5693\n",
            "Epoch 134/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7590 - acc: 0.8842 - val_loss: 2.3817 - val_acc: 0.5248\n",
            "Epoch 135/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7819 - acc: 0.8718 - val_loss: 1.8520 - val_acc: 0.5930\n",
            "Epoch 136/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7791 - acc: 0.8760 - val_loss: 2.0157 - val_acc: 0.5760\n",
            "Epoch 137/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7764 - acc: 0.8740 - val_loss: 2.3372 - val_acc: 0.5559\n",
            "Epoch 138/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7714 - acc: 0.8800 - val_loss: 2.4037 - val_acc: 0.5528\n",
            "Epoch 139/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7756 - acc: 0.8760 - val_loss: 2.0846 - val_acc: 0.5575\n",
            "Epoch 140/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7643 - acc: 0.8843 - val_loss: 2.5073 - val_acc: 0.5311\n",
            "Epoch 141/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7685 - acc: 0.8793 - val_loss: 1.9481 - val_acc: 0.5887\n",
            "Epoch 142/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7829 - acc: 0.8754 - val_loss: 1.9637 - val_acc: 0.5634\n",
            "Epoch 143/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7693 - acc: 0.8828 - val_loss: 1.9826 - val_acc: 0.5729\n",
            "Epoch 144/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7814 - acc: 0.8736 - val_loss: 2.5724 - val_acc: 0.5177\n",
            "Epoch 145/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7799 - acc: 0.8752 - val_loss: 2.0683 - val_acc: 0.5749\n",
            "Epoch 146/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7715 - acc: 0.8772 - val_loss: 2.0440 - val_acc: 0.5697\n",
            "Epoch 147/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7670 - acc: 0.8802 - val_loss: 1.8980 - val_acc: 0.5906\n",
            "Epoch 148/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7625 - acc: 0.8813 - val_loss: 2.3145 - val_acc: 0.5445\n",
            "Epoch 149/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7677 - acc: 0.8808 - val_loss: 2.2355 - val_acc: 0.5504\n",
            "Epoch 150/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7637 - acc: 0.8799 - val_loss: 3.1656 - val_acc: 0.4744\n",
            "Epoch 151/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7706 - acc: 0.8761 - val_loss: 2.1002 - val_acc: 0.5772\n",
            "Epoch 152/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7732 - acc: 0.8781 - val_loss: 2.0616 - val_acc: 0.5670\n",
            "Epoch 153/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7675 - acc: 0.8794 - val_loss: 2.0320 - val_acc: 0.5607\n",
            "Epoch 154/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7657 - acc: 0.8851 - val_loss: 1.9213 - val_acc: 0.5760\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|         | 6/256 [58:22<40:55:54, 589.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 6s 550us/step - loss: 6.0553 - acc: 0.2908 - val_loss: 5.3783 - val_acc: 0.2876\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 4.4344 - acc: 0.3623 - val_loss: 3.7377 - val_acc: 0.3771\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 3.2641 - acc: 0.4560 - val_loss: 2.9958 - val_acc: 0.3806\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 2.4793 - acc: 0.5233 - val_loss: 2.4057 - val_acc: 0.4247\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 1.9971 - acc: 0.5669 - val_loss: 1.9380 - val_acc: 0.5126\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 1.6951 - acc: 0.5932 - val_loss: 1.8737 - val_acc: 0.5035\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 1.5233 - acc: 0.6094 - val_loss: 1.6966 - val_acc: 0.5299\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 1.3827 - acc: 0.6325 - val_loss: 1.7182 - val_acc: 0.5063\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 1.3020 - acc: 0.6474 - val_loss: 1.5942 - val_acc: 0.5426\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 1.2472 - acc: 0.6601 - val_loss: 1.6981 - val_acc: 0.5359\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 1.2074 - acc: 0.6681 - val_loss: 1.6818 - val_acc: 0.5335\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 1.1797 - acc: 0.6763 - val_loss: 1.5634 - val_acc: 0.5327\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 1.1393 - acc: 0.6964 - val_loss: 1.4716 - val_acc: 0.5847\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 1.1240 - acc: 0.6996 - val_loss: 1.8553 - val_acc: 0.4992\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 1.1048 - acc: 0.7067 - val_loss: 1.5990 - val_acc: 0.5626\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 1.0774 - acc: 0.7193 - val_loss: 1.5109 - val_acc: 0.5650\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 1.0394 - acc: 0.7403 - val_loss: 1.7376 - val_acc: 0.5225\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 1.0608 - acc: 0.7340 - val_loss: 1.5024 - val_acc: 0.5910\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 1.0412 - acc: 0.7471 - val_loss: 2.0773 - val_acc: 0.5264\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 1.0373 - acc: 0.7460 - val_loss: 1.7229 - val_acc: 0.5520\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 1.0041 - acc: 0.7665 - val_loss: 1.6244 - val_acc: 0.5804\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 0.9812 - acc: 0.7771 - val_loss: 1.7105 - val_acc: 0.5626\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.9901 - acc: 0.7758 - val_loss: 1.8210 - val_acc: 0.5327\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.9684 - acc: 0.7830 - val_loss: 1.6260 - val_acc: 0.5934\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.9694 - acc: 0.7870 - val_loss: 1.6614 - val_acc: 0.5914\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.9620 - acc: 0.7902 - val_loss: 1.6791 - val_acc: 0.5745\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.9503 - acc: 0.7993 - val_loss: 1.6243 - val_acc: 0.6040\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.9242 - acc: 0.8104 - val_loss: 1.7850 - val_acc: 0.5863\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.9452 - acc: 0.8004 - val_loss: 1.9833 - val_acc: 0.5319\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.9379 - acc: 0.8104 - val_loss: 1.7329 - val_acc: 0.5717\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.9296 - acc: 0.8197 - val_loss: 1.9359 - val_acc: 0.5508\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8982 - acc: 0.8317 - val_loss: 1.7074 - val_acc: 0.5737\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8958 - acc: 0.8306 - val_loss: 1.7388 - val_acc: 0.5883\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8921 - acc: 0.8307 - val_loss: 1.6929 - val_acc: 0.5922\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.8860 - acc: 0.8329 - val_loss: 1.7710 - val_acc: 0.5894\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.8852 - acc: 0.8422 - val_loss: 1.7366 - val_acc: 0.6005\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8860 - acc: 0.8399 - val_loss: 2.4004 - val_acc: 0.5067\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8690 - acc: 0.8470 - val_loss: 2.0235 - val_acc: 0.5579\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.8566 - acc: 0.8473 - val_loss: 1.8825 - val_acc: 0.5776\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 0.8562 - acc: 0.8516 - val_loss: 2.1725 - val_acc: 0.5327\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8672 - acc: 0.8467 - val_loss: 1.7865 - val_acc: 0.5922\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8972 - acc: 0.8381 - val_loss: 1.9643 - val_acc: 0.5567\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.8795 - acc: 0.8469 - val_loss: 2.4187 - val_acc: 0.5236\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8566 - acc: 0.8571 - val_loss: 1.8839 - val_acc: 0.5946\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8358 - acc: 0.8666 - val_loss: 1.7924 - val_acc: 0.5981\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8352 - acc: 0.8633 - val_loss: 2.1107 - val_acc: 0.5662\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8288 - acc: 0.8646 - val_loss: 1.8737 - val_acc: 0.5831\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8340 - acc: 0.8637 - val_loss: 2.1151 - val_acc: 0.5453\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8669 - acc: 0.8565 - val_loss: 1.8629 - val_acc: 0.5863\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8462 - acc: 0.8617 - val_loss: 2.1043 - val_acc: 0.5406\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.9049 - acc: 0.8415 - val_loss: 2.2546 - val_acc: 0.5165\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8396 - acc: 0.8654 - val_loss: 1.8688 - val_acc: 0.5733\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8294 - acc: 0.8688 - val_loss: 2.2487 - val_acc: 0.5292\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8193 - acc: 0.8717 - val_loss: 2.2636 - val_acc: 0.5232\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8249 - acc: 0.8686 - val_loss: 2.2921 - val_acc: 0.5169\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8451 - acc: 0.8614 - val_loss: 1.9161 - val_acc: 0.5587\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8032 - acc: 0.8816 - val_loss: 2.3153 - val_acc: 0.5292\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8230 - acc: 0.8685 - val_loss: 1.8834 - val_acc: 0.5879\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7883 - acc: 0.8893 - val_loss: 2.3965 - val_acc: 0.5323\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8268 - acc: 0.8715 - val_loss: 2.0977 - val_acc: 0.5556\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8191 - acc: 0.8741 - val_loss: 2.0825 - val_acc: 0.5686\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8554 - acc: 0.8581 - val_loss: 1.9256 - val_acc: 0.5863\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8434 - acc: 0.8703 - val_loss: 1.8393 - val_acc: 0.6020\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8003 - acc: 0.8799 - val_loss: 2.3408 - val_acc: 0.5114\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7785 - acc: 0.8882 - val_loss: 2.1015 - val_acc: 0.5595\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7773 - acc: 0.8892 - val_loss: 2.1828 - val_acc: 0.5552\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7994 - acc: 0.8791 - val_loss: 1.9353 - val_acc: 0.5906\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8050 - acc: 0.8749 - val_loss: 1.7714 - val_acc: 0.6005\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7970 - acc: 0.8841 - val_loss: 2.6515 - val_acc: 0.5453\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8246 - acc: 0.8732 - val_loss: 2.5482 - val_acc: 0.4980\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8425 - acc: 0.8699 - val_loss: 2.0377 - val_acc: 0.5796\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8110 - acc: 0.8799 - val_loss: 1.9758 - val_acc: 0.5823\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8077 - acc: 0.8818 - val_loss: 2.1514 - val_acc: 0.5626\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8058 - acc: 0.8822 - val_loss: 1.9426 - val_acc: 0.5642\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8183 - acc: 0.8756 - val_loss: 2.3274 - val_acc: 0.5299\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7817 - acc: 0.8913 - val_loss: 2.0002 - val_acc: 0.5583\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.7659 - acc: 0.8947 - val_loss: 2.5042 - val_acc: 0.5280\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8038 - acc: 0.8816 - val_loss: 2.0011 - val_acc: 0.5847\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7849 - acc: 0.8883 - val_loss: 2.0107 - val_acc: 0.5902\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8104 - acc: 0.8759 - val_loss: 2.1293 - val_acc: 0.5666\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8317 - acc: 0.8692 - val_loss: 2.4911 - val_acc: 0.5051\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7888 - acc: 0.8859 - val_loss: 2.2770 - val_acc: 0.5583\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7913 - acc: 0.8878 - val_loss: 2.0466 - val_acc: 0.5697\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7745 - acc: 0.8929 - val_loss: 2.0997 - val_acc: 0.5548\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8180 - acc: 0.8767 - val_loss: 2.0093 - val_acc: 0.5847\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7957 - acc: 0.8858 - val_loss: 2.0166 - val_acc: 0.5894\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8016 - acc: 0.8854 - val_loss: 2.0240 - val_acc: 0.5757\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7886 - acc: 0.8873 - val_loss: 2.1035 - val_acc: 0.5552\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7971 - acc: 0.8815 - val_loss: 2.0456 - val_acc: 0.5835\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7895 - acc: 0.8905 - val_loss: 2.2237 - val_acc: 0.5697\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7834 - acc: 0.8896 - val_loss: 1.8868 - val_acc: 0.5950\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7812 - acc: 0.8875 - val_loss: 2.1544 - val_acc: 0.5662\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7781 - acc: 0.8915 - val_loss: 2.1578 - val_acc: 0.5532\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7888 - acc: 0.8862 - val_loss: 2.2379 - val_acc: 0.5646\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.8069 - acc: 0.8813 - val_loss: 2.0266 - val_acc: 0.5705\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7768 - acc: 0.8926 - val_loss: 2.1500 - val_acc: 0.5552\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8418 - acc: 0.8716 - val_loss: 2.0670 - val_acc: 0.5717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  3%|         | 7/256 [1:05:35<37:30:53, 542.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 499us/step - loss: 6.1050 - acc: 0.2830 - val_loss: 5.7128 - val_acc: 0.2805\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 3.9925 - acc: 0.3523 - val_loss: 3.2870 - val_acc: 0.3436\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 2.8333 - acc: 0.4285 - val_loss: 2.3881 - val_acc: 0.4480\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 2.2040 - acc: 0.4909 - val_loss: 2.5150 - val_acc: 0.3511\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.8567 - acc: 0.5206 - val_loss: 1.8025 - val_acc: 0.4827\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 1.6371 - acc: 0.5468 - val_loss: 1.6371 - val_acc: 0.4909\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 1.5007 - acc: 0.5664 - val_loss: 1.8148 - val_acc: 0.4756\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 1.4044 - acc: 0.5885 - val_loss: 2.0886 - val_acc: 0.4224\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 1.3266 - acc: 0.6026 - val_loss: 1.5125 - val_acc: 0.5225\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.2685 - acc: 0.6258 - val_loss: 1.7643 - val_acc: 0.4744\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 1.2160 - acc: 0.6321 - val_loss: 1.9042 - val_acc: 0.4886\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 1.1874 - acc: 0.6502 - val_loss: 1.5503 - val_acc: 0.5272\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 1.1378 - acc: 0.6678 - val_loss: 1.4474 - val_acc: 0.5583\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 1.1190 - acc: 0.6778 - val_loss: 2.6930 - val_acc: 0.3865\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 1.0989 - acc: 0.6888 - val_loss: 1.8341 - val_acc: 0.4704\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 1.0667 - acc: 0.7052 - val_loss: 1.6897 - val_acc: 0.5315\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 1.0487 - acc: 0.7126 - val_loss: 1.6072 - val_acc: 0.5331\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 1.0241 - acc: 0.7253 - val_loss: 1.5523 - val_acc: 0.5429\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 1.0114 - acc: 0.7337 - val_loss: 1.6863 - val_acc: 0.5461\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 1.0086 - acc: 0.7368 - val_loss: 1.6930 - val_acc: 0.5280\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.9946 - acc: 0.7448 - val_loss: 1.9669 - val_acc: 0.4917\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.9864 - acc: 0.7524 - val_loss: 1.5795 - val_acc: 0.5827\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.9670 - acc: 0.7637 - val_loss: 1.5352 - val_acc: 0.5725\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.9526 - acc: 0.7735 - val_loss: 2.0022 - val_acc: 0.5272\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.9565 - acc: 0.7747 - val_loss: 1.8029 - val_acc: 0.5236\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.9476 - acc: 0.7749 - val_loss: 3.0588 - val_acc: 0.3582\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.9473 - acc: 0.7795 - val_loss: 1.8952 - val_acc: 0.5307\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.9380 - acc: 0.7860 - val_loss: 1.6416 - val_acc: 0.5792\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.9153 - acc: 0.7957 - val_loss: 3.1470 - val_acc: 0.3960\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.9168 - acc: 0.8016 - val_loss: 2.1451 - val_acc: 0.4819\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.9026 - acc: 0.8018 - val_loss: 1.8775 - val_acc: 0.5240\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.9113 - acc: 0.8025 - val_loss: 1.9333 - val_acc: 0.5508\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.9031 - acc: 0.8095 - val_loss: 2.2615 - val_acc: 0.5071\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8988 - acc: 0.8102 - val_loss: 2.1429 - val_acc: 0.5280\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8936 - acc: 0.8134 - val_loss: 1.6700 - val_acc: 0.5839\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8910 - acc: 0.8150 - val_loss: 1.8445 - val_acc: 0.5567\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.8834 - acc: 0.8226 - val_loss: 1.6983 - val_acc: 0.5851\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8832 - acc: 0.8210 - val_loss: 2.1910 - val_acc: 0.4870\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8754 - acc: 0.8283 - val_loss: 1.8008 - val_acc: 0.5567\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8753 - acc: 0.8268 - val_loss: 2.0613 - val_acc: 0.5130\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8747 - acc: 0.8286 - val_loss: 1.6835 - val_acc: 0.5894\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8718 - acc: 0.8318 - val_loss: 2.0617 - val_acc: 0.5359\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8617 - acc: 0.8365 - val_loss: 2.0502 - val_acc: 0.5386\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8650 - acc: 0.8321 - val_loss: 2.1227 - val_acc: 0.5296\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8633 - acc: 0.8334 - val_loss: 2.6411 - val_acc: 0.4567\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8612 - acc: 0.8363 - val_loss: 1.8593 - val_acc: 0.5504\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8527 - acc: 0.8412 - val_loss: 2.5475 - val_acc: 0.5047\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8483 - acc: 0.8408 - val_loss: 2.2144 - val_acc: 0.5339\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8601 - acc: 0.8363 - val_loss: 1.6808 - val_acc: 0.5879\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8512 - acc: 0.8453 - val_loss: 1.8954 - val_acc: 0.5623\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8513 - acc: 0.8411 - val_loss: 1.9311 - val_acc: 0.5654\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8394 - acc: 0.8481 - val_loss: 1.6668 - val_acc: 0.6052\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8463 - acc: 0.8448 - val_loss: 1.9637 - val_acc: 0.5552\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8402 - acc: 0.8448 - val_loss: 1.9210 - val_acc: 0.5662\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8327 - acc: 0.8511 - val_loss: 1.9612 - val_acc: 0.5599\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8310 - acc: 0.8493 - val_loss: 1.7861 - val_acc: 0.5934\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8283 - acc: 0.8549 - val_loss: 2.1684 - val_acc: 0.5232\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8370 - acc: 0.8517 - val_loss: 2.3043 - val_acc: 0.5414\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8305 - acc: 0.8551 - val_loss: 2.3841 - val_acc: 0.4972\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8321 - acc: 0.8518 - val_loss: 1.7699 - val_acc: 0.5890\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8203 - acc: 0.8580 - val_loss: 2.8642 - val_acc: 0.5059\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8381 - acc: 0.8545 - val_loss: 1.8045 - val_acc: 0.5693\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8268 - acc: 0.8574 - val_loss: 2.0473 - val_acc: 0.5382\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8292 - acc: 0.8586 - val_loss: 2.2306 - val_acc: 0.5323\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8185 - acc: 0.8589 - val_loss: 3.3146 - val_acc: 0.4586\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8272 - acc: 0.8553 - val_loss: 2.5724 - val_acc: 0.4827\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8257 - acc: 0.8631 - val_loss: 2.0844 - val_acc: 0.5634\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8195 - acc: 0.8612 - val_loss: 1.7521 - val_acc: 0.6028\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8141 - acc: 0.8651 - val_loss: 2.1131 - val_acc: 0.5496\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8144 - acc: 0.8620 - val_loss: 1.7643 - val_acc: 0.5973\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8104 - acc: 0.8628 - val_loss: 2.1133 - val_acc: 0.5536\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8073 - acc: 0.8646 - val_loss: 1.9674 - val_acc: 0.5512\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8079 - acc: 0.8629 - val_loss: 2.5837 - val_acc: 0.4988\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8117 - acc: 0.8641 - val_loss: 2.4607 - val_acc: 0.5099\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.8143 - acc: 0.8651 - val_loss: 2.5499 - val_acc: 0.5284\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8060 - acc: 0.8675 - val_loss: 2.0032 - val_acc: 0.5465\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8132 - acc: 0.8620 - val_loss: 2.1414 - val_acc: 0.5595\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8024 - acc: 0.8646 - val_loss: 2.0993 - val_acc: 0.5666\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8232 - acc: 0.8569 - val_loss: 1.7853 - val_acc: 0.5839\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8143 - acc: 0.8613 - val_loss: 2.3311 - val_acc: 0.5674\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7933 - acc: 0.8720 - val_loss: 2.3916 - val_acc: 0.5197\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8023 - acc: 0.8710 - val_loss: 2.0799 - val_acc: 0.5449\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8019 - acc: 0.8712 - val_loss: 2.5435 - val_acc: 0.5165\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8103 - acc: 0.8631 - val_loss: 2.3288 - val_acc: 0.5079\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8005 - acc: 0.8687 - val_loss: 2.3323 - val_acc: 0.5441\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7946 - acc: 0.8700 - val_loss: 2.2780 - val_acc: 0.5433\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8162 - acc: 0.8636 - val_loss: 2.0087 - val_acc: 0.5615\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8072 - acc: 0.8681 - val_loss: 1.9782 - val_acc: 0.5646\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7941 - acc: 0.8757 - val_loss: 2.9804 - val_acc: 0.4586\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8037 - acc: 0.8654 - val_loss: 2.5488 - val_acc: 0.5102\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7891 - acc: 0.8777 - val_loss: 2.1478 - val_acc: 0.5536\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7891 - acc: 0.8755 - val_loss: 1.8452 - val_acc: 0.5768\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8035 - acc: 0.8687 - val_loss: 2.1991 - val_acc: 0.5512\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7901 - acc: 0.8746 - val_loss: 2.6283 - val_acc: 0.4842\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7862 - acc: 0.8769 - val_loss: 2.8151 - val_acc: 0.4783\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7960 - acc: 0.8747 - val_loss: 2.0156 - val_acc: 0.5737\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8011 - acc: 0.8689 - val_loss: 2.0949 - val_acc: 0.5733\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7978 - acc: 0.8706 - val_loss: 2.2246 - val_acc: 0.5331\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7973 - acc: 0.8775 - val_loss: 2.1347 - val_acc: 0.5583\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7920 - acc: 0.8721 - val_loss: 1.9845 - val_acc: 0.5496\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7903 - acc: 0.8767 - val_loss: 2.2199 - val_acc: 0.5280\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7801 - acc: 0.8774 - val_loss: 2.3672 - val_acc: 0.5489\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7944 - acc: 0.8708 - val_loss: 2.1374 - val_acc: 0.5536\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7899 - acc: 0.8737 - val_loss: 2.0424 - val_acc: 0.5607\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7887 - acc: 0.8742 - val_loss: 2.0719 - val_acc: 0.5737\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7904 - acc: 0.8738 - val_loss: 1.9951 - val_acc: 0.5725\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7805 - acc: 0.8793 - val_loss: 2.7993 - val_acc: 0.5122\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7956 - acc: 0.8728 - val_loss: 2.1413 - val_acc: 0.5642\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7834 - acc: 0.8779 - val_loss: 2.1100 - val_acc: 0.5485\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7775 - acc: 0.8790 - val_loss: 2.1223 - val_acc: 0.5686\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7920 - acc: 0.8757 - val_loss: 3.4618 - val_acc: 0.4484\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7859 - acc: 0.8773 - val_loss: 2.2811 - val_acc: 0.5445\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7821 - acc: 0.8775 - val_loss: 2.0275 - val_acc: 0.5626\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7814 - acc: 0.8777 - val_loss: 2.2864 - val_acc: 0.5394\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7771 - acc: 0.8819 - val_loss: 2.2589 - val_acc: 0.5500\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7852 - acc: 0.8813 - val_loss: 2.7610 - val_acc: 0.4925\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7807 - acc: 0.8784 - val_loss: 2.7794 - val_acc: 0.4854\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7892 - acc: 0.8755 - val_loss: 2.2296 - val_acc: 0.5532\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7709 - acc: 0.8791 - val_loss: 2.2862 - val_acc: 0.5567\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7800 - acc: 0.8798 - val_loss: 2.3048 - val_acc: 0.5512\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 5s 455us/step - loss: 0.7751 - acc: 0.8829 - val_loss: 2.2355 - val_acc: 0.5311\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7822 - acc: 0.8795 - val_loss: 2.3623 - val_acc: 0.5461\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7851 - acc: 0.8743 - val_loss: 2.0681 - val_acc: 0.5611\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7768 - acc: 0.8799 - val_loss: 3.0571 - val_acc: 0.4685\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7734 - acc: 0.8806 - val_loss: 2.1601 - val_acc: 0.5682\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7688 - acc: 0.8833 - val_loss: 2.3470 - val_acc: 0.5260\n",
            "Epoch 127/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7627 - acc: 0.8841 - val_loss: 1.8901 - val_acc: 0.5772\n",
            "Epoch 128/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7795 - acc: 0.8819 - val_loss: 2.4712 - val_acc: 0.5240\n",
            "Epoch 129/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7829 - acc: 0.8795 - val_loss: 2.0901 - val_acc: 0.5532\n",
            "Epoch 130/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7873 - acc: 0.8779 - val_loss: 2.0835 - val_acc: 0.5512\n",
            "Epoch 131/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7832 - acc: 0.8770 - val_loss: 2.0094 - val_acc: 0.5693\n",
            "Epoch 132/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7751 - acc: 0.8815 - val_loss: 1.8479 - val_acc: 0.5843\n",
            "Epoch 133/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7704 - acc: 0.8813 - val_loss: 2.1528 - val_acc: 0.5583\n",
            "Epoch 134/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7704 - acc: 0.8850 - val_loss: 1.8959 - val_acc: 0.5760\n",
            "Epoch 135/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7847 - acc: 0.8774 - val_loss: 2.0847 - val_acc: 0.5615\n",
            "Epoch 136/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7814 - acc: 0.8781 - val_loss: 1.9665 - val_acc: 0.5784\n",
            "Epoch 137/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7747 - acc: 0.8809 - val_loss: 2.1716 - val_acc: 0.5512\n",
            "Epoch 138/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7703 - acc: 0.8840 - val_loss: 2.7214 - val_acc: 0.5102\n",
            "Epoch 139/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7657 - acc: 0.8842 - val_loss: 2.8523 - val_acc: 0.4701\n",
            "Epoch 140/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7696 - acc: 0.8836 - val_loss: 2.1353 - val_acc: 0.5575\n",
            "Epoch 141/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7683 - acc: 0.8819 - val_loss: 2.1504 - val_acc: 0.5587\n",
            "Epoch 142/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7578 - acc: 0.8830 - val_loss: 2.5917 - val_acc: 0.5244\n",
            "Epoch 143/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7683 - acc: 0.8847 - val_loss: 2.6879 - val_acc: 0.5118\n",
            "Epoch 144/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7697 - acc: 0.8793 - val_loss: 2.7090 - val_acc: 0.4957\n",
            "Epoch 145/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7651 - acc: 0.8844 - val_loss: 2.4225 - val_acc: 0.5016\n",
            "Epoch 146/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7658 - acc: 0.8823 - val_loss: 2.7403 - val_acc: 0.4913\n",
            "Epoch 147/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7685 - acc: 0.8825 - val_loss: 2.4479 - val_acc: 0.5378\n",
            "Epoch 148/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7764 - acc: 0.8787 - val_loss: 2.0729 - val_acc: 0.5559\n",
            "Epoch 149/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7689 - acc: 0.8834 - val_loss: 3.1208 - val_acc: 0.4803\n",
            "Epoch 150/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7726 - acc: 0.8825 - val_loss: 3.7232 - val_acc: 0.4606\n",
            "Epoch 151/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7602 - acc: 0.8871 - val_loss: 2.3184 - val_acc: 0.5370\n",
            "Epoch 152/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7663 - acc: 0.8821 - val_loss: 1.9899 - val_acc: 0.5745\n",
            "Epoch 153/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7709 - acc: 0.8790 - val_loss: 2.2409 - val_acc: 0.5552\n",
            "Epoch 154/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7650 - acc: 0.8819 - val_loss: 2.6107 - val_acc: 0.5158\n",
            "Epoch 155/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7691 - acc: 0.8801 - val_loss: 2.1151 - val_acc: 0.5812\n",
            "Epoch 156/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7758 - acc: 0.8770 - val_loss: 2.3955 - val_acc: 0.5493\n",
            "Epoch 157/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7506 - acc: 0.8925 - val_loss: 2.4262 - val_acc: 0.5165\n",
            "Epoch 158/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7583 - acc: 0.8834 - val_loss: 2.3279 - val_acc: 0.5426\n",
            "Epoch 159/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7662 - acc: 0.8828 - val_loss: 2.1171 - val_acc: 0.5469\n",
            "Epoch 160/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7649 - acc: 0.8831 - val_loss: 2.4650 - val_acc: 0.5496\n",
            "Epoch 161/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7637 - acc: 0.8818 - val_loss: 2.4630 - val_acc: 0.5154\n",
            "Epoch 162/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7535 - acc: 0.8914 - val_loss: 2.2504 - val_acc: 0.5532\n",
            "Epoch 163/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7719 - acc: 0.8820 - val_loss: 2.0786 - val_acc: 0.5749\n",
            "Epoch 164/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7555 - acc: 0.8898 - val_loss: 2.0752 - val_acc: 0.5757\n",
            "Epoch 165/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7659 - acc: 0.8821 - val_loss: 2.2392 - val_acc: 0.5331\n",
            "Epoch 166/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7628 - acc: 0.8818 - val_loss: 3.0595 - val_acc: 0.4677\n",
            "Epoch 167/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7641 - acc: 0.8852 - val_loss: 3.2326 - val_acc: 0.4634\n",
            "Epoch 168/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7639 - acc: 0.8858 - val_loss: 1.9411 - val_acc: 0.5863\n",
            "Epoch 169/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7560 - acc: 0.8895 - val_loss: 2.3086 - val_acc: 0.5469\n",
            "Epoch 170/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7597 - acc: 0.8878 - val_loss: 2.2646 - val_acc: 0.5311\n",
            "Epoch 171/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7557 - acc: 0.8884 - val_loss: 2.1833 - val_acc: 0.5516\n",
            "Epoch 172/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7622 - acc: 0.8859 - val_loss: 2.5257 - val_acc: 0.5181\n",
            "Epoch 173/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7434 - acc: 0.8916 - val_loss: 2.2995 - val_acc: 0.5449\n",
            "Epoch 174/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7658 - acc: 0.8858 - val_loss: 2.9341 - val_acc: 0.5134\n",
            "Epoch 175/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7627 - acc: 0.8850 - val_loss: 1.9562 - val_acc: 0.5855\n",
            "Epoch 176/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7490 - acc: 0.8890 - val_loss: 2.6696 - val_acc: 0.4827\n",
            "Epoch 177/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7653 - acc: 0.8861 - val_loss: 2.1330 - val_acc: 0.5662\n",
            "Epoch 178/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7603 - acc: 0.8853 - val_loss: 2.0079 - val_acc: 0.5764\n",
            "Epoch 179/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7615 - acc: 0.8878 - val_loss: 2.2381 - val_acc: 0.5528\n",
            "Epoch 180/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7541 - acc: 0.8896 - val_loss: 2.2645 - val_acc: 0.5646\n",
            "Epoch 181/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7693 - acc: 0.8803 - val_loss: 2.2736 - val_acc: 0.5473\n",
            "Epoch 182/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7486 - acc: 0.8909 - val_loss: 1.9397 - val_acc: 0.5741\n",
            "Epoch 183/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7537 - acc: 0.8837 - val_loss: 2.3281 - val_acc: 0.5418\n",
            "Epoch 184/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7647 - acc: 0.8822 - val_loss: 2.2685 - val_acc: 0.5465\n",
            "Epoch 185/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7516 - acc: 0.8892 - val_loss: 2.8763 - val_acc: 0.4917\n",
            "Epoch 186/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7506 - acc: 0.8893 - val_loss: 2.3145 - val_acc: 0.5477\n",
            "Epoch 187/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7459 - acc: 0.8909 - val_loss: 2.1839 - val_acc: 0.5753\n",
            "Epoch 188/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7469 - acc: 0.8887 - val_loss: 2.1315 - val_acc: 0.5650\n",
            "Epoch 189/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7601 - acc: 0.8871 - val_loss: 2.5829 - val_acc: 0.5288\n",
            "Epoch 190/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7721 - acc: 0.8839 - val_loss: 2.9228 - val_acc: 0.4898\n",
            "Epoch 191/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7584 - acc: 0.8872 - val_loss: 2.6736 - val_acc: 0.5323\n",
            "Epoch 192/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7522 - acc: 0.8890 - val_loss: 2.6096 - val_acc: 0.5134\n",
            "Epoch 193/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7540 - acc: 0.8892 - val_loss: 2.3642 - val_acc: 0.5256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  3%|         | 8/256 [1:19:43<43:41:23, 634.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 538us/step - loss: 6.0741 - acc: 0.2914 - val_loss: 5.2931 - val_acc: 0.3089\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 4.4184 - acc: 0.3630 - val_loss: 3.7532 - val_acc: 0.3491\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 3.2883 - acc: 0.4421 - val_loss: 2.9038 - val_acc: 0.4046\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 2.5115 - acc: 0.5080 - val_loss: 2.7913 - val_acc: 0.3731\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 2.0307 - acc: 0.5522 - val_loss: 2.0196 - val_acc: 0.4724\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 1.6855 - acc: 0.5960 - val_loss: 1.8540 - val_acc: 0.4764\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 1.5377 - acc: 0.6053 - val_loss: 1.9903 - val_acc: 0.4582\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 1.4205 - acc: 0.6136 - val_loss: 1.5793 - val_acc: 0.5437\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 1.3076 - acc: 0.6446 - val_loss: 1.5569 - val_acc: 0.5638\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 1.2377 - acc: 0.6577 - val_loss: 1.6828 - val_acc: 0.5370\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 1.2091 - acc: 0.6669 - val_loss: 1.9001 - val_acc: 0.4728\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 1.1854 - acc: 0.6726 - val_loss: 1.6803 - val_acc: 0.5102\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 1.1464 - acc: 0.6825 - val_loss: 1.5011 - val_acc: 0.5512\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 1.1143 - acc: 0.7041 - val_loss: 1.4378 - val_acc: 0.5843\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 1.0952 - acc: 0.7080 - val_loss: 1.5508 - val_acc: 0.5654\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 1.0705 - acc: 0.7202 - val_loss: 1.6652 - val_acc: 0.5504\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 1.0828 - acc: 0.7189 - val_loss: 1.7013 - val_acc: 0.5288\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 1.0518 - acc: 0.7369 - val_loss: 1.5356 - val_acc: 0.5693\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 1.0379 - acc: 0.7406 - val_loss: 1.6204 - val_acc: 0.5567\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 1.0070 - acc: 0.7599 - val_loss: 1.7764 - val_acc: 0.5351\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.9992 - acc: 0.7637 - val_loss: 1.5942 - val_acc: 0.5823\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.9814 - acc: 0.7684 - val_loss: 1.6096 - val_acc: 0.5571\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.9857 - acc: 0.7751 - val_loss: 1.5363 - val_acc: 0.5973\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.9916 - acc: 0.7698 - val_loss: 1.6725 - val_acc: 0.5646\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.9786 - acc: 0.7825 - val_loss: 1.6837 - val_acc: 0.5737\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.9752 - acc: 0.7851 - val_loss: 1.6581 - val_acc: 0.5772\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.9439 - acc: 0.7995 - val_loss: 2.2296 - val_acc: 0.4984\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.9436 - acc: 0.7977 - val_loss: 2.6160 - val_acc: 0.4728\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.9314 - acc: 0.8071 - val_loss: 1.8476 - val_acc: 0.5650\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.9126 - acc: 0.8194 - val_loss: 1.6333 - val_acc: 0.5820\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.9183 - acc: 0.8129 - val_loss: 1.8909 - val_acc: 0.5556\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8976 - acc: 0.8207 - val_loss: 1.9041 - val_acc: 0.5528\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.9328 - acc: 0.8115 - val_loss: 1.7657 - val_acc: 0.5820\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.9053 - acc: 0.8255 - val_loss: 2.0352 - val_acc: 0.5280\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8823 - acc: 0.8321 - val_loss: 2.1911 - val_acc: 0.5335\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 5s 458us/step - loss: 0.8739 - acc: 0.8390 - val_loss: 1.7785 - val_acc: 0.5808\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8575 - acc: 0.8452 - val_loss: 1.8237 - val_acc: 0.5682\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8954 - acc: 0.8310 - val_loss: 1.7555 - val_acc: 0.5906\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8743 - acc: 0.8378 - val_loss: 1.8467 - val_acc: 0.5796\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8624 - acc: 0.8452 - val_loss: 1.9101 - val_acc: 0.5504\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8262 - acc: 0.8603 - val_loss: 1.9813 - val_acc: 0.5583\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8398 - acc: 0.8543 - val_loss: 1.8521 - val_acc: 0.5800\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8743 - acc: 0.8463 - val_loss: 2.2347 - val_acc: 0.5114\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8696 - acc: 0.8458 - val_loss: 1.9139 - val_acc: 0.5690\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.8358 - acc: 0.8566 - val_loss: 2.1426 - val_acc: 0.5453\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8646 - acc: 0.8484 - val_loss: 2.0127 - val_acc: 0.5678\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8409 - acc: 0.8596 - val_loss: 1.7925 - val_acc: 0.5709\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.8252 - acc: 0.8629 - val_loss: 2.1192 - val_acc: 0.5469\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.8320 - acc: 0.8584 - val_loss: 1.8980 - val_acc: 0.5646\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8588 - acc: 0.8481 - val_loss: 2.0106 - val_acc: 0.5678\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8503 - acc: 0.8537 - val_loss: 1.9700 - val_acc: 0.5697\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8556 - acc: 0.8532 - val_loss: 1.8058 - val_acc: 0.5843\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8195 - acc: 0.8663 - val_loss: 2.1652 - val_acc: 0.5366\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8529 - acc: 0.8531 - val_loss: 2.0685 - val_acc: 0.5701\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8337 - acc: 0.8653 - val_loss: 1.8950 - val_acc: 0.5599\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8479 - acc: 0.8570 - val_loss: 2.0248 - val_acc: 0.5626\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8129 - acc: 0.8742 - val_loss: 1.7650 - val_acc: 0.6001\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7870 - acc: 0.8803 - val_loss: 2.4400 - val_acc: 0.5232\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8298 - acc: 0.8645 - val_loss: 1.9767 - val_acc: 0.5843\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8155 - acc: 0.8687 - val_loss: 2.0578 - val_acc: 0.5599\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8339 - acc: 0.8629 - val_loss: 2.2633 - val_acc: 0.5284\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8228 - acc: 0.8685 - val_loss: 1.9254 - val_acc: 0.5812\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8303 - acc: 0.8629 - val_loss: 2.0961 - val_acc: 0.5623\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8212 - acc: 0.8683 - val_loss: 1.9456 - val_acc: 0.5938\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8014 - acc: 0.8760 - val_loss: 1.8575 - val_acc: 0.5883\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8017 - acc: 0.8727 - val_loss: 1.8309 - val_acc: 0.6017\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 0.8203 - acc: 0.8687 - val_loss: 2.0450 - val_acc: 0.5654\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 5s 465us/step - loss: 0.7593 - acc: 0.8895 - val_loss: 2.0716 - val_acc: 0.5871\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7561 - acc: 0.8882 - val_loss: 1.8295 - val_acc: 0.5851\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8344 - acc: 0.8616 - val_loss: 1.9785 - val_acc: 0.5894\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8579 - acc: 0.8583 - val_loss: 2.0043 - val_acc: 0.5831\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7870 - acc: 0.8845 - val_loss: 1.8625 - val_acc: 0.5914\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7840 - acc: 0.8780 - val_loss: 2.0338 - val_acc: 0.5674\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7814 - acc: 0.8866 - val_loss: 1.9002 - val_acc: 0.6001\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8663 - acc: 0.8581 - val_loss: 2.5794 - val_acc: 0.5051\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8465 - acc: 0.8661 - val_loss: 2.1087 - val_acc: 0.5433\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7620 - acc: 0.8948 - val_loss: 1.8082 - val_acc: 0.5985\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7570 - acc: 0.8911 - val_loss: 2.4425 - val_acc: 0.5292\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7964 - acc: 0.8758 - val_loss: 1.9740 - val_acc: 0.5784\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7695 - acc: 0.8871 - val_loss: 2.9729 - val_acc: 0.4582\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8224 - acc: 0.8669 - val_loss: 1.9677 - val_acc: 0.5855\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7916 - acc: 0.8802 - val_loss: 1.9794 - val_acc: 0.5760\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7652 - acc: 0.8879 - val_loss: 2.4256 - val_acc: 0.5248\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8042 - acc: 0.8734 - val_loss: 2.0039 - val_acc: 0.5623\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8106 - acc: 0.8745 - val_loss: 2.6034 - val_acc: 0.5173\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8221 - acc: 0.8711 - val_loss: 1.9456 - val_acc: 0.5847\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7708 - acc: 0.8902 - val_loss: 1.9606 - val_acc: 0.5808\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7223 - acc: 0.9054 - val_loss: 2.1594 - val_acc: 0.5469\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 0.7733 - acc: 0.8821 - val_loss: 1.9616 - val_acc: 0.5867\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8168 - acc: 0.8720 - val_loss: 2.6171 - val_acc: 0.5209\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7974 - acc: 0.8829 - val_loss: 1.8716 - val_acc: 0.5887\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.7749 - acc: 0.8889 - val_loss: 2.0213 - val_acc: 0.5579\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 5s 453us/step - loss: 0.8060 - acc: 0.8776 - val_loss: 2.6204 - val_acc: 0.5083\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7897 - acc: 0.8852 - val_loss: 2.1258 - val_acc: 0.5583\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7860 - acc: 0.8833 - val_loss: 2.8648 - val_acc: 0.4992\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7655 - acc: 0.8905 - val_loss: 1.9425 - val_acc: 0.5796\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7642 - acc: 0.8929 - val_loss: 2.0790 - val_acc: 0.5705\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7860 - acc: 0.8841 - val_loss: 2.1092 - val_acc: 0.5603\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8141 - acc: 0.8747 - val_loss: 2.2441 - val_acc: 0.5477\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8070 - acc: 0.8787 - val_loss: 2.2110 - val_acc: 0.5433\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7755 - acc: 0.8902 - val_loss: 2.1072 - val_acc: 0.5729\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7804 - acc: 0.8865 - val_loss: 2.0412 - val_acc: 0.5733\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7792 - acc: 0.8850 - val_loss: 2.1985 - val_acc: 0.5429\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7317 - acc: 0.9007 - val_loss: 2.1042 - val_acc: 0.5595\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7328 - acc: 0.8963 - val_loss: 1.9174 - val_acc: 0.5859\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7974 - acc: 0.8788 - val_loss: 2.1283 - val_acc: 0.5658\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7819 - acc: 0.8849 - val_loss: 2.4763 - val_acc: 0.5193\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7342 - acc: 0.8995 - val_loss: 2.1483 - val_acc: 0.5646\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|         | 9/256 [1:27:52<40:31:17, 590.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 510us/step - loss: 6.0740 - acc: 0.2880 - val_loss: 5.9301 - val_acc: 0.2707\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 4.0486 - acc: 0.3634 - val_loss: 3.3602 - val_acc: 0.3475\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 2.8720 - acc: 0.4395 - val_loss: 2.6424 - val_acc: 0.3846\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 2.2510 - acc: 0.4863 - val_loss: 2.1508 - val_acc: 0.4271\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.8803 - acc: 0.5204 - val_loss: 1.9806 - val_acc: 0.4279\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 1.6624 - acc: 0.5408 - val_loss: 1.8479 - val_acc: 0.4500\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 1.5159 - acc: 0.5674 - val_loss: 2.0535 - val_acc: 0.4295\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 1.3804 - acc: 0.5968 - val_loss: 1.9711 - val_acc: 0.4405\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 1.3243 - acc: 0.6148 - val_loss: 1.7289 - val_acc: 0.4965\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.2826 - acc: 0.6164 - val_loss: 2.0211 - val_acc: 0.4452\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 1.2328 - acc: 0.6349 - val_loss: 1.8616 - val_acc: 0.4752\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.1876 - acc: 0.6483 - val_loss: 1.9989 - val_acc: 0.5114\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 1.1575 - acc: 0.6608 - val_loss: 1.9405 - val_acc: 0.4547\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 1.1280 - acc: 0.6785 - val_loss: 2.3148 - val_acc: 0.4511\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 1.1034 - acc: 0.6849 - val_loss: 1.7816 - val_acc: 0.5311\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 1.0746 - acc: 0.6989 - val_loss: 1.5207 - val_acc: 0.5745\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.0594 - acc: 0.7115 - val_loss: 2.2914 - val_acc: 0.4535\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 1.0409 - acc: 0.7206 - val_loss: 2.5434 - val_acc: 0.3649\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 1.0170 - acc: 0.7304 - val_loss: 1.5602 - val_acc: 0.5500\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 1.0142 - acc: 0.7342 - val_loss: 1.6636 - val_acc: 0.5248\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.9932 - acc: 0.7490 - val_loss: 1.9555 - val_acc: 0.5059\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.9957 - acc: 0.7503 - val_loss: 1.9482 - val_acc: 0.4811\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.9783 - acc: 0.7596 - val_loss: 1.6633 - val_acc: 0.5461\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.9579 - acc: 0.7690 - val_loss: 2.3955 - val_acc: 0.4870\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.9572 - acc: 0.7714 - val_loss: 2.0431 - val_acc: 0.5162\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.9454 - acc: 0.7787 - val_loss: 1.6954 - val_acc: 0.5638\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.9462 - acc: 0.7762 - val_loss: 2.4381 - val_acc: 0.4854\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.9321 - acc: 0.7912 - val_loss: 2.3620 - val_acc: 0.4937\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.9204 - acc: 0.7933 - val_loss: 1.7382 - val_acc: 0.5359\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.9152 - acc: 0.7987 - val_loss: 1.6854 - val_acc: 0.5804\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.9122 - acc: 0.8012 - val_loss: 1.8872 - val_acc: 0.5559\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.9122 - acc: 0.8074 - val_loss: 1.7654 - val_acc: 0.5532\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.9152 - acc: 0.8026 - val_loss: 2.3859 - val_acc: 0.5177\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.9094 - acc: 0.8027 - val_loss: 2.2224 - val_acc: 0.5213\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.9027 - acc: 0.8095 - val_loss: 2.1178 - val_acc: 0.5386\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8959 - acc: 0.8088 - val_loss: 2.8911 - val_acc: 0.4543\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8865 - acc: 0.8170 - val_loss: 1.7188 - val_acc: 0.5654\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8877 - acc: 0.8179 - val_loss: 2.3084 - val_acc: 0.4752\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8860 - acc: 0.8193 - val_loss: 1.8179 - val_acc: 0.5595\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8650 - acc: 0.8281 - val_loss: 2.2663 - val_acc: 0.4941\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8710 - acc: 0.8291 - val_loss: 2.2513 - val_acc: 0.5173\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8739 - acc: 0.8296 - val_loss: 2.0167 - val_acc: 0.5378\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8679 - acc: 0.8272 - val_loss: 2.0273 - val_acc: 0.5221\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8628 - acc: 0.8330 - val_loss: 2.5020 - val_acc: 0.5110\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8661 - acc: 0.8318 - val_loss: 2.6100 - val_acc: 0.4712\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8601 - acc: 0.8325 - val_loss: 2.4145 - val_acc: 0.5193\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8615 - acc: 0.8370 - val_loss: 1.8188 - val_acc: 0.5536\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8533 - acc: 0.8385 - val_loss: 3.5206 - val_acc: 0.3849\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8556 - acc: 0.8342 - val_loss: 2.3931 - val_acc: 0.5075\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8526 - acc: 0.8378 - val_loss: 2.7263 - val_acc: 0.4610\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8475 - acc: 0.8397 - val_loss: 1.9542 - val_acc: 0.5623\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8439 - acc: 0.8425 - val_loss: 2.3384 - val_acc: 0.4980\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8456 - acc: 0.8439 - val_loss: 2.6279 - val_acc: 0.4760\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8490 - acc: 0.8417 - val_loss: 2.9313 - val_acc: 0.4708\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8361 - acc: 0.8518 - val_loss: 3.0258 - val_acc: 0.4768\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8595 - acc: 0.8421 - val_loss: 2.2214 - val_acc: 0.5339\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8312 - acc: 0.8520 - val_loss: 2.7500 - val_acc: 0.4488\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8343 - acc: 0.8519 - val_loss: 2.5658 - val_acc: 0.5071\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8317 - acc: 0.8529 - val_loss: 1.9583 - val_acc: 0.5591\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8310 - acc: 0.8544 - val_loss: 1.9537 - val_acc: 0.5595\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8319 - acc: 0.8490 - val_loss: 2.1887 - val_acc: 0.5374\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8340 - acc: 0.8571 - val_loss: 2.5069 - val_acc: 0.5004\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8258 - acc: 0.8528 - val_loss: 1.9266 - val_acc: 0.5646\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8250 - acc: 0.8543 - val_loss: 2.8165 - val_acc: 0.4905\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8329 - acc: 0.8489 - val_loss: 2.3622 - val_acc: 0.5126\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8195 - acc: 0.8578 - val_loss: 1.9697 - val_acc: 0.5623\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8247 - acc: 0.8541 - val_loss: 2.1303 - val_acc: 0.5402\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8202 - acc: 0.8601 - val_loss: 1.9340 - val_acc: 0.5587\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8214 - acc: 0.8585 - val_loss: 2.1923 - val_acc: 0.5130\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8200 - acc: 0.8608 - val_loss: 3.1349 - val_acc: 0.4464\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8118 - acc: 0.8633 - val_loss: 3.0064 - val_acc: 0.4606\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8217 - acc: 0.8568 - val_loss: 2.3827 - val_acc: 0.5307\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8314 - acc: 0.8537 - val_loss: 2.5091 - val_acc: 0.4669\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8135 - acc: 0.8608 - val_loss: 3.0019 - val_acc: 0.4858\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8235 - acc: 0.8664 - val_loss: 2.6835 - val_acc: 0.4693\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8077 - acc: 0.8639 - val_loss: 2.4766 - val_acc: 0.5268\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8133 - acc: 0.8668 - val_loss: 2.3831 - val_acc: 0.4941\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8090 - acc: 0.8623 - val_loss: 2.6587 - val_acc: 0.4921\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8153 - acc: 0.8607 - val_loss: 2.6891 - val_acc: 0.4835\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8072 - acc: 0.8636 - val_loss: 2.1275 - val_acc: 0.5319\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8128 - acc: 0.8609 - val_loss: 1.9477 - val_acc: 0.5615\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8114 - acc: 0.8669 - val_loss: 2.1517 - val_acc: 0.5449\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7951 - acc: 0.8701 - val_loss: 2.5327 - val_acc: 0.4945\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7979 - acc: 0.8688 - val_loss: 2.0761 - val_acc: 0.5603\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8146 - acc: 0.8617 - val_loss: 2.0835 - val_acc: 0.5567\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7983 - acc: 0.8693 - val_loss: 2.2506 - val_acc: 0.5441\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8036 - acc: 0.8663 - val_loss: 2.3466 - val_acc: 0.5236\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8015 - acc: 0.8663 - val_loss: 2.4793 - val_acc: 0.5051\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8061 - acc: 0.8678 - val_loss: 2.8672 - val_acc: 0.4634\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7973 - acc: 0.8666 - val_loss: 2.1314 - val_acc: 0.5493\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7970 - acc: 0.8677 - val_loss: 2.6724 - val_acc: 0.4736\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7995 - acc: 0.8689 - val_loss: 3.2255 - val_acc: 0.4649\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8059 - acc: 0.8630 - val_loss: 2.2381 - val_acc: 0.5441\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8005 - acc: 0.8685 - val_loss: 1.8999 - val_acc: 0.5733\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7901 - acc: 0.8708 - val_loss: 2.6937 - val_acc: 0.5055\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7990 - acc: 0.8658 - val_loss: 2.9895 - val_acc: 0.4988\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7842 - acc: 0.8739 - val_loss: 2.0738 - val_acc: 0.5686\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8038 - acc: 0.8634 - val_loss: 2.1904 - val_acc: 0.5292\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7885 - acc: 0.8732 - val_loss: 2.4295 - val_acc: 0.5130\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7986 - acc: 0.8701 - val_loss: 2.0876 - val_acc: 0.5654\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7847 - acc: 0.8718 - val_loss: 2.5119 - val_acc: 0.5067\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7931 - acc: 0.8711 - val_loss: 2.2320 - val_acc: 0.5299\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7899 - acc: 0.8757 - val_loss: 1.9194 - val_acc: 0.5709\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8016 - acc: 0.8660 - val_loss: 2.2989 - val_acc: 0.5347\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7971 - acc: 0.8691 - val_loss: 2.8169 - val_acc: 0.4783\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7865 - acc: 0.8773 - val_loss: 2.3872 - val_acc: 0.5359\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7937 - acc: 0.8676 - val_loss: 2.4061 - val_acc: 0.5209\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7824 - acc: 0.8789 - val_loss: 2.0756 - val_acc: 0.5469\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7940 - acc: 0.8668 - val_loss: 2.3978 - val_acc: 0.5165\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7871 - acc: 0.8749 - val_loss: 2.2890 - val_acc: 0.5299\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7828 - acc: 0.8728 - val_loss: 2.5276 - val_acc: 0.5177\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7923 - acc: 0.8738 - val_loss: 2.2422 - val_acc: 0.5386\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7698 - acc: 0.8807 - val_loss: 2.4934 - val_acc: 0.5146\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7818 - acc: 0.8789 - val_loss: 2.4186 - val_acc: 0.5418\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7798 - acc: 0.8767 - val_loss: 3.2311 - val_acc: 0.4366\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7941 - acc: 0.8726 - val_loss: 2.0579 - val_acc: 0.5741\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7900 - acc: 0.8705 - val_loss: 2.8091 - val_acc: 0.5008\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7822 - acc: 0.8790 - val_loss: 2.8616 - val_acc: 0.4901\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7825 - acc: 0.8737 - val_loss: 3.1001 - val_acc: 0.4835\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7793 - acc: 0.8770 - val_loss: 2.8973 - val_acc: 0.4641\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7838 - acc: 0.8738 - val_loss: 2.5944 - val_acc: 0.5311\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7771 - acc: 0.8792 - val_loss: 2.3667 - val_acc: 0.5296\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 5s 455us/step - loss: 0.7734 - acc: 0.8809 - val_loss: 2.4367 - val_acc: 0.5402\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7803 - acc: 0.8757 - val_loss: 3.3010 - val_acc: 0.4500\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7699 - acc: 0.8830 - val_loss: 2.0340 - val_acc: 0.5666\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7830 - acc: 0.8775 - val_loss: 2.1528 - val_acc: 0.5532\n",
            "Epoch 127/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7816 - acc: 0.8741 - val_loss: 2.2307 - val_acc: 0.5536\n",
            "Epoch 128/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7674 - acc: 0.8849 - val_loss: 2.0369 - val_acc: 0.5768\n",
            "Epoch 129/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7707 - acc: 0.8796 - val_loss: 2.4583 - val_acc: 0.5299\n",
            "Epoch 130/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7852 - acc: 0.8760 - val_loss: 1.9043 - val_acc: 0.5843\n",
            "Epoch 131/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7793 - acc: 0.8763 - val_loss: 2.4494 - val_acc: 0.5339\n",
            "Epoch 132/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7828 - acc: 0.8778 - val_loss: 2.2550 - val_acc: 0.5556\n",
            "Epoch 133/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7779 - acc: 0.8783 - val_loss: 2.0942 - val_acc: 0.5591\n",
            "Epoch 134/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7754 - acc: 0.8792 - val_loss: 2.0792 - val_acc: 0.5575\n",
            "Epoch 135/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7734 - acc: 0.8802 - val_loss: 3.4668 - val_acc: 0.4236\n",
            "Epoch 136/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7746 - acc: 0.8786 - val_loss: 2.0665 - val_acc: 0.5674\n",
            "Epoch 137/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7676 - acc: 0.8820 - val_loss: 2.8347 - val_acc: 0.5055\n",
            "Epoch 138/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7768 - acc: 0.8772 - val_loss: 3.1367 - val_acc: 0.4606\n",
            "Epoch 139/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7771 - acc: 0.8786 - val_loss: 2.4376 - val_acc: 0.5087\n",
            "Epoch 140/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7703 - acc: 0.8821 - val_loss: 2.2611 - val_acc: 0.5394\n",
            "Epoch 141/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7756 - acc: 0.8779 - val_loss: 2.2463 - val_acc: 0.5563\n",
            "Epoch 142/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7666 - acc: 0.8840 - val_loss: 3.1702 - val_acc: 0.5032\n",
            "Epoch 143/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7708 - acc: 0.8823 - val_loss: 3.1507 - val_acc: 0.4748\n",
            "Epoch 144/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7801 - acc: 0.8772 - val_loss: 2.3242 - val_acc: 0.5433\n",
            "Epoch 145/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7666 - acc: 0.8842 - val_loss: 2.3260 - val_acc: 0.5252\n",
            "Epoch 146/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7673 - acc: 0.8830 - val_loss: 2.3753 - val_acc: 0.5319\n",
            "Epoch 147/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7687 - acc: 0.8801 - val_loss: 2.5666 - val_acc: 0.5075\n",
            "Epoch 148/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7751 - acc: 0.8790 - val_loss: 2.2306 - val_acc: 0.5394\n",
            "Epoch 149/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7676 - acc: 0.8825 - val_loss: 2.6696 - val_acc: 0.5240\n",
            "Epoch 150/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7752 - acc: 0.8795 - val_loss: 2.2643 - val_acc: 0.5469\n",
            "Epoch 151/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7582 - acc: 0.8866 - val_loss: 3.0114 - val_acc: 0.4701\n",
            "Epoch 152/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7665 - acc: 0.8842 - val_loss: 2.5804 - val_acc: 0.5130\n",
            "Epoch 153/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7732 - acc: 0.8770 - val_loss: 2.6512 - val_acc: 0.5102\n",
            "Epoch 154/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7712 - acc: 0.8787 - val_loss: 2.4396 - val_acc: 0.5398\n",
            "Epoch 155/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7706 - acc: 0.8802 - val_loss: 3.3946 - val_acc: 0.4303\n",
            "Epoch 156/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7693 - acc: 0.8793 - val_loss: 2.0501 - val_acc: 0.5571\n",
            "Epoch 157/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7635 - acc: 0.8841 - val_loss: 2.6290 - val_acc: 0.5102\n",
            "Epoch 158/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7620 - acc: 0.8832 - val_loss: 2.9678 - val_acc: 0.4909\n",
            "Epoch 159/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7698 - acc: 0.8783 - val_loss: 2.2701 - val_acc: 0.5422\n",
            "Epoch 160/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7612 - acc: 0.8864 - val_loss: 3.2625 - val_acc: 0.4413\n",
            "Epoch 161/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7748 - acc: 0.8788 - val_loss: 3.4195 - val_acc: 0.4740\n",
            "Epoch 162/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7736 - acc: 0.8804 - val_loss: 2.4969 - val_acc: 0.5205\n",
            "Epoch 163/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7509 - acc: 0.8889 - val_loss: 2.5378 - val_acc: 0.5102\n",
            "Epoch 164/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7585 - acc: 0.8819 - val_loss: 2.8803 - val_acc: 0.4996\n",
            "Epoch 165/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7785 - acc: 0.8772 - val_loss: 3.4157 - val_acc: 0.4614\n",
            "Epoch 166/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7612 - acc: 0.8854 - val_loss: 2.7407 - val_acc: 0.4953\n",
            "Epoch 167/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7555 - acc: 0.8858 - val_loss: 2.5611 - val_acc: 0.5071\n",
            "Epoch 168/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7715 - acc: 0.8791 - val_loss: 3.2009 - val_acc: 0.4673\n",
            "Epoch 169/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7556 - acc: 0.8889 - val_loss: 2.8948 - val_acc: 0.4976\n",
            "Epoch 170/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7589 - acc: 0.8862 - val_loss: 2.5320 - val_acc: 0.5193\n",
            "Epoch 171/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7511 - acc: 0.8890 - val_loss: 2.8023 - val_acc: 0.4870\n",
            "Epoch 172/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7604 - acc: 0.8858 - val_loss: 2.4335 - val_acc: 0.5221\n",
            "Epoch 173/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7629 - acc: 0.8845 - val_loss: 2.3140 - val_acc: 0.5201\n",
            "Epoch 174/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7590 - acc: 0.8868 - val_loss: 2.3315 - val_acc: 0.5674\n",
            "Epoch 175/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7673 - acc: 0.8821 - val_loss: 2.8224 - val_acc: 0.4775\n",
            "Epoch 176/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7567 - acc: 0.8839 - val_loss: 3.2176 - val_acc: 0.4586\n",
            "Epoch 177/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7690 - acc: 0.8813 - val_loss: 2.2388 - val_acc: 0.5445\n",
            "Epoch 178/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7553 - acc: 0.8857 - val_loss: 2.5058 - val_acc: 0.5185\n",
            "Epoch 179/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7645 - acc: 0.8837 - val_loss: 3.4343 - val_acc: 0.4417\n",
            "Epoch 180/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7509 - acc: 0.8886 - val_loss: 2.5514 - val_acc: 0.5315\n",
            "Epoch 181/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7609 - acc: 0.8826 - val_loss: 2.3322 - val_acc: 0.5418\n",
            "Epoch 182/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7617 - acc: 0.8810 - val_loss: 2.2570 - val_acc: 0.5445\n",
            "Epoch 183/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7500 - acc: 0.8881 - val_loss: 2.5072 - val_acc: 0.5158\n",
            "Epoch 184/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7628 - acc: 0.8849 - val_loss: 2.3853 - val_acc: 0.5264\n",
            "Epoch 185/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7685 - acc: 0.8834 - val_loss: 2.4679 - val_acc: 0.5106\n",
            "Epoch 186/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7450 - acc: 0.8905 - val_loss: 2.4715 - val_acc: 0.5402\n",
            "Epoch 187/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7533 - acc: 0.8828 - val_loss: 3.2715 - val_acc: 0.4708\n",
            "Epoch 188/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7571 - acc: 0.8854 - val_loss: 2.4085 - val_acc: 0.5366\n",
            "Epoch 189/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7595 - acc: 0.8820 - val_loss: 3.7956 - val_acc: 0.4567\n",
            "Epoch 190/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7572 - acc: 0.8823 - val_loss: 2.8036 - val_acc: 0.5327\n",
            "Epoch 191/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7512 - acc: 0.8882 - val_loss: 3.2248 - val_acc: 0.4775\n",
            "Epoch 192/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7543 - acc: 0.8900 - val_loss: 2.6772 - val_acc: 0.4957\n",
            "Epoch 193/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7552 - acc: 0.8870 - val_loss: 2.3160 - val_acc: 0.5327\n",
            "Epoch 194/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7533 - acc: 0.8836 - val_loss: 2.9001 - val_acc: 0.4807\n",
            "Epoch 195/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7622 - acc: 0.8833 - val_loss: 2.0954 - val_acc: 0.5745\n",
            "Epoch 196/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7583 - acc: 0.8864 - val_loss: 2.4703 - val_acc: 0.5248\n",
            "Epoch 197/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7376 - acc: 0.8910 - val_loss: 2.7590 - val_acc: 0.5091\n",
            "Epoch 198/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7439 - acc: 0.8908 - val_loss: 4.0122 - val_acc: 0.4456\n",
            "Epoch 199/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7478 - acc: 0.8866 - val_loss: 3.1165 - val_acc: 0.4771\n",
            "Epoch 200/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7531 - acc: 0.8906 - val_loss: 2.2045 - val_acc: 0.5406\n",
            "Epoch 201/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7546 - acc: 0.8891 - val_loss: 4.3451 - val_acc: 0.4326\n",
            "Epoch 202/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7654 - acc: 0.8826 - val_loss: 1.9744 - val_acc: 0.5725\n",
            "Epoch 203/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7533 - acc: 0.8893 - val_loss: 2.1633 - val_acc: 0.5717\n",
            "Epoch 204/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7477 - acc: 0.8866 - val_loss: 2.1269 - val_acc: 0.5481\n",
            "Epoch 205/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7516 - acc: 0.8859 - val_loss: 3.1628 - val_acc: 0.4799\n",
            "Epoch 206/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7592 - acc: 0.8812 - val_loss: 2.2645 - val_acc: 0.5563\n",
            "Epoch 207/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7580 - acc: 0.8872 - val_loss: 2.2001 - val_acc: 0.5504\n",
            "Epoch 208/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7530 - acc: 0.8899 - val_loss: 2.5301 - val_acc: 0.5209\n",
            "Epoch 209/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7489 - acc: 0.8899 - val_loss: 2.4727 - val_acc: 0.5280\n",
            "Epoch 210/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7527 - acc: 0.8857 - val_loss: 2.8621 - val_acc: 0.4890\n",
            "Epoch 211/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7433 - acc: 0.8881 - val_loss: 2.4784 - val_acc: 0.5390\n",
            "Epoch 212/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7617 - acc: 0.8835 - val_loss: 2.5124 - val_acc: 0.5461\n",
            "Epoch 213/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7454 - acc: 0.8912 - val_loss: 3.6239 - val_acc: 0.4287\n",
            "Epoch 214/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7398 - acc: 0.8940 - val_loss: 2.2125 - val_acc: 0.5461\n",
            "Epoch 215/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7612 - acc: 0.8791 - val_loss: 2.6644 - val_acc: 0.4996\n",
            "Epoch 216/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7474 - acc: 0.8863 - val_loss: 3.1976 - val_acc: 0.4760\n",
            "Epoch 217/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7425 - acc: 0.8922 - val_loss: 2.1271 - val_acc: 0.5745\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|         | 10/256 [1:43:50<47:52:33, 700.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 540us/step - loss: 6.1259 - acc: 0.2922 - val_loss: 5.7010 - val_acc: 0.2983\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 4.4804 - acc: 0.3723 - val_loss: 3.8008 - val_acc: 0.3589\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 3.3155 - acc: 0.4539 - val_loss: 3.0327 - val_acc: 0.3865\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 2.5613 - acc: 0.5131 - val_loss: 2.3779 - val_acc: 0.4681\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 2.0499 - acc: 0.5633 - val_loss: 2.1966 - val_acc: 0.4456\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 1.7510 - acc: 0.5845 - val_loss: 1.8896 - val_acc: 0.4870\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 1.5458 - acc: 0.6103 - val_loss: 1.8385 - val_acc: 0.4874\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 1.4253 - acc: 0.6259 - val_loss: 1.6969 - val_acc: 0.5055\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 1.3461 - acc: 0.6377 - val_loss: 1.6313 - val_acc: 0.5225\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 1.2811 - acc: 0.6485 - val_loss: 1.8512 - val_acc: 0.4905\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 375us/step - loss: 1.2368 - acc: 0.6583 - val_loss: 1.4744 - val_acc: 0.5595\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 1.2098 - acc: 0.6656 - val_loss: 1.8038 - val_acc: 0.5189\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 1.2018 - acc: 0.6662 - val_loss: 1.8776 - val_acc: 0.5079\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 1.1573 - acc: 0.6902 - val_loss: 1.5693 - val_acc: 0.5611\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 1.0996 - acc: 0.7117 - val_loss: 1.7455 - val_acc: 0.5201\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 1.1073 - acc: 0.7095 - val_loss: 1.6084 - val_acc: 0.5603\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.1040 - acc: 0.7126 - val_loss: 1.5903 - val_acc: 0.5559\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 1.0882 - acc: 0.7227 - val_loss: 1.7891 - val_acc: 0.5366\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 1.0876 - acc: 0.7272 - val_loss: 1.7994 - val_acc: 0.5524\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 1.0649 - acc: 0.7467 - val_loss: 1.5594 - val_acc: 0.5753\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.9839 - acc: 0.7767 - val_loss: 1.6042 - val_acc: 0.5820\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 1.0133 - acc: 0.7627 - val_loss: 2.0209 - val_acc: 0.5051\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.9916 - acc: 0.7803 - val_loss: 1.7593 - val_acc: 0.5575\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.9677 - acc: 0.7842 - val_loss: 1.9599 - val_acc: 0.5327\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.9675 - acc: 0.7905 - val_loss: 2.4175 - val_acc: 0.4701\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 1.0419 - acc: 0.7637 - val_loss: 1.8796 - val_acc: 0.5504\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 1.0061 - acc: 0.7816 - val_loss: 1.6771 - val_acc: 0.5961\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.9681 - acc: 0.7956 - val_loss: 1.8925 - val_acc: 0.5457\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.9691 - acc: 0.7936 - val_loss: 1.8524 - val_acc: 0.5646\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.9294 - acc: 0.8153 - val_loss: 1.6169 - val_acc: 0.5946\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8643 - acc: 0.8384 - val_loss: 1.9902 - val_acc: 0.5173\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.9765 - acc: 0.7981 - val_loss: 1.6357 - val_acc: 0.5957\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.9090 - acc: 0.8245 - val_loss: 1.8875 - val_acc: 0.5477\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.9016 - acc: 0.8259 - val_loss: 2.2277 - val_acc: 0.5366\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.9314 - acc: 0.8178 - val_loss: 2.1798 - val_acc: 0.5126\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.9010 - acc: 0.8313 - val_loss: 1.8590 - val_acc: 0.5599\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.9192 - acc: 0.8247 - val_loss: 1.9837 - val_acc: 0.5441\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.9011 - acc: 0.8377 - val_loss: 1.8300 - val_acc: 0.5642\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8766 - acc: 0.8452 - val_loss: 1.9405 - val_acc: 0.5524\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8346 - acc: 0.8571 - val_loss: 1.7433 - val_acc: 0.5977\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8374 - acc: 0.8559 - val_loss: 1.8258 - val_acc: 0.5950\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8761 - acc: 0.8446 - val_loss: 2.0719 - val_acc: 0.5410\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.9114 - acc: 0.8356 - val_loss: 1.9209 - val_acc: 0.5406\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.9334 - acc: 0.8308 - val_loss: 2.1298 - val_acc: 0.5623\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8379 - acc: 0.8642 - val_loss: 1.9076 - val_acc: 0.5784\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8586 - acc: 0.8547 - val_loss: 2.1275 - val_acc: 0.5516\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8322 - acc: 0.8586 - val_loss: 1.9423 - val_acc: 0.5646\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8735 - acc: 0.8447 - val_loss: 2.2012 - val_acc: 0.5327\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8487 - acc: 0.8576 - val_loss: 1.9944 - val_acc: 0.5429\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8004 - acc: 0.8753 - val_loss: 1.8223 - val_acc: 0.5788\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8698 - acc: 0.8545 - val_loss: 1.9349 - val_acc: 0.5670\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8405 - acc: 0.8624 - val_loss: 1.9635 - val_acc: 0.5674\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8408 - acc: 0.8623 - val_loss: 1.9974 - val_acc: 0.5599\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8702 - acc: 0.8524 - val_loss: 1.9606 - val_acc: 0.5666\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8242 - acc: 0.8678 - val_loss: 2.0963 - val_acc: 0.5485\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8032 - acc: 0.8774 - val_loss: 2.1260 - val_acc: 0.5327\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7701 - acc: 0.8849 - val_loss: 2.4990 - val_acc: 0.5134\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.9091 - acc: 0.8358 - val_loss: 2.2032 - val_acc: 0.5445\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8314 - acc: 0.8685 - val_loss: 2.0612 - val_acc: 0.5477\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8043 - acc: 0.8769 - val_loss: 2.6070 - val_acc: 0.5110\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8263 - acc: 0.8625 - val_loss: 2.0809 - val_acc: 0.5709\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8107 - acc: 0.8740 - val_loss: 1.9527 - val_acc: 0.5816\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7638 - acc: 0.8892 - val_loss: 1.9014 - val_acc: 0.5993\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8874 - acc: 0.8454 - val_loss: 1.8838 - val_acc: 0.5902\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8084 - acc: 0.8756 - val_loss: 2.6547 - val_acc: 0.5173\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8529 - acc: 0.8580 - val_loss: 2.0229 - val_acc: 0.5879\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7813 - acc: 0.8828 - val_loss: 1.9972 - val_acc: 0.5816\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8394 - acc: 0.8661 - val_loss: 2.0664 - val_acc: 0.5528\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8275 - acc: 0.8713 - val_loss: 2.2010 - val_acc: 0.5260\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8261 - acc: 0.8686 - val_loss: 1.9511 - val_acc: 0.5847\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8037 - acc: 0.8819 - val_loss: 1.9680 - val_acc: 0.5729\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8480 - acc: 0.8662 - val_loss: 2.3598 - val_acc: 0.5378\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7901 - acc: 0.8868 - val_loss: 2.1906 - val_acc: 0.5532\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8862 - acc: 0.8538 - val_loss: 2.0619 - val_acc: 0.5473\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8498 - acc: 0.8674 - val_loss: 2.0374 - val_acc: 0.5839\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7597 - acc: 0.8956 - val_loss: 2.1448 - val_acc: 0.5591\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8086 - acc: 0.8751 - val_loss: 1.9259 - val_acc: 0.5914\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8695 - acc: 0.8597 - val_loss: 1.9817 - val_acc: 0.5847\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8252 - acc: 0.8742 - val_loss: 2.1156 - val_acc: 0.5662\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7935 - acc: 0.8875 - val_loss: 2.1053 - val_acc: 0.5567\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7670 - acc: 0.8960 - val_loss: 1.9530 - val_acc: 0.5883\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7709 - acc: 0.8871 - val_loss: 2.1729 - val_acc: 0.5414\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7909 - acc: 0.8851 - val_loss: 2.5000 - val_acc: 0.5299\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7869 - acc: 0.8846 - val_loss: 1.9492 - val_acc: 0.5906\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7887 - acc: 0.8800 - val_loss: 1.9474 - val_acc: 0.5650\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7962 - acc: 0.8799 - val_loss: 1.9835 - val_acc: 0.5646\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8963 - acc: 0.8486 - val_loss: 2.2764 - val_acc: 0.5410\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8186 - acc: 0.8771 - val_loss: 2.0965 - val_acc: 0.5536\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7765 - acc: 0.8915 - val_loss: 1.9314 - val_acc: 0.5788\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7155 - acc: 0.9098 - val_loss: 1.9550 - val_acc: 0.5800\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7294 - acc: 0.9016 - val_loss: 2.2985 - val_acc: 0.5496\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7518 - acc: 0.8956 - val_loss: 2.3168 - val_acc: 0.5544\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7683 - acc: 0.8864 - val_loss: 2.2797 - val_acc: 0.5457\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7980 - acc: 0.8772 - val_loss: 2.0521 - val_acc: 0.5800\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8137 - acc: 0.8734 - val_loss: 2.1293 - val_acc: 0.5563\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8111 - acc: 0.8812 - val_loss: 2.1461 - val_acc: 0.5686\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8193 - acc: 0.8822 - val_loss: 2.0482 - val_acc: 0.5890\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8299 - acc: 0.8760 - val_loss: 2.2038 - val_acc: 0.5477\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8330 - acc: 0.8694 - val_loss: 2.2276 - val_acc: 0.5559\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7797 - acc: 0.8916 - val_loss: 2.0779 - val_acc: 0.5847\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7198 - acc: 0.9112 - val_loss: 2.0515 - val_acc: 0.5686\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7144 - acc: 0.9099 - val_loss: 2.5402 - val_acc: 0.5493\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8074 - acc: 0.8724 - val_loss: 2.1738 - val_acc: 0.5496\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8047 - acc: 0.8820 - val_loss: 2.0968 - val_acc: 0.5670\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7657 - acc: 0.8957 - val_loss: 3.1574 - val_acc: 0.4661\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8070 - acc: 0.8746 - val_loss: 2.0379 - val_acc: 0.5567\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8043 - acc: 0.8805 - val_loss: 2.2894 - val_acc: 0.5532\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7635 - acc: 0.8951 - val_loss: 2.3159 - val_acc: 0.5288\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7424 - acc: 0.8982 - val_loss: 2.4088 - val_acc: 0.5177\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7708 - acc: 0.8871 - val_loss: 2.4974 - val_acc: 0.5343\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8244 - acc: 0.8732 - val_loss: 2.3434 - val_acc: 0.5418\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.9074 - acc: 0.8533 - val_loss: 2.1008 - val_acc: 0.5642\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8011 - acc: 0.8909 - val_loss: 2.0451 - val_acc: 0.5741\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7486 - acc: 0.9029 - val_loss: 2.0687 - val_acc: 0.5776\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7321 - acc: 0.9062 - val_loss: 1.9796 - val_acc: 0.5930\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7352 - acc: 0.9009 - val_loss: 1.9983 - val_acc: 0.5615\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7425 - acc: 0.8972 - val_loss: 1.9355 - val_acc: 0.5835\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7547 - acc: 0.8918 - val_loss: 2.3066 - val_acc: 0.5552\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7908 - acc: 0.8843 - val_loss: 2.0821 - val_acc: 0.5776\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8037 - acc: 0.8817 - val_loss: 2.1975 - val_acc: 0.5670\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7915 - acc: 0.8853 - val_loss: 2.1240 - val_acc: 0.5717\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8103 - acc: 0.8817 - val_loss: 2.2877 - val_acc: 0.5638\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|         | 11/256 [1:52:45<44:18:57, 651.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 499us/step - loss: 6.0633 - acc: 0.2946 - val_loss: 5.0199 - val_acc: 0.2778\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 3.9322 - acc: 0.3392 - val_loss: 3.0933 - val_acc: 0.3361\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 2.7620 - acc: 0.3952 - val_loss: 2.4166 - val_acc: 0.3676\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 2.1464 - acc: 0.4671 - val_loss: 2.2934 - val_acc: 0.3503\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 1.8062 - acc: 0.5105 - val_loss: 2.2394 - val_acc: 0.3810\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 1.6186 - acc: 0.5368 - val_loss: 1.7655 - val_acc: 0.4523\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 1.4782 - acc: 0.5604 - val_loss: 2.5715 - val_acc: 0.4425\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 1.3926 - acc: 0.5704 - val_loss: 3.1450 - val_acc: 0.3676\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 1.3371 - acc: 0.5906 - val_loss: 2.0825 - val_acc: 0.4200\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 1.2538 - acc: 0.6198 - val_loss: 1.7103 - val_acc: 0.4933\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 1.2179 - acc: 0.6296 - val_loss: 2.0483 - val_acc: 0.4374\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 1.1731 - acc: 0.6434 - val_loss: 1.8448 - val_acc: 0.4701\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 1.1469 - acc: 0.6582 - val_loss: 2.2199 - val_acc: 0.4586\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 1.1163 - acc: 0.6715 - val_loss: 1.4934 - val_acc: 0.5642\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 1.0887 - acc: 0.6861 - val_loss: 1.8179 - val_acc: 0.5138\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 1.0573 - acc: 0.6954 - val_loss: 2.1692 - val_acc: 0.4165\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.0583 - acc: 0.7002 - val_loss: 2.3576 - val_acc: 0.4240\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 1.0288 - acc: 0.7113 - val_loss: 2.1515 - val_acc: 0.4614\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 1.0227 - acc: 0.7196 - val_loss: 2.0528 - val_acc: 0.4779\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 1.0172 - acc: 0.7295 - val_loss: 1.7595 - val_acc: 0.5485\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.9987 - acc: 0.7380 - val_loss: 1.6591 - val_acc: 0.5473\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.9939 - acc: 0.7428 - val_loss: 1.6837 - val_acc: 0.5343\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.9708 - acc: 0.7552 - val_loss: 2.3506 - val_acc: 0.4634\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.9657 - acc: 0.7622 - val_loss: 2.0400 - val_acc: 0.4984\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.9670 - acc: 0.7627 - val_loss: 2.7659 - val_acc: 0.4011\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.9472 - acc: 0.7712 - val_loss: 2.5249 - val_acc: 0.4303\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.9415 - acc: 0.7782 - val_loss: 2.3834 - val_acc: 0.4180\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.9312 - acc: 0.7796 - val_loss: 2.3035 - val_acc: 0.5091\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.9255 - acc: 0.7884 - val_loss: 2.0693 - val_acc: 0.5130\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.9321 - acc: 0.7906 - val_loss: 1.8651 - val_acc: 0.5240\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.9212 - acc: 0.7880 - val_loss: 1.8964 - val_acc: 0.5288\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.9189 - acc: 0.7966 - val_loss: 1.8549 - val_acc: 0.5296\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.9009 - acc: 0.8027 - val_loss: 2.8291 - val_acc: 0.4456\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.9095 - acc: 0.8032 - val_loss: 2.9016 - val_acc: 0.4247\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8917 - acc: 0.8100 - val_loss: 2.6229 - val_acc: 0.4409\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.9048 - acc: 0.8025 - val_loss: 1.9304 - val_acc: 0.5331\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8933 - acc: 0.8109 - val_loss: 2.2691 - val_acc: 0.4779\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.8794 - acc: 0.8156 - val_loss: 2.1973 - val_acc: 0.4984\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.8822 - acc: 0.8186 - val_loss: 1.7272 - val_acc: 0.5528\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8718 - acc: 0.8184 - val_loss: 3.9855 - val_acc: 0.3605\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8765 - acc: 0.8248 - val_loss: 2.2407 - val_acc: 0.4799\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8674 - acc: 0.8218 - val_loss: 1.8151 - val_acc: 0.5607\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8594 - acc: 0.8322 - val_loss: 3.3082 - val_acc: 0.4121\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8635 - acc: 0.8337 - val_loss: 3.2308 - val_acc: 0.4484\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8563 - acc: 0.8334 - val_loss: 2.7959 - val_acc: 0.4389\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8507 - acc: 0.8378 - val_loss: 3.3834 - val_acc: 0.4334\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8430 - acc: 0.8391 - val_loss: 3.7637 - val_acc: 0.3617\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8496 - acc: 0.8357 - val_loss: 1.8886 - val_acc: 0.5433\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8485 - acc: 0.8382 - val_loss: 2.2311 - val_acc: 0.5217\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8449 - acc: 0.8434 - val_loss: 2.9933 - val_acc: 0.4736\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8465 - acc: 0.8397 - val_loss: 2.7969 - val_acc: 0.4500\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8325 - acc: 0.8485 - val_loss: 1.9722 - val_acc: 0.5449\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8358 - acc: 0.8445 - val_loss: 3.5712 - val_acc: 0.4326\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8387 - acc: 0.8445 - val_loss: 2.0412 - val_acc: 0.5374\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8405 - acc: 0.8431 - val_loss: 2.6099 - val_acc: 0.4720\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8419 - acc: 0.8444 - val_loss: 1.9197 - val_acc: 0.5386\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8319 - acc: 0.8504 - val_loss: 2.2564 - val_acc: 0.5339\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8288 - acc: 0.8489 - val_loss: 2.0504 - val_acc: 0.5351\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8299 - acc: 0.8492 - val_loss: 3.3145 - val_acc: 0.4464\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8347 - acc: 0.8468 - val_loss: 2.3056 - val_acc: 0.5189\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8286 - acc: 0.8529 - val_loss: 2.4297 - val_acc: 0.5000\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8352 - acc: 0.8477 - val_loss: 2.0081 - val_acc: 0.5429\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8223 - acc: 0.8563 - val_loss: 1.9355 - val_acc: 0.5745\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8187 - acc: 0.8555 - val_loss: 2.4203 - val_acc: 0.5055\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8150 - acc: 0.8580 - val_loss: 2.3992 - val_acc: 0.5099\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8179 - acc: 0.8531 - val_loss: 2.4996 - val_acc: 0.5169\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8165 - acc: 0.8582 - val_loss: 2.4226 - val_acc: 0.5311\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8215 - acc: 0.8580 - val_loss: 3.0665 - val_acc: 0.4886\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8058 - acc: 0.8613 - val_loss: 2.3179 - val_acc: 0.5047\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8087 - acc: 0.8652 - val_loss: 2.4394 - val_acc: 0.5008\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8150 - acc: 0.8589 - val_loss: 3.9877 - val_acc: 0.3676\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8103 - acc: 0.8614 - val_loss: 2.8086 - val_acc: 0.4972\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8149 - acc: 0.8614 - val_loss: 2.8214 - val_acc: 0.4527\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8171 - acc: 0.8629 - val_loss: 3.6734 - val_acc: 0.3802\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8167 - acc: 0.8589 - val_loss: 2.0876 - val_acc: 0.5489\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8119 - acc: 0.8625 - val_loss: 1.9746 - val_acc: 0.5496\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8085 - acc: 0.8638 - val_loss: 2.8575 - val_acc: 0.4437\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7995 - acc: 0.8662 - val_loss: 2.2311 - val_acc: 0.5264\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8076 - acc: 0.8636 - val_loss: 2.8998 - val_acc: 0.4752\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8047 - acc: 0.8692 - val_loss: 2.0533 - val_acc: 0.5536\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8065 - acc: 0.8645 - val_loss: 2.4723 - val_acc: 0.5079\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7995 - acc: 0.8681 - val_loss: 2.0843 - val_acc: 0.5496\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.8045 - acc: 0.8651 - val_loss: 2.8900 - val_acc: 0.4933\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7931 - acc: 0.8697 - val_loss: 2.6824 - val_acc: 0.5039\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8031 - acc: 0.8665 - val_loss: 2.1238 - val_acc: 0.5563\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7923 - acc: 0.8710 - val_loss: 3.4682 - val_acc: 0.4307\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8025 - acc: 0.8660 - val_loss: 2.1664 - val_acc: 0.5441\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7974 - acc: 0.8640 - val_loss: 2.2640 - val_acc: 0.5236\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7960 - acc: 0.8705 - val_loss: 2.5215 - val_acc: 0.5197\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7898 - acc: 0.8722 - val_loss: 2.4332 - val_acc: 0.5055\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7817 - acc: 0.8703 - val_loss: 2.8177 - val_acc: 0.4752\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7977 - acc: 0.8698 - val_loss: 2.3239 - val_acc: 0.5201\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7947 - acc: 0.8684 - val_loss: 2.7124 - val_acc: 0.4890\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7904 - acc: 0.8716 - val_loss: 3.5200 - val_acc: 0.4318\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7895 - acc: 0.8685 - val_loss: 2.4179 - val_acc: 0.5016\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7882 - acc: 0.8714 - val_loss: 1.8876 - val_acc: 0.5599\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7852 - acc: 0.8712 - val_loss: 2.7528 - val_acc: 0.4921\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7775 - acc: 0.8734 - val_loss: 3.1197 - val_acc: 0.4417\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7834 - acc: 0.8741 - val_loss: 2.9309 - val_acc: 0.5118\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7916 - acc: 0.8698 - val_loss: 2.3348 - val_acc: 0.5579\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7843 - acc: 0.8718 - val_loss: 2.3923 - val_acc: 0.5327\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7866 - acc: 0.8722 - val_loss: 2.4961 - val_acc: 0.5299\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7898 - acc: 0.8737 - val_loss: 2.1439 - val_acc: 0.5493\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7826 - acc: 0.8747 - val_loss: 2.9767 - val_acc: 0.4720\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7839 - acc: 0.8742 - val_loss: 2.1917 - val_acc: 0.5414\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7711 - acc: 0.8822 - val_loss: 2.8882 - val_acc: 0.4846\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7853 - acc: 0.8753 - val_loss: 2.1728 - val_acc: 0.5536\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7874 - acc: 0.8754 - val_loss: 2.8910 - val_acc: 0.4850\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7795 - acc: 0.8773 - val_loss: 2.7983 - val_acc: 0.4685\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7845 - acc: 0.8731 - val_loss: 2.2247 - val_acc: 0.5264\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7823 - acc: 0.8765 - val_loss: 2.4209 - val_acc: 0.5347\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7868 - acc: 0.8727 - val_loss: 2.5072 - val_acc: 0.4965\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7730 - acc: 0.8780 - val_loss: 2.4168 - val_acc: 0.5355\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7743 - acc: 0.8799 - val_loss: 2.4627 - val_acc: 0.5059\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7707 - acc: 0.8801 - val_loss: 3.3933 - val_acc: 0.4405\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7857 - acc: 0.8764 - val_loss: 2.0794 - val_acc: 0.5579\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7816 - acc: 0.8777 - val_loss: 2.2793 - val_acc: 0.5173\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7735 - acc: 0.8795 - val_loss: 3.5019 - val_acc: 0.4397\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7856 - acc: 0.8723 - val_loss: 2.2629 - val_acc: 0.5449\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7771 - acc: 0.8775 - val_loss: 3.3372 - val_acc: 0.4870\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7668 - acc: 0.8821 - val_loss: 2.0497 - val_acc: 0.5658\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7765 - acc: 0.8797 - val_loss: 2.6684 - val_acc: 0.5099\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7744 - acc: 0.8832 - val_loss: 1.9468 - val_acc: 0.5776\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7647 - acc: 0.8780 - val_loss: 2.4279 - val_acc: 0.5165\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7774 - acc: 0.8758 - val_loss: 3.1118 - val_acc: 0.5083\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7754 - acc: 0.8773 - val_loss: 2.2617 - val_acc: 0.5493\n",
            "Epoch 127/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7720 - acc: 0.8808 - val_loss: 3.2294 - val_acc: 0.4492\n",
            "Epoch 128/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7745 - acc: 0.8825 - val_loss: 2.7753 - val_acc: 0.5165\n",
            "Epoch 129/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7756 - acc: 0.8849 - val_loss: 2.4717 - val_acc: 0.5181\n",
            "Epoch 130/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7736 - acc: 0.8741 - val_loss: 2.2998 - val_acc: 0.5288\n",
            "Epoch 131/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7731 - acc: 0.8791 - val_loss: 2.5223 - val_acc: 0.5390\n",
            "Epoch 132/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7741 - acc: 0.8772 - val_loss: 2.4553 - val_acc: 0.5071\n",
            "Epoch 133/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7747 - acc: 0.8823 - val_loss: 2.2256 - val_acc: 0.5540\n",
            "Epoch 134/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7611 - acc: 0.8803 - val_loss: 3.3161 - val_acc: 0.4291\n",
            "Epoch 135/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7772 - acc: 0.8758 - val_loss: 2.4311 - val_acc: 0.5280\n",
            "Epoch 136/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7730 - acc: 0.8785 - val_loss: 2.7468 - val_acc: 0.4693\n",
            "Epoch 137/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.7685 - acc: 0.8831 - val_loss: 2.2939 - val_acc: 0.5378\n",
            "Epoch 138/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7778 - acc: 0.8782 - val_loss: 3.8104 - val_acc: 0.4192\n",
            "Epoch 139/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7695 - acc: 0.8814 - val_loss: 3.8486 - val_acc: 0.3972\n",
            "Epoch 140/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7636 - acc: 0.8866 - val_loss: 2.8627 - val_acc: 0.4701\n",
            "Epoch 141/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7784 - acc: 0.8789 - val_loss: 2.6385 - val_acc: 0.5114\n",
            "Epoch 142/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7690 - acc: 0.8822 - val_loss: 2.7493 - val_acc: 0.4874\n",
            "Epoch 143/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7674 - acc: 0.8788 - val_loss: 3.4983 - val_acc: 0.4338\n",
            "Epoch 144/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.7682 - acc: 0.8807 - val_loss: 3.7721 - val_acc: 0.4724\n",
            "Epoch 145/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7675 - acc: 0.8793 - val_loss: 4.3396 - val_acc: 0.3755\n",
            "Epoch 146/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7737 - acc: 0.8775 - val_loss: 3.5388 - val_acc: 0.4563\n",
            "Epoch 147/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7654 - acc: 0.8829 - val_loss: 2.8022 - val_acc: 0.5102\n",
            "Epoch 148/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7607 - acc: 0.8827 - val_loss: 2.3772 - val_acc: 0.5248\n",
            "Epoch 149/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7635 - acc: 0.8838 - val_loss: 3.5561 - val_acc: 0.4567\n",
            "Epoch 150/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7673 - acc: 0.8850 - val_loss: 2.7581 - val_acc: 0.4933\n",
            "Epoch 151/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.7645 - acc: 0.8838 - val_loss: 2.1088 - val_acc: 0.5619\n",
            "Epoch 152/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.7573 - acc: 0.8880 - val_loss: 2.0445 - val_acc: 0.5808\n",
            "Epoch 153/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.7751 - acc: 0.8767 - val_loss: 2.0648 - val_acc: 0.5768\n",
            "Epoch 154/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.7624 - acc: 0.8837 - val_loss: 2.3012 - val_acc: 0.5496\n",
            "Epoch 155/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.7653 - acc: 0.8791 - val_loss: 2.2215 - val_acc: 0.5378\n",
            "Epoch 156/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7504 - acc: 0.8847 - val_loss: 2.3499 - val_acc: 0.5189\n",
            "Epoch 157/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7737 - acc: 0.8761 - val_loss: 2.1742 - val_acc: 0.5721\n",
            "Epoch 158/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7500 - acc: 0.8885 - val_loss: 2.1591 - val_acc: 0.5587\n",
            "Epoch 159/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7532 - acc: 0.8885 - val_loss: 2.6956 - val_acc: 0.5075\n",
            "Epoch 160/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.7737 - acc: 0.8789 - val_loss: 2.5532 - val_acc: 0.5091\n",
            "Epoch 161/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7562 - acc: 0.8853 - val_loss: 2.0286 - val_acc: 0.5500\n",
            "Epoch 162/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7521 - acc: 0.8895 - val_loss: 2.0278 - val_acc: 0.5693\n",
            "Epoch 163/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7618 - acc: 0.8822 - val_loss: 3.7435 - val_acc: 0.4228\n",
            "Epoch 164/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7684 - acc: 0.8808 - val_loss: 2.3732 - val_acc: 0.5437\n",
            "Epoch 165/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.7637 - acc: 0.8787 - val_loss: 2.3052 - val_acc: 0.5579\n",
            "Epoch 166/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7472 - acc: 0.8889 - val_loss: 2.6020 - val_acc: 0.5173\n",
            "Epoch 167/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7477 - acc: 0.8903 - val_loss: 2.3670 - val_acc: 0.5426\n",
            "Epoch 168/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7643 - acc: 0.8798 - val_loss: 2.5009 - val_acc: 0.5154\n",
            "Epoch 169/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7515 - acc: 0.8898 - val_loss: 2.4146 - val_acc: 0.5173\n",
            "Epoch 170/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7492 - acc: 0.8906 - val_loss: 4.8052 - val_acc: 0.3519\n",
            "Epoch 171/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7667 - acc: 0.8818 - val_loss: 2.2990 - val_acc: 0.5634\n",
            "Epoch 172/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7598 - acc: 0.8830 - val_loss: 2.2099 - val_acc: 0.5366\n",
            "Epoch 173/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.7498 - acc: 0.8889 - val_loss: 2.4059 - val_acc: 0.5426\n",
            "Epoch 174/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7544 - acc: 0.8876 - val_loss: 3.2992 - val_acc: 0.4464\n",
            "Epoch 175/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7578 - acc: 0.8849 - val_loss: 2.5992 - val_acc: 0.5087\n",
            "Epoch 176/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7628 - acc: 0.8834 - val_loss: 3.1653 - val_acc: 0.4444\n",
            "Epoch 177/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.7603 - acc: 0.8874 - val_loss: 2.6216 - val_acc: 0.5087\n",
            "Epoch 178/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7516 - acc: 0.8852 - val_loss: 3.3603 - val_acc: 0.4775\n",
            "Epoch 179/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7624 - acc: 0.8847 - val_loss: 2.6130 - val_acc: 0.4823\n",
            "Epoch 180/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7612 - acc: 0.8826 - val_loss: 2.2384 - val_acc: 0.5623\n",
            "Epoch 181/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7418 - acc: 0.8917 - val_loss: 2.1365 - val_acc: 0.5808\n",
            "Epoch 182/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7538 - acc: 0.8873 - val_loss: 5.2114 - val_acc: 0.3838\n",
            "Epoch 183/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.7567 - acc: 0.8881 - val_loss: 2.1986 - val_acc: 0.5583\n",
            "Epoch 184/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.7468 - acc: 0.8896 - val_loss: 2.0068 - val_acc: 0.5859\n",
            "Epoch 185/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7628 - acc: 0.8834 - val_loss: 2.7181 - val_acc: 0.5051\n",
            "Epoch 186/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7597 - acc: 0.8872 - val_loss: 2.9978 - val_acc: 0.4630\n",
            "Epoch 187/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7570 - acc: 0.8872 - val_loss: 4.0285 - val_acc: 0.4236\n",
            "Epoch 188/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7644 - acc: 0.8815 - val_loss: 3.2472 - val_acc: 0.4724\n",
            "Epoch 189/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7420 - acc: 0.8958 - val_loss: 3.6747 - val_acc: 0.4232\n",
            "Epoch 190/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7504 - acc: 0.8919 - val_loss: 2.5125 - val_acc: 0.5197\n",
            "Epoch 191/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7537 - acc: 0.8844 - val_loss: 3.1713 - val_acc: 0.4464\n",
            "Epoch 192/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7585 - acc: 0.8827 - val_loss: 2.4522 - val_acc: 0.5410\n",
            "Epoch 193/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7591 - acc: 0.8862 - val_loss: 2.1577 - val_acc: 0.5575\n",
            "Epoch 194/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7558 - acc: 0.8861 - val_loss: 2.4112 - val_acc: 0.5307\n",
            "Epoch 195/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7649 - acc: 0.8851 - val_loss: 2.2695 - val_acc: 0.5500\n",
            "Epoch 196/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7608 - acc: 0.8851 - val_loss: 2.5067 - val_acc: 0.5339\n",
            "Epoch 197/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.7468 - acc: 0.8909 - val_loss: 2.5158 - val_acc: 0.5114\n",
            "Epoch 198/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7477 - acc: 0.8853 - val_loss: 2.2822 - val_acc: 0.5599\n",
            "Epoch 199/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7569 - acc: 0.8894 - val_loss: 3.3421 - val_acc: 0.4638\n",
            "Epoch 200/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7472 - acc: 0.8904 - val_loss: 3.5515 - val_acc: 0.4756\n",
            "Epoch 201/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.7486 - acc: 0.8879 - val_loss: 2.1636 - val_acc: 0.5571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|         | 12/256 [2:07:09<48:26:46, 714.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 522us/step - loss: 6.1404 - acc: 0.2924 - val_loss: 5.7415 - val_acc: 0.2794\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 4.5160 - acc: 0.3708 - val_loss: 3.9140 - val_acc: 0.3649\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 3.4850 - acc: 0.4349 - val_loss: 3.1484 - val_acc: 0.3932\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 2.6971 - acc: 0.5075 - val_loss: 2.5388 - val_acc: 0.4113\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 2.1672 - acc: 0.5504 - val_loss: 2.2521 - val_acc: 0.4622\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 1.8193 - acc: 0.5769 - val_loss: 1.9934 - val_acc: 0.4547\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 1.6916 - acc: 0.5743 - val_loss: 2.0119 - val_acc: 0.4527\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 1.5540 - acc: 0.5906 - val_loss: 1.7618 - val_acc: 0.4961\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 1.3585 - acc: 0.6347 - val_loss: 1.8911 - val_acc: 0.4929\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 1.3694 - acc: 0.6234 - val_loss: 1.7560 - val_acc: 0.5099\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 1.2999 - acc: 0.6349 - val_loss: 1.5268 - val_acc: 0.5323\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 1.2161 - acc: 0.6619 - val_loss: 1.5530 - val_acc: 0.5690\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 1.1956 - acc: 0.6769 - val_loss: 2.0712 - val_acc: 0.4578\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 1.1370 - acc: 0.6907 - val_loss: 1.5347 - val_acc: 0.5493\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 1.2833 - acc: 0.6482 - val_loss: 1.6890 - val_acc: 0.5339\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 1.2832 - acc: 0.6572 - val_loss: 1.6661 - val_acc: 0.5256\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.1306 - acc: 0.7017 - val_loss: 2.1062 - val_acc: 0.4980\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 1.1873 - acc: 0.6814 - val_loss: 1.6045 - val_acc: 0.5485\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 1.1354 - acc: 0.7010 - val_loss: 1.5701 - val_acc: 0.5733\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 1.0892 - acc: 0.7203 - val_loss: 1.8758 - val_acc: 0.4945\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.1772 - acc: 0.6918 - val_loss: 1.8299 - val_acc: 0.5122\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 1.1210 - acc: 0.7252 - val_loss: 1.5051 - val_acc: 0.5843\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 1.0624 - acc: 0.7390 - val_loss: 1.8641 - val_acc: 0.4925\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.0624 - acc: 0.7434 - val_loss: 1.7693 - val_acc: 0.5500\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.0913 - acc: 0.7296 - val_loss: 1.8223 - val_acc: 0.5284\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.0788 - acc: 0.7393 - val_loss: 1.5071 - val_acc: 0.5914\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.0367 - acc: 0.7574 - val_loss: 1.8918 - val_acc: 0.4996\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 1.0629 - acc: 0.7521 - val_loss: 1.5597 - val_acc: 0.5863\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8951 - acc: 0.8147 - val_loss: 1.9453 - val_acc: 0.5039\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 1.1610 - acc: 0.7275 - val_loss: 1.8194 - val_acc: 0.5619\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.1290 - acc: 0.7401 - val_loss: 1.7770 - val_acc: 0.5686\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.9919 - acc: 0.7831 - val_loss: 1.7470 - val_acc: 0.5697\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8824 - acc: 0.8262 - val_loss: 1.8615 - val_acc: 0.5516\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.9213 - acc: 0.8059 - val_loss: 1.9194 - val_acc: 0.5583\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 1.0991 - acc: 0.7457 - val_loss: 1.6598 - val_acc: 0.5729\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8961 - acc: 0.8268 - val_loss: 1.6304 - val_acc: 0.5942\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 1.0673 - acc: 0.7619 - val_loss: 1.7683 - val_acc: 0.5835\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.9055 - acc: 0.8239 - val_loss: 1.6552 - val_acc: 0.6064\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.9880 - acc: 0.7856 - val_loss: 1.9693 - val_acc: 0.5441\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8669 - acc: 0.8368 - val_loss: 1.9928 - val_acc: 0.5276\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 1.0006 - acc: 0.7862 - val_loss: 1.6719 - val_acc: 0.5816\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8517 - acc: 0.8525 - val_loss: 2.2195 - val_acc: 0.5177\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.2207 - acc: 0.7278 - val_loss: 1.7253 - val_acc: 0.5780\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.9409 - acc: 0.8262 - val_loss: 1.7943 - val_acc: 0.5682\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 1.0962 - acc: 0.7694 - val_loss: 1.8099 - val_acc: 0.5654\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 1.0461 - acc: 0.7885 - val_loss: 1.7619 - val_acc: 0.5796\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.9222 - acc: 0.8311 - val_loss: 1.6711 - val_acc: 0.6009\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8006 - acc: 0.8703 - val_loss: 2.0368 - val_acc: 0.5359\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.9636 - acc: 0.8037 - val_loss: 1.7291 - val_acc: 0.6005\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.9667 - acc: 0.8103 - val_loss: 1.7284 - val_acc: 0.5902\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8015 - acc: 0.8686 - val_loss: 1.9780 - val_acc: 0.5500\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8086 - acc: 0.8620 - val_loss: 1.7149 - val_acc: 0.5749\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 1.0424 - acc: 0.7873 - val_loss: 1.7587 - val_acc: 0.5784\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8197 - acc: 0.8681 - val_loss: 1.8400 - val_acc: 0.5701\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7677 - acc: 0.8787 - val_loss: 2.0403 - val_acc: 0.5209\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.0673 - acc: 0.7811 - val_loss: 1.8585 - val_acc: 0.5776\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.1250 - acc: 0.7664 - val_loss: 2.1113 - val_acc: 0.5532\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8853 - acc: 0.8542 - val_loss: 1.8844 - val_acc: 0.5760\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7925 - acc: 0.8779 - val_loss: 1.9864 - val_acc: 0.5599\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.8055 - acc: 0.8660 - val_loss: 2.0794 - val_acc: 0.5662\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.9799 - acc: 0.8107 - val_loss: 1.7839 - val_acc: 0.5942\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7634 - acc: 0.8893 - val_loss: 1.9174 - val_acc: 0.5449\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 1.0610 - acc: 0.7843 - val_loss: 1.7644 - val_acc: 0.5839\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8650 - acc: 0.8546 - val_loss: 1.8354 - val_acc: 0.5906\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7401 - acc: 0.8975 - val_loss: 2.0556 - val_acc: 0.5500\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.0122 - acc: 0.7919 - val_loss: 2.4084 - val_acc: 0.5362\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.9558 - acc: 0.8224 - val_loss: 2.0030 - val_acc: 0.5516\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.9839 - acc: 0.8183 - val_loss: 2.0607 - val_acc: 0.5433\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8452 - acc: 0.8608 - val_loss: 1.9560 - val_acc: 0.5559\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.9717 - acc: 0.8207 - val_loss: 2.2698 - val_acc: 0.5311\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.9699 - acc: 0.8243 - val_loss: 1.8095 - val_acc: 0.5906\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8154 - acc: 0.8768 - val_loss: 1.7570 - val_acc: 0.6032\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8307 - acc: 0.8649 - val_loss: 1.9189 - val_acc: 0.5843\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 1.1345 - acc: 0.7674 - val_loss: 1.7574 - val_acc: 0.6036\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8480 - acc: 0.8684 - val_loss: 1.9189 - val_acc: 0.5500\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.1132 - acc: 0.7802 - val_loss: 2.0895 - val_acc: 0.5296\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 1.2687 - acc: 0.7329 - val_loss: 1.7105 - val_acc: 0.5922\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.0601 - acc: 0.8057 - val_loss: 1.9215 - val_acc: 0.5473\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.0118 - acc: 0.8188 - val_loss: 1.6883 - val_acc: 0.6044\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7994 - acc: 0.8865 - val_loss: 1.8774 - val_acc: 0.5764\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.9541 - acc: 0.8260 - val_loss: 1.8321 - val_acc: 0.5737\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.9108 - acc: 0.8411 - val_loss: 1.7969 - val_acc: 0.5796\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7420 - acc: 0.9024 - val_loss: 1.9340 - val_acc: 0.5650\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7255 - acc: 0.9047 - val_loss: 1.7432 - val_acc: 0.6001\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7451 - acc: 0.8848 - val_loss: 2.3028 - val_acc: 0.5205\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 1.0333 - acc: 0.7917 - val_loss: 2.2357 - val_acc: 0.5445\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7836 - acc: 0.8849 - val_loss: 1.9760 - val_acc: 0.5847\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.9423 - acc: 0.8286 - val_loss: 2.1787 - val_acc: 0.5638\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7222 - acc: 0.9073 - val_loss: 2.3294 - val_acc: 0.5032\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8273 - acc: 0.8637 - val_loss: 1.9305 - val_acc: 0.5686\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7796 - acc: 0.8792 - val_loss: 2.2491 - val_acc: 0.5102\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 1.2008 - acc: 0.7473 - val_loss: 1.8152 - val_acc: 0.5690\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 1.0835 - acc: 0.7931 - val_loss: 1.7982 - val_acc: 0.5733\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7870 - acc: 0.8925 - val_loss: 2.0645 - val_acc: 0.5552\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7679 - acc: 0.8872 - val_loss: 1.9132 - val_acc: 0.5851\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.0482 - acc: 0.7911 - val_loss: 2.0586 - val_acc: 0.5729\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 1.0439 - acc: 0.8005 - val_loss: 1.9724 - val_acc: 0.5587\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.9938 - acc: 0.8187 - val_loss: 1.7702 - val_acc: 0.5993\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8624 - acc: 0.8646 - val_loss: 1.9251 - val_acc: 0.5823\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7078 - acc: 0.9155 - val_loss: 1.9041 - val_acc: 0.5788\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7692 - acc: 0.8815 - val_loss: 2.0464 - val_acc: 0.5575\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8931 - acc: 0.8433 - val_loss: 1.8653 - val_acc: 0.5713\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8475 - acc: 0.8653 - val_loss: 1.8480 - val_acc: 0.5954\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7824 - acc: 0.8844 - val_loss: 2.0909 - val_acc: 0.5556\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8401 - acc: 0.8643 - val_loss: 2.0297 - val_acc: 0.5512\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.9368 - acc: 0.8312 - val_loss: 2.1805 - val_acc: 0.5508\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7686 - acc: 0.8943 - val_loss: 2.0388 - val_acc: 0.5516\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8505 - acc: 0.8593 - val_loss: 2.0438 - val_acc: 0.5524\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.0280 - acc: 0.8011 - val_loss: 1.8186 - val_acc: 0.5934\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7880 - acc: 0.8883 - val_loss: 1.7322 - val_acc: 0.5938\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7310 - acc: 0.9034 - val_loss: 1.9899 - val_acc: 0.5662\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7078 - acc: 0.8989 - val_loss: 2.0144 - val_acc: 0.5701\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.6846 - acc: 0.9082 - val_loss: 1.9251 - val_acc: 0.5662\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7049 - acc: 0.8963 - val_loss: 1.9148 - val_acc: 0.5827\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8456 - acc: 0.8519 - val_loss: 2.3747 - val_acc: 0.5256\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8166 - acc: 0.8702 - val_loss: 2.7376 - val_acc: 0.5024\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.9867 - acc: 0.8149 - val_loss: 1.9068 - val_acc: 0.5934\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7381 - acc: 0.9062 - val_loss: 1.8610 - val_acc: 0.5764\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.9607 - acc: 0.8223 - val_loss: 2.1195 - val_acc: 0.5449\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 1.0960 - acc: 0.7831 - val_loss: 1.9984 - val_acc: 0.5638\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 1.1903 - acc: 0.7656 - val_loss: 2.3006 - val_acc: 0.5296\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 1.0028 - acc: 0.8260 - val_loss: 2.0485 - val_acc: 0.5619\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 1.0311 - acc: 0.8155 - val_loss: 1.8535 - val_acc: 0.5918\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8676 - acc: 0.8750 - val_loss: 1.7527 - val_acc: 0.6111\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7052 - acc: 0.9183 - val_loss: 2.2483 - val_acc: 0.5240\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7211 - acc: 0.9028 - val_loss: 1.9141 - val_acc: 0.5745\n",
            "Epoch 127/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7905 - acc: 0.8765 - val_loss: 1.8778 - val_acc: 0.5863\n",
            "Epoch 128/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.0840 - acc: 0.7842 - val_loss: 1.8287 - val_acc: 0.5804\n",
            "Epoch 129/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8222 - acc: 0.8768 - val_loss: 2.1194 - val_acc: 0.5493\n",
            "Epoch 130/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7828 - acc: 0.8869 - val_loss: 1.7795 - val_acc: 0.6017\n",
            "Epoch 131/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7599 - acc: 0.8913 - val_loss: 1.9196 - val_acc: 0.5883\n",
            "Epoch 132/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.6900 - acc: 0.9120 - val_loss: 2.0040 - val_acc: 0.5883\n",
            "Epoch 133/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8280 - acc: 0.8597 - val_loss: 1.9467 - val_acc: 0.5674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|         | 13/256 [2:16:52<45:35:33, 675.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 495us/step - loss: 6.0442 - acc: 0.2899 - val_loss: 7.0288 - val_acc: 0.2474\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 3.9094 - acc: 0.3494 - val_loss: 3.8891 - val_acc: 0.2644\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 2.7834 - acc: 0.4143 - val_loss: 2.9046 - val_acc: 0.3022\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 2.1981 - acc: 0.4664 - val_loss: 2.2606 - val_acc: 0.3645\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 1.8655 - acc: 0.5085 - val_loss: 2.3342 - val_acc: 0.3786\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 1.6422 - acc: 0.5381 - val_loss: 2.5387 - val_acc: 0.3814\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.5109 - acc: 0.5625 - val_loss: 2.4503 - val_acc: 0.3976\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 1.4236 - acc: 0.5817 - val_loss: 2.4684 - val_acc: 0.3708\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 1.3403 - acc: 0.6002 - val_loss: 2.1138 - val_acc: 0.3889\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.2797 - acc: 0.6158 - val_loss: 4.9824 - val_acc: 0.3097\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 1.2210 - acc: 0.6362 - val_loss: 1.9937 - val_acc: 0.4878\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.1879 - acc: 0.6470 - val_loss: 2.2517 - val_acc: 0.4161\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 1.1468 - acc: 0.6653 - val_loss: 1.7622 - val_acc: 0.4716\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 1.1234 - acc: 0.6816 - val_loss: 1.9920 - val_acc: 0.4787\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 1.1007 - acc: 0.6826 - val_loss: 2.3831 - val_acc: 0.4267\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.0795 - acc: 0.6961 - val_loss: 2.2885 - val_acc: 0.4708\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 1.0532 - acc: 0.7087 - val_loss: 1.6950 - val_acc: 0.5083\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 1.0429 - acc: 0.7142 - val_loss: 2.6274 - val_acc: 0.4468\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 1.0158 - acc: 0.7283 - val_loss: 2.0472 - val_acc: 0.4669\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 1.0056 - acc: 0.7341 - val_loss: 3.5497 - val_acc: 0.3298\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.9950 - acc: 0.7448 - val_loss: 1.8309 - val_acc: 0.5185\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.9858 - acc: 0.7498 - val_loss: 2.2278 - val_acc: 0.4925\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.9780 - acc: 0.7628 - val_loss: 3.4542 - val_acc: 0.4086\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.9629 - acc: 0.7643 - val_loss: 2.1118 - val_acc: 0.4913\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.9550 - acc: 0.7676 - val_loss: 3.5174 - val_acc: 0.3779\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.9528 - acc: 0.7745 - val_loss: 2.1103 - val_acc: 0.4803\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.9305 - acc: 0.7829 - val_loss: 2.8592 - val_acc: 0.4236\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.9417 - acc: 0.7853 - val_loss: 2.8221 - val_acc: 0.4031\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.9313 - acc: 0.7900 - val_loss: 2.3705 - val_acc: 0.4582\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.9218 - acc: 0.7904 - val_loss: 2.4354 - val_acc: 0.4385\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.9186 - acc: 0.7955 - val_loss: 2.0221 - val_acc: 0.5106\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.9112 - acc: 0.8050 - val_loss: 3.4020 - val_acc: 0.4027\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.9035 - acc: 0.8055 - val_loss: 3.2433 - val_acc: 0.3849\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.9045 - acc: 0.8078 - val_loss: 2.1173 - val_acc: 0.4811\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8810 - acc: 0.8163 - val_loss: 3.4422 - val_acc: 0.3916\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8885 - acc: 0.8128 - val_loss: 2.3044 - val_acc: 0.4807\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8813 - acc: 0.8200 - val_loss: 2.7111 - val_acc: 0.4456\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8798 - acc: 0.8242 - val_loss: 2.4619 - val_acc: 0.4716\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8857 - acc: 0.8196 - val_loss: 2.1337 - val_acc: 0.5016\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8662 - acc: 0.8252 - val_loss: 2.5511 - val_acc: 0.4492\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8669 - acc: 0.8281 - val_loss: 2.0068 - val_acc: 0.5394\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8725 - acc: 0.8264 - val_loss: 2.7209 - val_acc: 0.4641\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8634 - acc: 0.8316 - val_loss: 3.1439 - val_acc: 0.4318\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8562 - acc: 0.8362 - val_loss: 2.9126 - val_acc: 0.4949\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8659 - acc: 0.8315 - val_loss: 2.0433 - val_acc: 0.5244\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8542 - acc: 0.8387 - val_loss: 4.1832 - val_acc: 0.3637\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8510 - acc: 0.8396 - val_loss: 3.9602 - val_acc: 0.3448\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8565 - acc: 0.8374 - val_loss: 3.5349 - val_acc: 0.4149\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8559 - acc: 0.8390 - val_loss: 2.3655 - val_acc: 0.4819\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8489 - acc: 0.8365 - val_loss: 1.8858 - val_acc: 0.5465\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8412 - acc: 0.8443 - val_loss: 3.1597 - val_acc: 0.4354\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8391 - acc: 0.8449 - val_loss: 2.7658 - val_acc: 0.4724\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8413 - acc: 0.8451 - val_loss: 2.2560 - val_acc: 0.4976\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.8490 - acc: 0.8397 - val_loss: 5.6879 - val_acc: 0.3475\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8350 - acc: 0.8465 - val_loss: 3.5676 - val_acc: 0.4003\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8376 - acc: 0.8486 - val_loss: 3.2027 - val_acc: 0.4389\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8451 - acc: 0.8508 - val_loss: 2.1456 - val_acc: 0.5323\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8263 - acc: 0.8517 - val_loss: 1.8472 - val_acc: 0.5705\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8261 - acc: 0.8533 - val_loss: 2.3229 - val_acc: 0.5087\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8252 - acc: 0.8550 - val_loss: 2.6997 - val_acc: 0.4594\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.8163 - acc: 0.8546 - val_loss: 2.7338 - val_acc: 0.4716\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8376 - acc: 0.8513 - val_loss: 3.0911 - val_acc: 0.4764\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.8336 - acc: 0.8515 - val_loss: 2.5522 - val_acc: 0.4704\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8228 - acc: 0.8588 - val_loss: 2.9072 - val_acc: 0.4571\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8225 - acc: 0.8556 - val_loss: 2.9683 - val_acc: 0.4425\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8104 - acc: 0.8627 - val_loss: 2.4298 - val_acc: 0.5083\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8214 - acc: 0.8542 - val_loss: 2.3662 - val_acc: 0.5118\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8137 - acc: 0.8575 - val_loss: 2.8631 - val_acc: 0.4484\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8133 - acc: 0.8579 - val_loss: 2.4512 - val_acc: 0.4937\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8112 - acc: 0.8595 - val_loss: 2.4881 - val_acc: 0.5260\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8177 - acc: 0.8551 - val_loss: 2.5390 - val_acc: 0.5264\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8111 - acc: 0.8622 - val_loss: 1.8561 - val_acc: 0.5784\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7956 - acc: 0.8652 - val_loss: 2.5005 - val_acc: 0.4574\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8154 - acc: 0.8654 - val_loss: 2.1568 - val_acc: 0.5150\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7949 - acc: 0.8670 - val_loss: 3.3815 - val_acc: 0.4377\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8144 - acc: 0.8627 - val_loss: 3.2787 - val_acc: 0.4200\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7997 - acc: 0.8636 - val_loss: 2.8322 - val_acc: 0.4602\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8120 - acc: 0.8642 - val_loss: 2.7977 - val_acc: 0.4653\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8024 - acc: 0.8658 - val_loss: 2.6022 - val_acc: 0.4811\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8013 - acc: 0.8678 - val_loss: 3.0197 - val_acc: 0.4484\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8104 - acc: 0.8631 - val_loss: 2.6324 - val_acc: 0.4736\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7969 - acc: 0.8703 - val_loss: 3.0620 - val_acc: 0.4251\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7954 - acc: 0.8667 - val_loss: 2.4847 - val_acc: 0.4921\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7953 - acc: 0.8703 - val_loss: 2.1208 - val_acc: 0.5370\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7729 - acc: 0.8767 - val_loss: 4.0041 - val_acc: 0.4220\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7992 - acc: 0.8698 - val_loss: 2.8475 - val_acc: 0.4748\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8012 - acc: 0.8661 - val_loss: 6.0802 - val_acc: 0.3081\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8018 - acc: 0.8656 - val_loss: 2.7546 - val_acc: 0.4866\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7927 - acc: 0.8719 - val_loss: 3.4182 - val_acc: 0.4291\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7989 - acc: 0.8686 - val_loss: 5.3644 - val_acc: 0.3846\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8038 - acc: 0.8685 - val_loss: 3.0091 - val_acc: 0.4787\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7839 - acc: 0.8742 - val_loss: 2.2423 - val_acc: 0.5307\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7891 - acc: 0.8738 - val_loss: 3.0077 - val_acc: 0.4511\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7946 - acc: 0.8710 - val_loss: 3.2558 - val_acc: 0.4693\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7904 - acc: 0.8742 - val_loss: 2.7349 - val_acc: 0.4807\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7959 - acc: 0.8730 - val_loss: 1.9314 - val_acc: 0.5705\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7875 - acc: 0.8736 - val_loss: 2.3813 - val_acc: 0.5032\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7924 - acc: 0.8687 - val_loss: 3.8575 - val_acc: 0.4251\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7910 - acc: 0.8721 - val_loss: 2.9021 - val_acc: 0.4571\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7912 - acc: 0.8684 - val_loss: 3.1860 - val_acc: 0.4476\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7815 - acc: 0.8775 - val_loss: 4.8323 - val_acc: 0.3637\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7957 - acc: 0.8724 - val_loss: 2.5857 - val_acc: 0.5055\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7801 - acc: 0.8731 - val_loss: 2.7949 - val_acc: 0.4610\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7873 - acc: 0.8796 - val_loss: 2.4583 - val_acc: 0.5173\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7768 - acc: 0.8749 - val_loss: 2.2025 - val_acc: 0.5390\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|         | 14/256 [2:24:29<40:59:16, 609.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 529us/step - loss: 6.0670 - acc: 0.2929 - val_loss: 5.2312 - val_acc: 0.2908\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 4.4006 - acc: 0.3593 - val_loss: 3.6982 - val_acc: 0.3652\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 3.2596 - acc: 0.4456 - val_loss: 2.8394 - val_acc: 0.4444\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 2.5080 - acc: 0.5128 - val_loss: 2.4403 - val_acc: 0.4448\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 2.0031 - acc: 0.5710 - val_loss: 2.2138 - val_acc: 0.4310\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 1.7145 - acc: 0.5991 - val_loss: 1.8510 - val_acc: 0.5028\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 1.5257 - acc: 0.6234 - val_loss: 1.7396 - val_acc: 0.5083\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 1.4124 - acc: 0.6313 - val_loss: 1.8692 - val_acc: 0.4799\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 1.3308 - acc: 0.6486 - val_loss: 1.5284 - val_acc: 0.5524\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 1.2768 - acc: 0.6602 - val_loss: 1.6703 - val_acc: 0.5185\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 1.2037 - acc: 0.6774 - val_loss: 1.7019 - val_acc: 0.5244\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 1.1903 - acc: 0.6751 - val_loss: 1.8381 - val_acc: 0.5248\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.1427 - acc: 0.6977 - val_loss: 1.5594 - val_acc: 0.5623\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.1350 - acc: 0.6992 - val_loss: 1.5598 - val_acc: 0.5493\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 1.0833 - acc: 0.7275 - val_loss: 1.6118 - val_acc: 0.5567\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 1.0678 - acc: 0.7305 - val_loss: 1.5399 - val_acc: 0.5871\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 1.0551 - acc: 0.7442 - val_loss: 1.6317 - val_acc: 0.5757\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 1.0362 - acc: 0.7491 - val_loss: 1.6603 - val_acc: 0.5461\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 1.0332 - acc: 0.7510 - val_loss: 1.6477 - val_acc: 0.5520\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 1.0053 - acc: 0.7667 - val_loss: 1.5842 - val_acc: 0.5863\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.9864 - acc: 0.7808 - val_loss: 1.8033 - val_acc: 0.5737\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.9943 - acc: 0.7758 - val_loss: 1.9177 - val_acc: 0.5276\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.9822 - acc: 0.7882 - val_loss: 1.9754 - val_acc: 0.5288\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 1.0017 - acc: 0.7781 - val_loss: 1.8422 - val_acc: 0.5453\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.9604 - acc: 0.7982 - val_loss: 1.8120 - val_acc: 0.5500\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.9400 - acc: 0.8073 - val_loss: 1.7339 - val_acc: 0.5646\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.9218 - acc: 0.8095 - val_loss: 1.9705 - val_acc: 0.5528\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.9424 - acc: 0.8042 - val_loss: 1.9594 - val_acc: 0.5299\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.9415 - acc: 0.8175 - val_loss: 1.9110 - val_acc: 0.5378\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8805 - acc: 0.8342 - val_loss: 1.9240 - val_acc: 0.5563\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8765 - acc: 0.8372 - val_loss: 1.8717 - val_acc: 0.5599\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.9402 - acc: 0.8178 - val_loss: 2.5088 - val_acc: 0.5016\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.9219 - acc: 0.8253 - val_loss: 1.7031 - val_acc: 0.5961\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 0.8818 - acc: 0.8466 - val_loss: 1.8472 - val_acc: 0.5638\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.8669 - acc: 0.8494 - val_loss: 2.2617 - val_acc: 0.5449\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8539 - acc: 0.8514 - val_loss: 1.8431 - val_acc: 0.5623\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.8683 - acc: 0.8460 - val_loss: 1.8536 - val_acc: 0.5693\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8787 - acc: 0.8403 - val_loss: 1.9829 - val_acc: 0.5496\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8850 - acc: 0.8415 - val_loss: 1.9022 - val_acc: 0.5725\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8283 - acc: 0.8674 - val_loss: 2.0566 - val_acc: 0.5465\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8308 - acc: 0.8631 - val_loss: 2.4000 - val_acc: 0.5043\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8778 - acc: 0.8454 - val_loss: 1.8819 - val_acc: 0.5650\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8805 - acc: 0.8444 - val_loss: 2.0831 - val_acc: 0.5508\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8661 - acc: 0.8526 - val_loss: 1.8040 - val_acc: 0.5961\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8818 - acc: 0.8482 - val_loss: 1.7838 - val_acc: 0.5961\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8363 - acc: 0.8664 - val_loss: 2.1195 - val_acc: 0.5528\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8404 - acc: 0.8588 - val_loss: 2.0523 - val_acc: 0.5508\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7670 - acc: 0.8892 - val_loss: 1.7794 - val_acc: 0.5883\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8043 - acc: 0.8722 - val_loss: 2.1770 - val_acc: 0.5165\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8403 - acc: 0.8602 - val_loss: 2.2371 - val_acc: 0.5512\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8319 - acc: 0.8635 - val_loss: 2.0779 - val_acc: 0.5666\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8306 - acc: 0.8662 - val_loss: 1.9865 - val_acc: 0.5760\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8411 - acc: 0.8623 - val_loss: 1.8667 - val_acc: 0.5757\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8215 - acc: 0.8702 - val_loss: 2.1424 - val_acc: 0.5426\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8111 - acc: 0.8728 - val_loss: 1.9209 - val_acc: 0.5808\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8029 - acc: 0.8766 - val_loss: 2.2606 - val_acc: 0.5548\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8743 - acc: 0.8512 - val_loss: 1.9540 - val_acc: 0.5792\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8207 - acc: 0.8712 - val_loss: 1.8504 - val_acc: 0.5855\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8049 - acc: 0.8745 - val_loss: 2.2809 - val_acc: 0.5386\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7803 - acc: 0.8875 - val_loss: 2.0704 - val_acc: 0.5611\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7832 - acc: 0.8840 - val_loss: 1.8907 - val_acc: 0.5788\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8050 - acc: 0.8729 - val_loss: 2.0468 - val_acc: 0.5623\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7645 - acc: 0.8917 - val_loss: 2.0514 - val_acc: 0.5473\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7888 - acc: 0.8784 - val_loss: 2.0971 - val_acc: 0.5682\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8142 - acc: 0.8735 - val_loss: 2.0106 - val_acc: 0.5666\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8477 - acc: 0.8637 - val_loss: 2.1187 - val_acc: 0.5469\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8277 - acc: 0.8736 - val_loss: 1.9790 - val_acc: 0.5579\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7806 - acc: 0.8887 - val_loss: 2.0796 - val_acc: 0.5741\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8015 - acc: 0.8759 - val_loss: 1.9777 - val_acc: 0.5713\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8276 - acc: 0.8690 - val_loss: 1.8915 - val_acc: 0.5922\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8060 - acc: 0.8783 - val_loss: 2.1350 - val_acc: 0.5504\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7760 - acc: 0.8871 - val_loss: 2.1949 - val_acc: 0.5496\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7781 - acc: 0.8880 - val_loss: 2.0276 - val_acc: 0.5713\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7897 - acc: 0.8772 - val_loss: 2.1695 - val_acc: 0.5386\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7886 - acc: 0.8813 - val_loss: 2.1707 - val_acc: 0.5496\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7635 - acc: 0.8914 - val_loss: 2.0628 - val_acc: 0.5544\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7783 - acc: 0.8853 - val_loss: 2.3311 - val_acc: 0.5339\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8099 - acc: 0.8754 - val_loss: 2.0507 - val_acc: 0.5717\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7772 - acc: 0.8905 - val_loss: 2.2112 - val_acc: 0.5445\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8028 - acc: 0.8724 - val_loss: 2.3177 - val_acc: 0.5331\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8066 - acc: 0.8771 - val_loss: 1.9585 - val_acc: 0.5867\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7355 - acc: 0.9071 - val_loss: 2.0559 - val_acc: 0.5638\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7630 - acc: 0.8919 - val_loss: 1.8748 - val_acc: 0.5839\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7687 - acc: 0.8908 - val_loss: 2.0493 - val_acc: 0.5796\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7988 - acc: 0.8817 - val_loss: 1.8693 - val_acc: 0.6001\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7962 - acc: 0.8813 - val_loss: 2.4334 - val_acc: 0.5252\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8061 - acc: 0.8795 - val_loss: 2.1292 - val_acc: 0.5717\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8121 - acc: 0.8753 - val_loss: 2.1710 - val_acc: 0.5496\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7715 - acc: 0.8917 - val_loss: 2.1256 - val_acc: 0.5733\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7742 - acc: 0.8850 - val_loss: 2.2333 - val_acc: 0.5540\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7647 - acc: 0.8926 - val_loss: 1.9155 - val_acc: 0.5946\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7596 - acc: 0.8927 - val_loss: 1.9834 - val_acc: 0.5674\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8059 - acc: 0.8774 - val_loss: 2.2187 - val_acc: 0.5670\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7814 - acc: 0.8909 - val_loss: 2.3662 - val_acc: 0.5288\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7381 - acc: 0.9059 - val_loss: 2.2351 - val_acc: 0.5260\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7686 - acc: 0.8890 - val_loss: 2.0350 - val_acc: 0.5753\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7913 - acc: 0.8804 - val_loss: 2.0529 - val_acc: 0.5603\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7795 - acc: 0.8893 - val_loss: 2.1152 - val_acc: 0.5646\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7301 - acc: 0.9042 - val_loss: 2.3397 - val_acc: 0.5587\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7950 - acc: 0.8786 - val_loss: 1.9307 - val_acc: 0.5906\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8034 - acc: 0.8795 - val_loss: 1.9602 - val_acc: 0.5792\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7473 - acc: 0.8976 - val_loss: 2.1949 - val_acc: 0.5532\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.7629 - acc: 0.8919 - val_loss: 2.1722 - val_acc: 0.5650\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7502 - acc: 0.8950 - val_loss: 2.0459 - val_acc: 0.5717\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7710 - acc: 0.8897 - val_loss: 2.1370 - val_acc: 0.5571\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7969 - acc: 0.8815 - val_loss: 1.9580 - val_acc: 0.5800\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7810 - acc: 0.8838 - val_loss: 1.9875 - val_acc: 0.5662\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.7769 - acc: 0.8885 - val_loss: 1.8955 - val_acc: 0.6024\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7544 - acc: 0.8994 - val_loss: 1.8794 - val_acc: 0.5926\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 5s 443us/step - loss: 0.7388 - acc: 0.9021 - val_loss: 1.8699 - val_acc: 0.5879\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7345 - acc: 0.9030 - val_loss: 2.0377 - val_acc: 0.5784\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7221 - acc: 0.9047 - val_loss: 2.1031 - val_acc: 0.5674\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7376 - acc: 0.8996 - val_loss: 2.1175 - val_acc: 0.5784\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.7374 - acc: 0.9013 - val_loss: 2.1257 - val_acc: 0.5485\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7611 - acc: 0.8931 - val_loss: 2.1787 - val_acc: 0.5729\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8086 - acc: 0.8784 - val_loss: 2.3855 - val_acc: 0.5489\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7477 - acc: 0.9004 - val_loss: 1.9246 - val_acc: 0.5796\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7331 - acc: 0.9001 - val_loss: 2.0015 - val_acc: 0.5729\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7562 - acc: 0.8919 - val_loss: 2.9513 - val_acc: 0.4870\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7786 - acc: 0.8910 - val_loss: 2.1946 - val_acc: 0.5520\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7989 - acc: 0.8791 - val_loss: 2.2524 - val_acc: 0.5493\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7966 - acc: 0.8825 - val_loss: 2.6956 - val_acc: 0.5071\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7936 - acc: 0.8852 - val_loss: 2.2229 - val_acc: 0.5426\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7633 - acc: 0.8958 - val_loss: 2.0041 - val_acc: 0.5749\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7474 - acc: 0.8976 - val_loss: 2.2121 - val_acc: 0.5686\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.7462 - acc: 0.8986 - val_loss: 2.2058 - val_acc: 0.5469\n",
            "Epoch 127/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7572 - acc: 0.8938 - val_loss: 2.0546 - val_acc: 0.5678\n",
            "Epoch 128/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7277 - acc: 0.9039 - val_loss: 2.7711 - val_acc: 0.4771\n",
            "Epoch 129/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7510 - acc: 0.8956 - val_loss: 2.0715 - val_acc: 0.5504\n",
            "Epoch 130/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7622 - acc: 0.8921 - val_loss: 2.1694 - val_acc: 0.5465\n",
            "Epoch 131/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7879 - acc: 0.8865 - val_loss: 2.0075 - val_acc: 0.5930\n",
            "Epoch 132/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7311 - acc: 0.9083 - val_loss: 2.0404 - val_acc: 0.5906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|         | 15/256 [2:34:17<40:22:55, 603.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 485us/step - loss: 6.0783 - acc: 0.2900 - val_loss: 4.9681 - val_acc: 0.2963\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 3.8607 - acc: 0.3425 - val_loss: 3.0039 - val_acc: 0.3349\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 2.7427 - acc: 0.4060 - val_loss: 2.2971 - val_acc: 0.4015\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 2.1624 - acc: 0.4682 - val_loss: 1.8459 - val_acc: 0.4925\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.7930 - acc: 0.5284 - val_loss: 1.8119 - val_acc: 0.4602\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.6003 - acc: 0.5449 - val_loss: 1.5829 - val_acc: 0.5122\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.4674 - acc: 0.5693 - val_loss: 1.4778 - val_acc: 0.5441\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.3772 - acc: 0.5907 - val_loss: 1.5026 - val_acc: 0.5366\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.2979 - acc: 0.6156 - val_loss: 1.5358 - val_acc: 0.5303\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 1.2366 - acc: 0.6267 - val_loss: 1.5711 - val_acc: 0.5197\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 1.1843 - acc: 0.6500 - val_loss: 1.5482 - val_acc: 0.5051\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.1555 - acc: 0.6550 - val_loss: 1.3917 - val_acc: 0.5646\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 1.1120 - acc: 0.6671 - val_loss: 1.4880 - val_acc: 0.5670\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 1.0960 - acc: 0.6818 - val_loss: 1.7086 - val_acc: 0.5173\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.0709 - acc: 0.6949 - val_loss: 1.4166 - val_acc: 0.5871\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 1.0423 - acc: 0.7131 - val_loss: 1.3933 - val_acc: 0.5965\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 1.0409 - acc: 0.7107 - val_loss: 1.4093 - val_acc: 0.5820\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 1.0062 - acc: 0.7270 - val_loss: 1.6529 - val_acc: 0.5292\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.9982 - acc: 0.7375 - val_loss: 1.4021 - val_acc: 0.5954\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.9851 - acc: 0.7424 - val_loss: 1.6265 - val_acc: 0.5709\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.9670 - acc: 0.7549 - val_loss: 1.6047 - val_acc: 0.5599\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.9606 - acc: 0.7544 - val_loss: 1.9027 - val_acc: 0.5055\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.9486 - acc: 0.7634 - val_loss: 1.5707 - val_acc: 0.5583\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.9393 - acc: 0.7677 - val_loss: 1.9312 - val_acc: 0.5418\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.9343 - acc: 0.7796 - val_loss: 1.8047 - val_acc: 0.5284\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.9276 - acc: 0.7806 - val_loss: 1.7190 - val_acc: 0.5587\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.9190 - acc: 0.7812 - val_loss: 1.5887 - val_acc: 0.5717\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.9125 - acc: 0.7887 - val_loss: 1.6121 - val_acc: 0.5808\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.9034 - acc: 0.7972 - val_loss: 1.8472 - val_acc: 0.5623\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.9063 - acc: 0.7967 - val_loss: 1.7177 - val_acc: 0.5725\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8919 - acc: 0.8075 - val_loss: 1.8035 - val_acc: 0.5469\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8889 - acc: 0.8049 - val_loss: 1.9407 - val_acc: 0.5248\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8865 - acc: 0.8123 - val_loss: 1.7614 - val_acc: 0.5670\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8866 - acc: 0.8086 - val_loss: 1.7506 - val_acc: 0.5567\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8746 - acc: 0.8180 - val_loss: 1.7353 - val_acc: 0.5768\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8679 - acc: 0.8185 - val_loss: 1.9328 - val_acc: 0.5493\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8682 - acc: 0.8271 - val_loss: 1.7620 - val_acc: 0.5496\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8635 - acc: 0.8230 - val_loss: 1.7328 - val_acc: 0.5528\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8544 - acc: 0.8272 - val_loss: 2.0146 - val_acc: 0.5481\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8563 - acc: 0.8323 - val_loss: 1.9475 - val_acc: 0.5481\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8568 - acc: 0.8322 - val_loss: 1.7576 - val_acc: 0.5930\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8465 - acc: 0.8347 - val_loss: 1.7085 - val_acc: 0.5827\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8548 - acc: 0.8340 - val_loss: 1.9022 - val_acc: 0.5394\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8449 - acc: 0.8354 - val_loss: 1.7507 - val_acc: 0.5682\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8315 - acc: 0.8432 - val_loss: 1.9368 - val_acc: 0.5496\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8353 - acc: 0.8432 - val_loss: 1.9228 - val_acc: 0.5693\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8312 - acc: 0.8431 - val_loss: 1.8187 - val_acc: 0.5524\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8320 - acc: 0.8422 - val_loss: 2.1783 - val_acc: 0.5106\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8230 - acc: 0.8523 - val_loss: 1.8814 - val_acc: 0.5504\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8357 - acc: 0.8407 - val_loss: 1.7102 - val_acc: 0.5847\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8152 - acc: 0.8520 - val_loss: 1.6835 - val_acc: 0.5887\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8328 - acc: 0.8461 - val_loss: 2.0160 - val_acc: 0.5311\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8122 - acc: 0.8583 - val_loss: 1.7368 - val_acc: 0.5812\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8181 - acc: 0.8519 - val_loss: 1.6969 - val_acc: 0.5847\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8282 - acc: 0.8515 - val_loss: 1.7912 - val_acc: 0.5887\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8090 - acc: 0.8557 - val_loss: 1.7508 - val_acc: 0.5922\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8269 - acc: 0.8485 - val_loss: 1.9416 - val_acc: 0.5682\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8135 - acc: 0.8564 - val_loss: 1.8477 - val_acc: 0.5843\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.8152 - acc: 0.8530 - val_loss: 1.9244 - val_acc: 0.5686\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8165 - acc: 0.8549 - val_loss: 1.7049 - val_acc: 0.5985\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8025 - acc: 0.8605 - val_loss: 2.1049 - val_acc: 0.5429\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7930 - acc: 0.8643 - val_loss: 2.0407 - val_acc: 0.5634\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8145 - acc: 0.8547 - val_loss: 2.1578 - val_acc: 0.5244\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8130 - acc: 0.8586 - val_loss: 2.4460 - val_acc: 0.4917\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8028 - acc: 0.8628 - val_loss: 3.0107 - val_acc: 0.5043\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7991 - acc: 0.8634 - val_loss: 2.1705 - val_acc: 0.5311\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8037 - acc: 0.8617 - val_loss: 1.9008 - val_acc: 0.5823\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7989 - acc: 0.8657 - val_loss: 1.8593 - val_acc: 0.5658\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8002 - acc: 0.8655 - val_loss: 1.9996 - val_acc: 0.5477\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7961 - acc: 0.8645 - val_loss: 1.9154 - val_acc: 0.5749\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7897 - acc: 0.8704 - val_loss: 2.0178 - val_acc: 0.5524\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8007 - acc: 0.8646 - val_loss: 1.9912 - val_acc: 0.5595\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7964 - acc: 0.8678 - val_loss: 1.9337 - val_acc: 0.5757\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8013 - acc: 0.8614 - val_loss: 1.9993 - val_acc: 0.5808\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7809 - acc: 0.8691 - val_loss: 2.1114 - val_acc: 0.5284\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7964 - acc: 0.8629 - val_loss: 2.0496 - val_acc: 0.5670\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7887 - acc: 0.8685 - val_loss: 2.2573 - val_acc: 0.5165\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7937 - acc: 0.8694 - val_loss: 2.0074 - val_acc: 0.5654\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7915 - acc: 0.8679 - val_loss: 2.0472 - val_acc: 0.5607\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7985 - acc: 0.8630 - val_loss: 2.0675 - val_acc: 0.5292\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7920 - acc: 0.8701 - val_loss: 2.2734 - val_acc: 0.5284\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7873 - acc: 0.8694 - val_loss: 2.0182 - val_acc: 0.5496\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7944 - acc: 0.8686 - val_loss: 2.0258 - val_acc: 0.5599\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7890 - acc: 0.8693 - val_loss: 1.9966 - val_acc: 0.5465\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7787 - acc: 0.8735 - val_loss: 2.0749 - val_acc: 0.5638\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7886 - acc: 0.8704 - val_loss: 2.2328 - val_acc: 0.5315\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7885 - acc: 0.8730 - val_loss: 2.0677 - val_acc: 0.5666\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7895 - acc: 0.8782 - val_loss: 2.3402 - val_acc: 0.5240\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7910 - acc: 0.8710 - val_loss: 1.9184 - val_acc: 0.5737\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7717 - acc: 0.8770 - val_loss: 2.3373 - val_acc: 0.5394\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7775 - acc: 0.8721 - val_loss: 2.0503 - val_acc: 0.5733\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7891 - acc: 0.8698 - val_loss: 1.9509 - val_acc: 0.5796\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7700 - acc: 0.8758 - val_loss: 2.2318 - val_acc: 0.5559\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7784 - acc: 0.8783 - val_loss: 2.0637 - val_acc: 0.5678\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7895 - acc: 0.8697 - val_loss: 1.8805 - val_acc: 0.5788\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7761 - acc: 0.8737 - val_loss: 1.9660 - val_acc: 0.5634\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7818 - acc: 0.8748 - val_loss: 2.1792 - val_acc: 0.5347\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7697 - acc: 0.8800 - val_loss: 2.4265 - val_acc: 0.5508\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7815 - acc: 0.8768 - val_loss: 2.0731 - val_acc: 0.5615\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7738 - acc: 0.8783 - val_loss: 2.1097 - val_acc: 0.5619\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7664 - acc: 0.8774 - val_loss: 1.9481 - val_acc: 0.5729\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7849 - acc: 0.8731 - val_loss: 2.1771 - val_acc: 0.5583\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7791 - acc: 0.8780 - val_loss: 2.1751 - val_acc: 0.5512\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7747 - acc: 0.8780 - val_loss: 1.9599 - val_acc: 0.5784\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7766 - acc: 0.8771 - val_loss: 2.0159 - val_acc: 0.5792\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7614 - acc: 0.8820 - val_loss: 2.0973 - val_acc: 0.5662\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7600 - acc: 0.8840 - val_loss: 2.4447 - val_acc: 0.5039\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7702 - acc: 0.8785 - val_loss: 2.0492 - val_acc: 0.5500\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7627 - acc: 0.8829 - val_loss: 2.4436 - val_acc: 0.5307\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7764 - acc: 0.8733 - val_loss: 2.4034 - val_acc: 0.5260\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7592 - acc: 0.8866 - val_loss: 1.9539 - val_acc: 0.5835\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7687 - acc: 0.8818 - val_loss: 2.0747 - val_acc: 0.5686\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7763 - acc: 0.8782 - val_loss: 2.5248 - val_acc: 0.5102\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7709 - acc: 0.8820 - val_loss: 2.1187 - val_acc: 0.5559\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7768 - acc: 0.8795 - val_loss: 2.2256 - val_acc: 0.5429\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7637 - acc: 0.8816 - val_loss: 2.1559 - val_acc: 0.5686\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7751 - acc: 0.8760 - val_loss: 2.0546 - val_acc: 0.5426\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7693 - acc: 0.8799 - val_loss: 1.9780 - val_acc: 0.5678\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7628 - acc: 0.8804 - val_loss: 2.1779 - val_acc: 0.5493\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7643 - acc: 0.8814 - val_loss: 2.2292 - val_acc: 0.5733\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7596 - acc: 0.8827 - val_loss: 2.3112 - val_acc: 0.5327\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7657 - acc: 0.8801 - val_loss: 2.0149 - val_acc: 0.5717\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7652 - acc: 0.8820 - val_loss: 2.2151 - val_acc: 0.5556\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7606 - acc: 0.8835 - val_loss: 2.1374 - val_acc: 0.5603\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7540 - acc: 0.8809 - val_loss: 2.0906 - val_acc: 0.5812\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7451 - acc: 0.8872 - val_loss: 2.3470 - val_acc: 0.5292\n",
            "Epoch 127/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7672 - acc: 0.8792 - val_loss: 1.9940 - val_acc: 0.5717\n",
            "Epoch 128/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7631 - acc: 0.8788 - val_loss: 2.0793 - val_acc: 0.5745\n",
            "Epoch 129/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7597 - acc: 0.8814 - val_loss: 1.9623 - val_acc: 0.5697\n",
            "Epoch 130/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7578 - acc: 0.8844 - val_loss: 2.3542 - val_acc: 0.5315\n",
            "Epoch 131/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7493 - acc: 0.8891 - val_loss: 2.0172 - val_acc: 0.5646\n",
            "Epoch 132/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7644 - acc: 0.8832 - val_loss: 2.2873 - val_acc: 0.5579\n",
            "Epoch 133/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7504 - acc: 0.8863 - val_loss: 2.3318 - val_acc: 0.5296\n",
            "Epoch 134/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7663 - acc: 0.8827 - val_loss: 2.4152 - val_acc: 0.5327\n",
            "Epoch 135/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7489 - acc: 0.8860 - val_loss: 1.9210 - val_acc: 0.5713\n",
            "Epoch 136/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7519 - acc: 0.8861 - val_loss: 2.3919 - val_acc: 0.5402\n",
            "Epoch 137/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7610 - acc: 0.8808 - val_loss: 2.3316 - val_acc: 0.5611\n",
            "Epoch 138/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7571 - acc: 0.8836 - val_loss: 2.1507 - val_acc: 0.5615\n",
            "Epoch 139/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7521 - acc: 0.8843 - val_loss: 2.0800 - val_acc: 0.5642\n",
            "Epoch 140/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7484 - acc: 0.8869 - val_loss: 1.8452 - val_acc: 0.5894\n",
            "Epoch 141/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7511 - acc: 0.8856 - val_loss: 2.3547 - val_acc: 0.5398\n",
            "Epoch 142/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7544 - acc: 0.8858 - val_loss: 1.9624 - val_acc: 0.5792\n",
            "Epoch 143/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7496 - acc: 0.8874 - val_loss: 2.0379 - val_acc: 0.5630\n",
            "Epoch 144/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7465 - acc: 0.8840 - val_loss: 1.9999 - val_acc: 0.5823\n",
            "Epoch 145/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7566 - acc: 0.8867 - val_loss: 2.2619 - val_acc: 0.5536\n",
            "Epoch 146/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7482 - acc: 0.8884 - val_loss: 2.1661 - val_acc: 0.5615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|         | 16/256 [2:44:58<40:58:19, 614.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 506us/step - loss: 6.1153 - acc: 0.2901 - val_loss: 5.6969 - val_acc: 0.2908\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 4.5084 - acc: 0.3651 - val_loss: 3.8632 - val_acc: 0.3589\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 3.3506 - acc: 0.4441 - val_loss: 2.9831 - val_acc: 0.4204\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 2.5779 - acc: 0.5082 - val_loss: 2.3991 - val_acc: 0.4594\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 2.0741 - acc: 0.5483 - val_loss: 1.9685 - val_acc: 0.5102\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.7419 - acc: 0.5959 - val_loss: 1.8202 - val_acc: 0.5280\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 1.5399 - acc: 0.6202 - val_loss: 1.7016 - val_acc: 0.5299\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 1.4362 - acc: 0.6259 - val_loss: 1.7636 - val_acc: 0.4984\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 1.3323 - acc: 0.6412 - val_loss: 1.5693 - val_acc: 0.5528\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.2773 - acc: 0.6480 - val_loss: 1.5572 - val_acc: 0.5611\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 1.2416 - acc: 0.6632 - val_loss: 1.6616 - val_acc: 0.5256\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.1934 - acc: 0.6775 - val_loss: 1.4899 - val_acc: 0.5678\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 1.1411 - acc: 0.6958 - val_loss: 1.6954 - val_acc: 0.5315\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.1051 - acc: 0.7110 - val_loss: 1.7216 - val_acc: 0.5315\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 1.0704 - acc: 0.7246 - val_loss: 1.6179 - val_acc: 0.5512\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 1.0642 - acc: 0.7356 - val_loss: 1.4559 - val_acc: 0.6005\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 1.0502 - acc: 0.7376 - val_loss: 1.6055 - val_acc: 0.5575\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 1.0395 - acc: 0.7412 - val_loss: 1.5193 - val_acc: 0.5965\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.0112 - acc: 0.7588 - val_loss: 1.5793 - val_acc: 0.5954\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.0013 - acc: 0.7606 - val_loss: 1.7534 - val_acc: 0.5595\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 1.0140 - acc: 0.7642 - val_loss: 1.6045 - val_acc: 0.5820\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.9787 - acc: 0.7806 - val_loss: 1.6381 - val_acc: 0.5816\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.9689 - acc: 0.7865 - val_loss: 1.6093 - val_acc: 0.5835\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.9327 - acc: 0.8017 - val_loss: 1.5157 - val_acc: 0.6072\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.9500 - acc: 0.7926 - val_loss: 1.7475 - val_acc: 0.5800\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.9409 - acc: 0.8016 - val_loss: 1.7635 - val_acc: 0.5579\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.9521 - acc: 0.8032 - val_loss: 2.4213 - val_acc: 0.4870\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.9465 - acc: 0.8019 - val_loss: 1.9283 - val_acc: 0.5556\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8969 - acc: 0.8244 - val_loss: 1.7425 - val_acc: 0.6052\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.9058 - acc: 0.8262 - val_loss: 1.6796 - val_acc: 0.5855\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.9137 - acc: 0.8201 - val_loss: 1.8015 - val_acc: 0.5804\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8616 - acc: 0.8435 - val_loss: 2.2546 - val_acc: 0.5347\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8890 - acc: 0.8327 - val_loss: 2.0703 - val_acc: 0.5429\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8539 - acc: 0.8489 - val_loss: 1.8102 - val_acc: 0.5697\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8586 - acc: 0.8462 - val_loss: 2.0003 - val_acc: 0.5512\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8858 - acc: 0.8334 - val_loss: 2.3230 - val_acc: 0.5268\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8973 - acc: 0.8347 - val_loss: 1.7842 - val_acc: 0.5918\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8570 - acc: 0.8515 - val_loss: 1.8866 - val_acc: 0.5796\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8436 - acc: 0.8516 - val_loss: 2.0541 - val_acc: 0.5623\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8536 - acc: 0.8545 - val_loss: 1.8987 - val_acc: 0.5749\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8528 - acc: 0.8520 - val_loss: 2.0989 - val_acc: 0.5607\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8470 - acc: 0.8573 - val_loss: 1.8270 - val_acc: 0.5954\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8144 - acc: 0.8679 - val_loss: 1.8246 - val_acc: 0.5745\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8318 - acc: 0.8566 - val_loss: 2.1984 - val_acc: 0.5201\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8224 - acc: 0.8700 - val_loss: 2.5688 - val_acc: 0.4972\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8400 - acc: 0.8595 - val_loss: 1.7788 - val_acc: 0.5914\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.8098 - acc: 0.8754 - val_loss: 2.5540 - val_acc: 0.4886\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8010 - acc: 0.8678 - val_loss: 1.8392 - val_acc: 0.5725\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8456 - acc: 0.8574 - val_loss: 2.1037 - val_acc: 0.5410\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8510 - acc: 0.8612 - val_loss: 1.9448 - val_acc: 0.5764\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8361 - acc: 0.8665 - val_loss: 2.2446 - val_acc: 0.5524\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8288 - acc: 0.8688 - val_loss: 2.0485 - val_acc: 0.5536\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7967 - acc: 0.8749 - val_loss: 2.6305 - val_acc: 0.4909\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7989 - acc: 0.8779 - val_loss: 2.9962 - val_acc: 0.5158\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8268 - acc: 0.8662 - val_loss: 1.8910 - val_acc: 0.5733\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.7902 - acc: 0.8790 - val_loss: 1.8123 - val_acc: 0.5835\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7739 - acc: 0.8839 - val_loss: 1.8773 - val_acc: 0.5784\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8071 - acc: 0.8734 - val_loss: 1.9552 - val_acc: 0.5642\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7903 - acc: 0.8788 - val_loss: 2.3760 - val_acc: 0.5079\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8179 - acc: 0.8697 - val_loss: 2.1748 - val_acc: 0.5422\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.7990 - acc: 0.8796 - val_loss: 2.3520 - val_acc: 0.5236\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7904 - acc: 0.8796 - val_loss: 2.0857 - val_acc: 0.5540\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7879 - acc: 0.8829 - val_loss: 1.9584 - val_acc: 0.5733\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8263 - acc: 0.8722 - val_loss: 2.3746 - val_acc: 0.5256\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8031 - acc: 0.8801 - val_loss: 2.0959 - val_acc: 0.5563\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7861 - acc: 0.8813 - val_loss: 2.4944 - val_acc: 0.5095\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8170 - acc: 0.8734 - val_loss: 1.9493 - val_acc: 0.5800\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7969 - acc: 0.8816 - val_loss: 2.0593 - val_acc: 0.5556\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.8332 - acc: 0.8678 - val_loss: 2.0300 - val_acc: 0.5946\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7602 - acc: 0.8953 - val_loss: 1.9248 - val_acc: 0.5678\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7464 - acc: 0.8944 - val_loss: 2.2525 - val_acc: 0.5520\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7556 - acc: 0.8928 - val_loss: 2.1509 - val_acc: 0.5847\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7435 - acc: 0.8964 - val_loss: 1.9041 - val_acc: 0.5989\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7381 - acc: 0.8943 - val_loss: 1.8885 - val_acc: 0.5965\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7838 - acc: 0.8805 - val_loss: 2.4187 - val_acc: 0.5106\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8223 - acc: 0.8713 - val_loss: 1.9759 - val_acc: 0.5753\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8092 - acc: 0.8775 - val_loss: 2.4042 - val_acc: 0.5481\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7793 - acc: 0.8893 - val_loss: 1.9216 - val_acc: 0.5690\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7745 - acc: 0.8851 - val_loss: 1.8824 - val_acc: 0.5973\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7591 - acc: 0.8895 - val_loss: 2.1452 - val_acc: 0.5556\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7672 - acc: 0.8894 - val_loss: 2.0111 - val_acc: 0.5843\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8052 - acc: 0.8784 - val_loss: 2.2036 - val_acc: 0.5571\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7923 - acc: 0.8814 - val_loss: 2.4933 - val_acc: 0.5236\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7740 - acc: 0.8866 - val_loss: 1.9526 - val_acc: 0.5957\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7716 - acc: 0.8925 - val_loss: 2.0598 - val_acc: 0.5776\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7565 - acc: 0.8932 - val_loss: 2.1690 - val_acc: 0.5725\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7564 - acc: 0.8922 - val_loss: 2.1465 - val_acc: 0.5705\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7704 - acc: 0.8843 - val_loss: 2.1083 - val_acc: 0.5753\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7552 - acc: 0.8964 - val_loss: 2.5115 - val_acc: 0.5355\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 443us/step - loss: 0.7563 - acc: 0.8942 - val_loss: 2.3246 - val_acc: 0.5583\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7804 - acc: 0.8838 - val_loss: 1.9357 - val_acc: 0.5957\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7710 - acc: 0.8911 - val_loss: 1.9266 - val_acc: 0.5922\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7883 - acc: 0.8857 - val_loss: 2.0485 - val_acc: 0.5831\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7495 - acc: 0.8992 - val_loss: 2.0703 - val_acc: 0.5690\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  7%|         | 17/256 [2:52:00<36:58:24, 556.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 504us/step - loss: 6.0908 - acc: 0.2872 - val_loss: 5.3545 - val_acc: 0.2928\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 3.9815 - acc: 0.3301 - val_loss: 3.0456 - val_acc: 0.3629\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 2.8044 - acc: 0.4081 - val_loss: 2.3133 - val_acc: 0.4133\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 2.1804 - acc: 0.4692 - val_loss: 1.8741 - val_acc: 0.4819\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 1.8425 - acc: 0.5107 - val_loss: 1.9254 - val_acc: 0.4299\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 1.6233 - acc: 0.5414 - val_loss: 1.6924 - val_acc: 0.4764\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 1.4901 - acc: 0.5696 - val_loss: 1.7420 - val_acc: 0.4898\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 1.3769 - acc: 0.5949 - val_loss: 1.4384 - val_acc: 0.5481\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.3302 - acc: 0.5999 - val_loss: 1.4900 - val_acc: 0.5221\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 1.2543 - acc: 0.6286 - val_loss: 1.6357 - val_acc: 0.4988\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.2166 - acc: 0.6371 - val_loss: 1.9059 - val_acc: 0.4681\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 1.1627 - acc: 0.6528 - val_loss: 1.9196 - val_acc: 0.5059\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.1384 - acc: 0.6653 - val_loss: 1.4336 - val_acc: 0.5579\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 1.1091 - acc: 0.6768 - val_loss: 1.3824 - val_acc: 0.5705\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 1.0820 - acc: 0.6917 - val_loss: 1.4089 - val_acc: 0.5796\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.0634 - acc: 0.6973 - val_loss: 1.4807 - val_acc: 0.5477\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 1.0472 - acc: 0.7128 - val_loss: 1.9471 - val_acc: 0.4965\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 1.0252 - acc: 0.7220 - val_loss: 1.5297 - val_acc: 0.5571\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.0206 - acc: 0.7249 - val_loss: 1.7301 - val_acc: 0.5114\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 1.0058 - acc: 0.7293 - val_loss: 1.6928 - val_acc: 0.5225\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.9780 - acc: 0.7461 - val_loss: 1.5585 - val_acc: 0.5827\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.9739 - acc: 0.7525 - val_loss: 1.6148 - val_acc: 0.5406\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.9734 - acc: 0.7524 - val_loss: 1.5344 - val_acc: 0.5820\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.9503 - acc: 0.7644 - val_loss: 1.7060 - val_acc: 0.5485\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.9547 - acc: 0.7680 - val_loss: 1.5974 - val_acc: 0.5879\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.9435 - acc: 0.7732 - val_loss: 2.3208 - val_acc: 0.5067\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.9241 - acc: 0.7877 - val_loss: 1.6283 - val_acc: 0.5804\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.9188 - acc: 0.7904 - val_loss: 1.9246 - val_acc: 0.5095\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.9070 - acc: 0.7958 - val_loss: 1.6361 - val_acc: 0.5784\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.9154 - acc: 0.7913 - val_loss: 1.8435 - val_acc: 0.5406\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.9117 - acc: 0.7970 - val_loss: 1.5941 - val_acc: 0.5863\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8814 - acc: 0.8153 - val_loss: 1.8224 - val_acc: 0.5485\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8810 - acc: 0.8153 - val_loss: 1.9979 - val_acc: 0.5532\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8941 - acc: 0.8112 - val_loss: 1.7585 - val_acc: 0.5725\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8749 - acc: 0.8198 - val_loss: 1.7106 - val_acc: 0.5611\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8827 - acc: 0.8165 - val_loss: 1.9468 - val_acc: 0.5378\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.8782 - acc: 0.8222 - val_loss: 1.8279 - val_acc: 0.5686\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.8685 - acc: 0.8249 - val_loss: 1.6680 - val_acc: 0.5800\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.8551 - acc: 0.8329 - val_loss: 1.8964 - val_acc: 0.5567\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8622 - acc: 0.8290 - val_loss: 1.8006 - val_acc: 0.5615\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.8593 - acc: 0.8331 - val_loss: 2.6154 - val_acc: 0.4708\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.8670 - acc: 0.8303 - val_loss: 1.8831 - val_acc: 0.5469\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8537 - acc: 0.8328 - val_loss: 1.7720 - val_acc: 0.5642\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8503 - acc: 0.8403 - val_loss: 2.0880 - val_acc: 0.5374\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8372 - acc: 0.8445 - val_loss: 1.7949 - val_acc: 0.5749\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.8483 - acc: 0.8374 - val_loss: 2.0673 - val_acc: 0.5441\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8430 - acc: 0.8437 - val_loss: 2.1862 - val_acc: 0.5441\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8328 - acc: 0.8471 - val_loss: 1.9610 - val_acc: 0.5670\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8342 - acc: 0.8443 - val_loss: 1.7403 - val_acc: 0.5705\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8348 - acc: 0.8474 - val_loss: 2.3010 - val_acc: 0.5071\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8353 - acc: 0.8469 - val_loss: 1.8589 - val_acc: 0.5587\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.8356 - acc: 0.8422 - val_loss: 2.0456 - val_acc: 0.5410\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8362 - acc: 0.8510 - val_loss: 1.9405 - val_acc: 0.5481\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.8264 - acc: 0.8529 - val_loss: 2.0193 - val_acc: 0.5717\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8157 - acc: 0.8564 - val_loss: 2.2299 - val_acc: 0.5469\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8195 - acc: 0.8521 - val_loss: 1.9131 - val_acc: 0.5603\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8232 - acc: 0.8543 - val_loss: 1.9829 - val_acc: 0.5429\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.8130 - acc: 0.8600 - val_loss: 1.9191 - val_acc: 0.5835\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8152 - acc: 0.8561 - val_loss: 2.0300 - val_acc: 0.5327\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8136 - acc: 0.8572 - val_loss: 2.1274 - val_acc: 0.5449\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8183 - acc: 0.8614 - val_loss: 1.9083 - val_acc: 0.5835\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8068 - acc: 0.8616 - val_loss: 2.2224 - val_acc: 0.5221\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8145 - acc: 0.8563 - val_loss: 1.8115 - val_acc: 0.5816\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.8091 - acc: 0.8621 - val_loss: 1.8747 - val_acc: 0.5906\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8124 - acc: 0.8574 - val_loss: 1.9185 - val_acc: 0.5623\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7983 - acc: 0.8666 - val_loss: 1.8586 - val_acc: 0.5843\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.8140 - acc: 0.8597 - val_loss: 3.1925 - val_acc: 0.4874\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8132 - acc: 0.8621 - val_loss: 2.2444 - val_acc: 0.5260\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8020 - acc: 0.8631 - val_loss: 2.0661 - val_acc: 0.5690\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8101 - acc: 0.8609 - val_loss: 2.0576 - val_acc: 0.5508\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8050 - acc: 0.8639 - val_loss: 1.7745 - val_acc: 0.5871\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.7933 - acc: 0.8635 - val_loss: 1.8508 - val_acc: 0.5693\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7946 - acc: 0.8669 - val_loss: 2.4453 - val_acc: 0.5335\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8022 - acc: 0.8687 - val_loss: 2.2971 - val_acc: 0.5256\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7877 - acc: 0.8710 - val_loss: 2.1555 - val_acc: 0.5516\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7974 - acc: 0.8639 - val_loss: 1.9199 - val_acc: 0.5571\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8031 - acc: 0.8643 - val_loss: 1.9315 - val_acc: 0.5776\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7957 - acc: 0.8668 - val_loss: 1.8542 - val_acc: 0.5757\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7882 - acc: 0.8719 - val_loss: 2.0116 - val_acc: 0.5705\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7924 - acc: 0.8687 - val_loss: 2.0703 - val_acc: 0.5690\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7887 - acc: 0.8717 - val_loss: 1.8968 - val_acc: 0.5835\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7928 - acc: 0.8721 - val_loss: 2.1019 - val_acc: 0.5607\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7939 - acc: 0.8672 - val_loss: 2.2657 - val_acc: 0.5177\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8015 - acc: 0.8668 - val_loss: 2.2607 - val_acc: 0.5493\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8029 - acc: 0.8634 - val_loss: 1.8137 - val_acc: 0.5843\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7963 - acc: 0.8735 - val_loss: 1.7613 - val_acc: 0.5926\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7957 - acc: 0.8705 - val_loss: 2.0965 - val_acc: 0.5764\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7871 - acc: 0.8741 - val_loss: 2.0881 - val_acc: 0.5567\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7904 - acc: 0.8703 - val_loss: 2.2636 - val_acc: 0.5429\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7807 - acc: 0.8734 - val_loss: 1.9126 - val_acc: 0.5804\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7912 - acc: 0.8747 - val_loss: 1.9050 - val_acc: 0.5784\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7901 - acc: 0.8741 - val_loss: 2.3901 - val_acc: 0.5536\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7789 - acc: 0.8779 - val_loss: 2.3708 - val_acc: 0.5311\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7880 - acc: 0.8706 - val_loss: 2.1077 - val_acc: 0.5410\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7788 - acc: 0.8755 - val_loss: 1.9959 - val_acc: 0.5693\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7907 - acc: 0.8690 - val_loss: 1.8492 - val_acc: 0.5898\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7787 - acc: 0.8822 - val_loss: 2.1301 - val_acc: 0.5524\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7806 - acc: 0.8732 - val_loss: 2.0087 - val_acc: 0.5760\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7903 - acc: 0.8734 - val_loss: 1.9259 - val_acc: 0.5827\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7782 - acc: 0.8750 - val_loss: 1.8928 - val_acc: 0.5772\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7667 - acc: 0.8808 - val_loss: 1.9679 - val_acc: 0.5623\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7726 - acc: 0.8795 - val_loss: 1.8198 - val_acc: 0.5946\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7887 - acc: 0.8736 - val_loss: 2.3796 - val_acc: 0.5205\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7732 - acc: 0.8764 - val_loss: 2.0210 - val_acc: 0.5650\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7880 - acc: 0.8730 - val_loss: 1.8946 - val_acc: 0.5693\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7738 - acc: 0.8780 - val_loss: 2.0521 - val_acc: 0.5772\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7627 - acc: 0.8811 - val_loss: 2.2788 - val_acc: 0.5221\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7738 - acc: 0.8786 - val_loss: 1.9849 - val_acc: 0.5965\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7747 - acc: 0.8782 - val_loss: 2.3235 - val_acc: 0.5386\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7688 - acc: 0.8789 - val_loss: 2.0459 - val_acc: 0.5634\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7762 - acc: 0.8770 - val_loss: 2.2708 - val_acc: 0.5559\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7742 - acc: 0.8780 - val_loss: 2.1943 - val_acc: 0.5646\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7882 - acc: 0.8681 - val_loss: 1.9182 - val_acc: 0.5879\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7646 - acc: 0.8826 - val_loss: 2.0237 - val_acc: 0.5571\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7676 - acc: 0.8772 - val_loss: 1.7777 - val_acc: 0.5993\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7775 - acc: 0.8783 - val_loss: 1.9339 - val_acc: 0.5934\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7685 - acc: 0.8822 - val_loss: 2.0240 - val_acc: 0.5678\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7649 - acc: 0.8794 - val_loss: 1.9406 - val_acc: 0.5957\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.7669 - acc: 0.8813 - val_loss: 2.1676 - val_acc: 0.5611\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7717 - acc: 0.8786 - val_loss: 2.2276 - val_acc: 0.5500\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7647 - acc: 0.8819 - val_loss: 2.0036 - val_acc: 0.5623\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7674 - acc: 0.8795 - val_loss: 2.0150 - val_acc: 0.5493\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7662 - acc: 0.8821 - val_loss: 2.0361 - val_acc: 0.5757\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7723 - acc: 0.8736 - val_loss: 1.9881 - val_acc: 0.5863\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7541 - acc: 0.8836 - val_loss: 2.0503 - val_acc: 0.5867\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7783 - acc: 0.8770 - val_loss: 2.0819 - val_acc: 0.5709\n",
            "Epoch 127/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7619 - acc: 0.8864 - val_loss: 1.9549 - val_acc: 0.5615\n",
            "Epoch 128/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7678 - acc: 0.8805 - val_loss: 2.1111 - val_acc: 0.5776\n",
            "Epoch 129/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7693 - acc: 0.8767 - val_loss: 1.9324 - val_acc: 0.5800\n",
            "Epoch 130/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7589 - acc: 0.8822 - val_loss: 2.0113 - val_acc: 0.5768\n",
            "Epoch 131/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7714 - acc: 0.8819 - val_loss: 2.1054 - val_acc: 0.5599\n",
            "Epoch 132/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7715 - acc: 0.8821 - val_loss: 2.0364 - val_acc: 0.5768\n",
            "Epoch 133/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.7652 - acc: 0.8831 - val_loss: 2.3294 - val_acc: 0.5422\n",
            "Epoch 134/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.7683 - acc: 0.8789 - val_loss: 1.7298 - val_acc: 0.6127\n",
            "Epoch 135/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.7519 - acc: 0.8839 - val_loss: 2.0876 - val_acc: 0.5646\n",
            "Epoch 136/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.7650 - acc: 0.8826 - val_loss: 1.8990 - val_acc: 0.5823\n",
            "Epoch 137/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7604 - acc: 0.8847 - val_loss: 2.0362 - val_acc: 0.5875\n",
            "Epoch 138/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.7580 - acc: 0.8852 - val_loss: 2.2622 - val_acc: 0.5626\n",
            "Epoch 139/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7671 - acc: 0.8817 - val_loss: 1.9612 - val_acc: 0.5847\n",
            "Epoch 140/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7520 - acc: 0.8846 - val_loss: 1.9131 - val_acc: 0.6032\n",
            "Epoch 141/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7496 - acc: 0.8873 - val_loss: 2.0153 - val_acc: 0.5717\n",
            "Epoch 142/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.7621 - acc: 0.8845 - val_loss: 2.1283 - val_acc: 0.5634\n",
            "Epoch 143/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7539 - acc: 0.8866 - val_loss: 2.1432 - val_acc: 0.5638\n",
            "Epoch 144/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7674 - acc: 0.8789 - val_loss: 1.8728 - val_acc: 0.5946\n",
            "Epoch 145/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7587 - acc: 0.8873 - val_loss: 2.0275 - val_acc: 0.5780\n",
            "Epoch 146/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7538 - acc: 0.8853 - val_loss: 2.6886 - val_acc: 0.5055\n",
            "Epoch 147/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7616 - acc: 0.8799 - val_loss: 2.1565 - val_acc: 0.5737\n",
            "Epoch 148/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7650 - acc: 0.8866 - val_loss: 3.0256 - val_acc: 0.5020\n",
            "Epoch 149/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7520 - acc: 0.8888 - val_loss: 1.8210 - val_acc: 0.5918\n",
            "Epoch 150/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7475 - acc: 0.8885 - val_loss: 2.0110 - val_acc: 0.5894\n",
            "Epoch 151/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7581 - acc: 0.8840 - val_loss: 1.9762 - val_acc: 0.5879\n",
            "Epoch 152/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7527 - acc: 0.8894 - val_loss: 2.2829 - val_acc: 0.5579\n",
            "Epoch 153/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7567 - acc: 0.8837 - val_loss: 2.2335 - val_acc: 0.5623\n",
            "Epoch 154/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7555 - acc: 0.8860 - val_loss: 2.2707 - val_acc: 0.5370\n",
            "Epoch 155/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7558 - acc: 0.8844 - val_loss: 2.2459 - val_acc: 0.5552\n",
            "Epoch 156/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7494 - acc: 0.8898 - val_loss: 2.0205 - val_acc: 0.5556\n",
            "Epoch 157/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7497 - acc: 0.8935 - val_loss: 2.8034 - val_acc: 0.5028\n",
            "Epoch 158/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7429 - acc: 0.8920 - val_loss: 2.5107 - val_acc: 0.5473\n",
            "Epoch 159/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7414 - acc: 0.8906 - val_loss: 2.0097 - val_acc: 0.5847\n",
            "Epoch 160/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7598 - acc: 0.8815 - val_loss: 2.0379 - val_acc: 0.5823\n",
            "Epoch 161/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7532 - acc: 0.8871 - val_loss: 2.9354 - val_acc: 0.4669\n",
            "Epoch 162/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7478 - acc: 0.8882 - val_loss: 2.4437 - val_acc: 0.5205\n",
            "Epoch 163/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7333 - acc: 0.8890 - val_loss: 2.4103 - val_acc: 0.5473\n",
            "Epoch 164/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7416 - acc: 0.8909 - val_loss: 2.1230 - val_acc: 0.5701\n",
            "Epoch 165/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7445 - acc: 0.8873 - val_loss: 2.3491 - val_acc: 0.5481\n",
            "Epoch 166/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7496 - acc: 0.8864 - val_loss: 2.0911 - val_acc: 0.5595\n",
            "Epoch 167/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7467 - acc: 0.8892 - val_loss: 1.9549 - val_acc: 0.5796\n",
            "Epoch 168/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7439 - acc: 0.8873 - val_loss: 1.8252 - val_acc: 0.6036\n",
            "Epoch 169/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7518 - acc: 0.8882 - val_loss: 2.6356 - val_acc: 0.4992\n",
            "Epoch 170/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7396 - acc: 0.8914 - val_loss: 2.0570 - val_acc: 0.5737\n",
            "Epoch 171/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7559 - acc: 0.8908 - val_loss: 2.2011 - val_acc: 0.5556\n",
            "Epoch 172/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7584 - acc: 0.8839 - val_loss: 2.3685 - val_acc: 0.5418\n",
            "Epoch 173/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7525 - acc: 0.8874 - val_loss: 1.9403 - val_acc: 0.5867\n",
            "Epoch 174/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7596 - acc: 0.8840 - val_loss: 2.2071 - val_acc: 0.5496\n",
            "Epoch 175/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7479 - acc: 0.8848 - val_loss: 2.1667 - val_acc: 0.5469\n",
            "Epoch 176/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7371 - acc: 0.8942 - val_loss: 2.0348 - val_acc: 0.5820\n",
            "Epoch 177/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7427 - acc: 0.8926 - val_loss: 2.3174 - val_acc: 0.5390\n",
            "Epoch 178/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7454 - acc: 0.8890 - val_loss: 2.0795 - val_acc: 0.5634\n",
            "Epoch 179/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7555 - acc: 0.8835 - val_loss: 1.9746 - val_acc: 0.5890\n",
            "Epoch 180/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7392 - acc: 0.8916 - val_loss: 2.3580 - val_acc: 0.5457\n",
            "Epoch 181/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7352 - acc: 0.8898 - val_loss: 2.5707 - val_acc: 0.5496\n",
            "Epoch 182/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7467 - acc: 0.8880 - val_loss: 1.9362 - val_acc: 0.5946\n",
            "Epoch 183/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7486 - acc: 0.8840 - val_loss: 2.0559 - val_acc: 0.5804\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  7%|         | 18/256 [3:05:02<41:16:51, 624.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 540us/step - loss: 6.1113 - acc: 0.2931 - val_loss: 5.5328 - val_acc: 0.2912\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 377us/step - loss: 4.4727 - acc: 0.3627 - val_loss: 3.9143 - val_acc: 0.3314\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 3.3796 - acc: 0.4293 - val_loss: 3.0228 - val_acc: 0.4019\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 2.5983 - acc: 0.5003 - val_loss: 2.3456 - val_acc: 0.4752\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 2.0940 - acc: 0.5454 - val_loss: 2.1442 - val_acc: 0.4500\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 1.7872 - acc: 0.5863 - val_loss: 1.9905 - val_acc: 0.4393\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 377us/step - loss: 1.5782 - acc: 0.6046 - val_loss: 1.8133 - val_acc: 0.4941\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 1.4510 - acc: 0.6162 - val_loss: 1.9907 - val_acc: 0.4417\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 1.3729 - acc: 0.6287 - val_loss: 1.6612 - val_acc: 0.5292\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 1.2664 - acc: 0.6514 - val_loss: 1.4856 - val_acc: 0.5808\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 367us/step - loss: 1.2384 - acc: 0.6595 - val_loss: 1.5278 - val_acc: 0.5532\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 1.1886 - acc: 0.6746 - val_loss: 1.5199 - val_acc: 0.5544\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 1.1920 - acc: 0.6657 - val_loss: 1.6147 - val_acc: 0.5591\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 1.1479 - acc: 0.6926 - val_loss: 1.7158 - val_acc: 0.5433\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 1.1541 - acc: 0.6888 - val_loss: 2.0878 - val_acc: 0.4779\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 1.0803 - acc: 0.7144 - val_loss: 1.7626 - val_acc: 0.5130\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 1.1030 - acc: 0.7166 - val_loss: 2.2779 - val_acc: 0.4519\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 1.0309 - acc: 0.7415 - val_loss: 1.6558 - val_acc: 0.5559\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 1.0270 - acc: 0.7474 - val_loss: 2.1505 - val_acc: 0.4866\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 1.0612 - acc: 0.7380 - val_loss: 2.5380 - val_acc: 0.4590\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 1.0795 - acc: 0.7400 - val_loss: 1.6613 - val_acc: 0.5571\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.0228 - acc: 0.7648 - val_loss: 1.7835 - val_acc: 0.5398\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 1.1020 - acc: 0.7374 - val_loss: 1.7808 - val_acc: 0.5331\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 1.0352 - acc: 0.7627 - val_loss: 1.6440 - val_acc: 0.5985\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.9359 - acc: 0.8005 - val_loss: 1.7058 - val_acc: 0.5721\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.9617 - acc: 0.7963 - val_loss: 1.8644 - val_acc: 0.5504\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.9531 - acc: 0.7974 - val_loss: 1.8821 - val_acc: 0.5362\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.9788 - acc: 0.7926 - val_loss: 1.7935 - val_acc: 0.5717\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.9386 - acc: 0.8059 - val_loss: 1.6512 - val_acc: 0.5816\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8841 - acc: 0.8299 - val_loss: 1.8193 - val_acc: 0.5650\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.9216 - acc: 0.8157 - val_loss: 1.6645 - val_acc: 0.5989\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.9741 - acc: 0.8043 - val_loss: 2.1889 - val_acc: 0.5008\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.8931 - acc: 0.8305 - val_loss: 1.8051 - val_acc: 0.5753\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.9159 - acc: 0.8280 - val_loss: 2.2356 - val_acc: 0.4996\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.9968 - acc: 0.8079 - val_loss: 2.2474 - val_acc: 0.5154\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 1.0118 - acc: 0.8043 - val_loss: 1.9655 - val_acc: 0.5556\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.9671 - acc: 0.8165 - val_loss: 1.9370 - val_acc: 0.5721\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8818 - acc: 0.8513 - val_loss: 1.8448 - val_acc: 0.5623\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.9404 - acc: 0.8257 - val_loss: 2.4773 - val_acc: 0.4850\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8923 - acc: 0.8423 - val_loss: 1.9280 - val_acc: 0.5630\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.9141 - acc: 0.8318 - val_loss: 1.7968 - val_acc: 0.5910\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8819 - acc: 0.8500 - val_loss: 1.7519 - val_acc: 0.5997\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8635 - acc: 0.8539 - val_loss: 1.9599 - val_acc: 0.5422\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.8887 - acc: 0.8431 - val_loss: 2.0162 - val_acc: 0.5461\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8170 - acc: 0.8709 - val_loss: 1.8548 - val_acc: 0.5851\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.8030 - acc: 0.8723 - val_loss: 2.1593 - val_acc: 0.5355\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.8270 - acc: 0.8607 - val_loss: 1.8604 - val_acc: 0.5760\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8201 - acc: 0.8679 - val_loss: 2.2370 - val_acc: 0.5394\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8960 - acc: 0.8401 - val_loss: 2.3289 - val_acc: 0.5134\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.8382 - acc: 0.8651 - val_loss: 1.9143 - val_acc: 0.5977\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8270 - acc: 0.8648 - val_loss: 2.0334 - val_acc: 0.5772\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8654 - acc: 0.8553 - val_loss: 2.5156 - val_acc: 0.4976\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.9441 - acc: 0.8319 - val_loss: 2.0932 - val_acc: 0.5634\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.8734 - acc: 0.8588 - val_loss: 2.1493 - val_acc: 0.5390\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.8597 - acc: 0.8647 - val_loss: 1.8873 - val_acc: 0.5808\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.8024 - acc: 0.8815 - val_loss: 2.2008 - val_acc: 0.5556\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.9635 - acc: 0.8209 - val_loss: 1.8755 - val_acc: 0.5965\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7851 - acc: 0.8910 - val_loss: 1.8761 - val_acc: 0.5890\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7476 - acc: 0.8972 - val_loss: 2.0119 - val_acc: 0.5741\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.7393 - acc: 0.8958 - val_loss: 3.0984 - val_acc: 0.4957\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 1.0790 - acc: 0.7907 - val_loss: 2.1906 - val_acc: 0.5418\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.9482 - acc: 0.8318 - val_loss: 1.8368 - val_acc: 0.5914\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.8249 - acc: 0.8768 - val_loss: 1.7246 - val_acc: 0.6194\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7188 - acc: 0.9121 - val_loss: 1.9973 - val_acc: 0.5445\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.7724 - acc: 0.8844 - val_loss: 1.8418 - val_acc: 0.5800\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8072 - acc: 0.8755 - val_loss: 2.1883 - val_acc: 0.5540\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8677 - acc: 0.8561 - val_loss: 2.3091 - val_acc: 0.5299\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.9419 - acc: 0.8340 - val_loss: 1.9039 - val_acc: 0.5753\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8386 - acc: 0.8678 - val_loss: 2.4886 - val_acc: 0.5169\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.7935 - acc: 0.8879 - val_loss: 1.9481 - val_acc: 0.5827\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.7343 - acc: 0.9009 - val_loss: 1.7499 - val_acc: 0.6107\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.7978 - acc: 0.8733 - val_loss: 1.8726 - val_acc: 0.5985\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.8658 - acc: 0.8537 - val_loss: 2.2260 - val_acc: 0.5461\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.8941 - acc: 0.8523 - val_loss: 2.0317 - val_acc: 0.5626\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.9000 - acc: 0.8575 - val_loss: 2.0356 - val_acc: 0.5524\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.8149 - acc: 0.8849 - val_loss: 2.0208 - val_acc: 0.5729\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7288 - acc: 0.9081 - val_loss: 2.0917 - val_acc: 0.5603\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.7819 - acc: 0.8878 - val_loss: 2.1661 - val_acc: 0.5461\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7793 - acc: 0.8875 - val_loss: 1.8232 - val_acc: 0.6217\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.8182 - acc: 0.8717 - val_loss: 2.7654 - val_acc: 0.4984\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.9411 - acc: 0.8343 - val_loss: 2.3053 - val_acc: 0.5418\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8674 - acc: 0.8677 - val_loss: 2.0094 - val_acc: 0.5792\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.8776 - acc: 0.8620 - val_loss: 1.9284 - val_acc: 0.5831\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8810 - acc: 0.8633 - val_loss: 1.9643 - val_acc: 0.5843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  7%|         | 19/256 [3:10:55<35:44:12, 542.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 491us/step - loss: 6.0165 - acc: 0.2835 - val_loss: 5.4517 - val_acc: 0.2467\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 3.8996 - acc: 0.3464 - val_loss: 3.3904 - val_acc: 0.2959\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 2.8257 - acc: 0.4103 - val_loss: 2.7479 - val_acc: 0.3526\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 2.2012 - acc: 0.4717 - val_loss: 2.2706 - val_acc: 0.3783\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 1.8548 - acc: 0.5196 - val_loss: 2.1807 - val_acc: 0.4188\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 1.6634 - acc: 0.5304 - val_loss: 2.4785 - val_acc: 0.3873\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 1.5169 - acc: 0.5640 - val_loss: 2.2190 - val_acc: 0.3865\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 1.3905 - acc: 0.5868 - val_loss: 2.4254 - val_acc: 0.4098\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 1.3444 - acc: 0.5945 - val_loss: 2.4536 - val_acc: 0.3980\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 1.2654 - acc: 0.6250 - val_loss: 2.4816 - val_acc: 0.4039\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 1.2287 - acc: 0.6250 - val_loss: 1.9207 - val_acc: 0.4504\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 1.1761 - acc: 0.6521 - val_loss: 2.5425 - val_acc: 0.3834\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 1.1507 - acc: 0.6585 - val_loss: 3.2563 - val_acc: 0.3719\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 1.1179 - acc: 0.6732 - val_loss: 1.8249 - val_acc: 0.5142\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 1.0831 - acc: 0.6911 - val_loss: 1.5929 - val_acc: 0.5477\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 1.0775 - acc: 0.6905 - val_loss: 2.3199 - val_acc: 0.4606\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 1.0605 - acc: 0.7043 - val_loss: 2.2382 - val_acc: 0.4421\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 1.0403 - acc: 0.7130 - val_loss: 2.3637 - val_acc: 0.3913\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 1.0196 - acc: 0.7299 - val_loss: 2.7496 - val_acc: 0.4031\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 1.0089 - acc: 0.7326 - val_loss: 2.2772 - val_acc: 0.4523\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.9852 - acc: 0.7454 - val_loss: 2.6804 - val_acc: 0.3924\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.9789 - acc: 0.7533 - val_loss: 2.2493 - val_acc: 0.4626\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.9811 - acc: 0.7523 - val_loss: 2.7968 - val_acc: 0.4058\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.9576 - acc: 0.7683 - val_loss: 2.4320 - val_acc: 0.4188\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.9543 - acc: 0.7732 - val_loss: 2.7622 - val_acc: 0.4303\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.9384 - acc: 0.7801 - val_loss: 2.7357 - val_acc: 0.4267\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.9339 - acc: 0.7789 - val_loss: 2.0180 - val_acc: 0.5362\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.9155 - acc: 0.7935 - val_loss: 1.7598 - val_acc: 0.5697\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.9194 - acc: 0.7908 - val_loss: 2.8041 - val_acc: 0.4421\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.9070 - acc: 0.7971 - val_loss: 2.4496 - val_acc: 0.4972\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.9025 - acc: 0.8002 - val_loss: 2.1532 - val_acc: 0.5012\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8978 - acc: 0.8045 - val_loss: 3.3765 - val_acc: 0.4354\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.8872 - acc: 0.8104 - val_loss: 3.5473 - val_acc: 0.4043\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.8961 - acc: 0.8107 - val_loss: 2.1080 - val_acc: 0.5091\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.8888 - acc: 0.8098 - val_loss: 2.1271 - val_acc: 0.5299\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8831 - acc: 0.8203 - val_loss: 1.9299 - val_acc: 0.5256\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.8705 - acc: 0.8231 - val_loss: 2.1906 - val_acc: 0.5059\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8698 - acc: 0.8269 - val_loss: 3.7130 - val_acc: 0.4177\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.8684 - acc: 0.8270 - val_loss: 2.7410 - val_acc: 0.4464\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8635 - acc: 0.8272 - val_loss: 1.9886 - val_acc: 0.5398\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.8615 - acc: 0.8298 - val_loss: 2.9628 - val_acc: 0.4634\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.8669 - acc: 0.8301 - val_loss: 2.8695 - val_acc: 0.4413\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.8577 - acc: 0.8358 - val_loss: 2.8843 - val_acc: 0.4350\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8517 - acc: 0.8385 - val_loss: 2.7777 - val_acc: 0.4448\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8513 - acc: 0.8342 - val_loss: 2.6622 - val_acc: 0.4590\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.8576 - acc: 0.8351 - val_loss: 2.9604 - val_acc: 0.4330\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.8393 - acc: 0.8468 - val_loss: 2.4706 - val_acc: 0.4886\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8451 - acc: 0.8452 - val_loss: 3.2346 - val_acc: 0.4413\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.8435 - acc: 0.8456 - val_loss: 2.6075 - val_acc: 0.4673\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8355 - acc: 0.8452 - val_loss: 2.4576 - val_acc: 0.5158\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8369 - acc: 0.8437 - val_loss: 2.7082 - val_acc: 0.4795\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.8360 - acc: 0.8477 - val_loss: 2.5294 - val_acc: 0.4716\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.8345 - acc: 0.8498 - val_loss: 2.9443 - val_acc: 0.4539\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.8386 - acc: 0.8461 - val_loss: 2.7763 - val_acc: 0.4953\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8385 - acc: 0.8486 - val_loss: 2.2829 - val_acc: 0.5229\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.8294 - acc: 0.8527 - val_loss: 2.3039 - val_acc: 0.5039\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8299 - acc: 0.8515 - val_loss: 2.5218 - val_acc: 0.4622\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.8280 - acc: 0.8483 - val_loss: 2.3570 - val_acc: 0.5106\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8129 - acc: 0.8566 - val_loss: 2.3557 - val_acc: 0.4996\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.8232 - acc: 0.8558 - val_loss: 2.3929 - val_acc: 0.5256\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8304 - acc: 0.8486 - val_loss: 2.0994 - val_acc: 0.5429\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.8245 - acc: 0.8551 - val_loss: 2.8081 - val_acc: 0.4819\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8236 - acc: 0.8557 - val_loss: 1.9393 - val_acc: 0.5516\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8253 - acc: 0.8598 - val_loss: 3.1473 - val_acc: 0.4634\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.8195 - acc: 0.8539 - val_loss: 2.1077 - val_acc: 0.5299\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8216 - acc: 0.8583 - val_loss: 2.5053 - val_acc: 0.4768\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8118 - acc: 0.8613 - val_loss: 3.4083 - val_acc: 0.4212\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.8062 - acc: 0.8642 - val_loss: 4.5558 - val_acc: 0.3676\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.8143 - acc: 0.8610 - val_loss: 2.8765 - val_acc: 0.5059\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.8115 - acc: 0.8615 - val_loss: 1.7666 - val_acc: 0.5879\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.8164 - acc: 0.8609 - val_loss: 1.9981 - val_acc: 0.5406\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.8037 - acc: 0.8666 - val_loss: 2.5450 - val_acc: 0.4850\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.8060 - acc: 0.8646 - val_loss: 3.1539 - val_acc: 0.4456\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8189 - acc: 0.8596 - val_loss: 3.3281 - val_acc: 0.4523\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.8107 - acc: 0.8641 - val_loss: 2.7030 - val_acc: 0.4866\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8032 - acc: 0.8692 - val_loss: 2.9423 - val_acc: 0.4933\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8031 - acc: 0.8610 - val_loss: 2.9734 - val_acc: 0.4614\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8048 - acc: 0.8621 - val_loss: 2.3834 - val_acc: 0.4886\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8007 - acc: 0.8662 - val_loss: 2.2822 - val_acc: 0.5307\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7932 - acc: 0.8664 - val_loss: 2.3169 - val_acc: 0.5461\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7933 - acc: 0.8674 - val_loss: 2.4013 - val_acc: 0.5063\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7897 - acc: 0.8705 - val_loss: 2.7937 - val_acc: 0.4905\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7962 - acc: 0.8671 - val_loss: 3.7025 - val_acc: 0.4322\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7938 - acc: 0.8664 - val_loss: 1.9347 - val_acc: 0.5749\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7968 - acc: 0.8668 - val_loss: 2.1944 - val_acc: 0.5465\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7945 - acc: 0.8704 - val_loss: 2.6743 - val_acc: 0.5110\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7979 - acc: 0.8657 - val_loss: 3.3567 - val_acc: 0.4102\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7888 - acc: 0.8676 - val_loss: 2.9010 - val_acc: 0.4882\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8029 - acc: 0.8679 - val_loss: 2.2421 - val_acc: 0.5355\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7897 - acc: 0.8711 - val_loss: 3.5705 - val_acc: 0.4117\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7892 - acc: 0.8711 - val_loss: 2.7180 - val_acc: 0.5284\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7902 - acc: 0.8720 - val_loss: 2.6238 - val_acc: 0.5130\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8089 - acc: 0.8677 - val_loss: 2.6113 - val_acc: 0.4976\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7994 - acc: 0.8703 - val_loss: 3.1060 - val_acc: 0.4362\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7834 - acc: 0.8743 - val_loss: 2.9926 - val_acc: 0.4626\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7854 - acc: 0.8739 - val_loss: 2.2947 - val_acc: 0.5311\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7915 - acc: 0.8690 - val_loss: 2.2372 - val_acc: 0.5315\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7797 - acc: 0.8740 - val_loss: 4.2488 - val_acc: 0.3881\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7885 - acc: 0.8727 - val_loss: 2.3702 - val_acc: 0.5457\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7817 - acc: 0.8800 - val_loss: 3.2112 - val_acc: 0.4574\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.7896 - acc: 0.8764 - val_loss: 2.8628 - val_acc: 0.4649\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7933 - acc: 0.8719 - val_loss: 2.7243 - val_acc: 0.4823\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7847 - acc: 0.8732 - val_loss: 2.5257 - val_acc: 0.4984\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7822 - acc: 0.8742 - val_loss: 3.0917 - val_acc: 0.4460\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7875 - acc: 0.8746 - val_loss: 2.2631 - val_acc: 0.5315\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7886 - acc: 0.8762 - val_loss: 2.2994 - val_acc: 0.5248\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7940 - acc: 0.8686 - val_loss: 2.4081 - val_acc: 0.5296\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7864 - acc: 0.8732 - val_loss: 2.9752 - val_acc: 0.4559\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7789 - acc: 0.8769 - val_loss: 2.1643 - val_acc: 0.5536\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7754 - acc: 0.8766 - val_loss: 2.8216 - val_acc: 0.4921\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7686 - acc: 0.8806 - val_loss: 1.9560 - val_acc: 0.6009\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7769 - acc: 0.8775 - val_loss: 2.3677 - val_acc: 0.5118\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7711 - acc: 0.8824 - val_loss: 2.1429 - val_acc: 0.5461\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7804 - acc: 0.8762 - val_loss: 2.9639 - val_acc: 0.4972\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7841 - acc: 0.8767 - val_loss: 2.2718 - val_acc: 0.5343\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7802 - acc: 0.8793 - val_loss: 2.6465 - val_acc: 0.4921\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7688 - acc: 0.8842 - val_loss: 3.6461 - val_acc: 0.4326\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7668 - acc: 0.8816 - val_loss: 2.2286 - val_acc: 0.5244\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7779 - acc: 0.8781 - val_loss: 2.7435 - val_acc: 0.5016\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7732 - acc: 0.8805 - val_loss: 5.4200 - val_acc: 0.3388\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7791 - acc: 0.8797 - val_loss: 3.0392 - val_acc: 0.4708\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7806 - acc: 0.8768 - val_loss: 2.4740 - val_acc: 0.5051\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7686 - acc: 0.8803 - val_loss: 3.1805 - val_acc: 0.4728\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7846 - acc: 0.8752 - val_loss: 3.0646 - val_acc: 0.5000\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7629 - acc: 0.8847 - val_loss: 2.6144 - val_acc: 0.5020\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7618 - acc: 0.8865 - val_loss: 2.9060 - val_acc: 0.4961\n",
            "Epoch 127/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7731 - acc: 0.8795 - val_loss: 2.2460 - val_acc: 0.5445\n",
            "Epoch 128/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7691 - acc: 0.8847 - val_loss: 3.7144 - val_acc: 0.4381\n",
            "Epoch 129/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7643 - acc: 0.8807 - val_loss: 2.6868 - val_acc: 0.5158\n",
            "Epoch 130/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7774 - acc: 0.8761 - val_loss: 4.7504 - val_acc: 0.3771\n",
            "Epoch 131/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7744 - acc: 0.8815 - val_loss: 5.4416 - val_acc: 0.3191\n",
            "Epoch 132/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7726 - acc: 0.8854 - val_loss: 2.5550 - val_acc: 0.5067\n",
            "Epoch 133/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7683 - acc: 0.8800 - val_loss: 3.7973 - val_acc: 0.4653\n",
            "Epoch 134/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7746 - acc: 0.8804 - val_loss: 2.5479 - val_acc: 0.5189\n",
            "Epoch 135/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7686 - acc: 0.8819 - val_loss: 3.7095 - val_acc: 0.4157\n",
            "Epoch 136/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7543 - acc: 0.8831 - val_loss: 3.2364 - val_acc: 0.4567\n",
            "Epoch 137/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7536 - acc: 0.8889 - val_loss: 3.8818 - val_acc: 0.4247\n",
            "Epoch 138/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7616 - acc: 0.8849 - val_loss: 3.3183 - val_acc: 0.4547\n",
            "Epoch 139/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7720 - acc: 0.8812 - val_loss: 1.9455 - val_acc: 0.5961\n",
            "Epoch 140/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7741 - acc: 0.8799 - val_loss: 3.2124 - val_acc: 0.4649\n",
            "Epoch 141/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7710 - acc: 0.8826 - val_loss: 3.9619 - val_acc: 0.4031\n",
            "Epoch 142/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7664 - acc: 0.8829 - val_loss: 2.6121 - val_acc: 0.5043\n",
            "Epoch 143/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7576 - acc: 0.8888 - val_loss: 3.1059 - val_acc: 0.4866\n",
            "Epoch 144/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7775 - acc: 0.8767 - val_loss: 2.5522 - val_acc: 0.5402\n",
            "Epoch 145/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7706 - acc: 0.8803 - val_loss: 2.9704 - val_acc: 0.4677\n",
            "Epoch 146/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7618 - acc: 0.8852 - val_loss: 2.4911 - val_acc: 0.5327\n",
            "Epoch 147/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7623 - acc: 0.8836 - val_loss: 3.5979 - val_acc: 0.4504\n",
            "Epoch 148/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7677 - acc: 0.8802 - val_loss: 3.5165 - val_acc: 0.4724\n",
            "Epoch 149/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7605 - acc: 0.8859 - val_loss: 2.3201 - val_acc: 0.5595\n",
            "Epoch 150/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7552 - acc: 0.8868 - val_loss: 4.3048 - val_acc: 0.4247\n",
            "Epoch 151/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7619 - acc: 0.8834 - val_loss: 2.3447 - val_acc: 0.5323\n",
            "Epoch 152/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7657 - acc: 0.8807 - val_loss: 2.2235 - val_acc: 0.5524\n",
            "Epoch 153/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7591 - acc: 0.8858 - val_loss: 2.9913 - val_acc: 0.4957\n",
            "Epoch 154/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7632 - acc: 0.8848 - val_loss: 2.4819 - val_acc: 0.5063\n",
            "Epoch 155/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7556 - acc: 0.8860 - val_loss: 2.9336 - val_acc: 0.4953\n",
            "Epoch 156/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7558 - acc: 0.8857 - val_loss: 4.2732 - val_acc: 0.4362\n",
            "Epoch 157/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7502 - acc: 0.8887 - val_loss: 2.5429 - val_acc: 0.5095\n",
            "Epoch 158/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7550 - acc: 0.8860 - val_loss: 3.0665 - val_acc: 0.4701\n",
            "Epoch 159/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7750 - acc: 0.8785 - val_loss: 2.4317 - val_acc: 0.5508\n",
            "Epoch 160/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7577 - acc: 0.8853 - val_loss: 2.4285 - val_acc: 0.5307\n",
            "Epoch 161/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7640 - acc: 0.8837 - val_loss: 3.0304 - val_acc: 0.4559\n",
            "Epoch 162/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7697 - acc: 0.8827 - val_loss: 2.7861 - val_acc: 0.5138\n",
            "Epoch 163/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7725 - acc: 0.8805 - val_loss: 2.8486 - val_acc: 0.5020\n",
            "Epoch 164/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7541 - acc: 0.8880 - val_loss: 2.7872 - val_acc: 0.5102\n",
            "Epoch 165/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7621 - acc: 0.8837 - val_loss: 2.6302 - val_acc: 0.5059\n",
            "Epoch 166/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7553 - acc: 0.8868 - val_loss: 3.7295 - val_acc: 0.4590\n",
            "Epoch 167/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7491 - acc: 0.8925 - val_loss: 3.1387 - val_acc: 0.4614\n",
            "Epoch 168/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7560 - acc: 0.8891 - val_loss: 3.2822 - val_acc: 0.4681\n",
            "Epoch 169/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7569 - acc: 0.8869 - val_loss: 2.3539 - val_acc: 0.5221\n",
            "Epoch 170/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7502 - acc: 0.8907 - val_loss: 3.2620 - val_acc: 0.4472\n",
            "Epoch 171/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7610 - acc: 0.8885 - val_loss: 2.4909 - val_acc: 0.5426\n",
            "Epoch 172/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7545 - acc: 0.8878 - val_loss: 3.0794 - val_acc: 0.4689\n",
            "Epoch 173/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7563 - acc: 0.8861 - val_loss: 3.4034 - val_acc: 0.4649\n",
            "Epoch 174/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7537 - acc: 0.8870 - val_loss: 2.5024 - val_acc: 0.5134\n",
            "Epoch 175/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7505 - acc: 0.8847 - val_loss: 4.5343 - val_acc: 0.4015\n",
            "Epoch 176/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7557 - acc: 0.8869 - val_loss: 2.3465 - val_acc: 0.5142\n",
            "Epoch 177/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7484 - acc: 0.8906 - val_loss: 2.9446 - val_acc: 0.4638\n",
            "Epoch 178/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7595 - acc: 0.8846 - val_loss: 2.6251 - val_acc: 0.5181\n",
            "Epoch 179/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7542 - acc: 0.8896 - val_loss: 2.2715 - val_acc: 0.5607\n",
            "Epoch 180/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7699 - acc: 0.8818 - val_loss: 4.9333 - val_acc: 0.3889\n",
            "Epoch 181/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7677 - acc: 0.8874 - val_loss: 1.9755 - val_acc: 0.5890\n",
            "Epoch 182/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7601 - acc: 0.8854 - val_loss: 2.1184 - val_acc: 0.5749\n",
            "Epoch 183/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7537 - acc: 0.8899 - val_loss: 3.2567 - val_acc: 0.4752\n",
            "Epoch 184/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7636 - acc: 0.8820 - val_loss: 3.3151 - val_acc: 0.4748\n",
            "Epoch 185/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7567 - acc: 0.8874 - val_loss: 2.8279 - val_acc: 0.5035\n",
            "Epoch 186/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7483 - acc: 0.8921 - val_loss: 2.1935 - val_acc: 0.5449\n",
            "Epoch 187/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7515 - acc: 0.8888 - val_loss: 3.1321 - val_acc: 0.4736\n",
            "Epoch 188/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7516 - acc: 0.8878 - val_loss: 3.6206 - val_acc: 0.4811\n",
            "Epoch 189/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7503 - acc: 0.8914 - val_loss: 3.2599 - val_acc: 0.4752\n",
            "Epoch 190/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7557 - acc: 0.8872 - val_loss: 3.9443 - val_acc: 0.4370\n",
            "Epoch 191/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7592 - acc: 0.8883 - val_loss: 3.0193 - val_acc: 0.4405\n",
            "Epoch 192/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7569 - acc: 0.8884 - val_loss: 3.7820 - val_acc: 0.4173\n",
            "Epoch 193/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7536 - acc: 0.8897 - val_loss: 2.7352 - val_acc: 0.5110\n",
            "Epoch 194/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7543 - acc: 0.8878 - val_loss: 2.7400 - val_acc: 0.4894\n",
            "Epoch 195/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7547 - acc: 0.8880 - val_loss: 2.3935 - val_acc: 0.5292\n",
            "Epoch 196/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7460 - acc: 0.8885 - val_loss: 3.1450 - val_acc: 0.5110\n",
            "Epoch 197/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7637 - acc: 0.8866 - val_loss: 2.2811 - val_acc: 0.5351\n",
            "Epoch 198/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7471 - acc: 0.8911 - val_loss: 3.9056 - val_acc: 0.4200\n",
            "Epoch 199/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7560 - acc: 0.8863 - val_loss: 2.8851 - val_acc: 0.4901\n",
            "Epoch 200/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7527 - acc: 0.8872 - val_loss: 4.5724 - val_acc: 0.3885\n",
            "Epoch 201/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7446 - acc: 0.8907 - val_loss: 4.0614 - val_acc: 0.4188\n",
            "Epoch 202/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7563 - acc: 0.8872 - val_loss: 2.9007 - val_acc: 0.4732\n",
            "Epoch 203/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7463 - acc: 0.8903 - val_loss: 2.8066 - val_acc: 0.4933\n",
            "Epoch 204/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7484 - acc: 0.8912 - val_loss: 2.0670 - val_acc: 0.5713\n",
            "Epoch 205/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7476 - acc: 0.8888 - val_loss: 3.7273 - val_acc: 0.4752\n",
            "Epoch 206/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7436 - acc: 0.8906 - val_loss: 2.4585 - val_acc: 0.5110\n",
            "Epoch 207/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7518 - acc: 0.8914 - val_loss: 2.6424 - val_acc: 0.5118\n",
            "Epoch 208/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7439 - acc: 0.8934 - val_loss: 2.1560 - val_acc: 0.5563\n",
            "Epoch 209/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7490 - acc: 0.8892 - val_loss: 3.7810 - val_acc: 0.4314\n",
            "Epoch 210/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7415 - acc: 0.8928 - val_loss: 3.0370 - val_acc: 0.4771\n",
            "Epoch 211/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7452 - acc: 0.8909 - val_loss: 2.5686 - val_acc: 0.4933\n",
            "Epoch 212/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7525 - acc: 0.8907 - val_loss: 8.9567 - val_acc: 0.2770\n",
            "Epoch 213/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7524 - acc: 0.8872 - val_loss: 3.0051 - val_acc: 0.4720\n",
            "Epoch 214/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7417 - acc: 0.8913 - val_loss: 5.8494 - val_acc: 0.3448\n",
            "Epoch 215/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7525 - acc: 0.8879 - val_loss: 5.0279 - val_acc: 0.3983\n",
            "Epoch 216/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7490 - acc: 0.8899 - val_loss: 2.3642 - val_acc: 0.5469\n",
            "Epoch 217/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7375 - acc: 0.8929 - val_loss: 3.0536 - val_acc: 0.4945\n",
            "Epoch 218/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7451 - acc: 0.8891 - val_loss: 2.4036 - val_acc: 0.5382\n",
            "Epoch 219/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7458 - acc: 0.8930 - val_loss: 2.3850 - val_acc: 0.5240\n",
            "Epoch 220/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7534 - acc: 0.8875 - val_loss: 3.2205 - val_acc: 0.4708\n",
            "Epoch 221/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7460 - acc: 0.8872 - val_loss: 3.3122 - val_acc: 0.4539\n",
            "Epoch 222/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7373 - acc: 0.8957 - val_loss: 3.0357 - val_acc: 0.4736\n",
            "Epoch 223/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7443 - acc: 0.8937 - val_loss: 4.3396 - val_acc: 0.3991\n",
            "Epoch 224/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7531 - acc: 0.8880 - val_loss: 2.9833 - val_acc: 0.4957\n",
            "Epoch 225/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7437 - acc: 0.8938 - val_loss: 2.7113 - val_acc: 0.4752\n",
            "Epoch 226/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7503 - acc: 0.8904 - val_loss: 2.9610 - val_acc: 0.4838\n",
            "Epoch 227/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7482 - acc: 0.8916 - val_loss: 2.3028 - val_acc: 0.5599\n",
            "Epoch 228/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7386 - acc: 0.8951 - val_loss: 2.6743 - val_acc: 0.5209\n",
            "Epoch 229/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7466 - acc: 0.8931 - val_loss: 2.3232 - val_acc: 0.5402\n",
            "Epoch 230/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7547 - acc: 0.8842 - val_loss: 3.2679 - val_acc: 0.4708\n",
            "Epoch 231/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7393 - acc: 0.8911 - val_loss: 2.2016 - val_acc: 0.5481\n",
            "Epoch 232/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7519 - acc: 0.8888 - val_loss: 3.4718 - val_acc: 0.4255\n",
            "Epoch 233/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7389 - acc: 0.8919 - val_loss: 4.6977 - val_acc: 0.4287\n",
            "Epoch 234/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7528 - acc: 0.8910 - val_loss: 2.6363 - val_acc: 0.5169\n",
            "Epoch 235/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7449 - acc: 0.8923 - val_loss: 2.7902 - val_acc: 0.5240\n",
            "Epoch 236/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7595 - acc: 0.8858 - val_loss: 3.2433 - val_acc: 0.4681\n",
            "Epoch 237/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7443 - acc: 0.8933 - val_loss: 2.8498 - val_acc: 0.5063\n",
            "Epoch 238/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7360 - acc: 0.8948 - val_loss: 2.1022 - val_acc: 0.5910\n",
            "Epoch 239/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7256 - acc: 0.8947 - val_loss: 2.5453 - val_acc: 0.5402\n",
            "Epoch 240/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7518 - acc: 0.8902 - val_loss: 2.8274 - val_acc: 0.4905\n",
            "Epoch 241/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7382 - acc: 0.8954 - val_loss: 2.5197 - val_acc: 0.5339\n",
            "Epoch 242/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7396 - acc: 0.8945 - val_loss: 4.8682 - val_acc: 0.4350\n",
            "Epoch 243/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7459 - acc: 0.8943 - val_loss: 3.4170 - val_acc: 0.4405\n",
            "Epoch 244/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7424 - acc: 0.8930 - val_loss: 2.9279 - val_acc: 0.4933\n",
            "Epoch 245/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.7488 - acc: 0.8928 - val_loss: 2.3531 - val_acc: 0.5477\n",
            "Epoch 246/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7439 - acc: 0.8921 - val_loss: 2.4157 - val_acc: 0.5229\n",
            "Epoch 247/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7523 - acc: 0.8874 - val_loss: 2.5884 - val_acc: 0.5343\n",
            "Epoch 248/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.7377 - acc: 0.8983 - val_loss: 3.6423 - val_acc: 0.4850\n",
            "Epoch 249/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7410 - acc: 0.8919 - val_loss: 2.2400 - val_acc: 0.5378\n",
            "Epoch 250/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7293 - acc: 0.8955 - val_loss: 3.3616 - val_acc: 0.4539\n",
            "Epoch 251/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7521 - acc: 0.8885 - val_loss: 3.1292 - val_acc: 0.5095\n",
            "Epoch 252/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7403 - acc: 0.8938 - val_loss: 3.4734 - val_acc: 0.4779\n",
            "Epoch 253/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7425 - acc: 0.8952 - val_loss: 2.3075 - val_acc: 0.5496\n",
            "Epoch 254/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7442 - acc: 0.8913 - val_loss: 2.3728 - val_acc: 0.5465\n",
            "Epoch 255/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7249 - acc: 0.8958 - val_loss: 3.8382 - val_acc: 0.4224\n",
            "Epoch 256/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.7384 - acc: 0.8935 - val_loss: 2.5381 - val_acc: 0.5378\n",
            "Epoch 257/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7433 - acc: 0.8905 - val_loss: 3.9879 - val_acc: 0.4295\n",
            "Epoch 258/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7368 - acc: 0.8922 - val_loss: 2.2983 - val_acc: 0.5382\n",
            "Epoch 259/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7388 - acc: 0.8933 - val_loss: 3.7117 - val_acc: 0.4370\n",
            "Epoch 260/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7404 - acc: 0.8954 - val_loss: 2.7149 - val_acc: 0.5169\n",
            "Epoch 261/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7431 - acc: 0.8920 - val_loss: 2.4765 - val_acc: 0.5256\n",
            "Epoch 262/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7421 - acc: 0.8889 - val_loss: 2.5186 - val_acc: 0.5165\n",
            "Epoch 263/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7324 - acc: 0.8978 - val_loss: 3.5684 - val_acc: 0.4567\n",
            "Epoch 264/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7362 - acc: 0.8928 - val_loss: 2.5231 - val_acc: 0.5284\n",
            "Epoch 265/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7398 - acc: 0.8966 - val_loss: 3.6114 - val_acc: 0.4598\n",
            "Epoch 266/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7225 - acc: 0.9014 - val_loss: 2.8506 - val_acc: 0.5138\n",
            "Epoch 267/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7380 - acc: 0.8971 - val_loss: 2.7787 - val_acc: 0.5193\n",
            "Epoch 268/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7393 - acc: 0.8944 - val_loss: 3.2442 - val_acc: 0.4886\n",
            "Epoch 269/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7460 - acc: 0.8883 - val_loss: 2.5362 - val_acc: 0.4921\n",
            "Epoch 270/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7278 - acc: 0.8957 - val_loss: 3.3614 - val_acc: 0.4618\n",
            "Epoch 271/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7322 - acc: 0.8936 - val_loss: 3.0298 - val_acc: 0.4878\n",
            "Epoch 272/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7449 - acc: 0.8927 - val_loss: 4.2986 - val_acc: 0.3964\n",
            "Epoch 273/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7508 - acc: 0.8873 - val_loss: 3.0011 - val_acc: 0.4862\n",
            "Epoch 274/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7457 - acc: 0.8954 - val_loss: 2.4261 - val_acc: 0.5512\n",
            "Epoch 275/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7350 - acc: 0.8954 - val_loss: 3.9582 - val_acc: 0.4228\n",
            "Epoch 276/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7465 - acc: 0.8932 - val_loss: 2.7059 - val_acc: 0.5158\n",
            "Epoch 277/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7377 - acc: 0.8963 - val_loss: 2.7396 - val_acc: 0.5307\n",
            "Epoch 278/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.7281 - acc: 0.8953 - val_loss: 3.3159 - val_acc: 0.4819\n",
            "Epoch 279/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7477 - acc: 0.8903 - val_loss: 3.0631 - val_acc: 0.5099\n",
            "Epoch 280/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7321 - acc: 0.8934 - val_loss: 2.9677 - val_acc: 0.4697\n",
            "Epoch 281/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7361 - acc: 0.8947 - val_loss: 2.4342 - val_acc: 0.5500\n",
            "Epoch 282/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7361 - acc: 0.8902 - val_loss: 2.6799 - val_acc: 0.5087\n",
            "Epoch 283/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7434 - acc: 0.8944 - val_loss: 2.9546 - val_acc: 0.5185\n",
            "Epoch 284/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7380 - acc: 0.8958 - val_loss: 3.1239 - val_acc: 0.5221\n",
            "Epoch 285/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7365 - acc: 0.8952 - val_loss: 2.1596 - val_acc: 0.5682\n",
            "Epoch 286/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7369 - acc: 0.8929 - val_loss: 2.5095 - val_acc: 0.5181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|         | 20/256 [3:31:19<48:59:21, 747.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 530us/step - loss: 6.1040 - acc: 0.2886 - val_loss: 5.3232 - val_acc: 0.2876\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 4.4949 - acc: 0.3422 - val_loss: 3.8668 - val_acc: 0.3211\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 3.3439 - acc: 0.4271 - val_loss: 2.9731 - val_acc: 0.3755\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 2.5668 - acc: 0.4955 - val_loss: 2.4324 - val_acc: 0.4247\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 2.0734 - acc: 0.5410 - val_loss: 1.9828 - val_acc: 0.5185\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 1.7495 - acc: 0.5886 - val_loss: 1.8609 - val_acc: 0.4846\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 1.5278 - acc: 0.6132 - val_loss: 1.9311 - val_acc: 0.4551\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 1.4222 - acc: 0.6297 - val_loss: 1.6520 - val_acc: 0.5252\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 1.3296 - acc: 0.6433 - val_loss: 1.5163 - val_acc: 0.5721\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 1.2669 - acc: 0.6603 - val_loss: 1.7421 - val_acc: 0.5264\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 1.2079 - acc: 0.6702 - val_loss: 1.6338 - val_acc: 0.5362\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 1.1683 - acc: 0.6879 - val_loss: 2.0609 - val_acc: 0.4748\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 1.1403 - acc: 0.6994 - val_loss: 1.6442 - val_acc: 0.5591\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 1.0978 - acc: 0.7144 - val_loss: 2.0327 - val_acc: 0.4708\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 1.1106 - acc: 0.7103 - val_loss: 1.4359 - val_acc: 0.5898\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 1.0787 - acc: 0.7216 - val_loss: 1.5675 - val_acc: 0.5563\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 1.0764 - acc: 0.7336 - val_loss: 1.8139 - val_acc: 0.5059\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 1.0317 - acc: 0.7462 - val_loss: 1.8537 - val_acc: 0.5284\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.0385 - acc: 0.7465 - val_loss: 2.0003 - val_acc: 0.5000\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 1.0229 - acc: 0.7616 - val_loss: 1.7768 - val_acc: 0.5366\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.9995 - acc: 0.7696 - val_loss: 1.5410 - val_acc: 0.5930\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.9709 - acc: 0.7824 - val_loss: 1.6881 - val_acc: 0.5697\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.9899 - acc: 0.7748 - val_loss: 1.7437 - val_acc: 0.5630\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.9948 - acc: 0.7815 - val_loss: 1.7358 - val_acc: 0.5701\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.9526 - acc: 0.7959 - val_loss: 1.9619 - val_acc: 0.5351\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.9157 - acc: 0.8127 - val_loss: 1.9269 - val_acc: 0.5382\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.9632 - acc: 0.8007 - val_loss: 1.6802 - val_acc: 0.5827\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.9181 - acc: 0.8171 - val_loss: 1.5966 - val_acc: 0.5946\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.9016 - acc: 0.8234 - val_loss: 1.9876 - val_acc: 0.5493\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.9007 - acc: 0.8238 - val_loss: 1.6569 - val_acc: 0.5989\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.9180 - acc: 0.8227 - val_loss: 1.9276 - val_acc: 0.5658\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.9260 - acc: 0.8233 - val_loss: 1.8558 - val_acc: 0.5599\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.9040 - acc: 0.8320 - val_loss: 1.9979 - val_acc: 0.5445\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8781 - acc: 0.8416 - val_loss: 2.1737 - val_acc: 0.5264\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.9190 - acc: 0.8319 - val_loss: 2.0412 - val_acc: 0.5394\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8927 - acc: 0.8429 - val_loss: 1.7433 - val_acc: 0.5914\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8799 - acc: 0.8422 - val_loss: 1.9122 - val_acc: 0.5579\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8623 - acc: 0.8484 - val_loss: 1.9416 - val_acc: 0.5556\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.8372 - acc: 0.8612 - val_loss: 1.7498 - val_acc: 0.6024\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8465 - acc: 0.8543 - val_loss: 2.1764 - val_acc: 0.5473\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.8986 - acc: 0.8403 - val_loss: 2.1088 - val_acc: 0.5607\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8811 - acc: 0.8481 - val_loss: 1.8082 - val_acc: 0.5969\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8186 - acc: 0.8736 - val_loss: 2.0349 - val_acc: 0.5473\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8592 - acc: 0.8592 - val_loss: 2.4988 - val_acc: 0.4984\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8450 - acc: 0.8590 - val_loss: 2.0997 - val_acc: 0.5500\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8368 - acc: 0.8663 - val_loss: 2.2068 - val_acc: 0.5327\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8256 - acc: 0.8672 - val_loss: 2.0140 - val_acc: 0.5579\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8557 - acc: 0.8598 - val_loss: 2.0660 - val_acc: 0.5433\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8561 - acc: 0.8579 - val_loss: 2.2261 - val_acc: 0.5441\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8244 - acc: 0.8734 - val_loss: 2.1323 - val_acc: 0.5556\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7979 - acc: 0.8767 - val_loss: 1.9488 - val_acc: 0.5654\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8228 - acc: 0.8679 - val_loss: 2.2556 - val_acc: 0.5362\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7798 - acc: 0.8848 - val_loss: 1.9703 - val_acc: 0.5678\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7983 - acc: 0.8776 - val_loss: 1.8438 - val_acc: 0.5879\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7951 - acc: 0.8728 - val_loss: 1.9903 - val_acc: 0.5690\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.8322 - acc: 0.8693 - val_loss: 2.1386 - val_acc: 0.5504\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7893 - acc: 0.8830 - val_loss: 1.9314 - val_acc: 0.5875\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7938 - acc: 0.8831 - val_loss: 2.0582 - val_acc: 0.5745\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.9118 - acc: 0.8405 - val_loss: 1.8960 - val_acc: 0.5863\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8560 - acc: 0.8653 - val_loss: 2.1360 - val_acc: 0.5473\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7797 - acc: 0.8945 - val_loss: 2.0813 - val_acc: 0.5567\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7302 - acc: 0.9051 - val_loss: 1.9579 - val_acc: 0.5859\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7660 - acc: 0.8856 - val_loss: 2.0268 - val_acc: 0.5741\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8549 - acc: 0.8571 - val_loss: 2.0040 - val_acc: 0.5623\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8176 - acc: 0.8775 - val_loss: 1.8384 - val_acc: 0.6032\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8189 - acc: 0.8779 - val_loss: 1.8866 - val_acc: 0.5792\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7683 - acc: 0.8953 - val_loss: 1.9138 - val_acc: 0.5906\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7752 - acc: 0.8886 - val_loss: 1.9150 - val_acc: 0.5800\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7522 - acc: 0.8955 - val_loss: 2.3924 - val_acc: 0.5217\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8155 - acc: 0.8723 - val_loss: 1.9839 - val_acc: 0.5646\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8427 - acc: 0.8684 - val_loss: 2.5529 - val_acc: 0.4988\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8183 - acc: 0.8763 - val_loss: 1.8898 - val_acc: 0.5938\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.8108 - acc: 0.8754 - val_loss: 1.8916 - val_acc: 0.5839\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7914 - acc: 0.8864 - val_loss: 1.9321 - val_acc: 0.5757\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7701 - acc: 0.8909 - val_loss: 2.1394 - val_acc: 0.5619\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7696 - acc: 0.8876 - val_loss: 2.1999 - val_acc: 0.5626\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7865 - acc: 0.8831 - val_loss: 2.2229 - val_acc: 0.5611\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7372 - acc: 0.9039 - val_loss: 2.2695 - val_acc: 0.5493\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7665 - acc: 0.8872 - val_loss: 2.1576 - val_acc: 0.5496\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.8235 - acc: 0.8749 - val_loss: 2.0786 - val_acc: 0.5642\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7966 - acc: 0.8823 - val_loss: 2.0833 - val_acc: 0.5772\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7863 - acc: 0.8888 - val_loss: 2.4167 - val_acc: 0.5280\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|         | 21/256 [3:37:15<41:07:20, 629.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 484us/step - loss: 6.0581 - acc: 0.2933 - val_loss: 5.2301 - val_acc: 0.3144\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 4.0012 - acc: 0.3477 - val_loss: 3.2489 - val_acc: 0.3416\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 2.9257 - acc: 0.4178 - val_loss: 2.6923 - val_acc: 0.3621\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 2.2960 - acc: 0.4729 - val_loss: 2.3024 - val_acc: 0.3617\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 1.8897 - acc: 0.5175 - val_loss: 2.3663 - val_acc: 0.3708\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 1.6698 - acc: 0.5401 - val_loss: 2.7549 - val_acc: 0.3566\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 1.5266 - acc: 0.5693 - val_loss: 1.9676 - val_acc: 0.4295\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 1.4109 - acc: 0.5910 - val_loss: 4.3151 - val_acc: 0.2825\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 1.3351 - acc: 0.6041 - val_loss: 1.9158 - val_acc: 0.4752\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 1.2763 - acc: 0.6215 - val_loss: 2.3120 - val_acc: 0.3960\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 1.2222 - acc: 0.6366 - val_loss: 1.6937 - val_acc: 0.4913\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 1.1650 - acc: 0.6586 - val_loss: 2.0961 - val_acc: 0.4334\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 1.1490 - acc: 0.6646 - val_loss: 1.5996 - val_acc: 0.5359\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 1.1173 - acc: 0.6780 - val_loss: 1.6814 - val_acc: 0.5087\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 1.0922 - acc: 0.6886 - val_loss: 2.0038 - val_acc: 0.4665\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 1.0620 - acc: 0.6979 - val_loss: 1.6710 - val_acc: 0.4968\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 1.0368 - acc: 0.7175 - val_loss: 2.9124 - val_acc: 0.4113\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 1.0267 - acc: 0.7260 - val_loss: 2.3199 - val_acc: 0.4397\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 1.0072 - acc: 0.7374 - val_loss: 2.1455 - val_acc: 0.5000\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 1.0039 - acc: 0.7389 - val_loss: 1.7544 - val_acc: 0.5292\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.9781 - acc: 0.7574 - val_loss: 1.8927 - val_acc: 0.5366\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.9760 - acc: 0.7550 - val_loss: 1.8173 - val_acc: 0.5035\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.9576 - acc: 0.7637 - val_loss: 2.1976 - val_acc: 0.4701\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.9651 - acc: 0.7689 - val_loss: 2.0793 - val_acc: 0.5099\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.9379 - acc: 0.7785 - val_loss: 1.9519 - val_acc: 0.5071\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.9358 - acc: 0.7863 - val_loss: 1.9346 - val_acc: 0.5272\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.9287 - acc: 0.7871 - val_loss: 1.7654 - val_acc: 0.5504\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.9061 - acc: 0.8002 - val_loss: 2.6629 - val_acc: 0.4409\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.9060 - acc: 0.7984 - val_loss: 2.6065 - val_acc: 0.4945\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.9116 - acc: 0.7989 - val_loss: 2.1247 - val_acc: 0.5020\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8989 - acc: 0.8066 - val_loss: 1.9924 - val_acc: 0.5083\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.8844 - acc: 0.8129 - val_loss: 2.1157 - val_acc: 0.4941\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8906 - acc: 0.8100 - val_loss: 1.9573 - val_acc: 0.5331\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8847 - acc: 0.8169 - val_loss: 1.7923 - val_acc: 0.5737\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8819 - acc: 0.8225 - val_loss: 1.9234 - val_acc: 0.5520\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8820 - acc: 0.8220 - val_loss: 2.5066 - val_acc: 0.4720\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.8760 - acc: 0.8221 - val_loss: 1.9939 - val_acc: 0.5433\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8726 - acc: 0.8272 - val_loss: 2.0201 - val_acc: 0.5229\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8621 - acc: 0.8307 - val_loss: 1.9259 - val_acc: 0.5532\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8699 - acc: 0.8292 - val_loss: 3.3332 - val_acc: 0.3897\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.8626 - acc: 0.8309 - val_loss: 2.5667 - val_acc: 0.4760\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8484 - acc: 0.8391 - val_loss: 2.1252 - val_acc: 0.5457\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.8583 - acc: 0.8391 - val_loss: 4.2069 - val_acc: 0.3656\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8424 - acc: 0.8404 - val_loss: 2.0250 - val_acc: 0.5512\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8505 - acc: 0.8375 - val_loss: 2.8052 - val_acc: 0.4720\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8449 - acc: 0.8430 - val_loss: 3.4642 - val_acc: 0.4236\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.8369 - acc: 0.8455 - val_loss: 2.0093 - val_acc: 0.5327\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8335 - acc: 0.8477 - val_loss: 2.2084 - val_acc: 0.5169\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8348 - acc: 0.8474 - val_loss: 3.7764 - val_acc: 0.4330\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8380 - acc: 0.8486 - val_loss: 3.0624 - val_acc: 0.4413\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8389 - acc: 0.8492 - val_loss: 2.1933 - val_acc: 0.5284\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8315 - acc: 0.8487 - val_loss: 3.4524 - val_acc: 0.4102\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8454 - acc: 0.8464 - val_loss: 2.0250 - val_acc: 0.5429\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.8325 - acc: 0.8508 - val_loss: 2.3994 - val_acc: 0.4941\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.8213 - acc: 0.8516 - val_loss: 2.4833 - val_acc: 0.5024\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.8234 - acc: 0.8550 - val_loss: 2.9722 - val_acc: 0.4279\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.8216 - acc: 0.8560 - val_loss: 2.2117 - val_acc: 0.5217\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.8203 - acc: 0.8563 - val_loss: 3.3413 - val_acc: 0.4236\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.8044 - acc: 0.8646 - val_loss: 2.0838 - val_acc: 0.5623\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.8252 - acc: 0.8546 - val_loss: 2.2876 - val_acc: 0.5343\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.8094 - acc: 0.8625 - val_loss: 2.4542 - val_acc: 0.5055\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.8133 - acc: 0.8597 - val_loss: 3.1216 - val_acc: 0.4488\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.8031 - acc: 0.8657 - val_loss: 2.1024 - val_acc: 0.5433\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.8031 - acc: 0.8629 - val_loss: 2.3712 - val_acc: 0.5177\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.8113 - acc: 0.8592 - val_loss: 3.2263 - val_acc: 0.4137\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.8163 - acc: 0.8622 - val_loss: 2.2397 - val_acc: 0.5331\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.8021 - acc: 0.8624 - val_loss: 2.4326 - val_acc: 0.5138\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7946 - acc: 0.8707 - val_loss: 3.4910 - val_acc: 0.4212\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7993 - acc: 0.8651 - val_loss: 3.4677 - val_acc: 0.4720\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.8133 - acc: 0.8603 - val_loss: 2.0773 - val_acc: 0.5418\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.8123 - acc: 0.8663 - val_loss: 2.6338 - val_acc: 0.4921\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7932 - acc: 0.8702 - val_loss: 3.1567 - val_acc: 0.4689\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7948 - acc: 0.8724 - val_loss: 2.1743 - val_acc: 0.5343\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7986 - acc: 0.8688 - val_loss: 2.1343 - val_acc: 0.5398\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7968 - acc: 0.8693 - val_loss: 2.2216 - val_acc: 0.5280\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7927 - acc: 0.8702 - val_loss: 2.5416 - val_acc: 0.5165\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 0.8013 - acc: 0.8659 - val_loss: 2.3711 - val_acc: 0.5169\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7855 - acc: 0.8729 - val_loss: 2.3345 - val_acc: 0.5292\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.7917 - acc: 0.8735 - val_loss: 2.7405 - val_acc: 0.5035\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.8001 - acc: 0.8683 - val_loss: 1.9711 - val_acc: 0.5595\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7838 - acc: 0.8717 - val_loss: 2.3828 - val_acc: 0.5055\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7850 - acc: 0.8689 - val_loss: 3.1842 - val_acc: 0.4464\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7781 - acc: 0.8739 - val_loss: 2.4794 - val_acc: 0.5374\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7829 - acc: 0.8730 - val_loss: 2.1416 - val_acc: 0.5422\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 0.7960 - acc: 0.8694 - val_loss: 2.9196 - val_acc: 0.4618\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7891 - acc: 0.8727 - val_loss: 2.0452 - val_acc: 0.5567\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 0.7840 - acc: 0.8761 - val_loss: 2.1099 - val_acc: 0.5567\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7829 - acc: 0.8718 - val_loss: 2.7118 - val_acc: 0.4965\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.7836 - acc: 0.8735 - val_loss: 2.8571 - val_acc: 0.4874\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7678 - acc: 0.8787 - val_loss: 3.3961 - val_acc: 0.4338\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7783 - acc: 0.8771 - val_loss: 2.5056 - val_acc: 0.5181\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7863 - acc: 0.8715 - val_loss: 2.1808 - val_acc: 0.5410\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7826 - acc: 0.8778 - val_loss: 3.9446 - val_acc: 0.4058\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7912 - acc: 0.8716 - val_loss: 1.9992 - val_acc: 0.5508\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7889 - acc: 0.8698 - val_loss: 3.0082 - val_acc: 0.4708\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7768 - acc: 0.8770 - val_loss: 2.1698 - val_acc: 0.5512\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7689 - acc: 0.8778 - val_loss: 2.5025 - val_acc: 0.5256\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7788 - acc: 0.8759 - val_loss: 2.8528 - val_acc: 0.4685\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7853 - acc: 0.8753 - val_loss: 2.5002 - val_acc: 0.4882\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 0.7738 - acc: 0.8784 - val_loss: 2.2519 - val_acc: 0.5327\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 0.7787 - acc: 0.8763 - val_loss: 2.1864 - val_acc: 0.5213\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7713 - acc: 0.8778 - val_loss: 2.1629 - val_acc: 0.5315\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7749 - acc: 0.8775 - val_loss: 2.3720 - val_acc: 0.5154\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7771 - acc: 0.8763 - val_loss: 3.5814 - val_acc: 0.4145\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 373us/step - loss: 0.7728 - acc: 0.8759 - val_loss: 4.8859 - val_acc: 0.3991\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7761 - acc: 0.8780 - val_loss: 2.9850 - val_acc: 0.4681\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 371us/step - loss: 0.7656 - acc: 0.8866 - val_loss: 2.9685 - val_acc: 0.4685\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.7639 - acc: 0.8846 - val_loss: 2.9166 - val_acc: 0.4708\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7715 - acc: 0.8806 - val_loss: 2.4835 - val_acc: 0.5197\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7615 - acc: 0.8808 - val_loss: 2.5104 - val_acc: 0.5110\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7687 - acc: 0.8814 - val_loss: 2.6989 - val_acc: 0.4866\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7667 - acc: 0.8819 - val_loss: 2.2205 - val_acc: 0.5299\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7666 - acc: 0.8791 - val_loss: 1.9828 - val_acc: 0.5839\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7630 - acc: 0.8810 - val_loss: 2.9045 - val_acc: 0.5134\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7582 - acc: 0.8834 - val_loss: 2.7523 - val_acc: 0.4689\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.7637 - acc: 0.8831 - val_loss: 2.4089 - val_acc: 0.5319\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.7717 - acc: 0.8775 - val_loss: 2.8364 - val_acc: 0.4842\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7685 - acc: 0.8799 - val_loss: 2.1918 - val_acc: 0.5418\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7591 - acc: 0.8870 - val_loss: 2.1182 - val_acc: 0.5481\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7690 - acc: 0.8823 - val_loss: 2.6376 - val_acc: 0.4835\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.7550 - acc: 0.8867 - val_loss: 2.6798 - val_acc: 0.4988\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7580 - acc: 0.8847 - val_loss: 3.9990 - val_acc: 0.4259\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7619 - acc: 0.8826 - val_loss: 2.7722 - val_acc: 0.4779\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.7607 - acc: 0.8836 - val_loss: 2.1127 - val_acc: 0.5528\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7473 - acc: 0.8894 - val_loss: 2.3975 - val_acc: 0.5035\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7560 - acc: 0.8849 - val_loss: 2.5044 - val_acc: 0.5268\n",
            "Epoch 127/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7565 - acc: 0.8846 - val_loss: 2.2118 - val_acc: 0.5615\n",
            "Epoch 128/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7480 - acc: 0.8864 - val_loss: 2.2395 - val_acc: 0.5516\n",
            "Epoch 129/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7569 - acc: 0.8835 - val_loss: 2.6841 - val_acc: 0.4949\n",
            "Epoch 130/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.7439 - acc: 0.8908 - val_loss: 2.8154 - val_acc: 0.4933\n",
            "Epoch 131/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7614 - acc: 0.8843 - val_loss: 2.1414 - val_acc: 0.5374\n",
            "Epoch 132/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7629 - acc: 0.8844 - val_loss: 2.1109 - val_acc: 0.5485\n",
            "Epoch 133/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7524 - acc: 0.8881 - val_loss: 2.2235 - val_acc: 0.5359\n",
            "Epoch 134/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7544 - acc: 0.8879 - val_loss: 2.4301 - val_acc: 0.5398\n",
            "Epoch 135/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7585 - acc: 0.8860 - val_loss: 2.5727 - val_acc: 0.5225\n",
            "Epoch 136/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7598 - acc: 0.8853 - val_loss: 2.5734 - val_acc: 0.5225\n",
            "Epoch 137/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7508 - acc: 0.8895 - val_loss: 3.3138 - val_acc: 0.4618\n",
            "Epoch 138/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7541 - acc: 0.8856 - val_loss: 2.4739 - val_acc: 0.5217\n",
            "Epoch 139/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7573 - acc: 0.8844 - val_loss: 3.5466 - val_acc: 0.4507\n",
            "Epoch 140/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7504 - acc: 0.8865 - val_loss: 2.4458 - val_acc: 0.5043\n",
            "Epoch 141/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7554 - acc: 0.8886 - val_loss: 2.8448 - val_acc: 0.4838\n",
            "Epoch 142/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7632 - acc: 0.8804 - val_loss: 3.9393 - val_acc: 0.4090\n",
            "Epoch 143/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7469 - acc: 0.8912 - val_loss: 3.2490 - val_acc: 0.4661\n",
            "Epoch 144/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7531 - acc: 0.8887 - val_loss: 2.0521 - val_acc: 0.5544\n",
            "Epoch 145/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7511 - acc: 0.8908 - val_loss: 2.6989 - val_acc: 0.4949\n",
            "Epoch 146/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7502 - acc: 0.8878 - val_loss: 2.2106 - val_acc: 0.5701\n",
            "Epoch 147/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7484 - acc: 0.8884 - val_loss: 2.4528 - val_acc: 0.5296\n",
            "Epoch 148/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7515 - acc: 0.8862 - val_loss: 3.9444 - val_acc: 0.3897\n",
            "Epoch 149/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7359 - acc: 0.8920 - val_loss: 2.1655 - val_acc: 0.5441\n",
            "Epoch 150/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.7629 - acc: 0.8811 - val_loss: 3.2928 - val_acc: 0.4606\n",
            "Epoch 151/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7568 - acc: 0.8866 - val_loss: 2.0236 - val_acc: 0.5745\n",
            "Epoch 152/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7561 - acc: 0.8838 - val_loss: 2.3986 - val_acc: 0.5236\n",
            "Epoch 153/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7523 - acc: 0.8895 - val_loss: 2.3059 - val_acc: 0.5236\n",
            "Epoch 154/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7488 - acc: 0.8856 - val_loss: 2.2894 - val_acc: 0.5441\n",
            "Epoch 155/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7484 - acc: 0.8916 - val_loss: 2.2809 - val_acc: 0.5473\n",
            "Epoch 156/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7478 - acc: 0.8907 - val_loss: 3.1413 - val_acc: 0.4850\n",
            "Epoch 157/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7525 - acc: 0.8856 - val_loss: 3.1630 - val_acc: 0.4626\n",
            "Epoch 158/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7533 - acc: 0.8875 - val_loss: 2.1459 - val_acc: 0.5591\n",
            "Epoch 159/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7540 - acc: 0.8871 - val_loss: 2.2394 - val_acc: 0.5528\n",
            "Epoch 160/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7564 - acc: 0.8853 - val_loss: 2.7752 - val_acc: 0.4708\n",
            "Epoch 161/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7414 - acc: 0.8922 - val_loss: 3.0630 - val_acc: 0.4531\n",
            "Epoch 162/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7501 - acc: 0.8888 - val_loss: 2.4031 - val_acc: 0.5288\n",
            "Epoch 163/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7478 - acc: 0.8868 - val_loss: 2.1847 - val_acc: 0.5607\n",
            "Epoch 164/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7312 - acc: 0.8948 - val_loss: 2.8587 - val_acc: 0.4957\n",
            "Epoch 165/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7511 - acc: 0.8881 - val_loss: 2.6221 - val_acc: 0.5087\n",
            "Epoch 166/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7495 - acc: 0.8888 - val_loss: 2.2780 - val_acc: 0.5311\n",
            "Epoch 167/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7415 - acc: 0.8921 - val_loss: 3.4179 - val_acc: 0.4098\n",
            "Epoch 168/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7494 - acc: 0.8880 - val_loss: 2.0605 - val_acc: 0.5650\n",
            "Epoch 169/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7438 - acc: 0.8904 - val_loss: 2.7573 - val_acc: 0.5091\n",
            "Epoch 170/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7504 - acc: 0.8879 - val_loss: 3.1562 - val_acc: 0.4890\n",
            "Epoch 171/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7504 - acc: 0.8875 - val_loss: 2.6193 - val_acc: 0.4858\n",
            "Epoch 172/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7407 - acc: 0.8923 - val_loss: 2.7989 - val_acc: 0.5362\n",
            "Epoch 173/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7479 - acc: 0.8861 - val_loss: 2.2395 - val_acc: 0.5252\n",
            "Epoch 174/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7391 - acc: 0.8946 - val_loss: 3.3482 - val_acc: 0.4173\n",
            "Epoch 175/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7454 - acc: 0.8902 - val_loss: 3.5717 - val_acc: 0.4515\n",
            "Epoch 176/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7505 - acc: 0.8892 - val_loss: 2.6979 - val_acc: 0.5102\n",
            "Epoch 177/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7462 - acc: 0.8917 - val_loss: 2.6119 - val_acc: 0.5000\n",
            "Epoch 178/300\n",
            "10152/10152 [==============================] - 4s 442us/step - loss: 0.7518 - acc: 0.8880 - val_loss: 1.9487 - val_acc: 0.5792\n",
            "Epoch 179/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7359 - acc: 0.8980 - val_loss: 2.9857 - val_acc: 0.5055\n",
            "Epoch 180/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7500 - acc: 0.8878 - val_loss: 2.0888 - val_acc: 0.5477\n",
            "Epoch 181/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7479 - acc: 0.8885 - val_loss: 2.0911 - val_acc: 0.5650\n",
            "Epoch 182/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.7486 - acc: 0.8891 - val_loss: 2.2580 - val_acc: 0.5386\n",
            "Epoch 183/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7424 - acc: 0.8908 - val_loss: 2.9147 - val_acc: 0.4768\n",
            "Epoch 184/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.7460 - acc: 0.8870 - val_loss: 3.1166 - val_acc: 0.4622\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  9%|         | 22/256 [3:49:51<43:23:48, 667.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 6s 588us/step - loss: 6.1532 - acc: 0.2915 - val_loss: 5.3730 - val_acc: 0.2998\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 4.5384 - acc: 0.3456 - val_loss: 3.8584 - val_acc: 0.3519\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 3.4077 - acc: 0.4338 - val_loss: 2.9772 - val_acc: 0.4023\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 2.6055 - acc: 0.5065 - val_loss: 2.3819 - val_acc: 0.4720\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 2.0991 - acc: 0.5578 - val_loss: 2.0235 - val_acc: 0.5169\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 1.7889 - acc: 0.5861 - val_loss: 1.8061 - val_acc: 0.5390\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 1.5815 - acc: 0.6056 - val_loss: 1.7409 - val_acc: 0.5426\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 1.4409 - acc: 0.6301 - val_loss: 1.6032 - val_acc: 0.5351\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 1.3208 - acc: 0.6560 - val_loss: 1.7392 - val_acc: 0.5071\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 1.2406 - acc: 0.6690 - val_loss: 1.7508 - val_acc: 0.5240\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 1.2084 - acc: 0.6782 - val_loss: 1.8051 - val_acc: 0.5359\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 1.1343 - acc: 0.6957 - val_loss: 1.5164 - val_acc: 0.5859\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 1.1369 - acc: 0.6982 - val_loss: 1.7788 - val_acc: 0.5280\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 1.1137 - acc: 0.7109 - val_loss: 1.7440 - val_acc: 0.5130\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 5s 454us/step - loss: 1.0775 - acc: 0.7236 - val_loss: 1.5121 - val_acc: 0.5721\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 1.0790 - acc: 0.7250 - val_loss: 1.8171 - val_acc: 0.5248\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 1.0530 - acc: 0.7378 - val_loss: 1.6641 - val_acc: 0.5532\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 1.0265 - acc: 0.7490 - val_loss: 1.6989 - val_acc: 0.5272\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 1.0123 - acc: 0.7600 - val_loss: 1.5669 - val_acc: 0.5812\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.9949 - acc: 0.7704 - val_loss: 2.0409 - val_acc: 0.5000\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.9811 - acc: 0.7778 - val_loss: 1.6719 - val_acc: 0.5678\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.9775 - acc: 0.7767 - val_loss: 1.9295 - val_acc: 0.5355\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.9574 - acc: 0.7908 - val_loss: 1.9798 - val_acc: 0.5087\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.9403 - acc: 0.8001 - val_loss: 2.0533 - val_acc: 0.5150\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 5s 452us/step - loss: 0.9315 - acc: 0.8018 - val_loss: 2.1471 - val_acc: 0.5099\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.9535 - acc: 0.8006 - val_loss: 1.7059 - val_acc: 0.5760\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.9037 - acc: 0.8238 - val_loss: 2.1715 - val_acc: 0.4980\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.9208 - acc: 0.8157 - val_loss: 1.9760 - val_acc: 0.5426\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 5s 451us/step - loss: 0.8990 - acc: 0.8286 - val_loss: 1.6929 - val_acc: 0.5709\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 5s 450us/step - loss: 0.9334 - acc: 0.8147 - val_loss: 1.8267 - val_acc: 0.5796\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 5s 448us/step - loss: 0.8736 - acc: 0.8398 - val_loss: 1.7576 - val_acc: 0.5745\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 5s 445us/step - loss: 0.9015 - acc: 0.8287 - val_loss: 2.0016 - val_acc: 0.5374\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.9006 - acc: 0.8319 - val_loss: 1.9853 - val_acc: 0.5489\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 5s 449us/step - loss: 0.8766 - acc: 0.8414 - val_loss: 2.0773 - val_acc: 0.5272\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 5s 447us/step - loss: 0.8547 - acc: 0.8462 - val_loss: 1.7845 - val_acc: 0.5879\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 5s 446us/step - loss: 0.8645 - acc: 0.8440 - val_loss: 1.8345 - val_acc: 0.5879\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.8857 - acc: 0.8404 - val_loss: 2.1388 - val_acc: 0.5414\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8550 - acc: 0.8544 - val_loss: 2.0427 - val_acc: 0.5489\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8708 - acc: 0.8476 - val_loss: 2.3701 - val_acc: 0.5110\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.8707 - acc: 0.8454 - val_loss: 1.7372 - val_acc: 0.6024\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.8122 - acc: 0.8706 - val_loss: 1.8927 - val_acc: 0.5701\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.8178 - acc: 0.8675 - val_loss: 1.8319 - val_acc: 0.5784\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.7991 - acc: 0.8732 - val_loss: 2.2108 - val_acc: 0.5599\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.8344 - acc: 0.8674 - val_loss: 1.8370 - val_acc: 0.5894\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.8302 - acc: 0.8653 - val_loss: 2.1649 - val_acc: 0.5579\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.8442 - acc: 0.8620 - val_loss: 2.0138 - val_acc: 0.5721\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.8279 - acc: 0.8678 - val_loss: 1.9446 - val_acc: 0.5654\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.8381 - acc: 0.8639 - val_loss: 2.2807 - val_acc: 0.5087\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.8243 - acc: 0.8696 - val_loss: 1.9272 - val_acc: 0.5745\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.8151 - acc: 0.8731 - val_loss: 1.9605 - val_acc: 0.5670\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.8159 - acc: 0.8739 - val_loss: 1.7910 - val_acc: 0.5910\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7900 - acc: 0.8805 - val_loss: 1.9362 - val_acc: 0.5638\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8104 - acc: 0.8737 - val_loss: 2.1384 - val_acc: 0.5453\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.8460 - acc: 0.8599 - val_loss: 1.9137 - val_acc: 0.5851\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8364 - acc: 0.8672 - val_loss: 1.8009 - val_acc: 0.5867\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8037 - acc: 0.8823 - val_loss: 2.5130 - val_acc: 0.5213\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7829 - acc: 0.8846 - val_loss: 2.1637 - val_acc: 0.5319\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7744 - acc: 0.8866 - val_loss: 2.1407 - val_acc: 0.5469\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7871 - acc: 0.8830 - val_loss: 1.8758 - val_acc: 0.5827\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8257 - acc: 0.8708 - val_loss: 2.1307 - val_acc: 0.5634\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.8453 - acc: 0.8659 - val_loss: 2.2090 - val_acc: 0.5339\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.8028 - acc: 0.8775 - val_loss: 2.0059 - val_acc: 0.5607\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.7570 - acc: 0.8972 - val_loss: 1.9510 - val_acc: 0.5804\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7294 - acc: 0.9013 - val_loss: 1.8640 - val_acc: 0.5887\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7624 - acc: 0.8851 - val_loss: 2.0971 - val_acc: 0.5394\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.8343 - acc: 0.8604 - val_loss: 2.1119 - val_acc: 0.5638\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.8158 - acc: 0.8764 - val_loss: 2.0445 - val_acc: 0.5489\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7910 - acc: 0.8863 - val_loss: 2.0205 - val_acc: 0.5843\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7924 - acc: 0.8823 - val_loss: 2.0316 - val_acc: 0.5567\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7855 - acc: 0.8872 - val_loss: 2.1979 - val_acc: 0.5654\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.7608 - acc: 0.8955 - val_loss: 1.8392 - val_acc: 0.6005\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.7493 - acc: 0.8986 - val_loss: 1.9122 - val_acc: 0.5950\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7662 - acc: 0.8882 - val_loss: 2.4095 - val_acc: 0.5197\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.7735 - acc: 0.8878 - val_loss: 2.1013 - val_acc: 0.5863\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7682 - acc: 0.8899 - val_loss: 1.9723 - val_acc: 0.5792\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7692 - acc: 0.8890 - val_loss: 2.1582 - val_acc: 0.5721\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7946 - acc: 0.8834 - val_loss: 2.1485 - val_acc: 0.5642\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7836 - acc: 0.8894 - val_loss: 2.2061 - val_acc: 0.5496\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7811 - acc: 0.8890 - val_loss: 2.0335 - val_acc: 0.5843\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.7446 - acc: 0.8980 - val_loss: 1.8643 - val_acc: 0.6087\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.7934 - acc: 0.8778 - val_loss: 2.1868 - val_acc: 0.5493\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8224 - acc: 0.8737 - val_loss: 2.0909 - val_acc: 0.5630\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7537 - acc: 0.8992 - val_loss: 1.9220 - val_acc: 0.5835\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7461 - acc: 0.8978 - val_loss: 1.8324 - val_acc: 0.5875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  9%|         | 23/256 [3:55:51<37:14:39, 575.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 476us/step - loss: 6.0656 - acc: 0.2823 - val_loss: 6.6205 - val_acc: 0.2498\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 3.8410 - acc: 0.3294 - val_loss: 3.0852 - val_acc: 0.3113\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 2.6883 - acc: 0.3791 - val_loss: 2.5011 - val_acc: 0.3633\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 2.1334 - acc: 0.4437 - val_loss: 1.9298 - val_acc: 0.4338\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 1.8053 - acc: 0.5011 - val_loss: 2.1698 - val_acc: 0.4106\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 1.5982 - acc: 0.5332 - val_loss: 1.7444 - val_acc: 0.4460\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 1.4738 - acc: 0.5580 - val_loss: 1.8192 - val_acc: 0.4693\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 1.3811 - acc: 0.5764 - val_loss: 1.6740 - val_acc: 0.4728\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 1.3116 - acc: 0.5961 - val_loss: 2.4535 - val_acc: 0.4023\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 1.2377 - acc: 0.6192 - val_loss: 1.8312 - val_acc: 0.4980\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 1.2026 - acc: 0.6361 - val_loss: 1.4856 - val_acc: 0.5414\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 1.1500 - acc: 0.6557 - val_loss: 1.6143 - val_acc: 0.5122\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 1.1241 - acc: 0.6649 - val_loss: 1.6234 - val_acc: 0.5201\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 1.1115 - acc: 0.6741 - val_loss: 1.6646 - val_acc: 0.5268\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 1.0677 - acc: 0.6899 - val_loss: 1.8222 - val_acc: 0.5012\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 1.0557 - acc: 0.7016 - val_loss: 1.6909 - val_acc: 0.5051\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 1.0327 - acc: 0.7140 - val_loss: 2.0028 - val_acc: 0.4645\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 1.0225 - acc: 0.7153 - val_loss: 2.0466 - val_acc: 0.4630\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.9866 - acc: 0.7352 - val_loss: 1.9654 - val_acc: 0.4736\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.9845 - acc: 0.7415 - val_loss: 1.6824 - val_acc: 0.5481\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.9653 - acc: 0.7513 - val_loss: 2.0482 - val_acc: 0.4878\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.9615 - acc: 0.7540 - val_loss: 1.5990 - val_acc: 0.5481\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.9584 - acc: 0.7578 - val_loss: 1.9205 - val_acc: 0.4949\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.9383 - acc: 0.7729 - val_loss: 2.0263 - val_acc: 0.4594\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.9422 - acc: 0.7752 - val_loss: 2.0517 - val_acc: 0.4961\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.9239 - acc: 0.7781 - val_loss: 1.7319 - val_acc: 0.5465\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 440us/step - loss: 0.9168 - acc: 0.7885 - val_loss: 1.9998 - val_acc: 0.5165\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.9138 - acc: 0.7913 - val_loss: 2.0148 - val_acc: 0.5185\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8989 - acc: 0.7979 - val_loss: 1.7478 - val_acc: 0.5477\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.9021 - acc: 0.8005 - val_loss: 2.3092 - val_acc: 0.4594\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8891 - acc: 0.8112 - val_loss: 2.1373 - val_acc: 0.5142\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 431us/step - loss: 0.8786 - acc: 0.8139 - val_loss: 2.0167 - val_acc: 0.5229\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8711 - acc: 0.8194 - val_loss: 2.1799 - val_acc: 0.5051\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8858 - acc: 0.8114 - val_loss: 1.8808 - val_acc: 0.5347\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8659 - acc: 0.8246 - val_loss: 1.8732 - val_acc: 0.5611\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8664 - acc: 0.8231 - val_loss: 1.8760 - val_acc: 0.5410\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8610 - acc: 0.8260 - val_loss: 2.2743 - val_acc: 0.5016\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8616 - acc: 0.8252 - val_loss: 2.2729 - val_acc: 0.4941\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8490 - acc: 0.8359 - val_loss: 2.0742 - val_acc: 0.5232\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8482 - acc: 0.8330 - val_loss: 2.3051 - val_acc: 0.5035\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8549 - acc: 0.8319 - val_loss: 2.0795 - val_acc: 0.5496\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8360 - acc: 0.8390 - val_loss: 1.9912 - val_acc: 0.5453\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.8337 - acc: 0.8408 - val_loss: 2.1098 - val_acc: 0.5296\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.8181 - acc: 0.8461 - val_loss: 1.9910 - val_acc: 0.5508\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8369 - acc: 0.8415 - val_loss: 1.9130 - val_acc: 0.5559\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8257 - acc: 0.8477 - val_loss: 2.6281 - val_acc: 0.4775\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8225 - acc: 0.8463 - val_loss: 1.8811 - val_acc: 0.5851\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 432us/step - loss: 0.8156 - acc: 0.8542 - val_loss: 2.6358 - val_acc: 0.4909\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.8154 - acc: 0.8515 - val_loss: 2.5506 - val_acc: 0.5213\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.8128 - acc: 0.8547 - val_loss: 2.4314 - val_acc: 0.4961\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8064 - acc: 0.8531 - val_loss: 1.7838 - val_acc: 0.5887\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8169 - acc: 0.8507 - val_loss: 2.0049 - val_acc: 0.5638\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.8068 - acc: 0.8589 - val_loss: 2.5514 - val_acc: 0.4913\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8038 - acc: 0.8590 - val_loss: 2.2030 - val_acc: 0.5315\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.8105 - acc: 0.8546 - val_loss: 2.2348 - val_acc: 0.5205\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.8069 - acc: 0.8582 - val_loss: 2.1174 - val_acc: 0.5205\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.8040 - acc: 0.8630 - val_loss: 2.1074 - val_acc: 0.5402\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7997 - acc: 0.8620 - val_loss: 2.4458 - val_acc: 0.5043\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7989 - acc: 0.8610 - val_loss: 2.6498 - val_acc: 0.5008\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8023 - acc: 0.8613 - val_loss: 2.1761 - val_acc: 0.5536\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7987 - acc: 0.8616 - val_loss: 2.1609 - val_acc: 0.5303\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.8020 - acc: 0.8655 - val_loss: 2.5115 - val_acc: 0.5067\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7903 - acc: 0.8666 - val_loss: 2.1022 - val_acc: 0.5493\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7885 - acc: 0.8694 - val_loss: 2.2690 - val_acc: 0.5126\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7878 - acc: 0.8682 - val_loss: 3.0658 - val_acc: 0.4464\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7932 - acc: 0.8640 - val_loss: 2.6121 - val_acc: 0.4894\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7943 - acc: 0.8638 - val_loss: 1.9588 - val_acc: 0.5587\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.7841 - acc: 0.8682 - val_loss: 2.5397 - val_acc: 0.5047\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7936 - acc: 0.8679 - val_loss: 4.1830 - val_acc: 0.4082\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7933 - acc: 0.8613 - val_loss: 2.4477 - val_acc: 0.4980\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.7811 - acc: 0.8716 - val_loss: 1.9919 - val_acc: 0.5658\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7855 - acc: 0.8735 - val_loss: 2.3477 - val_acc: 0.5315\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7807 - acc: 0.8729 - val_loss: 1.9950 - val_acc: 0.5804\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7793 - acc: 0.8746 - val_loss: 1.9782 - val_acc: 0.5721\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 0.7752 - acc: 0.8784 - val_loss: 2.3758 - val_acc: 0.5524\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 0.7752 - acc: 0.8760 - val_loss: 2.7978 - val_acc: 0.4886\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 0.7705 - acc: 0.8831 - val_loss: 2.1431 - val_acc: 0.5280\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 0.7807 - acc: 0.8707 - val_loss: 2.1767 - val_acc: 0.5426\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7671 - acc: 0.8772 - val_loss: 2.9375 - val_acc: 0.4909\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 377us/step - loss: 0.7762 - acc: 0.8721 - val_loss: 2.4406 - val_acc: 0.5110\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 0.7703 - acc: 0.8771 - val_loss: 2.9497 - val_acc: 0.4720\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.7677 - acc: 0.8771 - val_loss: 2.3872 - val_acc: 0.5351\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 0.7627 - acc: 0.8752 - val_loss: 2.1068 - val_acc: 0.5654\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 0.7741 - acc: 0.8773 - val_loss: 2.0211 - val_acc: 0.5701\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 0.7605 - acc: 0.8823 - val_loss: 2.5244 - val_acc: 0.4929\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 375us/step - loss: 0.7678 - acc: 0.8798 - val_loss: 2.6030 - val_acc: 0.5118\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 0.7646 - acc: 0.8823 - val_loss: 2.6411 - val_acc: 0.4901\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7766 - acc: 0.8739 - val_loss: 2.9037 - val_acc: 0.4917\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.7611 - acc: 0.8792 - val_loss: 2.0240 - val_acc: 0.5319\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 0.7663 - acc: 0.8794 - val_loss: 2.1191 - val_acc: 0.5682\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 374us/step - loss: 0.7579 - acc: 0.8811 - val_loss: 2.1753 - val_acc: 0.5559\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 376us/step - loss: 0.7532 - acc: 0.8824 - val_loss: 2.0961 - val_acc: 0.5465\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 371us/step - loss: 0.7532 - acc: 0.8840 - val_loss: 2.7815 - val_acc: 0.5055\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 367us/step - loss: 0.7626 - acc: 0.8794 - val_loss: 2.2502 - val_acc: 0.5355\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 365us/step - loss: 0.7489 - acc: 0.8840 - val_loss: 1.9829 - val_acc: 0.5650\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 373us/step - loss: 0.7524 - acc: 0.8814 - val_loss: 2.1456 - val_acc: 0.5449\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 365us/step - loss: 0.7462 - acc: 0.8856 - val_loss: 1.9363 - val_acc: 0.5757\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 373us/step - loss: 0.7539 - acc: 0.8825 - val_loss: 2.3477 - val_acc: 0.5209\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 372us/step - loss: 0.7559 - acc: 0.8852 - val_loss: 2.0244 - val_acc: 0.5646\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7546 - acc: 0.8788 - val_loss: 2.8823 - val_acc: 0.4953\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7538 - acc: 0.8866 - val_loss: 2.8224 - val_acc: 0.4669\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7538 - acc: 0.8851 - val_loss: 2.0405 - val_acc: 0.5753\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7550 - acc: 0.8791 - val_loss: 2.6324 - val_acc: 0.5150\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.7608 - acc: 0.8833 - val_loss: 2.4371 - val_acc: 0.5142\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7637 - acc: 0.8805 - val_loss: 2.0306 - val_acc: 0.5690\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7382 - acc: 0.8911 - val_loss: 2.4060 - val_acc: 0.5118\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7489 - acc: 0.8897 - val_loss: 2.6346 - val_acc: 0.4980\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 4s 374us/step - loss: 0.7555 - acc: 0.8849 - val_loss: 2.4942 - val_acc: 0.5063\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 0.7544 - acc: 0.8825 - val_loss: 2.7771 - val_acc: 0.4980\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 371us/step - loss: 0.7543 - acc: 0.8840 - val_loss: 2.2902 - val_acc: 0.5402\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 377us/step - loss: 0.7520 - acc: 0.8784 - val_loss: 2.2411 - val_acc: 0.5457\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 372us/step - loss: 0.7582 - acc: 0.8835 - val_loss: 2.8684 - val_acc: 0.5012\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 0.7573 - acc: 0.8830 - val_loss: 3.0850 - val_acc: 0.4925\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 372us/step - loss: 0.7384 - acc: 0.8912 - val_loss: 2.1308 - val_acc: 0.5461\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 365us/step - loss: 0.7452 - acc: 0.8908 - val_loss: 2.1450 - val_acc: 0.5650\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 364us/step - loss: 0.7555 - acc: 0.8855 - val_loss: 2.6920 - val_acc: 0.4988\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 368us/step - loss: 0.7517 - acc: 0.8871 - val_loss: 2.0637 - val_acc: 0.5638\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 365us/step - loss: 0.7482 - acc: 0.8878 - val_loss: 2.2791 - val_acc: 0.5355\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 364us/step - loss: 0.7483 - acc: 0.8831 - val_loss: 2.2143 - val_acc: 0.5461\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 365us/step - loss: 0.7446 - acc: 0.8910 - val_loss: 2.0420 - val_acc: 0.5788\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 368us/step - loss: 0.7388 - acc: 0.8883 - val_loss: 2.5182 - val_acc: 0.5122\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 4s 364us/step - loss: 0.7395 - acc: 0.8912 - val_loss: 2.3897 - val_acc: 0.5500\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 4s 370us/step - loss: 0.7459 - acc: 0.8932 - val_loss: 2.1876 - val_acc: 0.5465\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 4s 363us/step - loss: 0.7475 - acc: 0.8893 - val_loss: 2.4035 - val_acc: 0.5004\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 4s 366us/step - loss: 0.7532 - acc: 0.8891 - val_loss: 2.4342 - val_acc: 0.5398\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 4s 364us/step - loss: 0.7402 - acc: 0.8903 - val_loss: 2.7663 - val_acc: 0.4905\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  9%|         | 24/256 [4:04:30<36:00:06, 558.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 507us/step - loss: 6.2027 - acc: 0.2845 - val_loss: 5.4593 - val_acc: 0.2943\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 364us/step - loss: 4.6042 - acc: 0.3509 - val_loss: 3.9675 - val_acc: 0.3353\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 354us/step - loss: 3.4822 - acc: 0.4251 - val_loss: 3.2039 - val_acc: 0.3558\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 3s 344us/step - loss: 2.6356 - acc: 0.5172 - val_loss: 2.4798 - val_acc: 0.4460\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 349us/step - loss: 2.1307 - acc: 0.5613 - val_loss: 2.2048 - val_acc: 0.4515\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 349us/step - loss: 1.8021 - acc: 0.5869 - val_loss: 1.8382 - val_acc: 0.5335\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 349us/step - loss: 1.5894 - acc: 0.6151 - val_loss: 1.9978 - val_acc: 0.5110\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 3s 344us/step - loss: 1.4521 - acc: 0.6287 - val_loss: 1.6815 - val_acc: 0.5240\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 345us/step - loss: 1.3512 - acc: 0.6442 - val_loss: 1.5756 - val_acc: 0.5544\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 349us/step - loss: 1.2800 - acc: 0.6579 - val_loss: 1.6876 - val_acc: 0.5359\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 355us/step - loss: 1.2313 - acc: 0.6683 - val_loss: 1.6726 - val_acc: 0.5323\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 350us/step - loss: 1.1676 - acc: 0.6912 - val_loss: 1.7955 - val_acc: 0.5244\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 365us/step - loss: 1.1295 - acc: 0.7023 - val_loss: 1.7094 - val_acc: 0.5335\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 376us/step - loss: 1.0918 - acc: 0.7140 - val_loss: 1.5352 - val_acc: 0.5705\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 1.0894 - acc: 0.7227 - val_loss: 1.6318 - val_acc: 0.5583\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 374us/step - loss: 1.0635 - acc: 0.7322 - val_loss: 1.5423 - val_acc: 0.5693\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 376us/step - loss: 1.0687 - acc: 0.7310 - val_loss: 1.9819 - val_acc: 0.4917\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 1.0332 - acc: 0.7496 - val_loss: 1.7758 - val_acc: 0.5362\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 1.0231 - acc: 0.7571 - val_loss: 1.8681 - val_acc: 0.5343\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 377us/step - loss: 1.0046 - acc: 0.7664 - val_loss: 1.6803 - val_acc: 0.5587\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 375us/step - loss: 1.0141 - acc: 0.7714 - val_loss: 1.7546 - val_acc: 0.5607\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 0.9591 - acc: 0.7907 - val_loss: 1.6824 - val_acc: 0.5871\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 377us/step - loss: 0.9804 - acc: 0.7852 - val_loss: 1.8502 - val_acc: 0.5359\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.9652 - acc: 0.7992 - val_loss: 1.7359 - val_acc: 0.5757\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.9720 - acc: 0.7909 - val_loss: 2.0408 - val_acc: 0.5311\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.9292 - acc: 0.8107 - val_loss: 1.6967 - val_acc: 0.5760\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.9173 - acc: 0.8210 - val_loss: 1.6682 - val_acc: 0.5934\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.9081 - acc: 0.8195 - val_loss: 1.9135 - val_acc: 0.5520\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.8788 - acc: 0.8369 - val_loss: 1.7247 - val_acc: 0.5823\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.9245 - acc: 0.8201 - val_loss: 2.2894 - val_acc: 0.4976\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.9157 - acc: 0.8294 - val_loss: 2.0255 - val_acc: 0.5398\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.9074 - acc: 0.8321 - val_loss: 1.8403 - val_acc: 0.5686\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.8848 - acc: 0.8355 - val_loss: 2.0021 - val_acc: 0.5390\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.8617 - acc: 0.8467 - val_loss: 1.8692 - val_acc: 0.5820\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.8671 - acc: 0.8499 - val_loss: 1.8340 - val_acc: 0.5776\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.8250 - acc: 0.8603 - val_loss: 1.7349 - val_acc: 0.5973\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.8317 - acc: 0.8574 - val_loss: 1.8088 - val_acc: 0.5776\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8677 - acc: 0.8468 - val_loss: 2.1216 - val_acc: 0.5485\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.8674 - acc: 0.8487 - val_loss: 1.7305 - val_acc: 0.5922\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.8346 - acc: 0.8640 - val_loss: 1.9292 - val_acc: 0.5646\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8219 - acc: 0.8657 - val_loss: 2.3044 - val_acc: 0.5272\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8117 - acc: 0.8698 - val_loss: 2.4381 - val_acc: 0.5366\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8304 - acc: 0.8641 - val_loss: 1.9799 - val_acc: 0.5816\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8406 - acc: 0.8599 - val_loss: 2.0381 - val_acc: 0.5493\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8430 - acc: 0.8626 - val_loss: 2.1933 - val_acc: 0.5402\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8051 - acc: 0.8738 - val_loss: 1.8026 - val_acc: 0.5843\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8243 - acc: 0.8700 - val_loss: 1.8298 - val_acc: 0.5808\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7847 - acc: 0.8814 - val_loss: 2.1621 - val_acc: 0.5335\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.8133 - acc: 0.8714 - val_loss: 2.2443 - val_acc: 0.5461\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7652 - acc: 0.8904 - val_loss: 1.9588 - val_acc: 0.5749\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.8278 - acc: 0.8664 - val_loss: 2.0956 - val_acc: 0.5626\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.8099 - acc: 0.8737 - val_loss: 1.8777 - val_acc: 0.6005\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7990 - acc: 0.8763 - val_loss: 1.8675 - val_acc: 0.5776\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7743 - acc: 0.8903 - val_loss: 1.8494 - val_acc: 0.5851\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7772 - acc: 0.8856 - val_loss: 1.8471 - val_acc: 0.6135\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.8107 - acc: 0.8732 - val_loss: 2.2672 - val_acc: 0.5481\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 374us/step - loss: 0.7914 - acc: 0.8886 - val_loss: 1.9587 - val_acc: 0.5890\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 0.7819 - acc: 0.8875 - val_loss: 2.0032 - val_acc: 0.5839\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7987 - acc: 0.8779 - val_loss: 1.9057 - val_acc: 0.5906\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7932 - acc: 0.8828 - val_loss: 1.8940 - val_acc: 0.5662\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7763 - acc: 0.8904 - val_loss: 2.0129 - val_acc: 0.5615\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7469 - acc: 0.8933 - val_loss: 2.1394 - val_acc: 0.5666\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.7723 - acc: 0.8880 - val_loss: 2.0668 - val_acc: 0.5693\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8247 - acc: 0.8712 - val_loss: 2.8025 - val_acc: 0.4957\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8201 - acc: 0.8747 - val_loss: 2.1597 - val_acc: 0.5690\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7667 - acc: 0.8965 - val_loss: 2.1635 - val_acc: 0.5571\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7422 - acc: 0.9006 - val_loss: 1.9764 - val_acc: 0.5686\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7193 - acc: 0.9083 - val_loss: 2.1637 - val_acc: 0.5528\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7700 - acc: 0.8881 - val_loss: 2.7356 - val_acc: 0.4980\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8194 - acc: 0.8733 - val_loss: 2.1512 - val_acc: 0.5630\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8221 - acc: 0.8731 - val_loss: 2.5623 - val_acc: 0.5075\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.7829 - acc: 0.8896 - val_loss: 1.9116 - val_acc: 0.5808\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7666 - acc: 0.8948 - val_loss: 1.9335 - val_acc: 0.5910\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7294 - acc: 0.9082 - val_loss: 1.9981 - val_acc: 0.5855\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.7219 - acc: 0.9071 - val_loss: 2.1727 - val_acc: 0.5701\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7444 - acc: 0.8980 - val_loss: 2.2513 - val_acc: 0.5457\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7756 - acc: 0.8879 - val_loss: 2.3350 - val_acc: 0.5591\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7570 - acc: 0.8938 - val_loss: 2.2001 - val_acc: 0.5544\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7382 - acc: 0.9006 - val_loss: 2.4343 - val_acc: 0.5457\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.7286 - acc: 0.9022 - val_loss: 1.9928 - val_acc: 0.5871\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7457 - acc: 0.8984 - val_loss: 2.0234 - val_acc: 0.5670\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.7757 - acc: 0.8877 - val_loss: 2.4122 - val_acc: 0.5335\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7816 - acc: 0.8853 - val_loss: 2.0825 - val_acc: 0.5745\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7529 - acc: 0.8980 - val_loss: 1.9242 - val_acc: 0.5788\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.7465 - acc: 0.8962 - val_loss: 2.0880 - val_acc: 0.5709\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7254 - acc: 0.9062 - val_loss: 2.5905 - val_acc: 0.5118\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7441 - acc: 0.8963 - val_loss: 2.1021 - val_acc: 0.5697\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7542 - acc: 0.8933 - val_loss: 2.1963 - val_acc: 0.5575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|         | 25/256 [4:10:21<31:50:03, 496.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 467us/step - loss: 6.0950 - acc: 0.2793 - val_loss: 5.7037 - val_acc: 0.2730\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 3.9772 - acc: 0.3184 - val_loss: 3.0302 - val_acc: 0.3034\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 2.7447 - acc: 0.3941 - val_loss: 2.3578 - val_acc: 0.3424\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 2.1407 - acc: 0.4607 - val_loss: 1.9899 - val_acc: 0.4125\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 1.8357 - acc: 0.4991 - val_loss: 1.8126 - val_acc: 0.4842\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 1.6258 - acc: 0.5352 - val_loss: 1.6647 - val_acc: 0.4823\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 1.4842 - acc: 0.5592 - val_loss: 2.2676 - val_acc: 0.4157\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 1.4015 - acc: 0.5830 - val_loss: 1.5570 - val_acc: 0.5099\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 1.3384 - acc: 0.5939 - val_loss: 1.8002 - val_acc: 0.4551\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 1.2605 - acc: 0.6209 - val_loss: 1.4259 - val_acc: 0.5473\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 1.2242 - acc: 0.6325 - val_loss: 1.7886 - val_acc: 0.4638\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 1.1829 - acc: 0.6498 - val_loss: 1.8148 - val_acc: 0.4704\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 1.1504 - acc: 0.6607 - val_loss: 1.4400 - val_acc: 0.5650\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 1.1176 - acc: 0.6754 - val_loss: 1.9417 - val_acc: 0.4535\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 1.0953 - acc: 0.6871 - val_loss: 1.4827 - val_acc: 0.5666\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 1.0627 - acc: 0.6978 - val_loss: 1.5301 - val_acc: 0.5481\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 1.0319 - acc: 0.7132 - val_loss: 1.6050 - val_acc: 0.5433\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 1.0266 - acc: 0.7229 - val_loss: 1.7457 - val_acc: 0.5256\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 1.0091 - acc: 0.7335 - val_loss: 1.4259 - val_acc: 0.5812\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.9896 - acc: 0.7453 - val_loss: 1.6174 - val_acc: 0.5461\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.9831 - acc: 0.7533 - val_loss: 1.6096 - val_acc: 0.5567\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.9573 - acc: 0.7635 - val_loss: 1.5793 - val_acc: 0.5725\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.9534 - acc: 0.7686 - val_loss: 1.5700 - val_acc: 0.5457\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.9465 - acc: 0.7721 - val_loss: 1.8442 - val_acc: 0.5307\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.9308 - acc: 0.7791 - val_loss: 1.9782 - val_acc: 0.5260\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.9303 - acc: 0.7798 - val_loss: 1.9194 - val_acc: 0.5284\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.9095 - acc: 0.7916 - val_loss: 1.7034 - val_acc: 0.5701\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.9094 - acc: 0.7949 - val_loss: 2.4860 - val_acc: 0.4370\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.9129 - acc: 0.7990 - val_loss: 1.5931 - val_acc: 0.5772\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.8899 - acc: 0.8104 - val_loss: 2.0937 - val_acc: 0.4996\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.8857 - acc: 0.8095 - val_loss: 1.6805 - val_acc: 0.5835\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.8863 - acc: 0.8120 - val_loss: 1.9874 - val_acc: 0.5496\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.8719 - acc: 0.8156 - val_loss: 1.6118 - val_acc: 0.5902\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.8762 - acc: 0.8165 - val_loss: 1.7076 - val_acc: 0.5709\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.8624 - acc: 0.8262 - val_loss: 1.7677 - val_acc: 0.5729\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.8599 - acc: 0.8280 - val_loss: 2.0339 - val_acc: 0.5236\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.8511 - acc: 0.8331 - val_loss: 1.7787 - val_acc: 0.5780\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.8626 - acc: 0.8287 - val_loss: 1.9620 - val_acc: 0.5489\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8545 - acc: 0.8305 - val_loss: 1.7450 - val_acc: 0.5831\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.8476 - acc: 0.8358 - val_loss: 1.8888 - val_acc: 0.5749\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8412 - acc: 0.8359 - val_loss: 1.9280 - val_acc: 0.5548\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.8340 - acc: 0.8451 - val_loss: 1.7388 - val_acc: 0.5910\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.8469 - acc: 0.8397 - val_loss: 2.2989 - val_acc: 0.4905\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8326 - acc: 0.8451 - val_loss: 1.9713 - val_acc: 0.5571\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.8304 - acc: 0.8419 - val_loss: 1.9510 - val_acc: 0.5670\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.8244 - acc: 0.8462 - val_loss: 1.8433 - val_acc: 0.5800\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.8352 - acc: 0.8452 - val_loss: 1.8709 - val_acc: 0.5768\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.8299 - acc: 0.8507 - val_loss: 2.3654 - val_acc: 0.4933\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.8193 - acc: 0.8522 - val_loss: 1.9685 - val_acc: 0.5548\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8094 - acc: 0.8622 - val_loss: 2.2219 - val_acc: 0.5335\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.8201 - acc: 0.8517 - val_loss: 2.2907 - val_acc: 0.5024\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.8242 - acc: 0.8541 - val_loss: 2.0666 - val_acc: 0.5634\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8164 - acc: 0.8580 - val_loss: 2.0122 - val_acc: 0.5827\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.8040 - acc: 0.8606 - val_loss: 1.6985 - val_acc: 0.5902\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.8051 - acc: 0.8621 - val_loss: 2.0099 - val_acc: 0.5642\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8060 - acc: 0.8646 - val_loss: 2.2009 - val_acc: 0.5386\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8017 - acc: 0.8639 - val_loss: 2.1718 - val_acc: 0.5465\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.8153 - acc: 0.8604 - val_loss: 3.0274 - val_acc: 0.4078\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.8006 - acc: 0.8664 - val_loss: 2.6499 - val_acc: 0.5209\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8012 - acc: 0.8661 - val_loss: 2.0816 - val_acc: 0.5705\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7996 - acc: 0.8662 - val_loss: 1.9322 - val_acc: 0.5642\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.8145 - acc: 0.8628 - val_loss: 1.9120 - val_acc: 0.5760\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.8026 - acc: 0.8637 - val_loss: 1.9555 - val_acc: 0.5701\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.7950 - acc: 0.8680 - val_loss: 2.0335 - val_acc: 0.5548\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7967 - acc: 0.8692 - val_loss: 1.7965 - val_acc: 0.6001\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.7878 - acc: 0.8743 - val_loss: 2.4214 - val_acc: 0.5059\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.8113 - acc: 0.8638 - val_loss: 2.8876 - val_acc: 0.5063\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7968 - acc: 0.8679 - val_loss: 2.3969 - val_acc: 0.5059\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7863 - acc: 0.8752 - val_loss: 2.5188 - val_acc: 0.5071\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7963 - acc: 0.8707 - val_loss: 2.9317 - val_acc: 0.5165\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.8030 - acc: 0.8661 - val_loss: 2.1105 - val_acc: 0.5536\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7859 - acc: 0.8743 - val_loss: 2.0188 - val_acc: 0.5776\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7804 - acc: 0.8729 - val_loss: 3.2316 - val_acc: 0.4251\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7828 - acc: 0.8718 - val_loss: 2.4378 - val_acc: 0.5272\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7802 - acc: 0.8735 - val_loss: 2.3038 - val_acc: 0.5418\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7888 - acc: 0.8681 - val_loss: 2.8218 - val_acc: 0.4775\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7912 - acc: 0.8705 - val_loss: 1.9968 - val_acc: 0.5591\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.7780 - acc: 0.8747 - val_loss: 1.7666 - val_acc: 0.5930\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7790 - acc: 0.8734 - val_loss: 2.0001 - val_acc: 0.5634\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7764 - acc: 0.8757 - val_loss: 2.4572 - val_acc: 0.5099\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7852 - acc: 0.8797 - val_loss: 2.0119 - val_acc: 0.5626\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7681 - acc: 0.8779 - val_loss: 3.1056 - val_acc: 0.4791\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.7802 - acc: 0.8759 - val_loss: 2.6956 - val_acc: 0.4898\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7799 - acc: 0.8748 - val_loss: 2.5518 - val_acc: 0.4811\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7616 - acc: 0.8840 - val_loss: 1.8844 - val_acc: 0.5729\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7891 - acc: 0.8763 - val_loss: 2.3199 - val_acc: 0.5386\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.7937 - acc: 0.8683 - val_loss: 2.0445 - val_acc: 0.5788\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7734 - acc: 0.8788 - val_loss: 1.9606 - val_acc: 0.5611\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7764 - acc: 0.8787 - val_loss: 2.2898 - val_acc: 0.5232\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7926 - acc: 0.8734 - val_loss: 2.0350 - val_acc: 0.5335\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7652 - acc: 0.8792 - val_loss: 1.8557 - val_acc: 0.5851\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7731 - acc: 0.8799 - val_loss: 2.5465 - val_acc: 0.4645\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7775 - acc: 0.8757 - val_loss: 2.6749 - val_acc: 0.5185\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7735 - acc: 0.8790 - val_loss: 1.9504 - val_acc: 0.5630\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7776 - acc: 0.8804 - val_loss: 2.3304 - val_acc: 0.5177\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7661 - acc: 0.8809 - val_loss: 2.4678 - val_acc: 0.5264\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7758 - acc: 0.8785 - val_loss: 2.1067 - val_acc: 0.5729\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7710 - acc: 0.8787 - val_loss: 3.3123 - val_acc: 0.4429\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7709 - acc: 0.8791 - val_loss: 2.0957 - val_acc: 0.5571\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7740 - acc: 0.8766 - val_loss: 2.5675 - val_acc: 0.5276\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.7543 - acc: 0.8860 - val_loss: 1.9718 - val_acc: 0.5772\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7746 - acc: 0.8778 - val_loss: 2.1839 - val_acc: 0.5540\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7622 - acc: 0.8824 - val_loss: 2.1624 - val_acc: 0.5481\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7691 - acc: 0.8826 - val_loss: 2.5261 - val_acc: 0.4909\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7622 - acc: 0.8831 - val_loss: 2.4883 - val_acc: 0.5079\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.7587 - acc: 0.8818 - val_loss: 2.1149 - val_acc: 0.5548\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.7655 - acc: 0.8814 - val_loss: 2.3374 - val_acc: 0.5489\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7620 - acc: 0.8837 - val_loss: 1.9458 - val_acc: 0.5812\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.7591 - acc: 0.8882 - val_loss: 3.0187 - val_acc: 0.4783\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.7706 - acc: 0.8781 - val_loss: 2.3826 - val_acc: 0.5248\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.7591 - acc: 0.8863 - val_loss: 1.9134 - val_acc: 0.5843\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7542 - acc: 0.8873 - val_loss: 2.4557 - val_acc: 0.5154\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.7558 - acc: 0.8851 - val_loss: 2.1739 - val_acc: 0.5776\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7625 - acc: 0.8847 - val_loss: 2.0927 - val_acc: 0.5753\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.7619 - acc: 0.8844 - val_loss: 2.1861 - val_acc: 0.5433\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.7613 - acc: 0.8846 - val_loss: 2.1444 - val_acc: 0.5493\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.7538 - acc: 0.8864 - val_loss: 2.0034 - val_acc: 0.5910\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7483 - acc: 0.8892 - val_loss: 1.9989 - val_acc: 0.5654\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7749 - acc: 0.8799 - val_loss: 2.7021 - val_acc: 0.5240\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7515 - acc: 0.8867 - val_loss: 1.9840 - val_acc: 0.5823\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7593 - acc: 0.8830 - val_loss: 2.6835 - val_acc: 0.5016\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7460 - acc: 0.8893 - val_loss: 2.2868 - val_acc: 0.5587\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7554 - acc: 0.8828 - val_loss: 1.9681 - val_acc: 0.5646\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7612 - acc: 0.8824 - val_loss: 2.4952 - val_acc: 0.5020\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7453 - acc: 0.8911 - val_loss: 2.1800 - val_acc: 0.5705\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7562 - acc: 0.8852 - val_loss: 2.0941 - val_acc: 0.5532\n",
            "Epoch 127/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7490 - acc: 0.8894 - val_loss: 2.0491 - val_acc: 0.5623\n",
            "Epoch 128/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7476 - acc: 0.8925 - val_loss: 2.0319 - val_acc: 0.5626\n",
            "Epoch 129/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7452 - acc: 0.8904 - val_loss: 2.3073 - val_acc: 0.5366\n",
            "Epoch 130/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.7541 - acc: 0.8834 - val_loss: 2.0591 - val_acc: 0.5646\n",
            "Epoch 131/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7390 - acc: 0.8931 - val_loss: 2.0278 - val_acc: 0.5741\n",
            "Epoch 132/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.7556 - acc: 0.8881 - val_loss: 2.3025 - val_acc: 0.5437\n",
            "Epoch 133/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7551 - acc: 0.8832 - val_loss: 2.2012 - val_acc: 0.5595\n",
            "Epoch 134/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7581 - acc: 0.8837 - val_loss: 2.6378 - val_acc: 0.4929\n",
            "Epoch 135/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7454 - acc: 0.8872 - val_loss: 2.3478 - val_acc: 0.5359\n",
            "Epoch 136/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.7511 - acc: 0.8846 - val_loss: 2.6128 - val_acc: 0.5248\n",
            "Epoch 137/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7462 - acc: 0.8885 - val_loss: 2.1150 - val_acc: 0.5426\n",
            "Epoch 138/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7465 - acc: 0.8894 - val_loss: 4.2008 - val_acc: 0.3637\n",
            "Epoch 139/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7372 - acc: 0.8925 - val_loss: 2.9668 - val_acc: 0.5035\n",
            "Epoch 140/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7466 - acc: 0.8910 - val_loss: 2.2992 - val_acc: 0.5362\n",
            "Epoch 141/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.7441 - acc: 0.8868 - val_loss: 2.6665 - val_acc: 0.5106\n",
            "Epoch 142/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.7520 - acc: 0.8857 - val_loss: 3.0924 - val_acc: 0.4775\n",
            "Epoch 143/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7500 - acc: 0.8887 - val_loss: 2.0400 - val_acc: 0.5630\n",
            "Epoch 144/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7484 - acc: 0.8861 - val_loss: 2.5138 - val_acc: 0.5422\n",
            "Epoch 145/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7374 - acc: 0.8943 - val_loss: 1.8883 - val_acc: 0.5961\n",
            "Epoch 146/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7518 - acc: 0.8845 - val_loss: 2.1925 - val_acc: 0.5516\n",
            "Epoch 147/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.7429 - acc: 0.8914 - val_loss: 2.4856 - val_acc: 0.5185\n",
            "Epoch 148/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7492 - acc: 0.8895 - val_loss: 2.5277 - val_acc: 0.5276\n",
            "Epoch 149/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7428 - acc: 0.8895 - val_loss: 2.0961 - val_acc: 0.5729\n",
            "Epoch 150/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7390 - acc: 0.8919 - val_loss: 2.2136 - val_acc: 0.5591\n",
            "Epoch 151/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7313 - acc: 0.8931 - val_loss: 2.2033 - val_acc: 0.5284\n",
            "Epoch 152/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7418 - acc: 0.8880 - val_loss: 2.2135 - val_acc: 0.5414\n",
            "Epoch 153/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7310 - acc: 0.8946 - val_loss: 2.1322 - val_acc: 0.5623\n",
            "Epoch 154/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7381 - acc: 0.8907 - val_loss: 2.3083 - val_acc: 0.5292\n",
            "Epoch 155/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7412 - acc: 0.8914 - val_loss: 3.7686 - val_acc: 0.4307\n",
            "Epoch 156/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7368 - acc: 0.8911 - val_loss: 2.5513 - val_acc: 0.5193\n",
            "Epoch 157/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.7409 - acc: 0.8891 - val_loss: 2.1011 - val_acc: 0.5768\n",
            "Epoch 158/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7427 - acc: 0.8886 - val_loss: 2.4087 - val_acc: 0.5508\n",
            "Epoch 159/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7447 - acc: 0.8891 - val_loss: 2.2605 - val_acc: 0.5457\n",
            "Epoch 160/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7289 - acc: 0.8945 - val_loss: 2.3211 - val_acc: 0.5382\n",
            "Epoch 161/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7328 - acc: 0.8920 - val_loss: 1.9869 - val_acc: 0.5804\n",
            "Epoch 162/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7272 - acc: 0.8948 - val_loss: 2.1779 - val_acc: 0.5540\n",
            "Epoch 163/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7280 - acc: 0.8955 - val_loss: 2.0242 - val_acc: 0.5540\n",
            "Epoch 164/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7284 - acc: 0.8955 - val_loss: 2.3799 - val_acc: 0.5410\n",
            "Epoch 165/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7415 - acc: 0.8903 - val_loss: 2.0289 - val_acc: 0.5721\n",
            "Epoch 166/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7359 - acc: 0.8942 - val_loss: 2.3925 - val_acc: 0.5398\n",
            "Epoch 167/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7271 - acc: 0.8973 - val_loss: 2.2419 - val_acc: 0.5776\n",
            "Epoch 168/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7362 - acc: 0.8931 - val_loss: 2.4011 - val_acc: 0.5197\n",
            "Epoch 169/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7391 - acc: 0.8941 - val_loss: 2.3202 - val_acc: 0.5493\n",
            "Epoch 170/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7392 - acc: 0.8946 - val_loss: 2.4712 - val_acc: 0.5390\n",
            "Epoch 171/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7347 - acc: 0.8918 - val_loss: 2.9726 - val_acc: 0.4862\n",
            "Epoch 172/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.7315 - acc: 0.8925 - val_loss: 2.5385 - val_acc: 0.5284\n",
            "Epoch 173/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7366 - acc: 0.8915 - val_loss: 2.3323 - val_acc: 0.5366\n",
            "Epoch 174/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7197 - acc: 0.8954 - val_loss: 2.5242 - val_acc: 0.5221\n",
            "Epoch 175/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7413 - acc: 0.8916 - val_loss: 3.0092 - val_acc: 0.4531\n",
            "Epoch 176/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7259 - acc: 0.8967 - val_loss: 2.3905 - val_acc: 0.5319\n",
            "Epoch 177/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7368 - acc: 0.8918 - val_loss: 2.0761 - val_acc: 0.5717\n",
            "Epoch 178/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.7254 - acc: 0.8966 - val_loss: 2.3925 - val_acc: 0.5343\n",
            "Epoch 179/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.7153 - acc: 0.9001 - val_loss: 2.5613 - val_acc: 0.5355\n",
            "Epoch 180/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.7305 - acc: 0.8947 - val_loss: 2.0656 - val_acc: 0.5674\n",
            "Epoch 181/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7418 - acc: 0.8879 - val_loss: 2.0745 - val_acc: 0.5693\n",
            "Epoch 182/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7255 - acc: 0.8982 - val_loss: 2.6195 - val_acc: 0.5099\n",
            "Epoch 183/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7353 - acc: 0.8924 - val_loss: 2.0820 - val_acc: 0.5603\n",
            "Epoch 184/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7279 - acc: 0.8941 - val_loss: 2.4381 - val_acc: 0.5177\n",
            "Epoch 185/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7297 - acc: 0.8962 - val_loss: 2.4475 - val_acc: 0.5205\n",
            "Epoch 186/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 0.7258 - acc: 0.8962 - val_loss: 2.8071 - val_acc: 0.5035\n",
            "Epoch 187/300\n",
            "10152/10152 [==============================] - 4s 368us/step - loss: 0.7343 - acc: 0.8914 - val_loss: 2.3111 - val_acc: 0.5327\n",
            "Epoch 188/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7363 - acc: 0.8964 - val_loss: 2.2411 - val_acc: 0.5366\n",
            "Epoch 189/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7281 - acc: 0.8971 - val_loss: 3.1875 - val_acc: 0.4807\n",
            "Epoch 190/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7198 - acc: 0.8990 - val_loss: 3.3748 - val_acc: 0.4559\n",
            "Epoch 191/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 0.7205 - acc: 0.8983 - val_loss: 2.0527 - val_acc: 0.5729\n",
            "Epoch 192/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7360 - acc: 0.8878 - val_loss: 2.6385 - val_acc: 0.5177\n",
            "Epoch 193/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 0.7312 - acc: 0.8954 - val_loss: 3.2930 - val_acc: 0.4504\n",
            "Epoch 194/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7341 - acc: 0.8935 - val_loss: 2.3530 - val_acc: 0.5701\n",
            "Epoch 195/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 0.7258 - acc: 0.8957 - val_loss: 2.3140 - val_acc: 0.5414\n",
            "Epoch 196/300\n",
            "10152/10152 [==============================] - 4s 375us/step - loss: 0.7288 - acc: 0.8955 - val_loss: 1.9186 - val_acc: 0.5796\n",
            "Epoch 197/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 0.7270 - acc: 0.8970 - val_loss: 2.6569 - val_acc: 0.5087\n",
            "Epoch 198/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 0.7290 - acc: 0.8932 - val_loss: 2.0884 - val_acc: 0.5776\n",
            "Epoch 199/300\n",
            "10152/10152 [==============================] - 4s 374us/step - loss: 0.7294 - acc: 0.8945 - val_loss: 2.1702 - val_acc: 0.5717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|         | 26/256 [4:23:41<37:32:09, 587.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 476us/step - loss: 6.1647 - acc: 0.2935 - val_loss: 5.5771 - val_acc: 0.2904\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 376us/step - loss: 4.6084 - acc: 0.3559 - val_loss: 3.9587 - val_acc: 0.3519\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 369us/step - loss: 3.4493 - acc: 0.4406 - val_loss: 3.2572 - val_acc: 0.3775\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 362us/step - loss: 2.6645 - acc: 0.5086 - val_loss: 2.4872 - val_acc: 0.4476\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 369us/step - loss: 2.1482 - acc: 0.5577 - val_loss: 2.1846 - val_acc: 0.4590\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 366us/step - loss: 1.8059 - acc: 0.5903 - val_loss: 1.8789 - val_acc: 0.4901\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 362us/step - loss: 1.5795 - acc: 0.6169 - val_loss: 1.8028 - val_acc: 0.5162\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 370us/step - loss: 1.4207 - acc: 0.6471 - val_loss: 1.9644 - val_acc: 0.4661\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 365us/step - loss: 1.3442 - acc: 0.6501 - val_loss: 1.6244 - val_acc: 0.5441\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 365us/step - loss: 1.2797 - acc: 0.6659 - val_loss: 2.1672 - val_acc: 0.4582\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 366us/step - loss: 1.1973 - acc: 0.6867 - val_loss: 1.7630 - val_acc: 0.4937\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 362us/step - loss: 1.1682 - acc: 0.6953 - val_loss: 1.5133 - val_acc: 0.5634\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 364us/step - loss: 1.1362 - acc: 0.7035 - val_loss: 1.8428 - val_acc: 0.5193\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 369us/step - loss: 1.1269 - acc: 0.7123 - val_loss: 1.7389 - val_acc: 0.5229\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 1.0841 - acc: 0.7294 - val_loss: 1.5800 - val_acc: 0.5709\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 1.0769 - acc: 0.7301 - val_loss: 1.8122 - val_acc: 0.5205\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 1.0207 - acc: 0.7566 - val_loss: 1.5929 - val_acc: 0.5875\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 1.0210 - acc: 0.7555 - val_loss: 2.1178 - val_acc: 0.5162\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 1.0186 - acc: 0.7599 - val_loss: 2.0007 - val_acc: 0.4882\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 1.0305 - acc: 0.7598 - val_loss: 1.6323 - val_acc: 0.5784\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.9533 - acc: 0.7926 - val_loss: 1.8241 - val_acc: 0.5504\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.9587 - acc: 0.7952 - val_loss: 1.8388 - val_acc: 0.5500\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 0.9814 - acc: 0.7819 - val_loss: 1.7867 - val_acc: 0.5713\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.9835 - acc: 0.7877 - val_loss: 1.8485 - val_acc: 0.5500\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.9596 - acc: 0.7997 - val_loss: 2.1186 - val_acc: 0.5225\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.9257 - acc: 0.8158 - val_loss: 1.8242 - val_acc: 0.5489\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.9108 - acc: 0.8196 - val_loss: 2.6516 - val_acc: 0.4421\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.9263 - acc: 0.8125 - val_loss: 1.8348 - val_acc: 0.5567\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.9146 - acc: 0.8257 - val_loss: 2.0250 - val_acc: 0.5493\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.9031 - acc: 0.8282 - val_loss: 1.8518 - val_acc: 0.5662\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.8755 - acc: 0.8381 - val_loss: 1.8389 - val_acc: 0.5619\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.8693 - acc: 0.8462 - val_loss: 1.8168 - val_acc: 0.5859\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.8620 - acc: 0.8478 - val_loss: 1.7748 - val_acc: 0.5969\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.8265 - acc: 0.8621 - val_loss: 1.9151 - val_acc: 0.5760\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.8594 - acc: 0.8485 - val_loss: 1.8375 - val_acc: 0.5780\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.8590 - acc: 0.8518 - val_loss: 2.3329 - val_acc: 0.5130\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.8631 - acc: 0.8513 - val_loss: 1.8567 - val_acc: 0.5768\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.8231 - acc: 0.8624 - val_loss: 1.8470 - val_acc: 0.5764\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.8455 - acc: 0.8585 - val_loss: 1.8304 - val_acc: 0.5808\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.8280 - acc: 0.8638 - val_loss: 1.8539 - val_acc: 0.5973\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.8306 - acc: 0.8607 - val_loss: 1.9461 - val_acc: 0.5662\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.8376 - acc: 0.8654 - val_loss: 2.2037 - val_acc: 0.5496\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.8296 - acc: 0.8671 - val_loss: 2.1330 - val_acc: 0.5382\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.8260 - acc: 0.8670 - val_loss: 2.0151 - val_acc: 0.5662\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7958 - acc: 0.8781 - val_loss: 2.3001 - val_acc: 0.5272\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.8602 - acc: 0.8564 - val_loss: 2.2032 - val_acc: 0.5508\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.8318 - acc: 0.8680 - val_loss: 1.9161 - val_acc: 0.5946\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.8189 - acc: 0.8717 - val_loss: 2.0905 - val_acc: 0.5489\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.8198 - acc: 0.8752 - val_loss: 1.8499 - val_acc: 0.5887\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7825 - acc: 0.8823 - val_loss: 2.1147 - val_acc: 0.5445\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7993 - acc: 0.8786 - val_loss: 1.9882 - val_acc: 0.5757\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7919 - acc: 0.8797 - val_loss: 2.5055 - val_acc: 0.5130\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.8411 - acc: 0.8627 - val_loss: 2.3323 - val_acc: 0.5276\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.8125 - acc: 0.8792 - val_loss: 1.8409 - val_acc: 0.6032\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7744 - acc: 0.8897 - val_loss: 1.9406 - val_acc: 0.5800\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7707 - acc: 0.8903 - val_loss: 2.0536 - val_acc: 0.5650\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7476 - acc: 0.9009 - val_loss: 1.8708 - val_acc: 0.5969\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7825 - acc: 0.8795 - val_loss: 2.1163 - val_acc: 0.5355\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7904 - acc: 0.8816 - val_loss: 2.1076 - val_acc: 0.5678\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.8231 - acc: 0.8704 - val_loss: 2.1404 - val_acc: 0.5678\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7876 - acc: 0.8847 - val_loss: 1.9527 - val_acc: 0.5867\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7873 - acc: 0.8845 - val_loss: 1.9661 - val_acc: 0.5745\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.8066 - acc: 0.8799 - val_loss: 2.1318 - val_acc: 0.5520\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.7718 - acc: 0.8903 - val_loss: 2.0723 - val_acc: 0.5493\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.8053 - acc: 0.8801 - val_loss: 2.3049 - val_acc: 0.5540\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7450 - acc: 0.9013 - val_loss: 2.0884 - val_acc: 0.5796\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7440 - acc: 0.9014 - val_loss: 2.5013 - val_acc: 0.5150\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.8015 - acc: 0.8768 - val_loss: 2.0419 - val_acc: 0.5851\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7582 - acc: 0.8992 - val_loss: 2.0262 - val_acc: 0.5741\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.7512 - acc: 0.8955 - val_loss: 2.2594 - val_acc: 0.5374\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7324 - acc: 0.9017 - val_loss: 2.0545 - val_acc: 0.5571\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.7440 - acc: 0.8982 - val_loss: 1.9490 - val_acc: 0.5985\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 0.7539 - acc: 0.8969 - val_loss: 1.9292 - val_acc: 0.6032\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7892 - acc: 0.8834 - val_loss: 2.0598 - val_acc: 0.5768\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.8241 - acc: 0.8726 - val_loss: 2.2030 - val_acc: 0.5607\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 0.7821 - acc: 0.8920 - val_loss: 2.1619 - val_acc: 0.5985\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 0.7396 - acc: 0.9061 - val_loss: 1.9975 - val_acc: 0.5745\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7482 - acc: 0.8980 - val_loss: 2.1650 - val_acc: 0.5686\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7892 - acc: 0.8850 - val_loss: 2.6767 - val_acc: 0.5229\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.8387 - acc: 0.8650 - val_loss: 2.1315 - val_acc: 0.5559\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7844 - acc: 0.8914 - val_loss: 1.9041 - val_acc: 0.5942\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.7397 - acc: 0.9042 - val_loss: 2.3763 - val_acc: 0.5386\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.7429 - acc: 0.9026 - val_loss: 1.8929 - val_acc: 0.6013\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.7073 - acc: 0.9112 - val_loss: 1.8859 - val_acc: 0.6009\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7138 - acc: 0.9064 - val_loss: 2.1997 - val_acc: 0.5690\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.7660 - acc: 0.8847 - val_loss: 2.0973 - val_acc: 0.5630\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.8074 - acc: 0.8778 - val_loss: 1.9230 - val_acc: 0.5993\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.7449 - acc: 0.9011 - val_loss: 1.9812 - val_acc: 0.5788\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7524 - acc: 0.9025 - val_loss: 2.0625 - val_acc: 0.5961\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7873 - acc: 0.8859 - val_loss: 1.9104 - val_acc: 0.5890\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7684 - acc: 0.8925 - val_loss: 1.9161 - val_acc: 0.5867\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7396 - acc: 0.9061 - val_loss: 2.1290 - val_acc: 0.5642\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.7520 - acc: 0.8984 - val_loss: 1.9736 - val_acc: 0.5942\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7259 - acc: 0.9060 - val_loss: 1.9885 - val_acc: 0.5823\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7156 - acc: 0.9081 - val_loss: 1.9021 - val_acc: 0.5977\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7345 - acc: 0.8972 - val_loss: 1.9112 - val_acc: 0.6005\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7232 - acc: 0.9065 - val_loss: 2.2249 - val_acc: 0.5630\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7600 - acc: 0.8919 - val_loss: 2.5696 - val_acc: 0.5473\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.7721 - acc: 0.8897 - val_loss: 2.2505 - val_acc: 0.5508\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.7193 - acc: 0.9086 - val_loss: 2.0658 - val_acc: 0.5859\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7457 - acc: 0.8975 - val_loss: 2.3037 - val_acc: 0.5548\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.7384 - acc: 0.9020 - val_loss: 2.1821 - val_acc: 0.5670\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.7330 - acc: 0.9048 - val_loss: 2.0428 - val_acc: 0.5678\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7534 - acc: 0.8988 - val_loss: 2.2028 - val_acc: 0.5827\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 11%|         | 27/256 [4:30:36<34:04:18, 535.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 462us/step - loss: 6.0723 - acc: 0.2858 - val_loss: 5.6809 - val_acc: 0.2569\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 3.9769 - acc: 0.3457 - val_loss: 3.0654 - val_acc: 0.3574\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 2.8349 - acc: 0.4175 - val_loss: 2.3369 - val_acc: 0.4180\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 2.2220 - acc: 0.4715 - val_loss: 1.9505 - val_acc: 0.4854\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 1.8900 - acc: 0.5093 - val_loss: 1.8239 - val_acc: 0.4779\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 1.6502 - acc: 0.5485 - val_loss: 1.8481 - val_acc: 0.4224\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 1.5143 - acc: 0.5703 - val_loss: 1.6830 - val_acc: 0.4831\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 1.4175 - acc: 0.5840 - val_loss: 1.6245 - val_acc: 0.4984\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 1.3442 - acc: 0.5956 - val_loss: 1.5628 - val_acc: 0.5299\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 1.2662 - acc: 0.6274 - val_loss: 1.6915 - val_acc: 0.5099\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 1.2279 - acc: 0.6372 - val_loss: 1.4852 - val_acc: 0.5473\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 1.1809 - acc: 0.6574 - val_loss: 1.4300 - val_acc: 0.5508\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 1.1459 - acc: 0.6639 - val_loss: 1.5018 - val_acc: 0.5386\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 1.1192 - acc: 0.6677 - val_loss: 1.7671 - val_acc: 0.4980\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 374us/step - loss: 1.0848 - acc: 0.6916 - val_loss: 1.6753 - val_acc: 0.5213\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 1.0628 - acc: 0.7049 - val_loss: 1.5390 - val_acc: 0.5311\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 1.0448 - acc: 0.7141 - val_loss: 1.5477 - val_acc: 0.5457\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 377us/step - loss: 1.0155 - acc: 0.7255 - val_loss: 1.7697 - val_acc: 0.5457\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 1.0044 - acc: 0.7373 - val_loss: 2.3380 - val_acc: 0.4334\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 0.9824 - acc: 0.7472 - val_loss: 1.5220 - val_acc: 0.5887\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 0.9718 - acc: 0.7450 - val_loss: 1.6242 - val_acc: 0.5729\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 0.9609 - acc: 0.7567 - val_loss: 1.6595 - val_acc: 0.5489\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.9515 - acc: 0.7661 - val_loss: 1.5449 - val_acc: 0.5788\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 377us/step - loss: 0.9476 - acc: 0.7732 - val_loss: 1.5874 - val_acc: 0.5898\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.9319 - acc: 0.7792 - val_loss: 2.0897 - val_acc: 0.4913\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.9191 - acc: 0.7852 - val_loss: 1.7577 - val_acc: 0.5567\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 374us/step - loss: 0.9110 - acc: 0.7918 - val_loss: 2.6196 - val_acc: 0.4748\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 0.9101 - acc: 0.7947 - val_loss: 2.2400 - val_acc: 0.4886\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.8990 - acc: 0.7991 - val_loss: 2.2626 - val_acc: 0.4775\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 374us/step - loss: 0.8975 - acc: 0.8021 - val_loss: 1.7595 - val_acc: 0.5682\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.8893 - acc: 0.8074 - val_loss: 1.9286 - val_acc: 0.5390\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 372us/step - loss: 0.8785 - acc: 0.8126 - val_loss: 1.6953 - val_acc: 0.5626\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.8742 - acc: 0.8126 - val_loss: 1.8346 - val_acc: 0.5489\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.8679 - acc: 0.8170 - val_loss: 1.9954 - val_acc: 0.5292\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.8714 - acc: 0.8212 - val_loss: 1.6612 - val_acc: 0.5898\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 0.8679 - acc: 0.8217 - val_loss: 2.4064 - val_acc: 0.4980\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 0.8571 - acc: 0.8323 - val_loss: 2.0454 - val_acc: 0.5339\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 374us/step - loss: 0.8550 - acc: 0.8340 - val_loss: 1.7968 - val_acc: 0.5682\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 377us/step - loss: 0.8576 - acc: 0.8276 - val_loss: 1.7758 - val_acc: 0.5737\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 0.8543 - acc: 0.8332 - val_loss: 1.6863 - val_acc: 0.5942\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.8499 - acc: 0.8362 - val_loss: 1.6992 - val_acc: 0.5879\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 376us/step - loss: 0.8312 - acc: 0.8445 - val_loss: 2.0120 - val_acc: 0.5370\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.8488 - acc: 0.8362 - val_loss: 1.8059 - val_acc: 0.5701\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 374us/step - loss: 0.8359 - acc: 0.8430 - val_loss: 2.0838 - val_acc: 0.5575\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 375us/step - loss: 0.8321 - acc: 0.8474 - val_loss: 1.9774 - val_acc: 0.5508\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 377us/step - loss: 0.8350 - acc: 0.8452 - val_loss: 2.7347 - val_acc: 0.4905\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 375us/step - loss: 0.8292 - acc: 0.8434 - val_loss: 2.2931 - val_acc: 0.5284\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 377us/step - loss: 0.8283 - acc: 0.8500 - val_loss: 2.0080 - val_acc: 0.5473\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.8236 - acc: 0.8535 - val_loss: 1.9100 - val_acc: 0.5630\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.8231 - acc: 0.8506 - val_loss: 1.9920 - val_acc: 0.5331\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 0.8188 - acc: 0.8540 - val_loss: 2.5452 - val_acc: 0.4764\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 0.8198 - acc: 0.8537 - val_loss: 1.8990 - val_acc: 0.5599\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 373us/step - loss: 0.8166 - acc: 0.8560 - val_loss: 1.8332 - val_acc: 0.5855\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.8290 - acc: 0.8513 - val_loss: 2.1667 - val_acc: 0.5481\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 0.8112 - acc: 0.8577 - val_loss: 1.8027 - val_acc: 0.5721\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 0.8038 - acc: 0.8613 - val_loss: 2.1151 - val_acc: 0.5351\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.8041 - acc: 0.8555 - val_loss: 1.9237 - val_acc: 0.5611\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 0.8130 - acc: 0.8554 - val_loss: 1.8173 - val_acc: 0.5717\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 377us/step - loss: 0.8008 - acc: 0.8638 - val_loss: 2.0966 - val_acc: 0.5433\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 0.8162 - acc: 0.8545 - val_loss: 2.0393 - val_acc: 0.5520\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 0.7988 - acc: 0.8644 - val_loss: 1.9458 - val_acc: 0.5804\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 0.8126 - acc: 0.8579 - val_loss: 1.9313 - val_acc: 0.5847\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.8042 - acc: 0.8609 - val_loss: 2.2915 - val_acc: 0.5114\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7985 - acc: 0.8628 - val_loss: 2.4375 - val_acc: 0.5071\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.8029 - acc: 0.8610 - val_loss: 2.0285 - val_acc: 0.5721\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.8000 - acc: 0.8652 - val_loss: 2.1740 - val_acc: 0.5496\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7781 - acc: 0.8726 - val_loss: 1.9591 - val_acc: 0.5729\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.8054 - acc: 0.8642 - val_loss: 2.1274 - val_acc: 0.5441\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7971 - acc: 0.8644 - val_loss: 2.0777 - val_acc: 0.5355\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7788 - acc: 0.8740 - val_loss: 1.8505 - val_acc: 0.5906\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7996 - acc: 0.8660 - val_loss: 2.2609 - val_acc: 0.5579\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7857 - acc: 0.8695 - val_loss: 1.7959 - val_acc: 0.5839\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7814 - acc: 0.8733 - val_loss: 1.8686 - val_acc: 0.5946\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7823 - acc: 0.8713 - val_loss: 2.1867 - val_acc: 0.5493\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7902 - acc: 0.8695 - val_loss: 2.5797 - val_acc: 0.5091\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7783 - acc: 0.8698 - val_loss: 2.0983 - val_acc: 0.5473\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7841 - acc: 0.8687 - val_loss: 1.9945 - val_acc: 0.5603\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7885 - acc: 0.8707 - val_loss: 1.9797 - val_acc: 0.5583\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7842 - acc: 0.8711 - val_loss: 2.0774 - val_acc: 0.5729\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7807 - acc: 0.8734 - val_loss: 2.0876 - val_acc: 0.5674\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7708 - acc: 0.8766 - val_loss: 2.0882 - val_acc: 0.5559\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7740 - acc: 0.8804 - val_loss: 3.0811 - val_acc: 0.4571\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 0.7790 - acc: 0.8728 - val_loss: 1.9985 - val_acc: 0.5851\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.7733 - acc: 0.8768 - val_loss: 1.7966 - val_acc: 0.6103\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7706 - acc: 0.8756 - val_loss: 2.1122 - val_acc: 0.5552\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7669 - acc: 0.8813 - val_loss: 1.9802 - val_acc: 0.5678\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7654 - acc: 0.8797 - val_loss: 1.9434 - val_acc: 0.5721\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.7700 - acc: 0.8760 - val_loss: 2.1253 - val_acc: 0.5567\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.7650 - acc: 0.8812 - val_loss: 1.9630 - val_acc: 0.5674\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.7669 - acc: 0.8776 - val_loss: 2.1222 - val_acc: 0.5556\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7751 - acc: 0.8738 - val_loss: 1.9302 - val_acc: 0.5918\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7640 - acc: 0.8801 - val_loss: 2.2715 - val_acc: 0.5485\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7722 - acc: 0.8762 - val_loss: 2.2698 - val_acc: 0.5465\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7725 - acc: 0.8800 - val_loss: 2.3854 - val_acc: 0.5169\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7743 - acc: 0.8759 - val_loss: 2.2072 - val_acc: 0.5469\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 377us/step - loss: 0.7767 - acc: 0.8745 - val_loss: 2.1700 - val_acc: 0.5489\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7679 - acc: 0.8792 - val_loss: 2.1230 - val_acc: 0.5437\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7666 - acc: 0.8790 - val_loss: 2.0307 - val_acc: 0.5587\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 380us/step - loss: 0.7629 - acc: 0.8827 - val_loss: 1.9487 - val_acc: 0.5721\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7561 - acc: 0.8822 - val_loss: 1.9709 - val_acc: 0.5816\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7652 - acc: 0.8799 - val_loss: 2.0252 - val_acc: 0.5749\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7595 - acc: 0.8806 - val_loss: 1.8557 - val_acc: 0.5800\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.7674 - acc: 0.8769 - val_loss: 2.5222 - val_acc: 0.5256\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7557 - acc: 0.8836 - val_loss: 2.2542 - val_acc: 0.5433\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7658 - acc: 0.8784 - val_loss: 2.0988 - val_acc: 0.5693\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7595 - acc: 0.8804 - val_loss: 2.8131 - val_acc: 0.4645\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7619 - acc: 0.8825 - val_loss: 2.1204 - val_acc: 0.5461\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7620 - acc: 0.8791 - val_loss: 1.8908 - val_acc: 0.5823\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7622 - acc: 0.8835 - val_loss: 2.0862 - val_acc: 0.5690\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7519 - acc: 0.8829 - val_loss: 2.2789 - val_acc: 0.5248\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7452 - acc: 0.8858 - val_loss: 1.9530 - val_acc: 0.5855\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.7593 - acc: 0.8847 - val_loss: 1.9788 - val_acc: 0.5725\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.7566 - acc: 0.8825 - val_loss: 1.9400 - val_acc: 0.5709\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7444 - acc: 0.8888 - val_loss: 2.1210 - val_acc: 0.5741\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7571 - acc: 0.8865 - val_loss: 1.9717 - val_acc: 0.5981\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7486 - acc: 0.8877 - val_loss: 1.9590 - val_acc: 0.5784\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7452 - acc: 0.8892 - val_loss: 2.2023 - val_acc: 0.5461\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7427 - acc: 0.8900 - val_loss: 2.3973 - val_acc: 0.5299\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7617 - acc: 0.8840 - val_loss: 2.9498 - val_acc: 0.4795\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7520 - acc: 0.8848 - val_loss: 2.2867 - val_acc: 0.5193\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7376 - acc: 0.8866 - val_loss: 2.6090 - val_acc: 0.5059\n",
            "Epoch 122/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7445 - acc: 0.8894 - val_loss: 2.0684 - val_acc: 0.5642\n",
            "Epoch 123/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7483 - acc: 0.8846 - val_loss: 2.0452 - val_acc: 0.5575\n",
            "Epoch 124/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7535 - acc: 0.8859 - val_loss: 1.9305 - val_acc: 0.5823\n",
            "Epoch 125/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.7454 - acc: 0.8857 - val_loss: 3.3120 - val_acc: 0.4779\n",
            "Epoch 126/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7443 - acc: 0.8894 - val_loss: 2.3580 - val_acc: 0.5260\n",
            "Epoch 127/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7417 - acc: 0.8888 - val_loss: 1.9772 - val_acc: 0.5890\n",
            "Epoch 128/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7467 - acc: 0.8863 - val_loss: 2.5151 - val_acc: 0.5402\n",
            "Epoch 129/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7404 - acc: 0.8889 - val_loss: 2.1110 - val_acc: 0.5670\n",
            "Epoch 130/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.7437 - acc: 0.8895 - val_loss: 2.1010 - val_acc: 0.5705\n",
            "Epoch 131/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7418 - acc: 0.8895 - val_loss: 1.9490 - val_acc: 0.5863\n",
            "Epoch 132/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7406 - acc: 0.8938 - val_loss: 1.8297 - val_acc: 0.5879\n",
            "Epoch 133/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7421 - acc: 0.8936 - val_loss: 2.2752 - val_acc: 0.5607\n",
            "Epoch 134/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7406 - acc: 0.8896 - val_loss: 2.1921 - val_acc: 0.5615\n",
            "Epoch 135/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7375 - acc: 0.8896 - val_loss: 2.0330 - val_acc: 0.5855\n",
            "Epoch 136/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.7444 - acc: 0.8875 - val_loss: 2.5704 - val_acc: 0.5000\n",
            "Epoch 137/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7472 - acc: 0.8888 - val_loss: 2.0615 - val_acc: 0.5768\n",
            "Epoch 138/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7481 - acc: 0.8908 - val_loss: 2.8333 - val_acc: 0.5043\n",
            "Epoch 139/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7399 - acc: 0.8891 - val_loss: 1.9281 - val_acc: 0.5898\n",
            "Epoch 140/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7440 - acc: 0.8909 - val_loss: 2.0550 - val_acc: 0.5686\n",
            "Epoch 141/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7413 - acc: 0.8894 - val_loss: 1.9530 - val_acc: 0.5772\n",
            "Epoch 142/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7483 - acc: 0.8915 - val_loss: 1.9436 - val_acc: 0.5977\n",
            "Epoch 143/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7538 - acc: 0.8842 - val_loss: 1.9200 - val_acc: 0.5879\n",
            "Epoch 144/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7483 - acc: 0.8854 - val_loss: 2.3282 - val_acc: 0.5567\n",
            "Epoch 145/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7432 - acc: 0.8880 - val_loss: 2.0804 - val_acc: 0.5658\n",
            "Epoch 146/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 0.7381 - acc: 0.8901 - val_loss: 2.4485 - val_acc: 0.5146\n",
            "Epoch 147/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7479 - acc: 0.8880 - val_loss: 2.2096 - val_acc: 0.5642\n",
            "Epoch 148/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.7346 - acc: 0.8940 - val_loss: 2.1096 - val_acc: 0.5603\n",
            "Epoch 149/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7382 - acc: 0.8910 - val_loss: 2.2888 - val_acc: 0.5469\n",
            "Epoch 150/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.7385 - acc: 0.8903 - val_loss: 2.0863 - val_acc: 0.5540\n",
            "Epoch 151/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.7508 - acc: 0.8837 - val_loss: 2.5900 - val_acc: 0.5177\n",
            "Epoch 152/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7337 - acc: 0.8961 - val_loss: 2.9458 - val_acc: 0.4945\n",
            "Epoch 153/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.7452 - acc: 0.8884 - val_loss: 2.0690 - val_acc: 0.5839\n",
            "Epoch 154/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7430 - acc: 0.8896 - val_loss: 2.0631 - val_acc: 0.5820\n",
            "Epoch 155/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.7367 - acc: 0.8906 - val_loss: 2.1896 - val_acc: 0.5879\n",
            "Epoch 156/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7318 - acc: 0.8944 - val_loss: 2.0179 - val_acc: 0.5867\n",
            "Epoch 157/300\n",
            "10152/10152 [==============================] - 4s 381us/step - loss: 0.7382 - acc: 0.8922 - val_loss: 2.2432 - val_acc: 0.5579\n",
            "Epoch 158/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7433 - acc: 0.8886 - val_loss: 2.0595 - val_acc: 0.5993\n",
            "Epoch 159/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7354 - acc: 0.8932 - val_loss: 2.8572 - val_acc: 0.4756\n",
            "Epoch 160/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7327 - acc: 0.8938 - val_loss: 1.9480 - val_acc: 0.5808\n",
            "Epoch 161/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.7353 - acc: 0.8914 - val_loss: 2.0902 - val_acc: 0.5780\n",
            "Epoch 162/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7235 - acc: 0.8959 - val_loss: 2.3123 - val_acc: 0.5725\n",
            "Epoch 163/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7441 - acc: 0.8921 - val_loss: 2.4790 - val_acc: 0.5362\n",
            "Epoch 164/300\n",
            "10152/10152 [==============================] - 4s 379us/step - loss: 0.7339 - acc: 0.8910 - val_loss: 2.7419 - val_acc: 0.4953\n",
            "Epoch 165/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7291 - acc: 0.8953 - val_loss: 2.1397 - val_acc: 0.5658\n",
            "Epoch 166/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7471 - acc: 0.8864 - val_loss: 2.0519 - val_acc: 0.5823\n",
            "Epoch 167/300\n",
            "10152/10152 [==============================] - 4s 385us/step - loss: 0.7506 - acc: 0.8863 - val_loss: 2.0596 - val_acc: 0.5709\n",
            "Epoch 168/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7280 - acc: 0.8971 - val_loss: 2.3045 - val_acc: 0.5493\n",
            "Epoch 169/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7402 - acc: 0.8890 - val_loss: 2.6518 - val_acc: 0.5366\n",
            "Epoch 170/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7252 - acc: 0.8937 - val_loss: 2.5093 - val_acc: 0.5339\n",
            "Epoch 171/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.7343 - acc: 0.8918 - val_loss: 2.0120 - val_acc: 0.5855\n",
            "Epoch 172/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7336 - acc: 0.8915 - val_loss: 2.0155 - val_acc: 0.5863\n",
            "Epoch 173/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7420 - acc: 0.8886 - val_loss: 2.3070 - val_acc: 0.5461\n",
            "Epoch 174/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7305 - acc: 0.8962 - val_loss: 2.2852 - val_acc: 0.5481\n",
            "Epoch 175/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.7416 - acc: 0.8879 - val_loss: 2.0922 - val_acc: 0.5713\n",
            "Epoch 176/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.7298 - acc: 0.8974 - val_loss: 2.3893 - val_acc: 0.5493\n",
            "Epoch 177/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7167 - acc: 0.8977 - val_loss: 3.1680 - val_acc: 0.4850\n",
            "Epoch 178/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7335 - acc: 0.8915 - val_loss: 2.8751 - val_acc: 0.4886\n",
            "Epoch 179/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7397 - acc: 0.8880 - val_loss: 3.1138 - val_acc: 0.4957\n",
            "Epoch 180/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7381 - acc: 0.8900 - val_loss: 2.3109 - val_acc: 0.5445\n",
            "Epoch 181/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7334 - acc: 0.8916 - val_loss: 2.1266 - val_acc: 0.5658\n",
            "Epoch 182/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.7276 - acc: 0.8951 - val_loss: 2.0566 - val_acc: 0.5914\n",
            "Epoch 183/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.7288 - acc: 0.8969 - val_loss: 1.9938 - val_acc: 0.5808\n",
            "Epoch 184/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 0.7371 - acc: 0.8924 - val_loss: 2.0266 - val_acc: 0.5760\n",
            "Epoch 185/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7269 - acc: 0.8994 - val_loss: 1.9963 - val_acc: 0.5776\n",
            "Epoch 186/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7224 - acc: 0.8946 - val_loss: 2.2527 - val_acc: 0.5524\n",
            "Epoch 187/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7274 - acc: 0.8945 - val_loss: 1.9091 - val_acc: 0.5989\n",
            "Epoch 188/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7341 - acc: 0.8954 - val_loss: 2.2072 - val_acc: 0.5520\n",
            "Epoch 189/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.7402 - acc: 0.8913 - val_loss: 2.7868 - val_acc: 0.5315\n",
            "Epoch 190/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7230 - acc: 0.8945 - val_loss: 2.0235 - val_acc: 0.5827\n",
            "Epoch 191/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.7332 - acc: 0.8935 - val_loss: 2.1281 - val_acc: 0.5879\n",
            "Epoch 192/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.7271 - acc: 0.8944 - val_loss: 2.8300 - val_acc: 0.4988\n",
            "Epoch 193/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7303 - acc: 0.8925 - val_loss: 2.2196 - val_acc: 0.5489\n",
            "Epoch 194/300\n",
            "10152/10152 [==============================] - 4s 399us/step - loss: 0.7231 - acc: 0.8961 - val_loss: 2.3536 - val_acc: 0.5493\n",
            "Epoch 195/300\n",
            "10152/10152 [==============================] - 4s 382us/step - loss: 0.7220 - acc: 0.8972 - val_loss: 2.2149 - val_acc: 0.5725\n",
            "Epoch 196/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7305 - acc: 0.8920 - val_loss: 2.3578 - val_acc: 0.5485\n",
            "Epoch 197/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 0.7220 - acc: 0.8982 - val_loss: 2.2808 - val_acc: 0.5563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 11%|         | 28/256 [4:43:42<38:40:57, 610.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 480us/step - loss: 6.0726 - acc: 0.2916 - val_loss: 5.7004 - val_acc: 0.2778\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 367us/step - loss: 4.4931 - acc: 0.3724 - val_loss: 3.8819 - val_acc: 0.3448\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 369us/step - loss: 3.3606 - acc: 0.4383 - val_loss: 2.9372 - val_acc: 0.4145\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 373us/step - loss: 2.5581 - acc: 0.5168 - val_loss: 2.3251 - val_acc: 0.4878\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 2.0447 - acc: 0.5699 - val_loss: 2.0157 - val_acc: 0.5055\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 1.7363 - acc: 0.5985 - val_loss: 2.2749 - val_acc: 0.4121\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 1.5632 - acc: 0.6108 - val_loss: 1.8465 - val_acc: 0.5142\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 1.4443 - acc: 0.6192 - val_loss: 1.9395 - val_acc: 0.4744\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 1.3443 - acc: 0.6404 - val_loss: 1.5793 - val_acc: 0.5544\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 1.2310 - acc: 0.6702 - val_loss: 1.5704 - val_acc: 0.5595\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 1.2202 - acc: 0.6690 - val_loss: 1.8540 - val_acc: 0.4945\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 1.2141 - acc: 0.6692 - val_loss: 1.5331 - val_acc: 0.5634\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 387us/step - loss: 1.1089 - acc: 0.7063 - val_loss: 1.7703 - val_acc: 0.5209\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 1.0894 - acc: 0.7171 - val_loss: 1.8122 - val_acc: 0.4957\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 1.1148 - acc: 0.6983 - val_loss: 1.5463 - val_acc: 0.5705\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 1.0647 - acc: 0.7276 - val_loss: 1.5768 - val_acc: 0.5623\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 1.0684 - acc: 0.7280 - val_loss: 1.6665 - val_acc: 0.5418\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 1.0410 - acc: 0.7384 - val_loss: 1.6743 - val_acc: 0.5457\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 1.0281 - acc: 0.7502 - val_loss: 1.8630 - val_acc: 0.5075\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 1.0006 - acc: 0.7619 - val_loss: 1.6030 - val_acc: 0.5808\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.9974 - acc: 0.7725 - val_loss: 1.6318 - val_acc: 0.5745\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.9714 - acc: 0.7804 - val_loss: 1.6753 - val_acc: 0.5713\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.9793 - acc: 0.7765 - val_loss: 1.8415 - val_acc: 0.5426\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.9665 - acc: 0.7895 - val_loss: 1.5732 - val_acc: 0.5890\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.9904 - acc: 0.7845 - val_loss: 1.9223 - val_acc: 0.5362\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.9442 - acc: 0.8047 - val_loss: 1.6731 - val_acc: 0.6005\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.9132 - acc: 0.8125 - val_loss: 1.7973 - val_acc: 0.5532\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8982 - acc: 0.8239 - val_loss: 2.1596 - val_acc: 0.5087\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.9036 - acc: 0.8240 - val_loss: 2.3796 - val_acc: 0.5142\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.9126 - acc: 0.8234 - val_loss: 1.8980 - val_acc: 0.5587\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.9403 - acc: 0.8112 - val_loss: 2.3242 - val_acc: 0.4925\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.9011 - acc: 0.8287 - val_loss: 1.8984 - val_acc: 0.5548\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.8810 - acc: 0.8335 - val_loss: 1.7699 - val_acc: 0.5898\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.8348 - acc: 0.8535 - val_loss: 1.7591 - val_acc: 0.5863\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.9082 - acc: 0.8274 - val_loss: 1.7433 - val_acc: 0.5867\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8619 - acc: 0.8480 - val_loss: 1.8199 - val_acc: 0.5910\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.8749 - acc: 0.8446 - val_loss: 2.0241 - val_acc: 0.5437\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8575 - acc: 0.8498 - val_loss: 1.8148 - val_acc: 0.5757\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.8623 - acc: 0.8502 - val_loss: 2.0251 - val_acc: 0.5536\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8212 - acc: 0.8658 - val_loss: 1.7461 - val_acc: 0.5808\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.8381 - acc: 0.8619 - val_loss: 1.7621 - val_acc: 0.5930\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8596 - acc: 0.8531 - val_loss: 1.8893 - val_acc: 0.5816\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8471 - acc: 0.8576 - val_loss: 1.8954 - val_acc: 0.5772\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8095 - acc: 0.8724 - val_loss: 1.8581 - val_acc: 0.5705\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8003 - acc: 0.8740 - val_loss: 2.3176 - val_acc: 0.5240\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.8004 - acc: 0.8724 - val_loss: 2.0543 - val_acc: 0.5426\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.8392 - acc: 0.8615 - val_loss: 1.9165 - val_acc: 0.5879\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7904 - acc: 0.8780 - val_loss: 1.9089 - val_acc: 0.5757\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8787 - acc: 0.8510 - val_loss: 1.9259 - val_acc: 0.5757\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.8426 - acc: 0.8628 - val_loss: 2.1991 - val_acc: 0.5280\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.8128 - acc: 0.8735 - val_loss: 1.9928 - val_acc: 0.5666\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.8281 - acc: 0.8688 - val_loss: 1.8636 - val_acc: 0.5812\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 430us/step - loss: 0.7988 - acc: 0.8753 - val_loss: 2.2430 - val_acc: 0.5390\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8321 - acc: 0.8692 - val_loss: 2.1707 - val_acc: 0.5615\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7967 - acc: 0.8793 - val_loss: 1.8863 - val_acc: 0.5847\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7608 - acc: 0.8935 - val_loss: 1.9209 - val_acc: 0.5784\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7280 - acc: 0.9005 - val_loss: 2.0181 - val_acc: 0.5619\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7933 - acc: 0.8728 - val_loss: 1.8975 - val_acc: 0.5985\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7856 - acc: 0.8848 - val_loss: 2.1878 - val_acc: 0.5485\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.8317 - acc: 0.8665 - val_loss: 2.1753 - val_acc: 0.5599\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7462 - acc: 0.8984 - val_loss: 2.1466 - val_acc: 0.5473\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7986 - acc: 0.8755 - val_loss: 2.0288 - val_acc: 0.5796\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.8023 - acc: 0.8763 - val_loss: 2.0599 - val_acc: 0.5784\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8231 - acc: 0.8717 - val_loss: 2.1089 - val_acc: 0.5485\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8137 - acc: 0.8729 - val_loss: 1.8603 - val_acc: 0.6040\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.7973 - acc: 0.8823 - val_loss: 2.1893 - val_acc: 0.5630\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7925 - acc: 0.8852 - val_loss: 2.0123 - val_acc: 0.5808\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7800 - acc: 0.8891 - val_loss: 1.7839 - val_acc: 0.6127\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7939 - acc: 0.8813 - val_loss: 2.4800 - val_acc: 0.5319\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 429us/step - loss: 0.7841 - acc: 0.8847 - val_loss: 1.9455 - val_acc: 0.5753\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7860 - acc: 0.8864 - val_loss: 1.9184 - val_acc: 0.5934\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7568 - acc: 0.8965 - val_loss: 2.1409 - val_acc: 0.5638\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7081 - acc: 0.9117 - val_loss: 1.8483 - val_acc: 0.5902\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.6950 - acc: 0.9070 - val_loss: 1.9070 - val_acc: 0.5942\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.7401 - acc: 0.8952 - val_loss: 2.0241 - val_acc: 0.5835\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.8130 - acc: 0.8713 - val_loss: 2.2487 - val_acc: 0.5323\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.8173 - acc: 0.8765 - val_loss: 2.0746 - val_acc: 0.5563\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8175 - acc: 0.8778 - val_loss: 2.0505 - val_acc: 0.5721\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7785 - acc: 0.8915 - val_loss: 2.0760 - val_acc: 0.5701\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.7328 - acc: 0.9038 - val_loss: 2.0601 - val_acc: 0.5764\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 434us/step - loss: 0.7024 - acc: 0.9115 - val_loss: 2.2455 - val_acc: 0.5485\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7108 - acc: 0.9041 - val_loss: 2.2055 - val_acc: 0.5532\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7228 - acc: 0.9008 - val_loss: 1.9373 - val_acc: 0.5697\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7820 - acc: 0.8813 - val_loss: 2.1796 - val_acc: 0.5843\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7572 - acc: 0.8936 - val_loss: 2.0011 - val_acc: 0.5843\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7334 - acc: 0.8998 - val_loss: 2.0840 - val_acc: 0.5670\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.8390 - acc: 0.8622 - val_loss: 2.2733 - val_acc: 0.5426\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7713 - acc: 0.8920 - val_loss: 1.7884 - val_acc: 0.6064\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7161 - acc: 0.9076 - val_loss: 2.2407 - val_acc: 0.5493\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.7707 - acc: 0.8853 - val_loss: 2.3844 - val_acc: 0.5386\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.8115 - acc: 0.8739 - val_loss: 2.5614 - val_acc: 0.5225\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7204 - acc: 0.9083 - val_loss: 2.0100 - val_acc: 0.5867\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7125 - acc: 0.9095 - val_loss: 2.2878 - val_acc: 0.5374\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7545 - acc: 0.8921 - val_loss: 2.5798 - val_acc: 0.5225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 11%|        | 29/256 [4:50:19<34:27:53, 546.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 489us/step - loss: 6.1273 - acc: 0.2828 - val_loss: 5.4884 - val_acc: 0.2987\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 4.0435 - acc: 0.3280 - val_loss: 3.4080 - val_acc: 0.3042\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 2.7990 - acc: 0.3924 - val_loss: 2.7550 - val_acc: 0.2746\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 2.1983 - acc: 0.4625 - val_loss: 2.2469 - val_acc: 0.3751\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 1.8680 - acc: 0.5065 - val_loss: 2.7730 - val_acc: 0.3223\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 1.6523 - acc: 0.5306 - val_loss: 2.2269 - val_acc: 0.4303\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 1.4996 - acc: 0.5659 - val_loss: 1.9473 - val_acc: 0.4582\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 1.4024 - acc: 0.5841 - val_loss: 2.3011 - val_acc: 0.4322\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 1.3322 - acc: 0.5978 - val_loss: 1.6451 - val_acc: 0.4799\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 1.2677 - acc: 0.6217 - val_loss: 1.6991 - val_acc: 0.4831\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 1.2337 - acc: 0.6320 - val_loss: 2.2143 - val_acc: 0.4815\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 1.1703 - acc: 0.6543 - val_loss: 1.5942 - val_acc: 0.5154\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 1.1271 - acc: 0.6702 - val_loss: 2.4546 - val_acc: 0.4050\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 1.1224 - acc: 0.6786 - val_loss: 2.4161 - val_acc: 0.4121\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 1.0840 - acc: 0.6910 - val_loss: 2.5568 - val_acc: 0.3790\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 1.0630 - acc: 0.7063 - val_loss: 1.5705 - val_acc: 0.5654\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 1.0288 - acc: 0.7214 - val_loss: 1.9722 - val_acc: 0.5079\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 1.0147 - acc: 0.7285 - val_loss: 1.6080 - val_acc: 0.5374\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 1.0031 - acc: 0.7307 - val_loss: 1.8224 - val_acc: 0.5146\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.9964 - acc: 0.7386 - val_loss: 1.8435 - val_acc: 0.5292\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.9835 - acc: 0.7484 - val_loss: 2.6989 - val_acc: 0.4232\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.9662 - acc: 0.7537 - val_loss: 2.0798 - val_acc: 0.4882\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.9474 - acc: 0.7669 - val_loss: 3.4023 - val_acc: 0.3909\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.9490 - acc: 0.7687 - val_loss: 2.4566 - val_acc: 0.4736\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.9313 - acc: 0.7769 - val_loss: 2.0681 - val_acc: 0.5473\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.9286 - acc: 0.7814 - val_loss: 2.2858 - val_acc: 0.5114\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.9091 - acc: 0.7921 - val_loss: 1.9774 - val_acc: 0.5032\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.9118 - acc: 0.7917 - val_loss: 2.3088 - val_acc: 0.4689\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.9010 - acc: 0.8022 - val_loss: 1.6854 - val_acc: 0.5741\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.8988 - acc: 0.8031 - val_loss: 2.5959 - val_acc: 0.4803\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8952 - acc: 0.8056 - val_loss: 2.1069 - val_acc: 0.5095\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8940 - acc: 0.8058 - val_loss: 2.9723 - val_acc: 0.4433\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.8923 - acc: 0.8112 - val_loss: 2.0756 - val_acc: 0.5197\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.8763 - acc: 0.8167 - val_loss: 2.0103 - val_acc: 0.5359\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8731 - acc: 0.8188 - val_loss: 3.2253 - val_acc: 0.4224\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.8728 - acc: 0.8180 - val_loss: 2.5195 - val_acc: 0.4641\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.8651 - acc: 0.8254 - val_loss: 2.7546 - val_acc: 0.4740\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.8515 - acc: 0.8309 - val_loss: 2.4223 - val_acc: 0.4878\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.8600 - acc: 0.8275 - val_loss: 3.5488 - val_acc: 0.4638\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8630 - acc: 0.8287 - val_loss: 1.8572 - val_acc: 0.5559\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8526 - acc: 0.8311 - val_loss: 1.9569 - val_acc: 0.5500\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8477 - acc: 0.8312 - val_loss: 2.0296 - val_acc: 0.5323\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.8496 - acc: 0.8342 - val_loss: 1.8988 - val_acc: 0.5583\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8466 - acc: 0.8360 - val_loss: 2.1092 - val_acc: 0.5252\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8451 - acc: 0.8352 - val_loss: 3.1363 - val_acc: 0.4795\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.8404 - acc: 0.8419 - val_loss: 2.5502 - val_acc: 0.4874\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 0.8240 - acc: 0.8505 - val_loss: 2.9640 - val_acc: 0.4551\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.8250 - acc: 0.8488 - val_loss: 2.2070 - val_acc: 0.5272\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.8304 - acc: 0.8471 - val_loss: 2.6804 - val_acc: 0.4886\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.8332 - acc: 0.8480 - val_loss: 2.3282 - val_acc: 0.5264\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.8234 - acc: 0.8503 - val_loss: 2.8848 - val_acc: 0.4582\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 383us/step - loss: 0.8225 - acc: 0.8490 - val_loss: 4.0741 - val_acc: 0.3365\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8231 - acc: 0.8542 - val_loss: 2.3089 - val_acc: 0.5035\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8295 - acc: 0.8514 - val_loss: 2.4485 - val_acc: 0.4870\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.8189 - acc: 0.8516 - val_loss: 1.9074 - val_acc: 0.5764\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8112 - acc: 0.8570 - val_loss: 2.3857 - val_acc: 0.5083\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.8108 - acc: 0.8590 - val_loss: 2.1363 - val_acc: 0.5335\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8152 - acc: 0.8570 - val_loss: 3.0496 - val_acc: 0.4740\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.8065 - acc: 0.8628 - val_loss: 2.1104 - val_acc: 0.5327\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.8112 - acc: 0.8601 - val_loss: 2.7371 - val_acc: 0.5099\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.8054 - acc: 0.8615 - val_loss: 1.9674 - val_acc: 0.5426\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.8095 - acc: 0.8602 - val_loss: 2.9352 - val_acc: 0.4275\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.8002 - acc: 0.8622 - val_loss: 2.5371 - val_acc: 0.5020\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 0.7963 - acc: 0.8683 - val_loss: 2.0521 - val_acc: 0.5390\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 386us/step - loss: 0.7992 - acc: 0.8652 - val_loss: 2.0266 - val_acc: 0.5611\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.8082 - acc: 0.8595 - val_loss: 1.7916 - val_acc: 0.6020\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7878 - acc: 0.8693 - val_loss: 2.3673 - val_acc: 0.5240\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8029 - acc: 0.8642 - val_loss: 2.5147 - val_acc: 0.5150\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.7926 - acc: 0.8680 - val_loss: 2.6187 - val_acc: 0.5032\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 424us/step - loss: 0.7847 - acc: 0.8716 - val_loss: 1.9770 - val_acc: 0.5571\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.7866 - acc: 0.8710 - val_loss: 3.0483 - val_acc: 0.4783\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.7992 - acc: 0.8642 - val_loss: 2.5489 - val_acc: 0.4819\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7900 - acc: 0.8697 - val_loss: 2.0842 - val_acc: 0.5587\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7862 - acc: 0.8690 - val_loss: 2.5994 - val_acc: 0.4898\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7836 - acc: 0.8697 - val_loss: 2.2487 - val_acc: 0.5268\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 441us/step - loss: 0.7833 - acc: 0.8723 - val_loss: 2.3603 - val_acc: 0.5154\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 5s 444us/step - loss: 0.7900 - acc: 0.8682 - val_loss: 3.2172 - val_acc: 0.4287\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7805 - acc: 0.8740 - val_loss: 2.4540 - val_acc: 0.5189\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 438us/step - loss: 0.7950 - acc: 0.8656 - val_loss: 3.6278 - val_acc: 0.4043\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7805 - acc: 0.8765 - val_loss: 3.2190 - val_acc: 0.4866\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7802 - acc: 0.8781 - val_loss: 2.5499 - val_acc: 0.4819\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 437us/step - loss: 0.7821 - acc: 0.8735 - val_loss: 3.0442 - val_acc: 0.4622\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7785 - acc: 0.8766 - val_loss: 2.1643 - val_acc: 0.5406\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7881 - acc: 0.8706 - val_loss: 2.9861 - val_acc: 0.4850\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7777 - acc: 0.8760 - val_loss: 2.6967 - val_acc: 0.5083\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 436us/step - loss: 0.7908 - acc: 0.8690 - val_loss: 2.0520 - val_acc: 0.5729\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7804 - acc: 0.8736 - val_loss: 2.2585 - val_acc: 0.5288\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 439us/step - loss: 0.7850 - acc: 0.8727 - val_loss: 2.3165 - val_acc: 0.5209\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 433us/step - loss: 0.7723 - acc: 0.8771 - val_loss: 2.9706 - val_acc: 0.4748\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 435us/step - loss: 0.7651 - acc: 0.8805 - val_loss: 2.6458 - val_acc: 0.5236\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7741 - acc: 0.8774 - val_loss: 2.0095 - val_acc: 0.5666\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7768 - acc: 0.8767 - val_loss: 2.4888 - val_acc: 0.5079\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.7812 - acc: 0.8771 - val_loss: 2.5781 - val_acc: 0.4744\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.7667 - acc: 0.8794 - val_loss: 2.2837 - val_acc: 0.5378\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7642 - acc: 0.8791 - val_loss: 2.8437 - val_acc: 0.5142\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.7817 - acc: 0.8739 - val_loss: 2.4718 - val_acc: 0.5260\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7758 - acc: 0.8767 - val_loss: 2.4066 - val_acc: 0.5095\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.7617 - acc: 0.8783 - val_loss: 2.4822 - val_acc: 0.4921\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7585 - acc: 0.8817 - val_loss: 2.2740 - val_acc: 0.5299\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 396us/step - loss: 0.7811 - acc: 0.8719 - val_loss: 2.4252 - val_acc: 0.5106\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7527 - acc: 0.8852 - val_loss: 2.0474 - val_acc: 0.5855\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7632 - acc: 0.8837 - val_loss: 3.7288 - val_acc: 0.4507\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.7625 - acc: 0.8856 - val_loss: 2.4427 - val_acc: 0.5296\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.7617 - acc: 0.8841 - val_loss: 3.1981 - val_acc: 0.4417\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7730 - acc: 0.8786 - val_loss: 2.6022 - val_acc: 0.5032\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7641 - acc: 0.8836 - val_loss: 2.7233 - val_acc: 0.4815\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.7669 - acc: 0.8827 - val_loss: 2.2720 - val_acc: 0.5296\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.7706 - acc: 0.8800 - val_loss: 2.9424 - val_acc: 0.4823\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7627 - acc: 0.8851 - val_loss: 1.9962 - val_acc: 0.5847\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.7590 - acc: 0.8862 - val_loss: 3.4496 - val_acc: 0.4460\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.7766 - acc: 0.8773 - val_loss: 2.3572 - val_acc: 0.5410\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.7579 - acc: 0.8845 - val_loss: 2.2246 - val_acc: 0.5422\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7566 - acc: 0.8844 - val_loss: 2.5091 - val_acc: 0.5134\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.7652 - acc: 0.8787 - val_loss: 2.0130 - val_acc: 0.5611\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7546 - acc: 0.8844 - val_loss: 2.9959 - val_acc: 0.4870\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.7668 - acc: 0.8798 - val_loss: 4.1539 - val_acc: 0.3838\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.7576 - acc: 0.8853 - val_loss: 2.1519 - val_acc: 0.5508\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.7548 - acc: 0.8834 - val_loss: 2.3675 - val_acc: 0.5244\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.7596 - acc: 0.8831 - val_loss: 3.2837 - val_acc: 0.4606\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.7543 - acc: 0.8843 - val_loss: 2.8559 - val_acc: 0.4854\n",
            "Epoch 121/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7574 - acc: 0.8880 - val_loss: 2.8780 - val_acc: 0.4878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|        | 30/256 [4:58:43<33:31:02, 533.91s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 495us/step - loss: 6.0884 - acc: 0.2857 - val_loss: 5.2415 - val_acc: 0.3077\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 378us/step - loss: 4.4586 - acc: 0.3708 - val_loss: 3.7995 - val_acc: 0.3534\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 372us/step - loss: 3.3579 - acc: 0.4371 - val_loss: 2.9086 - val_acc: 0.4362\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 384us/step - loss: 2.6452 - acc: 0.4935 - val_loss: 2.3637 - val_acc: 0.4835\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 368us/step - loss: 2.0828 - acc: 0.5591 - val_loss: 2.1303 - val_acc: 0.4630\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 388us/step - loss: 1.7603 - acc: 0.5898 - val_loss: 1.7932 - val_acc: 0.5347\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 377us/step - loss: 1.5398 - acc: 0.6206 - val_loss: 1.6443 - val_acc: 0.5465\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 389us/step - loss: 1.4168 - acc: 0.6336 - val_loss: 1.8862 - val_acc: 0.4724\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 1.3134 - acc: 0.6526 - val_loss: 1.6631 - val_acc: 0.5288\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 1.2743 - acc: 0.6590 - val_loss: 1.8980 - val_acc: 0.5032\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 1.2243 - acc: 0.6711 - val_loss: 1.5699 - val_acc: 0.5437\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 1.1491 - acc: 0.7026 - val_loss: 1.5828 - val_acc: 0.5630\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 1.1311 - acc: 0.7044 - val_loss: 1.6543 - val_acc: 0.5362\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 1.0952 - acc: 0.7210 - val_loss: 1.6533 - val_acc: 0.5426\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 1.0883 - acc: 0.7273 - val_loss: 2.1002 - val_acc: 0.5134\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 1.0795 - acc: 0.7318 - val_loss: 1.5674 - val_acc: 0.5697\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 1.0444 - acc: 0.7481 - val_loss: 1.7281 - val_acc: 0.5284\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 1.0422 - acc: 0.7497 - val_loss: 1.7789 - val_acc: 0.5461\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 1.0012 - acc: 0.7678 - val_loss: 1.6907 - val_acc: 0.5670\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 1.0136 - acc: 0.7683 - val_loss: 1.8707 - val_acc: 0.5201\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 1.0122 - acc: 0.7695 - val_loss: 1.6816 - val_acc: 0.5745\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.9651 - acc: 0.7895 - val_loss: 1.6433 - val_acc: 0.5725\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.9803 - acc: 0.7877 - val_loss: 2.2289 - val_acc: 0.4618\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.9333 - acc: 0.8080 - val_loss: 1.7584 - val_acc: 0.5378\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.9505 - acc: 0.8042 - val_loss: 1.8100 - val_acc: 0.5760\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.9670 - acc: 0.7987 - val_loss: 1.9770 - val_acc: 0.5552\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.9233 - acc: 0.8173 - val_loss: 1.9618 - val_acc: 0.5544\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.9277 - acc: 0.8180 - val_loss: 1.9617 - val_acc: 0.5311\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.9326 - acc: 0.8175 - val_loss: 1.6885 - val_acc: 0.5780\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.9164 - acc: 0.8256 - val_loss: 1.6912 - val_acc: 0.5847\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.8702 - acc: 0.8461 - val_loss: 1.9196 - val_acc: 0.5540\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.8777 - acc: 0.8404 - val_loss: 2.3300 - val_acc: 0.5347\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.8886 - acc: 0.8399 - val_loss: 2.0554 - val_acc: 0.5437\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.8106 - acc: 0.8697 - val_loss: 1.9691 - val_acc: 0.5493\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.8370 - acc: 0.8555 - val_loss: 1.8714 - val_acc: 0.5788\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.9036 - acc: 0.8365 - val_loss: 2.0658 - val_acc: 0.5264\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.9309 - acc: 0.8291 - val_loss: 1.9515 - val_acc: 0.5674\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8544 - acc: 0.8641 - val_loss: 1.8795 - val_acc: 0.5686\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.8027 - acc: 0.8736 - val_loss: 2.0493 - val_acc: 0.5654\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.8525 - acc: 0.8570 - val_loss: 2.4700 - val_acc: 0.4933\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.8493 - acc: 0.8599 - val_loss: 2.0386 - val_acc: 0.5579\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.8562 - acc: 0.8578 - val_loss: 1.8941 - val_acc: 0.5906\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.8453 - acc: 0.8647 - val_loss: 1.8203 - val_acc: 0.5898\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7974 - acc: 0.8824 - val_loss: 1.8199 - val_acc: 0.5737\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.8289 - acc: 0.8660 - val_loss: 2.0221 - val_acc: 0.5556\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8341 - acc: 0.8661 - val_loss: 1.8394 - val_acc: 0.5859\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.8126 - acc: 0.8740 - val_loss: 2.6083 - val_acc: 0.5303\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.8223 - acc: 0.8724 - val_loss: 1.9349 - val_acc: 0.5887\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.8329 - acc: 0.8668 - val_loss: 2.2714 - val_acc: 0.5402\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.8041 - acc: 0.8801 - val_loss: 2.1054 - val_acc: 0.5512\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.8260 - acc: 0.8692 - val_loss: 2.1384 - val_acc: 0.5390\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 421us/step - loss: 0.7984 - acc: 0.8801 - val_loss: 2.0509 - val_acc: 0.5823\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7569 - acc: 0.8963 - val_loss: 2.2197 - val_acc: 0.5524\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7715 - acc: 0.8868 - val_loss: 2.3516 - val_acc: 0.5276\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7850 - acc: 0.8839 - val_loss: 2.1978 - val_acc: 0.5540\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7872 - acc: 0.8815 - val_loss: 1.9432 - val_acc: 0.5772\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.8324 - acc: 0.8681 - val_loss: 2.0639 - val_acc: 0.5579\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8233 - acc: 0.8741 - val_loss: 2.0023 - val_acc: 0.5587\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.8311 - acc: 0.8675 - val_loss: 2.3245 - val_acc: 0.5236\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7963 - acc: 0.8825 - val_loss: 1.9625 - val_acc: 0.5784\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7894 - acc: 0.8864 - val_loss: 2.1935 - val_acc: 0.5749\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.8056 - acc: 0.8780 - val_loss: 2.1908 - val_acc: 0.5500\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7930 - acc: 0.8875 - val_loss: 2.0727 - val_acc: 0.5729\n",
            "Epoch 64/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7810 - acc: 0.8869 - val_loss: 1.9464 - val_acc: 0.5772\n",
            "Epoch 65/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7495 - acc: 0.8994 - val_loss: 2.1643 - val_acc: 0.5603\n",
            "Epoch 66/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.8004 - acc: 0.8748 - val_loss: 2.2761 - val_acc: 0.5292\n",
            "Epoch 67/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7888 - acc: 0.8843 - val_loss: 2.0390 - val_acc: 0.5666\n",
            "Epoch 68/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.7642 - acc: 0.8910 - val_loss: 1.8220 - val_acc: 0.6009\n",
            "Epoch 69/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7343 - acc: 0.9044 - val_loss: 2.0544 - val_acc: 0.5839\n",
            "Epoch 70/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7625 - acc: 0.8892 - val_loss: 2.0162 - val_acc: 0.5678\n",
            "Epoch 71/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.7774 - acc: 0.8879 - val_loss: 1.8381 - val_acc: 0.5894\n",
            "Epoch 72/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.7794 - acc: 0.8866 - val_loss: 2.1005 - val_acc: 0.5646\n",
            "Epoch 73/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7515 - acc: 0.8980 - val_loss: 2.3670 - val_acc: 0.5232\n",
            "Epoch 74/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.7810 - acc: 0.8902 - val_loss: 2.0445 - val_acc: 0.5942\n",
            "Epoch 75/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.7649 - acc: 0.8921 - val_loss: 2.3544 - val_acc: 0.5418\n",
            "Epoch 76/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7592 - acc: 0.8944 - val_loss: 1.9704 - val_acc: 0.5729\n",
            "Epoch 77/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7772 - acc: 0.8890 - val_loss: 2.5413 - val_acc: 0.5536\n",
            "Epoch 78/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.7658 - acc: 0.8969 - val_loss: 2.2494 - val_acc: 0.5658\n",
            "Epoch 79/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7529 - acc: 0.8934 - val_loss: 2.9086 - val_acc: 0.5083\n",
            "Epoch 80/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7338 - acc: 0.9053 - val_loss: 2.3450 - val_acc: 0.5445\n",
            "Epoch 81/300\n",
            "10152/10152 [==============================] - 4s 425us/step - loss: 0.7185 - acc: 0.9038 - val_loss: 2.0132 - val_acc: 0.5690\n",
            "Epoch 82/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7513 - acc: 0.8948 - val_loss: 3.1577 - val_acc: 0.4744\n",
            "Epoch 83/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.8381 - acc: 0.8678 - val_loss: 2.2834 - val_acc: 0.5359\n",
            "Epoch 84/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7586 - acc: 0.8969 - val_loss: 2.1541 - val_acc: 0.5583\n",
            "Epoch 85/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7680 - acc: 0.8914 - val_loss: 2.3058 - val_acc: 0.5331\n",
            "Epoch 86/300\n",
            "10152/10152 [==============================] - 4s 426us/step - loss: 0.7519 - acc: 0.8968 - val_loss: 2.0219 - val_acc: 0.5831\n",
            "Epoch 87/300\n",
            "10152/10152 [==============================] - 4s 423us/step - loss: 0.7197 - acc: 0.9093 - val_loss: 1.9672 - val_acc: 0.5894\n",
            "Epoch 88/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7255 - acc: 0.9055 - val_loss: 1.8856 - val_acc: 0.5926\n",
            "Epoch 89/300\n",
            "10152/10152 [==============================] - 4s 427us/step - loss: 0.7163 - acc: 0.9048 - val_loss: 2.4017 - val_acc: 0.5508\n",
            "Epoch 90/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7519 - acc: 0.8921 - val_loss: 2.3507 - val_acc: 0.5414\n",
            "Epoch 91/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.7787 - acc: 0.8892 - val_loss: 2.1112 - val_acc: 0.5760\n",
            "Epoch 92/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7331 - acc: 0.9032 - val_loss: 2.2693 - val_acc: 0.5489\n",
            "Epoch 93/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7456 - acc: 0.8967 - val_loss: 2.1052 - val_acc: 0.5717\n",
            "Epoch 94/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 0.7114 - acc: 0.9082 - val_loss: 2.0469 - val_acc: 0.5745\n",
            "Epoch 95/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7492 - acc: 0.8937 - val_loss: 2.1736 - val_acc: 0.5642\n",
            "Epoch 96/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7627 - acc: 0.8930 - val_loss: 2.8538 - val_acc: 0.5095\n",
            "Epoch 97/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7866 - acc: 0.8821 - val_loss: 1.8549 - val_acc: 0.5946\n",
            "Epoch 98/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7395 - acc: 0.9040 - val_loss: 2.0273 - val_acc: 0.5792\n",
            "Epoch 99/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7228 - acc: 0.9044 - val_loss: 2.0464 - val_acc: 0.5910\n",
            "Epoch 100/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.6933 - acc: 0.9181 - val_loss: 1.9207 - val_acc: 0.5930\n",
            "Epoch 101/300\n",
            "10152/10152 [==============================] - 4s 420us/step - loss: 0.7085 - acc: 0.9066 - val_loss: 2.1789 - val_acc: 0.5717\n",
            "Epoch 102/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.7468 - acc: 0.8955 - val_loss: 2.1211 - val_acc: 0.5690\n",
            "Epoch 103/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7569 - acc: 0.8948 - val_loss: 2.1747 - val_acc: 0.5693\n",
            "Epoch 104/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7457 - acc: 0.9004 - val_loss: 2.4022 - val_acc: 0.5433\n",
            "Epoch 105/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.7217 - acc: 0.9072 - val_loss: 2.0781 - val_acc: 0.5599\n",
            "Epoch 106/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7257 - acc: 0.9061 - val_loss: 1.9971 - val_acc: 0.5839\n",
            "Epoch 107/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7250 - acc: 0.9056 - val_loss: 2.0696 - val_acc: 0.5879\n",
            "Epoch 108/300\n",
            "10152/10152 [==============================] - 4s 416us/step - loss: 0.7335 - acc: 0.9034 - val_loss: 2.4070 - val_acc: 0.5232\n",
            "Epoch 109/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7430 - acc: 0.8987 - val_loss: 2.3230 - val_acc: 0.5532\n",
            "Epoch 110/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7594 - acc: 0.8937 - val_loss: 1.9676 - val_acc: 0.5701\n",
            "Epoch 111/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7531 - acc: 0.8962 - val_loss: 2.1399 - val_acc: 0.5485\n",
            "Epoch 112/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 0.7279 - acc: 0.9044 - val_loss: 2.1940 - val_acc: 0.5713\n",
            "Epoch 113/300\n",
            "10152/10152 [==============================] - 4s 415us/step - loss: 0.7490 - acc: 0.8996 - val_loss: 2.0426 - val_acc: 0.5745\n",
            "Epoch 114/300\n",
            "10152/10152 [==============================] - 4s 428us/step - loss: 0.7454 - acc: 0.9023 - val_loss: 2.1673 - val_acc: 0.5406\n",
            "Epoch 115/300\n",
            "10152/10152 [==============================] - 4s 419us/step - loss: 0.7499 - acc: 0.8953 - val_loss: 2.1285 - val_acc: 0.5552\n",
            "Epoch 116/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7233 - acc: 0.9062 - val_loss: 1.9616 - val_acc: 0.6072\n",
            "Epoch 117/300\n",
            "10152/10152 [==============================] - 4s 422us/step - loss: 0.7186 - acc: 0.9103 - val_loss: 2.1679 - val_acc: 0.5662\n",
            "Epoch 118/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7306 - acc: 0.9027 - val_loss: 2.5528 - val_acc: 0.5185\n",
            "Epoch 119/300\n",
            "10152/10152 [==============================] - 4s 417us/step - loss: 0.7798 - acc: 0.8906 - val_loss: 2.5898 - val_acc: 0.5433\n",
            "Epoch 120/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 0.7979 - acc: 0.8863 - val_loss: 2.7643 - val_acc: 0.5288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|        | 31/256 [5:07:10<32:51:13, 525.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10152 samples, validate on 2538 samples\n",
            "Epoch 1/300\n",
            "10152/10152 [==============================] - 5s 485us/step - loss: 6.1414 - acc: 0.2852 - val_loss: 5.6975 - val_acc: 0.3034\n",
            "Epoch 2/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 4.0809 - acc: 0.3431 - val_loss: 3.2716 - val_acc: 0.3641\n",
            "Epoch 3/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 2.8687 - acc: 0.4239 - val_loss: 2.3194 - val_acc: 0.4606\n",
            "Epoch 4/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 2.2657 - acc: 0.4698 - val_loss: 2.2263 - val_acc: 0.3810\n",
            "Epoch 5/300\n",
            "10152/10152 [==============================] - 4s 418us/step - loss: 1.9148 - acc: 0.5072 - val_loss: 1.8725 - val_acc: 0.4492\n",
            "Epoch 6/300\n",
            "10152/10152 [==============================] - 4s 413us/step - loss: 1.6859 - acc: 0.5411 - val_loss: 1.8549 - val_acc: 0.4606\n",
            "Epoch 7/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 1.5224 - acc: 0.5687 - val_loss: 2.1873 - val_acc: 0.4429\n",
            "Epoch 8/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 1.4256 - acc: 0.5867 - val_loss: 2.0156 - val_acc: 0.4468\n",
            "Epoch 9/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 1.3399 - acc: 0.6086 - val_loss: 2.2994 - val_acc: 0.3869\n",
            "Epoch 10/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 1.2758 - acc: 0.6190 - val_loss: 1.5625 - val_acc: 0.5130\n",
            "Epoch 11/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 1.2248 - acc: 0.6398 - val_loss: 1.5090 - val_acc: 0.5370\n",
            "Epoch 12/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 1.1845 - acc: 0.6505 - val_loss: 1.3971 - val_acc: 0.5808\n",
            "Epoch 13/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 1.1498 - acc: 0.6701 - val_loss: 2.5613 - val_acc: 0.4251\n",
            "Epoch 14/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 1.1181 - acc: 0.6791 - val_loss: 1.5014 - val_acc: 0.5477\n",
            "Epoch 15/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 1.0831 - acc: 0.6901 - val_loss: 1.6326 - val_acc: 0.5737\n",
            "Epoch 16/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 1.0679 - acc: 0.7016 - val_loss: 1.5414 - val_acc: 0.5378\n",
            "Epoch 17/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 1.0390 - acc: 0.7160 - val_loss: 2.0016 - val_acc: 0.4795\n",
            "Epoch 18/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 1.0149 - acc: 0.7280 - val_loss: 1.6310 - val_acc: 0.5276\n",
            "Epoch 19/300\n",
            "10152/10152 [==============================] - 4s 412us/step - loss: 1.0024 - acc: 0.7352 - val_loss: 2.2206 - val_acc: 0.4819\n",
            "Epoch 20/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.9789 - acc: 0.7482 - val_loss: 2.0205 - val_acc: 0.5311\n",
            "Epoch 21/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.9786 - acc: 0.7507 - val_loss: 2.2796 - val_acc: 0.4421\n",
            "Epoch 22/300\n",
            "10152/10152 [==============================] - 4s 414us/step - loss: 0.9643 - acc: 0.7560 - val_loss: 1.8884 - val_acc: 0.5118\n",
            "Epoch 23/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.9623 - acc: 0.7616 - val_loss: 1.7785 - val_acc: 0.5366\n",
            "Epoch 24/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.9340 - acc: 0.7799 - val_loss: 2.0046 - val_acc: 0.5067\n",
            "Epoch 25/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.9439 - acc: 0.7749 - val_loss: 1.6824 - val_acc: 0.5662\n",
            "Epoch 26/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.9267 - acc: 0.7884 - val_loss: 1.6247 - val_acc: 0.5654\n",
            "Epoch 27/300\n",
            "10152/10152 [==============================] - 4s 401us/step - loss: 0.9118 - acc: 0.7954 - val_loss: 1.6362 - val_acc: 0.5725\n",
            "Epoch 28/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.9121 - acc: 0.7926 - val_loss: 1.7535 - val_acc: 0.5642\n",
            "Epoch 29/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.9025 - acc: 0.7973 - val_loss: 1.6708 - val_acc: 0.5721\n",
            "Epoch 30/300\n",
            "10152/10152 [==============================] - 4s 411us/step - loss: 0.8962 - acc: 0.8071 - val_loss: 1.9431 - val_acc: 0.5110\n",
            "Epoch 31/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8997 - acc: 0.8054 - val_loss: 1.9169 - val_acc: 0.5347\n",
            "Epoch 32/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.8777 - acc: 0.8143 - val_loss: 2.1221 - val_acc: 0.5024\n",
            "Epoch 33/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8965 - acc: 0.8125 - val_loss: 1.6183 - val_acc: 0.5914\n",
            "Epoch 34/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8731 - acc: 0.8224 - val_loss: 1.9162 - val_acc: 0.5429\n",
            "Epoch 35/300\n",
            "10152/10152 [==============================] - 4s 402us/step - loss: 0.8708 - acc: 0.8247 - val_loss: 1.9421 - val_acc: 0.5473\n",
            "Epoch 36/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.8604 - acc: 0.8284 - val_loss: 2.3055 - val_acc: 0.5284\n",
            "Epoch 37/300\n",
            "10152/10152 [==============================] - 4s 397us/step - loss: 0.8650 - acc: 0.8282 - val_loss: 1.9716 - val_acc: 0.5362\n",
            "Epoch 38/300\n",
            "10152/10152 [==============================] - 4s 392us/step - loss: 0.8505 - acc: 0.8395 - val_loss: 2.0196 - val_acc: 0.5177\n",
            "Epoch 39/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8533 - acc: 0.8339 - val_loss: 1.9543 - val_acc: 0.5449\n",
            "Epoch 40/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.8520 - acc: 0.8376 - val_loss: 2.2369 - val_acc: 0.4957\n",
            "Epoch 41/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.8532 - acc: 0.8367 - val_loss: 2.2293 - val_acc: 0.5091\n",
            "Epoch 42/300\n",
            "10152/10152 [==============================] - 4s 393us/step - loss: 0.8336 - acc: 0.8438 - val_loss: 1.9095 - val_acc: 0.5437\n",
            "Epoch 43/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.8343 - acc: 0.8441 - val_loss: 1.9398 - val_acc: 0.5583\n",
            "Epoch 44/300\n",
            "10152/10152 [==============================] - 4s 398us/step - loss: 0.8304 - acc: 0.8461 - val_loss: 2.3130 - val_acc: 0.5091\n",
            "Epoch 45/300\n",
            "10152/10152 [==============================] - 4s 409us/step - loss: 0.8291 - acc: 0.8529 - val_loss: 1.8416 - val_acc: 0.5772\n",
            "Epoch 46/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8275 - acc: 0.8501 - val_loss: 2.0446 - val_acc: 0.5272\n",
            "Epoch 47/300\n",
            "10152/10152 [==============================] - 4s 400us/step - loss: 0.8242 - acc: 0.8518 - val_loss: 2.1098 - val_acc: 0.5240\n",
            "Epoch 48/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8249 - acc: 0.8529 - val_loss: 2.8745 - val_acc: 0.4775\n",
            "Epoch 49/300\n",
            "10152/10152 [==============================] - 4s 407us/step - loss: 0.8221 - acc: 0.8518 - val_loss: 1.9162 - val_acc: 0.5686\n",
            "Epoch 50/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.8130 - acc: 0.8594 - val_loss: 2.0382 - val_acc: 0.5662\n",
            "Epoch 51/300\n",
            "10152/10152 [==============================] - 4s 408us/step - loss: 0.8308 - acc: 0.8493 - val_loss: 1.7786 - val_acc: 0.5883\n",
            "Epoch 52/300\n",
            "10152/10152 [==============================] - 4s 403us/step - loss: 0.8203 - acc: 0.8553 - val_loss: 2.6672 - val_acc: 0.4480\n",
            "Epoch 53/300\n",
            "10152/10152 [==============================] - 4s 405us/step - loss: 0.8073 - acc: 0.8604 - val_loss: 1.9340 - val_acc: 0.5591\n",
            "Epoch 54/300\n",
            "10152/10152 [==============================] - 4s 410us/step - loss: 0.8145 - acc: 0.8565 - val_loss: 2.1045 - val_acc: 0.5402\n",
            "Epoch 55/300\n",
            "10152/10152 [==============================] - 4s 406us/step - loss: 0.8149 - acc: 0.8607 - val_loss: 2.0208 - val_acc: 0.5347\n",
            "Epoch 56/300\n",
            "10152/10152 [==============================] - 4s 404us/step - loss: 0.7962 - acc: 0.8665 - val_loss: 2.4994 - val_acc: 0.4890\n",
            "Epoch 57/300\n",
            "10152/10152 [==============================] - 4s 394us/step - loss: 0.8070 - acc: 0.8596 - val_loss: 2.1157 - val_acc: 0.5579\n",
            "Epoch 58/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.8145 - acc: 0.8584 - val_loss: 1.9008 - val_acc: 0.5709\n",
            "Epoch 59/300\n",
            "10152/10152 [==============================] - 4s 395us/step - loss: 0.8023 - acc: 0.8653 - val_loss: 1.8332 - val_acc: 0.5682\n",
            "Epoch 60/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.8016 - acc: 0.8624 - val_loss: 2.2180 - val_acc: 0.5083\n",
            "Epoch 61/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.8028 - acc: 0.8664 - val_loss: 2.0625 - val_acc: 0.5504\n",
            "Epoch 62/300\n",
            "10152/10152 [==============================] - 4s 390us/step - loss: 0.8060 - acc: 0.8629 - val_loss: 1.9345 - val_acc: 0.5812\n",
            "Epoch 63/300\n",
            "10152/10152 [==============================] - 4s 391us/step - loss: 0.7991 - acc: 0.8681 - val_loss: 2.6371 - val_acc: 0.4984\n",
            "Epoch 64/300\n",
            " 3792/10152 [==========>...................] - ETA: 2s - loss: 0.7440 - acc: 0.8908"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-155-5788078d8d88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEEG_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mexperiment_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EEG_test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/talos/scan/Scan.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, params, model, experiment_name, x_val, y_val, val_split, random_method, seed, performance_target, fraction_limit, round_limit, time_limit, boolean_limit, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, minimize_loss, disable_progress_bar, print_params, clear_session, save_weights)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# input parameters section ends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_runtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/talos/scan/Scan.py\u001b[0m in \u001b[0;36m_runtime\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mscan_run\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/talos/scan/scan_run.py\u001b[0m in \u001b[0;36mscan_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# otherwise proceed with next permutation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mscan_round\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/talos/scan/scan_round.py\u001b[0m in \u001b[0;36mscan_round\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mingest_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/talos/model/ingest_model.py\u001b[0m in \u001b[0;36mingest_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                       self.round_params)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-134-6d202f31c80d>\u001b[0m in \u001b[0;36mEEG_model\u001b[0;34m(x_train, y_train, x_val, y_val, params)\u001b[0m\n\u001b[1;32m     66\u001b[0m                         \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                         \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                        )\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ZEnAoZncWW",
        "colab_type": "code",
        "outputId": "6c2fa7ed-babd-4990-a3a7-615c8193645f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "t.model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-156-3058c150ad5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAN-AGzWg9Ph",
        "colab_type": "code",
        "outputId": "7f77ad29-6762-43b2-89d0-0c5426e34201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.arange(0, 0.5, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O5TFAzNn88P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}